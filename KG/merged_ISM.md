#Key: [sherman_precisely_2004]

# A Precisely Controlled DNA Biped Walking Device

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is a molecular walking motor built from DNA. It consists of two main components: a DNA biped (two double-helical domains connected by flexible linkers, each with a single-stranded 'foot') and a DNA track or footpath (a rigid triple crossover (TX) molecule with single-stranded 'footholds'). The purpose is to demonstrate precise, externally controlled movement (walking) of the biped along the footpath at the nanometer scale. Input 'set' DNA strands are used to attach the biped's feet to specific footholds via hybridization. 'Unset' strands, complementary to the set strands and containing biotin tags, are used to detach the feet by sequestering the set strands. Psoralen molecules attached to the feet allow for UV-induced covalent cross-linking to the footholds (when attached), enabling unambiguous determination of the walker's state (position) via gel electrophoresis. The system allows controlled forward and backward stepping.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name        | Value        | Units       | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :-------------------- | :----------: | :---------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Key parameters defining the physical structure and operating conditions are listed. Step size is explicitly mentioned as an approximation based on DNA domain width. Concentration changes due to additions are noted.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source driving the walking motion comes from the chemical potential energy released during the hybridization of complementary DNA strands (set strands binding to feet/footholds, unset strands binding to set strands). The free energy change (ΔG) associated with DNA hybridization provides the thermodynamic driving force for strand displacement and attachment/detachment steps. UV light is also an energy input required for the psoralen cross-linking (state verification), but not for the walking motion itself.

### **2.2 Energy Transduction**

    *   Content: Chemical potential energy from DNA hybridization is transduced into mechanical work (movement of the biped). Specifically:
        1.  **Detachment (Unsetting):** Addition of an unset strand leads to hybridization with the set strand's toehold, followed by branch migration. The favorable free energy change of forming the unset:set duplex removes the set strand from the foot/foothold complex. This releases the foot, converting chemical energy into increased conformational freedom/potential energy of the biped/foot.
        2.  **Attachment (Setting):** Addition of a set strand, complementary to a free foot and a target foothold, leads to hybridization. The favorable free energy change drives the formation of the ternary complex (foot:set_strand:foothold), constraining the biped's position and converting chemical energy into mechanical constraint/positional change.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any information or data to assess the thermodynamic efficiency of converting the chemical energy of hybridization into mechanical work (walking). Quantifying this would require measuring the free energy changes of all hybridization steps and the actual work done against entropic/viscous forces, which is not done here. Qualitatively, DNA strand displacement processes are driven by free energy differences but are generally considered inefficient in terms of work output versus total energy change, with much energy lost as heat.

### **2.4 Energy Dissipation**

    *   Content: Energy is primarily dissipated as heat (entropy increase) during the irreversible hybridization and strand displacement reactions. Viscous drag from moving the DNA structures through the buffer solution also dissipates energy, although likely minimal at these slow, controlled speeds. Energy from UV light absorbed during cross-linking is partially dissipated as heat. The removal of the unset:set duplexes using streptavidin beads also involves energy dissipation related to binding and mechanical separation (though this is external manipulation).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term (minutes to hours demonstrated)
*    Units: Qualitative Descriptor

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 3 (for the explicitly demonstrated walk)
*   Units: distinct states (1A,2B; 1A,2C; 1B,2C)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: High (Qualitative)

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Component Annealing   | 48    | hours | Exp. Proc. | Explicit          | Time for initial slow cooling from 95°C to RT. |
        | Set Strand Incubation | 20    | minutes | Exp. Proc. | Explicit          | Time allowed after adding SS1A before adding SS2B. |
    *   **Note:** Only explicitly mentioned or reasonably inferred timescales are included. Kinetics of DNA reactions are not measured.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behavior is controlled, sequential, nanometer-scale locomotion (walking) of a DNA biped along a pre-defined DNA track. The direction (forward/backward) and specific footholds used are determined by the sequential addition of specific external DNA 'fuel' strands (set and unset strands). A secondary behavior is the ability to report its state (foot positions) via UV-induced psoralen cross-linking and gel electrophoresis.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary behavior (walking, i.e., changing state) is validated using psoralen cross-linking followed by denaturing PAGE (Figure 5). An aliquot is taken at each step, UV irradiated, and analyzed. The expected molecular weight shifts corresponding to specific foot-foothold covalent linkages confirm the state transitions. Control: The presence/absence of specific bands matches the expected state based on the added set/unset strands. The non-denaturing gel (Figure 4) validates the structural integrity of the overall complex at the operating temperature. Reproducibility is suggested by the ability to walk back to the initial state showing the original gel pattern. Limitations include potential for incomplete reactions/cross-linking (yields noted ~50%), and analysis is done on ensemble averages.

---

#Key: [man_homeostasis_2019]

# Homeostasis and soft robotics in the design of feeling machines

__Paper Type:__ Theoretical/Computational * (The paper proposes a new class of machines and discusses the theoretical underpinnings and potential enabling technologies, but does not present a specific implemented system or experimental results)*

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper proposes a new class of intelligent machines organized according to the principles of life regulation (homeostasis). These machines possess physical bodies, built using soft robotics principles, which incorporate vulnerability ("risk-to-self") and must be maintained within a narrow range of viability states. Their core function is self-preservation, driven by internal states analogous to biological feelings, which emerge from the homeostatic regulation process. Components include a soft, vulnerable body embedded with sensors (interoceptive, proprioceptive, exteroceptive) and actuators, coupled with a computational architecture (potentially inspired by deep learning, specifically Deep Boltzmann Machines or similar for cross-modal abstraction) that maps sensory data onto homeostatic states and generates behavior aimed at maintaining viability. The purpose is to create machines with intrinsic motivation (self-interest based on survival), improved functionality/adaptability in dynamic environments, and to serve as a platform for investigating consciousness and feeling.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name            | Value                  | Units        | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit)    |
        | :------------------------ | :--------------------- | :----------- | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters are conceptual or illustrative examples mentioned in the text, not quantified measurements of an implemented system.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Not explicitly specified, but implicitly necessary for any physical robot. Could be electrical (battery) or potentially harvested from the environment depending on the specific design (concept mentioned briefly in passing on p. 452 in relation to self-healing skin). Homeostatic regulation inherently involves managing internal energy levels (mentioned alongside temperature, p.447 & p. 448).

### **2.2 Energy Transduction**

    *   Content: Energy would be transduced from the input source (e.g., battery) to power sensors, actuators, and the computational system. Sensory inputs (mechanical, thermal, chemical, electromagnetic etc.) are transduced into signals processed by the computational system to assess homeostatic state. Computational outputs generate signals transduced into actions via actuators, consuming energy. The entire homeostatic process revolves around managing internal energy states.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Energy efficiency is not discussed or quantified in the paper.

### **2.4 Energy Dissipation**

    *   Content: Not explicitly discussed. Implicitly, any physical implementation would involve dissipation (e.g., heat from computation/actuation, mechanical friction, material deformation losses in soft body). The concept of decay/dissolution is central to the motivation (maintaining self against tendency toward dissolution), but physical dissipation mechanisms are not analyzed.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: Specific rules are not defined. Conceptually, they involve:
        1.  Sensors detecting internal states (temperature, energy level, strain, damage) and external conditions.
        2.  A computational process evaluating the current state relative to the optimal homeostatic range (viability).
        3.  Generating "feeling equivalents" representing deviation from optimal state (positive for favorable, negative for unfavorable).
        4.  Selecting actions (via actuators) aimed at minimizing negative feeling equivalents and maximizing positive ones, thereby restoring/maintaining homeostasis.
        5.  Learning/updating associations between external stimuli/actions and internal state consequences. (Inspired by refs like Damasio, Kolchinsky & Wolpert, Friston).
    * **Implicit/Explicit**: Implicit

### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**

    *   Content: The emergent global order is the maintenance of the machine's state within its viability range (homeostasis) across dynamic and unpredictable environments. This implies stable functioning, self-preservation, and potentially complex, adaptive behaviors driven by the "feeling equivalents."
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Hybrid / Neuromorphic (Potentially)

### **5.3 Computational Primitive:**

    *   Content: The core computational primitive is the mapping of multimodal sensory inputs (internal state sensors like strain, temperature, energy; external sensors) to an assessment of the system's homeostatic state relative to its viability range, resulting in a "feeling equivalent" signal that guides action selection. A secondary primitive is the cross-modal association learning (e.g., linking visual input to potential impact on body integrity/homeostasis). This involves feature extraction, integration, comparison to setpoints/ranges, and potentially predictive modeling (influenced by Friston ref 2).
    *   **Sub-Type (if applicable):** Homeostatic State Evaluation, Cross-Modal Association Learning.

### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description        | Value         | Units   | Source                            | Implicit/Explicit   | Justification                             |
        | :--------------------------- | :------------ | :------ | :-------------------------------- | :------------------: | :---------------------------------------- |

    *   **Note:** All timescales are implicit necessities of the proposed system but are not quantified or discussed in detail.

### **6.2 Active Inference:**

    *   Content: Partial/Unclear
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** Predictive Accuracy (of homeostatic consequences of actions), KL Divergence (between predicted and desired state distributions), Model Evidence/Marginal Likelihood (if using Bayesian framework), Timescale of Anticipatory Actions.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The primary mechanism proposed is learning driven by homeostatic feedback ("feeling equivalents"). Actions leading to states closer to optimal homeostasis (positive feeling) are reinforced, while those leading to danger/damage (negative feeling) are suppressed. This involves learning associations between exteroceptive data (environment), proprioceptive data (body state/action), and interoceptive data (internal homeostatic state). The paper suggests deep learning techniques (like DBMs) are suitable for learning cross-modal representations that could underpin this. It also discusses (and differentiates from) Reinforcement Learning (RL), suggesting a related but distinct approach grounded in biological homeostasis rather than arbitrary reward functions (p. 450). The specific algorithms or update rules are not detailed.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior sought is **affective regulation of behavior for self-preservation**. This manifests as:
        1.  **Goal-directed actions** aimed at maintaining homeostasis (seeking resources, avoiding threats).
        2.  **Adaptive responses** to novel situations based on learned associations and internal "feeling equivalents."
        3.  **Creative problem-solving** in service of survival.
        4.  Exhibiting **equivalents to feeling** (internal states that report on viability and motivate action).
        5.  Potentially exhibiting **empathy** if designed with appropriate rules (proposed p. 450-451).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper is theoretical; no validation is presented. Validation would require building or simulating these machines and testing their ability to:
        1. Maintain viability (homeostasis) across varied and challenging conditions.
        2. Exhibit adaptive behaviors not explicitly programmed.
        3. Demonstrate measurable internal states correlating with "feeling equivalents" and influencing behavior as predicted.
        4. Compare performance/robustness against conventional robot designs.
        (Box 1 suggests using these machines as a research platform to investigate these very questions).

---

#Key: [zhang_active_2024]

# Active machine learning model for the dynamic simulation and growth mechanisms of carbon on metal surface

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is an active machine learning framework for simulating the dynamic growth of carbon nanostructures on metal surfaces. It combines Molecular Dynamics (MD) and time-stamped force-biased Monte Carlo (tfMC) simulations with a Gaussian Approximation Potential (GAP) machine learning potential (termed CGM-MLP) trained on-the-fly using Density Functional Theory (DFT) calculations. The components include: the GAP model, Smooth Overlap of Atomic Positions (SOAP) descriptors for structure selection, DFT (CP2K) for generating training data (energies, forces), MD/tfMC simulation engine (LAMMPS with QUIP), and an active learning loop that selects new structures for DFT calculation based on similarity metrics (Dave, Dmax). The purpose is to accurately and efficiently simulate the complex, dynamic processes of substrate-catalyzed carbon growth (e.g., graphene on Cu(111)), capture rare events, and understand growth mechanisms to design substrates for desired nanostructures.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Listed key parameters characterizing the active learning simulation implementation and specific simulation runs.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input during the simulation's deposition phase is the kinetic energy of the impinging carbon atoms directed towards the metal substrate. Specific values are tested. DFT calculations provide the potential energy landscape information used to train the MLP, which guides the energy evolution during MD/tfMC.
    *   Value: 2.5 - 10.0 (tested range)
    *   Units: eV (per incident C atom)

### **2.2 Energy Transduction**

    *   Content: Incident kinetic energy is transduced into several forms upon impact: 1) Potential energy changes associated with adsorption onto the surface or diffusion into the subsurface. 2) Potential energy changes associated with C-C and C-metal bond formation/breaking (e.g., dimer, chain, ring formation). 3) Thermal energy within the substrate (Cu atoms) as kinetic energy is absorbed and dissipated (simulated via thermostatting in MD). 4) Energy associated with defect creation (e.g., creating adatoms/vacancies on the Cu surface). The CGM-MLP calculates potential energy and forces, mediating the conversion between kinetic and potential energy according to learned DFT data. The tfMC method uses force biasing, influencing the trajectory based on potential energy gradients to accelerate sampling.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Energy efficiency in the sense of energy conversion/harvesting is not a relevant metric for this system. The goal is computational efficiency and accuracy in simulating physical energy transformations (kinetic to potential/thermal). The computational efficiency is implicitly high compared to pure AIMD, allowing larger scales, but a numerical efficiency score isn't provided or applicable in a thermodynamic sense. The accuracy of the energy calculations is assessed via MAE relative to DFT (<0.05 eV/atom).

### **2.4 Energy Dissipation**

    *   Content: The primary energy dissipation mechanism is the thermalization of the incident carbon atom's kinetic energy and energy released during bond formation into the metal substrate (lattice vibrations/phonons). This is implicitly handled by the thermostat applied during the MD simulation phase (NVT ensemble mentioned, velocity rescaling wall also used for dissipation). High kinetic energy impacts (e.g., 10 eV) can also lead to energy dissipation through bond breaking (Fig 3b). Quantification of dissipation rates is not provided, but it's implicitly managed to maintain system temperature. Qualitative assessment: Dissipation is crucial and effectively managed by the simulation method (MD thermostatting).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**

    *   **ML Potential Memory:** This memory (the trained potential) is highly stable once trained (low degradation mentioned implicitly via stable low MAE). It is re-writable through the active learning process (adding new DFT data). Capacity depends on the size and diversity of the training set (hundreds to thousands of configurations implicitly). Read-out accuracy is measured by the potential's prediction error vs DFT (Force MAE ~0.43 eV/Å, Energy MAE ~0.013 eV/atom). This memory is closer to a learned model or knowledge base. (Score contribution: ~7/10 for this aspect).
    *   **Physical System State Memory:** This is the structural arrangement of atoms. Retention depends on the stability of formed structures and energy barriers for rearrangement (e.g., dimers, chains, graphene islands are persistent configurations). Capacity relates to the vast number of possible atomic configurations. Read-out is implicitly perfect within the simulation (the configuration *is* the state). It's less like rewritable 'data' memory and more like the physical state history common to any evolving physical system. (Score contribution: ~3/10 for this aspect in a cognitive sense).
    *   **Combined:** The overall score reflects a system where memory is crucial (MLP) and physical state history matters, but it doesn't strongly resemble biological memory types or easily reconfigurable digital memory. It lacks features like selective recall or associative memory in a cognitive sense. The memory is primarily representational (MLP) or structural (physical state). The score of 5 reflects the significant role of the learned potential (higher aspect) combined with the less 'cognitive' physical state memory (lower aspect).

### **3.3 Memory Retention Time:**

*   Value: Long-term (MLP); Variable/Simulation-dependent (Physical State)
    *   **ML Potential Memory:** Once trained (within an active learning loop or finalized), the GAP potential is static unless retrained. Its "retention" is effectively permanent for the duration of its use. Degradation isn't discussed, suggesting it's assumed stable. Classified as Long-term.
    *   **Physical System State Memory:** The persistence of a specific atomic configuration (e.g., a C dimer) depends on the energy barriers for diffusion or reaction, temperature, and subsequent impacts. Some states (like stable graphene nuclei) can persist for the simulation duration, while others (transient C monomers on surface) are short-lived. Retention is thus variable and depends on the specific physical state and simulation conditions.

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Variable (MLP); Very High (Physical State)
*   Units: Number of training configurations (MLP); Number of possible atomic states (Physical State)
    *   **ML Potential Memory:** Capacity relates to the complexity it can represent, influenced by the size and diversity of the training dataset. The paper implies datasets grow during active learning, reaching adequate coverage (low MAE). The final dataset size isn't explicitly stated but typically involves thousands of configurations for GAP.
    *   **Physical System State Memory:** Capacity is the astronomically large number of possible arrangements of C and metal atoms in the simulation box.

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: MLP: Energy MAE ~0.013 eV/atom, Force MAE ~0.43 eV/Å. Physical State: 100%
*   Units: eV/atom, eV/Å (MLP); % (Physical State)
    *   **ML Potential Memory:** "Readout" accuracy is the accuracy with which the potential predicts energy/forces compared to the ground truth (DFT). The paper explicitly states converged MAE values.
    *   **Physical System State Memory:** Within the simulation, the current atomic configuration *is* the state; readout is inherently perfect by definition.

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | MLP_MAE_E | MLP Energy Mean Absolute Error vs DFT | ~0.013 | eV/atom | MLP MemoryNode Attribute | Results (p.3) | Explicit | Measures accuracy of energy prediction (memory readout). |
    | MLP_MAE_F | MLP Force Mean Absolute Error vs DFT | ~0.43 | eV/Å | MLP MemoryNode Attribute | Results (p.3) | Explicit | Measures accuracy of force prediction (memory readout). |
    | PhysicalStatePersistence | Time a specific configuration lasts | Variable | fs / ps / SimulationSteps | PhysicalState MemoryNode Attribute | Fig 2 / Dynamics | Implicit | Qualitative observation from simulation dynamics. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are implicitly encoded within the CGM-MLP (Gaussian Approximation Potential). This potential function takes the local atomic environment (positions of neighbours within a cutoff radius, typically ~3.7 Å for C-C/C-Cu based on GAP defaults/Method section) of each atom as input and outputs the potential energy contribution and forces acting on that atom. The rules are complex, non-linear functions (sums of 2-body, 3-body, and SOAP many-body terms) fitted to DFT data, effectively representing the quantum mechanical interactions governing bonding, repulsion, adsorption, and diffusion at the atomic scale. Specific mathematical forms are standard for GAP (Ref 32, 34) but not fully reiterated in the paper. Key aspects derived from these rules include preferred subsurface sites for C monomers, stability of C dimers on the surface, energetic favorability of chains vs. rings (Fig 4b), and energy barriers for diffusion and reaction (Fig 4a, 4c).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | GAP | 2-body Interaction | Cutoff | 3.7 | Å | Methods | Explicit | Defines range of pairwise interactions. |
    | GAP | 3-body Interaction | Cutoff | 2.5 | Å | Methods | Explicit | Defines range of triplet interactions. |
    | GAP | SOAP descriptor | Cutoff | 3.7 | Å | Methods | Explicit | Defines range of many-body environment. |
    | GAP | SOAP descriptor | l_max | 4 | Dimensionless | Methods | Explicit | Angular resolution of density expansion. |
    | GAP | SOAP descriptor | n_max | 12 | Dimensionless | Methods | Explicit | Radial resolution of density expansion. |
    | CI-NEB | Dimer Formation Barrier | E_barrier | ~0.06 | eV/atom | Fig 4a | Explicit | Barrier for C monomer pair to form dimer. |
    | CI-NEB | Chain Formation Barrier (from dimers) | E_barrier | < 0.1 | eV/atom | Fig 4b | Explicit | Barrier for 4 dimers forming C8 chain. |
    | CI-NEB | Ring Formation Barrier (from dimers) | E_barrier | ~0.2 | eV/atom | Fig 4b | Explicit | Barrier for 4 dimers forming C8 ring. |
    | CI-NEB | Graphene Growth Barrier (Cu-passivated) | E_barrier | < 0.95 | eV/atom | Fig 4c | Explicit | Barrier for adding C3 unit to edge. |

### **4.3 Global Order:**

    *   Content: The emergent global order depends on deposition conditions (primarily incident kinetic energy, Ek). Low Ek (2.5 eV) leads to 1D carbon chains. Moderate Ek (5.0-7.5 eV) leads to the formation of 2D hexagonal carbon rings and small graphene islands/nuclei. High Ek (10 eV) disrupts ring formation, leading to more disordered structures or fragmented chains. Order also includes preferred subsurface location for C monomers and surface location for C dimers, and specific orientations of graphene islands relative to the Cu(111) substrate (~10° misalignment observed, Fig 2b).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Dynamics | MD Integration | Timestep | 0.5 | fs | Explicit | Governs how forces translate to motion. | Methods |
| Dynamics | tfMC Moves | Max Displacement (Δ) | 0.18 | Å | Explicit | Governs MC move size. | Methods |
| Selection | Active Learning | Dave threshold (Save) | 0.08 | Dimensionless | Explicit | Determines if average structure difference warrants adding to training. | Fig 1c |
| Selection | Active Learning | Dmax threshold (Smax) | 0.08 | Dimensionless | Explicit | Determines if max local structure difference warrants adding to training. | Fig 1c |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Structure | Carbon Ring Count | Number of Hexagonal Rings | ~0-35 (at 100 atoms) | Count | Explicit | Quantified based on simulation snapshots. | Coordination number analysis (cutoff 1.8Å) | Fig 3a |
| Structure | Hybridization | sp/sp2/sp3 content | Variable % | % | Explicit | Quantified based on coordination number. | Coordination number analysis (cutoff 1.8Å) | Fig 3a |
| Orientation | Graphene on Cu(111) | Angle offset from substrate lattice | ~10 | Degrees | Explicit | Measured from final structure. | Benchmark angle proposed by Ding et al. (Ref 19) | Fig 2b discussion |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip to Module 6.)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | MD Timestep | 0.5 | fs | Methods | Explicit | Fundamental step for integrating equations of motion. |
        | MD Equilibration/Run Duration (per deposition) | 2 | ps | Methods | Explicit | Duration of MD run after each deposition event before tfMC. |
        | tfMC Simulation | Not specified | Steps/Time | Methods | Implicit | Duration/extent of tfMC sampling not explicitly given in time units, described as enhancing sampling of rare events. |
        | Deposition Rate | 1 atom per (2 ps MD + tfMC duration) | atoms/time | Methods / Implied | Implicit | Rate determined by simulation cycle structure. |
        | Diffusion Time (Monomer/Dimer) | Variable (Implicitly simulated) | ps / ns | Fig 2, Fig 4a | Implicit | Barrier heights (Fig 4a) imply timescales, dynamical observations (Fig 2) show events occurring over simulation time. |
        | Structure Formation (e.g., Ring Closure) | Variable (Implicitly simulated) | ps / ns | Fig 2 / Dynamics | Implicit | Observed dynamically during simulations over many steps. |
    *   **Note:** The simulations cover timescales from femtoseconds (MD steps) to potentially nanoseconds or longer (cumulative simulation time for 100 atoms), capturing fast impacts and slower diffusion/rearrangement processes.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No (for the simulated physical system); Yes (for the simulation method/MLP)
        *   **Simulated Physical System:** The simulated carbon/metal system changes its structure over time due to deposition and atomic interactions (self-organization), but this is governed by fixed physical laws approximated by the (potentially adapting) MLP. The material itself doesn't change its *rules* of behavior or properties based on history to improve performance in a learning sense. It rearranges according to energy minimization.
        *   **Simulation Method (MLP):** The active learning framework exhibits adaptive plasticity. The CGM-MLP potential *adapts* by incorporating new DFT data points selected during the simulation, improving its accuracy (predictive performance) over the course of the training process. This change persists and improves future simulation steps guided by the updated potential.


### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors observed are the dynamic processes of carbon atom deposition, diffusion (surface and subsurface), and aggregation/self-organization on various metal substrates (Cu(111), Cr(110), Ti(001), O-Cu(111)). Specific emergent behaviors include:
        1.  Spontaneous diffusion of C monomers into the Cu(111) subsurface octahedral sites.
        2.  Formation and surface migration of C dimers.
        3.  Formation of 1D carbon chains, particularly at low incident energy.
        4.  Nucleation and growth of hexagonal carbon rings and 2D graphene islands, often involving Cu adatoms or steps, at moderate incident energy.
        5.  Breaking of carbon rings/bonds under high-energy ion impact.
        6.  Substrate-dependent crystallinity and nucleation rates (Cu > Cr > Ti).
        7.  Inhibition of graphene nucleation by surface oxygen contamination.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation of emergent behaviors relies on several methods:
        1.  **Comparison with DFT:** Energy barriers (CI-NEB calculations in Fig 4) and relative stabilities (e.g., monomer subsurface preference, chain vs ring favorability) calculated with CGM-MLP are directly compared against static DFT calculations, showing good agreement (errors generally < 0.2 eV/atom for barriers).
        2.  **Comparison with Previous Studies:** Observed mechanisms (e.g., subsurface monomer, dimer migration, role of adatoms/steps, chain stability) are explicitly stated to be consistent with findings from previous DFT studies and simulations (Refs 1, 4, 5, 7, 15, 18, 19, 20, 22, 52, 53).
        3.  **Comparison with Experiment:** Simulated trends in carbon film crystallinity and nucleation rate across different metal substrates (Cu > Cr > Ti) are shown to match experimental observations via HRTEM/SAED (Fig 5). The inhibiting effect of oxygen contamination also aligns with experimental findings (Ref 55).
        4.  **Visualization and Analysis:** Simulation trajectories are visualized (Fig 2, Supp Movies) and analyzed (e.g., ring counting, hybridization analysis in Fig 3a) to characterize the emergent structures and processes.
        5.  **Reproducibility:** Mention of repeated runs (Supplementary Fig 12) provides some evidence for reproducibility.

---

#Key: [decelle_inferring_2024]

# Inferring effective couplings with Restricted Boltzmann Machines

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### 1.1 System Description

    *   Content: The system described is a computational method for inferring the parameters (external fields H, pairwise couplings J(2), and higher-order couplings J(n)) of a generalized Ising model (GIM) from data generated by such a model. The core idea is to train a Restricted Boltzmann Machine (RBM) on the data and then use a derived analytical mapping to translate the learned RBM parameters (weights W, visible biases b, hidden biases c) into the effective GIM parameters. The purpose is to provide an interpretable physical model (GIM) that captures complex, potentially high-order correlations in binary datasets, leveraging the representational power and training efficiency of RBMs compared to traditional Boltzmann Machines or directly fitting high-order GIMs. The components are the RBM (a bipartite graph neural network/energy-based model with binary visible and hidden units) and the mathematical framework mapping the RBM's marginal visible distribution to the GIM's Boltzmann distribution.

### 1.2 Implementation Clarity


### 1.3 Key Parameters

        | Parameter Name | Value         | Units   | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :-----------: | :-----: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters relate to the numerical experiments used to validate the method. J is the energy unit of the ground truth Ising model.

## M2: Energy Flow

### 2.1 Energy Input


### 2.2 Energy Transduction


### 2.3 Energy Efficiency


### 2.4 Energy Dissipation


## M3: Memory

### 3.1 Memory Presence:

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding to M3.2 and M3.3.)**

### 3.2 Memory Type:**


### 3.3 Memory Retention Time:**

*   Value: Long-term (potentially permanent post-training)

### 3.4 Memory Capacity (Optional - if applicable)**

*  Value: Nv*Nh + Nv + Nh
*   Units: parameters

### 3.5 Readout Accuracy (Optional - if applicable)**

*   Value: Variable (quantified by 1 - ∆J(n), 1 - ∆H)
*   Units: (dimensionless, accuracy)

### 3.6 Degradation Rate (Optional - if applicable)**
    *   Value: 0 (post-training)

### 3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### 3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :-------------------- | :----------: | :----------: | :-------------------: | :------------ |:-----------------:| :-----------------:|
    | ∆J(n)     | Normalized MSE for n-body couplings | 0.09 - 1.0+  | dimensionless | `ReadoutAccuracy`  | Section 3, Figs 3, 5, 6, 7, 8, 9, 10, 15, 16 | Explicit          | Defined in Eq. 17, measures error in inferred couplings (memory readout). |
    | ∆H        | Normalized MSE for fields | ~0 (good)    | dimensionless | `ReadoutAccuracy`  | Section 3.2 (Fig 8a)| Explicit          | Analogous to Eq. 17, measures error in inferred fields. |

---

## M4: Self-Organization and Emergent Order

### 4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping to Module 5.)**

## M5: Computation

### 5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping to Module 6.)**

## M6: Temporal Dynamics

### 6.1 Timescales:**

        | Timescale Description                 | Value                 | Units     | Source       | Implicit/Explicit | Justification |
        | :------------------------------------ | :-------------------: | :-------: | :----------- | :----------------: | :------------: |
        | RBM Training Update Time (1 step)     | ~1.4 x 10^-4 (GPU, K=1) | s         | Table 1      | Explicit          | Empirical time measured for one gradient update step. |
        | Total RBM Training Time (Example)     | ~12                   | minutes   | Appendix G   | Mixed             | Explicitly stated typical time based on 10^5 updates, K=50. |
        | MCMC Steps per Gradient (K)         | 10, 50, 500, 5000     | steps     | Section 3.6  | Explicit          | Parameter of the training algorithm. |
        | GIM Inference Time (J(2), Nv=50, Nh=100) | ~0.15                 | s         | Table 1      | Explicit          | Empirical time for calculating pairwise couplings post-training. |
        | GIM Inference Time (J(3), Nv=50, Nh=100) | ~0.20                 | s         | Table 1      | Explicit          | Empirical time for calculating 3-body couplings post-training. |
        | Log-Likelihood Estimation (AIS) Time| ~2.3                  | s         | Table 1      | Explicit          | Empirical time for approximating partition function/likelihood. |

### 6.2 Active Inference:**

    *   Content: Unclear/Partial

## M7: Adaptation

### 7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", include M7.2)**

### 7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is the RBM training algorithm, specifically Gradient Ascent on the log-likelihood function (Eq. 25). The parameters (W, b, c) are updated iteratively (Eq. under A.2) in the direction that increases the likelihood of the observed training data under the RBM's probability distribution. The gradient calculation involves comparing statistics computed on the training data (positive phase) with statistics computed on samples generated by the RBM model itself (negative phase, estimated via MCMC like PCD-K). This process adjusts the parameters to make the RBM's equilibrium distribution better match the empirical data distribution. This is a form of unsupervised learning based on maximizing likelihood / minimizing KL divergence.

## M8: Emergent Behaviors

### 8.1 Behavior Description:**

    *   Content: The primary functional behavior of the system is the accurate inference of the underlying interaction parameters (external fields H, pairwise couplings J(2), and higher-order couplings J(n)) of a generalized Ising model (GIM) given a dataset of equilibrium samples generated from that GIM. This involves: 1) Training an RBM on the data. 2) Applying the derived analytical mapping (Eqs. 12-14) to the trained RBM parameters to calculate the GIM couplings. The system effectively learns the "rules" (couplings) of the data-generating process. A secondary behavior is the generation of new samples resembling the training data using the trained RBM (mentioned in Intro, Fig 1).

### 8.2 Behavior Robustness:**


### 8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary behavior (coupling inference) is validated through extensive controlled numerical experiments (Section 3). Ground truth GIMs with known parameters (H*, J*(n)) are used to generate datasets. RBMs are trained on these datasets, and the inferred parameters (H, J(n)) are compared to the ground truth using quantitative error metrics (∆H, ∆J(n), Eq. 17). Results are presented comparing inferred vs. true coupling matrices (Figs 3e, 8b, 11, 12, 13, 15) and histograms (Figs 3d, 4, 11, 12, 13, 14, 15). Comparisons are made with alternative methods (BM, BP, Cossu et al., Bulso et al.) (Sections 3.1, 3.5). Robustness is tested by varying dataset size, RBM size, temperature, and training methods (Sections 3.1, 3.3, 3.6). Limitations are discussed (Section 4, Appendix E). Reproducibility is supported by providing code (Code availability section). This constitutes strong quantitative validation within the computational domain.

---

#Key: [rauba_self-healing_2024]

# Self-Healing Machine Learning: A Framework for Autonomous Adaptation in Real-World Environments

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper proposes Self-Healing Machine Learning (SHML), an algorithmic framework designed to address performance degradation in deployed machine learning (ML) models (f) caused by distributional shifts in the data generating process (DGP). Unlike existing "reason-agnostic" methods, SHML aims to autonomously diagnose the *reason* for degradation and propose *diagnosis-based* corrective actions. The SHML system is defined as a tuple ⟨H,f⟩, where f is the deployed ML model and H is a "healing mechanism" that interacts with the environment and modulates f's behavior. H comprises four components: I. Monitoring (detects degradation, Eq. 4), II. Diagnosis (identifies reasons Z for degradation, outputting a distribution ζ over Z, Eq. 5), III. Adaptation (selects actions 'a' from space A based on diagnosis via policy π(⋅∣ζ), Eq. 6), and IV. Testing (evaluates proposed actions, Eq. 7). The goal is to find the optimal action a* that minimizes expected loss under the shifted DGP (Eq. 8). The paper also introduces H-LLM, the first specific SHML algorithm, which utilizes Large Language Models (LLMs) for the diagnosis (generating reasons and confidence scores) and adaptation (proposing textual actions) stages, leveraging LLM capabilities for hypothesis generation, contextual understanding, and agency. H-LLM uses statistical drift detectors for monitoring and evaluates actions on a backtesting window. The purpose is to maintain optimal ML model performance autonomously in dynamic real-world environments where data distributions change over time.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Diagnosis Candidates (k in H-LLM) | Not specified (implied >=1) | Count | Sec 5.3 | Implicit | Low (Not specified) | Inferred from "generates k candidate reasons" |

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: [Yes]

### **3.2 Memory Type:**

    *   **Retention:** Can be long-term (model parameters), medium-term (data buffer for diagnosis, backtesting window), or short-term (LLM context). Explicit mention of using `k` previous time points for monitoring (Sec 5.3.I). Configurable.
    *   **Capacity:** Depends on model size (`f`), data buffer size, LLM context window size. Potentially large but not unlimited. Not explicitly quantified.
    *   **Read-out Accuracy:** Model prediction accuracy is measurable. Diagnosis accuracy is qualitatively assessed (Sec 6.4, Fig 5 shows KL divergence improving). LLM output fidelity depends on the model and prompts.
    The system allows modification (retraining `f`, updating diagnosis `ζ`). It stores information that influences future states. Score reflects functional memory capabilities but lacks the physical multi-stability often associated with material memory.

### **3.3 Memory Retention Time:**

*   Value: Variable / Configurable
*    Units: Time steps / Data batches / Seconds (depending on context)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: Varied (See Exp. Sec 6)
*   Units: Accuracy (%), KL-Divergence

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Diagnosis Quality | KL Divergence between estimated and true corruption source | Varies (Fig 5) | Dimensionless | `DiagnosisStateMemory` attribute | Sec 6.4, Fig 5 | Explicit | Metric explicitly measured and reported. |
    | Model Accuracy | Predictive accuracy post-adaptation | Varies (Tables 4, 5, 9, etc.) | % | `ModelParameterMemory` (indirectly via model performance) | Sec 6 | Explicit | Metric explicitly measured and reported. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: [No]

**(Conditional: Section Skipped as M4.1 is "No")**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: [Yes]

### **5.2 Computation Type:**

    *   Content: [Hybrid] (Digital + Symbolic/Linguistic)

### **5.3 Computational Primitive:**

    *   Content: Multiple primitives are used:
        *   Statistical Hypothesis Testing (e.g., DDM for drift detection in Monitoring).
        *   Machine Learning Inference (e.g., prediction by model `f`).
        *   Machine Learning Training (e.g., retraining `f` as an adaptation action).
        *   Natural Language Generation/Reasoning (LLM-based diagnosis and adaptation proposal in H-LLM via CoT and MC sampling).
        *   Metric Calculation (e.g., Loss/Accuracy computation in Testing).
    *   **Sub-Type (if applicable):** Statistical Test, Neural Network Inference/Training, LLM Inference (Chain-of-Thought, Monte Carlo Sampling), Performance Metric Calculation.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Data Arrival | Batch or Streaming (1 sample/batch) | Data points per time step | Sec 3.1 | Explicit | Setting described explicitly. |
        | Drift Detection Delay | Variable (depends on shift magnitude, detector sensitivity) | Time steps / Samples | Fig 4 ("Avg Recovery Time"), Sec 6.3, Appendix D.1 | Explicit | Concept discussed and plotted, depends on parameters. |
        | Diagnosis/Adaptation Cycle (H-LLM) | 20-40 (overhead) | Seconds | Appendix B.5 | Explicit | Stated explicitly. |
        | Backtesting Window Duration | Variable (t' - t*) | Time steps / Samples | Sec B.4, Alg 1 | Explicit | Defined as the period between shift and detection completion. Size varied in experiments (Fig 6). |

### **6.2 Active Inference:**

    *   Content: [Partial/Unclear]
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:**
        *   **Prediction Error Reduction:** Measure the change in model prediction error (e.g., cross-entropy) before and after adaptation, averaged over chosen actions.
        *   **Diagnosis Accuracy vs. Action Optimality:** Correlate the accuracy/certainty of the diagnosis ζ (e.g., KL-divergence in Fig 5, or H(ζ) from Def 1) with the sub-optimality of the chosen action (difference from true optimal action's risk).
        *   **Policy Complexity/Adaptability:** Measure the entropy or dimensionality of the action distribution π(⋅|ζ) and how it changes over time or with diagnosis certainty.
        *   **Anticipation Timescale:** Qualitatively assess if adaptation actions address the *root cause* proactively versus reactively correcting symptoms.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: [Yes]

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism follows a four-stage process implemented by the healing mechanism `H`:
        1.  **Monitoring:** Detect performance degradation potentially due to DGP shift (e.g., using DDM). Output: signal `s_t` (Eq 4).
        2.  **Diagnosis:** Identify the likely reasons `z` for degradation using current/past data and context `c`. Output: diagnosis distribution `ζ` over reasons Z (Eq 5). In H-LLM, this uses LLM reasoning on data summaries.
        3.  **Adaptation (Action Selection):** Sample candidate actions `a` from a policy `π(⋅∣ζ)` conditioned on the diagnosis (Eq 6). Actions can include retraining `f` (potentially on filtered data), changing `f`'s architecture, modifying input data, etc. In H-LLM, LLM proposes actions based on diagnosis.
        4.  **Testing:** Evaluate the performance `R(a)` of candidate actions `a` on a relevant distribution (e.g., backtesting window `D_test`, Eq 7).
        5.  **Implementation:** Select and implement the best-performing action `a*` (Eq 8), modifying `f` or its operational context for future predictions.
        This mechanism represents a form of diagnosis-guided, feedback-driven adaptation aimed at optimizing performance under environmental change. It resembles a form of structured problem-solving or control loop.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary designed behavior is the *autonomous maintenance of predictive performance* of an ML model (`f`) in the face of changing data distributions (distribution shifts, concept drift). This involves detecting performance degradation, diagnosing the cause, selecting and testing appropriate corrective actions (e.g., retraining, data filtering, model modification), and implementing the best action. A key behavior of H-LLM is *generating plausible diagnoses* for model failure and *proposing targeted adaptation strategies* using LLM reasoning.

### **8.2 Behavior Robustness:**

        *   **To Data Corruption:** H-LLM shows significantly better performance than baseline adaptation methods when data is corrupted (varying `k` columns, `τ` percentage, Sec 6.1, Table 4; Sec 6.2, Table 5). Robustness decreases as corruption becomes extreme.
        *   **To Drift Detection Errors:** SHML shows greater robustness to false positive drift detections compared to traditional systems because actions are only implemented if testing shows improvement (Sec 6.3, Fig 4).
        *   **To Shift Type/Magnitude:** Diagnosis quality improves with more apparent shifts (Sec 6.4, Fig 5), suggesting robustness depends on diagnosability. Adaptation effectiveness depends on data quality and test set size (Sec 6.5, Fig 6).
        *   **Across Datasets/Models:** H-LLM provides benefits across multiple datasets (Sec 6.2, Table 5) and model architectures (Sec D.7, Table 11).
        Limitations exist (Sec 7): success depends on accurate diagnosis and effective action proposal, which can be challenging. The score reflects demonstrated improvement over baselines but acknowledges potential fragility in complex scenarios or with poor LLM performance.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper validates the *designed behavior* (improved adaptation performance) through extensive simulation experiments.
        *   **Operational Definitions:** Performance is defined by accuracy (Tables 4, 5, 8, 9, 10, 11) or KL-Divergence for diagnosis (Fig 5). Components like monitoring, diagnosis, adaptation, testing are operationally defined by their inputs/outputs/goals (Sec 3.3, Sec 5.3).
        *   **Control Experiments:** Performance is compared against several baseline adaptation methods (No retraining, Partial Updating, New model training, Ensemble Method) and ablated versions of H-LLM (Table 10).
        *   **Quantitative Analysis:** Results are presented with means and standard deviations over multiple runs (e.g., Tables 4, 5). Statistical significance isn't formally tested but comparisons are clear.
        *   **Robustness Demonstrated:** Performance is evaluated under varying conditions (corruption levels, drift sensitivity, datasets, models) (Sec 6, Appendix D).
        *   **Limitations:** Validation relies on simulated environments and specific drift/corruption types. Real-world complexity may pose further challenges (acknowledged in Sec 7). Claims are primarily about the effectiveness of the *designed* adaptation framework, not about spontaneously emergent behaviors beyond this goal.

---

#Key: [yu_hydrogels_2020]

# Hydrogels as dynamic memory with forgetting ability

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system utilizes polyampholyte (PA) hydrogels (e.g., copolymer of sodium p-styrenesulphonate (NaSS) and methyl chloride quarternized N,N-dimethylamino ethylacrylate (DMAEA-Q)) containing dynamic ionic bonds to achieve dynamic memory with spontaneous, programmable forgetting. The system works by exploiting the asymmetric kinetics of water uptake (swelling) in a hot bath (T_L, learning) and water release (shrinking) in a cold bath (T_F, forgetting), coupled with a rapid, thermally-induced transparency-to-opaque transition upon cooling due to the formation of a frustrated structure. Information (e.g., 2D patterns via a mask) is encoded during the thermal learning phase (duration t_L), stored as the altered water content and frustrated structure, retrieved visually via the opacity, and spontaneously forgotten as the gel shrinks back to equilibrium in the cold bath over a forgetting time (t_F) proportional to t_L. The purpose is to mimic brain-like dynamic memory characteristics in a synthetic soft material.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters are central to the system's operation, defining the material dimensions, thermal conditions, and the kinetics underlying the memory effect.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input is thermal energy transferred from the hot water bath (at T_L) to the hydrogel during the "learning" phase.

### **2.2 Energy Transduction**

    *   Content: 1. **Thermal to Kinetic/Potential (Swelling):** Heat transfer from the hot bath increases the gel's internal energy, decreases the association of ionic bonds, increases osmotic pressure, driving water diffusion into the gel (swelling). Energy is stored in the strained polymer network and the increased hydration state. 2. **Potential to Kinetic (Shrinking/Forgetting):** Upon moving to the cold bath, the stored potential energy drives water expulsion (shrinking) as the system returns to equilibrium. 3. **Thermal to Structural (Frustration):** Rapid cooling causes fast re-association of ionic bonds before water can diffuse out, trapping water and forming a thermodynamically metastable, light-scattering frustrated structure. Energy is stored in this non-equilibrium structure. 4. **Structural to Thermal (Forgetting):** As the frustrated structure slowly relaxes and water diffuses out, the stored structural energy is dissipated as heat.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any metrics for energy efficiency. Qualitatively, the process is likely very inefficient from an energy storage perspective. The amount of thermal energy input required to induce the small water uptake (17 wt%) and subsequent structural change is likely large compared to the energy stored in the frustrated state. The primary function is information storage/processing, not energy storage. Efficiency is Low.

### **2.4 Energy Dissipation**

    *   Content: 1. **Heat Loss:** Heat is dissipated from the gel to the surrounding bath (hot or cold) via conduction and convection. This occurs during initial heating, cooling, and throughout the swelling/shrinking processes. 2. **Viscous Dissipation:** Energy is dissipated due to the friction associated with polymer chain movement and water flow through the polymer network during swelling and shrinking. 3. **Structural Relaxation:** The relaxation of the non-equilibrium frustrated structure back to the equilibrium transparent state involves the dissipation of stored potential energy, likely as heat. Quantification: Not provided. Qualitative Assessment: Heat loss to surroundings is likely the dominant dissipation pathway. Viscous dissipation during slow shrinking might be significant relative to the stored structural energy. Dissipation during structural relaxation drives the forgetting process. Overall dissipation is High.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", including M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: 115 - 715 (Tunable range shown); Max ~28 (for saturated learning)
*    Units: minutes; hours

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Variable (Inverse of Forgetting Time, t_F)
    *   Units: min^-1 or h^-1

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Learning Efficiency (t_F / t_L) | Ratio of forgetting time to learning time | ~63 (for T_L=80, T_F=25) | Dimensionless | `MemoryNode` or `EncodingEdge` attribute `learningEfficiency` | Fig 4C, Eq. 1 | Explicit | Explicitly calculated and discussed as a key parameter related to D_sw/D_sh. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: 1. **Ionic Bonding:** Temperature-dependent association/dissociation kinetics of ionic bonds between polymer chains (fast, ~microseconds at 25°C). Cooling rapidly increases association. 2. **Water Diffusion:** Cooperative diffusion of water into/out of the polymer network is governed by osmotic pressure gradients and network structure (slow, D_sw >> D_sh). 3. **Thermal Conduction:** Heat transfer through the gel (fast, seconds). Rule: Upon rapid cooling (T_L -> T_F), Rule 1 (bond association) and Rule 3 (thermal equilibration) are much faster than Rule 2 (water diffusion). This kinetic mismatch prevents water absorbed at T_L from escaping before polymer chains aggregate via ionic bonds, leading to trapped water domains and polymer-rich regions on the ~300nm scale (frustrated structure).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | 1 | Ionic Bond Kinetics | Association Time (at 25°C) | ~ several | µs | Text (citing [29]) | Explicit | Stated in text. |
    | 2 | Water Diffusion (Swelling) | D_sw (at 80°C) | 2.3 x 10^-10 | m^2/s | Fig 2D, SI Figs S2-S5 | Explicit | Measured value. |
    | 2 | Water Diffusion (Shrinking) | D_sh (at 25°C) | 3.8 x 10^-12 | m^2/s | Fig 2D, SI Figs S2-S5 | Explicit | Measured value. |
    | 3 | Thermal Conduction | Conduction Time (across ~1mm gel) | ~ several | s | SI Appendix, Fig S9 | Explicit | Calculated/estimated in SI. |

### **4.3 Global Order:**

    *   Content: The globally emergent order is the macroscopic opaque state of the hydrogel, resulting from the formation of the light-scattering "frustrated structure" with a characteristic length scale of ~300 nm throughout the bulk material (or in the learned regions).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Explicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| 1 | Ionic Bond Kinetics vs Temp | Association Rate | Increases with decreasing T | (relative) | Explicit | Fundamental property discussed. | Design Principle, Asymmetric Kinetics Section |
| 2 | Water Diffusivity vs Temp | D_sw, D_sh | D_sw > D_sh, both likely T-dependent | m^2/s | Explicit | D_sw and D_sh measured at specific T; T-dependence implied. | Fig 2D, Asymmetric Kinetics Section |
| 3 | Kinetic Mismatch | τ_diffusion / τ_bond_assoc | >> 1 | Dimensionless | Explicit | Core principle: water diffusion much slower than bond formation on cooling. | Asymmetric Kinetics Section, SI Appendix S9 |
| 3 | Kinetic Mismatch | τ_diffusion / τ_heat_cond | >> 1 | Dimensionless | Explicit | Core principle: water diffusion much slower than heat conduction. | Asymmetric Kinetics Section, SI Appendix S9 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| 1 | Optical State | Turbidity (1-Transmittance) | ~0 (Transparent) to Max > 0 (Opaque) | Dimensionless | Explicit | Primary indicator of the frustrated state; measured. | Transmittance Measurement (Methods), Hot/Cold Bath Cycling | Fig 2B/C |
| 2 | Structural State | Characteristic Length Scale | ~300 | nm | Explicit (in SI) | SEM detects characteristic size of heterogeneity in opaque state. | SEM Measurement | SI Appendix Fig S8 |
| 3 | Hydration State | Water Content Change | Up to 17% (relative) | wt % | Explicit | Correlated with swelling/shrinking and opacity change. | Gravimetric Measurement | Fig 2B, Asymmetric Kinetics Section |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 0

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Ionic Bond Association (25°C) | ~ several | µs | Text (citing [29]) | Explicit | Stated in text. |
        | Thermal Conduction (across gel) | ~ several | s | SI Appendix, Fig S9 | Explicit | Calculated/estimated in SI. |
        | Turbidity Change (Opaque Formation) | < 4 | s | Fig 2C, Movie S1 | Explicit | Observed visually and timed. |
        | Swelling Time (Characteristic, t_L < τ_e) | 2 - 11 (tested range); ~0.6 (equilibrium τ_e) | min; h | Fig 4C; Fig 2B/Text | Explicit | t_L is controlled input; τ_e measured. |
        | Shrinking/Forgetting Time (t_F) | 115 - 715 (tunable range); ~28 (saturation) | min; h | Fig 4C; Fig 2B | Explicit | Measured as transparency/size recovery time. |
        | Swelling Diffusion Timescale (τ_sw ~ d^2/4D_sw) | ~0.6 | h | Calculation using d=1.15mm, D_sw=2.3e-10 m²/s | Implicit | Calculated from explicitly measured parameters. |
        | Shrinking Diffusion Timescale (τ_sh ~ d^2/4D_sh) | ~29 | h | Calculation using d=1.15mm, D_sh=3.8e-12 m²/s | Implicit | Calculated from explicitly measured parameters. |
    *   **Note:** The system exhibits a wide range of timescales, from microseconds (bond kinetics) to seconds (thermal conduction, opacity onset) to minutes/hours (swelling/shrinking, memory retention). The crucial asymmetry D_sw >> D_sh leads to t_F >> t_L.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: 1. **Dynamic Memory:** Encoding thermal history (T_L, t_L) as a persistent but decaying physical state (opacity, size). 2. **Spontaneous Forgetting:** Automatic decay of the memory state over time (t_F) back to equilibrium. 3. **Programmable Forgetting:** The forgetting time (t_F) can be controlled by adjusting the learning time (t_L) and temperature (T_L). 4. **Visual Readout:** Information retrieval via the macroscopic opacity change. 5. **Pattern Storage:** Ability to store spatially defined (2D) information using masks. 6. **Asymmetric Kinetics:** Fast swelling (learning) and slow shrinking (forgetting) driven by D_sw >> D_sh. 7. **Frustrated Structure Formation:** Spontaneous emergence of a metastable, opaque structure upon rapid cooling. 8. **Controlled Drug Release (SI Appendix, Fig S12):** Thermal learning modifies release kinetics.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Emergent behaviors (frustrated structure formation, resulting opacity, asymmetric kinetics) are validated through: 1. **Direct Observation:** Optical images and movies clearly show the transparent-to-opaque transition upon cooling and subsequent slow fading (Fig 2C, 3B, 4A, Movie S1, S5). 2. **Quantitative Measurement:** Transmittance measurements quantify the opacity change over time (Fig 2B). Size measurements quantify swelling/shrinking kinetics (Fig 2B). Diffusion coefficients D_sw and D_sh are quantitatively determined (Fig 2D, SI). 3. **Control Experiments:** Comparison with conventional chemical gels (showing no such effects, SI Fig S1) and experiments inhibiting water uptake (preventing frustration, Text). 4. **Structural Analysis:** SEM confirms the ~300 nm heterogeneity in the opaque state (SI Fig S8). DSC suggests bound water content is not the cause (SI Fig S6). 5. **Reproducibility:** Consistent results across different patterns, learning times, and temperatures (Fig 3-5). Limitations: Direct, real-time observation of the nanoscale structure formation is absent. Theoretical modeling is conceptual (Eq. 1) rather than a full simulation of the emergence.

---

#Key: [pak_generalized_2014]

# Generalized squirming motion of a sphere

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### 1.1 System Description

    *   Content: The system described is a theoretical model of a spherical "squirmer" of radius 'a' undergoing motion in an incompressible fluid at zero Reynolds number (Stokes flow). The squirmer generates motion through prescribed surface velocities (tangential and, in appendices, radial) over its continuous spherical surface. This models the locomotion of microorganisms like ciliates (Opalina) or flagellate colonies (Volvox) by representing the collective action of cilia/flagella as a surface velocity field. The paper derives the analytical solution for the flow field surrounding the squirmer for arbitrary, non-axisymmetric surface motion and computes the resulting 3D translational and rotational swimming kinematics. The purpose is to generalize the classical axisymmetric squirmer model to 3D, understand the resulting flow structures in terms of fundamental singularities, and provide a tool for modeling hydrodynamic interactions and collective locomotion in biological physics. Components are the sphere itself and the surrounding incompressible fluid governed by Stokes equations.

### 1.2 Implementation Clarity


### 1.3 Key Parameters

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### 2.1 Energy Input

    *   Content: The energy input is the mechanical work done by the prescribed surface velocities (squirming motion) of the sphere acting against the viscous stress of the surrounding fluid. This represents the biological energy expended by the microorganism to actuate its cilia or flagella. The paper calculates the rate of work (Power, P).
    *   Value: P (Calculated, depends on coefficients)
    *   Units: W (Watts)

### 2.2 Energy Transduction

    *   Content: The primary energy transduction is the conversion of the mechanical work done by the moving surface into kinetic energy of the fluid (creating flow) and ultimately dissipation into heat due to fluid viscosity. A portion of the work is converted into the kinetic energy associated with the bulk translation and rotation of the squirmer itself (swimming).

### 2.3 Energy Efficiency

    *   Justification/Metrics: The paper defines hydrodynamic efficiency E = 6πηaU² / P (Eq. 79), which is the ratio of work needed to drag a passive sphere at speed U to the work done by the squirmer to achieve speed U. The paper calculates P (Eq. 77, 80, 161) and U (Eq. 60, 157). The efficiency depends explicitly on the specific squirming mode coefficients (Bmn, Cmn, etc.). It notes that only modes B01, B11, B̃11 contribute to propulsion, and including any other modes decreases efficiency (Section 4.4). No specific numerical efficiency values are calculated for general cases, only the formula is provided.

### 2.4 Energy Dissipation

    *   Content: In Stokes flow (zero Reynolds number), all work done on the fluid is instantaneously dissipated as heat due to viscosity. The rate of energy dissipation is equal to the rate of work done by the squirmer, P, as calculated in Section 4.4 (Eqs. 77, 80) and Appendix C (Eq. 161). The mechanism is viscous friction within the fluid. The paper explicitly calculates P: P = ∫ n·σ·v dS (Eq. 76), which represents the rate of work done by surface tractions, equivalent to the viscous dissipation rate in the fluid volume for a neutrally buoyant swimmer in Stokes flow.

## M3: Memory

### 3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8.)**

### 3.2 Memory Type:**


### 3.3 Memory Retention Time:**


### 3.4 Memory Capacity (Optional - if applicable)


### 3.5 Readout Accuracy (Optional - if applicable)


### 3.6 Degradation Rate (Optional - if applicable)

### 3.7 Memory Operations Energy Cost (Optional - if applicable)
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### 3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### 4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

### 4.2 Local Interaction Rules:**


### 4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### 4.3 Global Order:**


### 4.4 Predictability of Global Order:**


### 4.5. Local Interaction Rules (for Self-Organization)
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### 4.6. Globally Emergent Order and Order Parameters
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### 4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### 5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

### 5.2 Computation Type:**


### 5.3 Computational Primitive:**


### 5.4 Embodied Computational Units
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### 6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Fluid response time | ~0 (Instantaneous) | s | Section 2 | Implicit | Stokes flow assumes instantaneous propagation of disturbances. |
        | Surface velocity variation | Arbitrary (depends on `Bm_n(t)`, `Cm_n(t)`) | s⁻¹ (characteristic frequency) | Intro/Formulation | Implicit | The surface velocities can be time-dependent (unsteady squirming), but no specific timescale is inherent to the general model; it depends on the prescribed actuation. Section 4.4 mentions time averaging for unsteady efficiency. |
        | Swimming period (if periodic motion) | Arbitrary (T) | s | Section 4.4 | Implicit | Depends on the periodicity of the prescribed surface motion, if any. |
    *   **Note:** The primary dynamics described by the Stokes equations are instantaneous in response to boundary conditions. Any relevant timescales are imposed externally via the time-dependence of the surface squirming motion, which is arbitrary in the general formulation.

### 6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### 7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

### 7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### 8.1 Behavior Description:**

    *   Content: The primary behaviors are three-dimensional locomotion (translation and rotation) of the spherical squirmer and the generation of a specific, analytically determined flow field in the surrounding fluid. The specific trajectory (straight, circular, helical) depends on the symmetry (or lack thereof) of the prescribed surface squirming motion (Eqs. 60, 61, 72-75). The flow field can be decomposed into fundamental Stokes flow singularities (Stokeslet, potential dipole, stresslet, rotlet, etc.), with the relative strengths determined by the squirming coefficients (Section 3.2, 4.3, Tables 1-3).

### 8.2 Behavior Robustness:**


### 8.3 CT-GIN Emergent Behavior Validation

---

#Key: [zhang_classical_2025]

# Classical sorting algorithms as a model of morphogenesis: Self-sorting arrays reveal unexpected competencies in a minimal model of basal intelligence

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is a computational model using classical sorting algorithms (Bubble, Insertion, Selection) to simulate biological morphogenesis, specifically the process of establishing order along an axis (like organ placement). The core components are 'cells', represented as elements in an array with integer 'values'. The system's function is to sort these cells based on their values. Two main implementations are studied: 1) Traditional, top-down algorithms where a central controller manages the array, and 2) Novel 'cell-view' algorithms where each cell acts as an autonomous agent, executing sorting rules based only on local interactions with neighbors (distributed, bottom-up control). The purpose is to model morphogenesis as a sorting task, explore the capabilities of these algorithms under biologically-inspired constraints (distributed control, unreliable components/'Frozen Cells'), and identify emergent 'basal intelligence' behaviors like error tolerance, delayed gratification, and aggregation in chimeric arrays (mix of algotypes).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Cell Value Range (Unique) | 1 to 100 | integers | Fig 3 caption (implied), Discussion (p.17) | Mixed | Medium | Implied range for 100 unique cells |

    *   **Note:** Key parameters governing the simulations are explicitly stated or clearly inferable from the results and methods sections.

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8.)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", proceeding to M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content:
        *   **Cell-view Bubble Sort:** Each cell compares its value with its immediate left and right neighbors. If `value > right_neighbor.value`, swap right. If `value < left_neighbor.value`, swap left. Interactions are strictly pairwise and adjacent.
        *   **Cell-view Insertion Sort:** Each cell can view all cells to its left. It moves left by swapping with its left neighbor if `value < left_neighbor.value` *and* all cells to its RIGHJT (Correction noted from paper text - Fig 2 shows cell knows cells to left, rule says moves left if value < left neighbor when cells to left are sorted) - Re-checking Methods text: "Each cell is able to view all cells to its left, and can swap only with its left neighbor. Active cell moves to the left if cells to the left have been sorted, and if the value of the active cell is smaller than that of its left neighbor." - This seems slightly contradictory or unclearly worded regarding when movement occurs relative to sorting status of neighbors. Figure 2 implies simpler comparison. Assuming the simpler pairwise swap based on Fig 2B's local depiction: If `value < left_neighbor.value`, swap left (conditional on some state, possibly local sortedness to left, but exact condition implementation based on text vs figure is slightly ambiguous). Interaction involves viewing left neighbors, acting on the immediate left neighbor.
        *   **Cell-view Selection Sort:** Each cell has a target position (initially leftmost). It views the cell currently at its target position. If `active_cell.value < target_position_cell.value`, swap places. If swap denied (or after swap?), target position might shift right (details slightly unclear on update rule). Interaction involves viewing and potentially swapping with a non-adjacent cell at the target position.
        *   **General Rules:** Cells operate in parallel (multi-threaded simulation). Frozen cells may fail to initiate or participate in swaps. Chimeric arrays have cells following different rules simultaneously.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The primary emergent global order is a fully sorted array of cells (integers in monotonic increasing order). A secondary emergent spatial order observed *during* the sorting process in chimeric arrays is the transient aggregation or clustering of cells with the same Algotype.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| P1 | Sortedness | Sortedness Value | 0-100 | % | Explicit | Measures fraction of cells in correct monotonic order | Methods | Methods, Fig 3 |
| P2 | Monotonicity | Monotonicity Error | 0 to N (100) | Count | Explicit | Number of adjacent pairs violating order | Methods | Methods, Fig 5 |
| P3 | Clustering (Chimeric) | Aggregation Value | 0-100 | % | Explicit | % of cells with same-Algotype neighbors | Methods | Methods, Fig 8 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Total Sorting Steps (Mean, Swap Only, Bubble, N=100) | ~2449 | Steps | Fig 4A, Results | Explicit | Average computational steps (swaps) |
        | Total Sorting Steps (Mean, Swap Only, Insertion, N=100) | ~2483 | Steps | Fig 4A, Results | Explicit | Average computational steps (swaps) |
        | Total Sorting Steps (Mean, Swap Only, Selection, N=100) | ~1096 | Steps | Fig 4A, Results | Explicit | Average computational steps (swaps) |
        | Peak Aggregation Time (% of Process, Bubble-Selection) | 42 | % | Results (Fig 8) | Explicit | Point of max clustering |
        | Peak Aggregation Time (% of Process, Bubble-Insertion) | 21 | % | Results (Fig 8) | Explicit | Point of max clustering |
    *   **Note:** Time is measured discretely in simulation "steps," representing comparisons or swaps. Absolute time (e.g., seconds) is not relevant for this computational model.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content:
        1.  **Self-Sorting (Cell-View):** The primary behavior is the achievement of a globally sorted array through decentralized, local interactions of autonomous cells (Bubble, Insertion, Selection variants).
        2.  **Error Tolerance:** The ability of cell-view algorithms (especially Bubble and Selection, depending on Frozen Cell type) to achieve a higher degree of sortedness compared to traditional algorithms in the presence of 'Frozen Cells' (non-participating elements).
        3.  **Delayed Gratification (DG):** An emergent trajectory characteristic, particularly in the presence of Frozen Cells, where the system temporarily moves *away* from the goal state (decreased Sortedness) to navigate around the obstacle, ultimately achieving a better sorted state later. Observed significantly in Bubble and Insertion sorts.
        4.  **Algotype Aggregation (Clustering):** In chimeric arrays (mixed Algotypes), cells with the same algorithm transiently cluster together spatially during the sorting process, even though the algorithms don't explicitly encode for this. This emergence depends on the combination of Algotypes.
        5.  **Competitive Equilibrium (Opposing Goals):** In chimeric arrays where Algotypes attempt to sort in opposite directions, the system reaches a stable, partially sorted equilibrium state rather than sorting completely or oscillating indefinitely.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors are validated through quantitative simulation and statistical analysis.
        *   **Operational Definitions:** Metrics like Sortedness Value, Monotonicity Error, Delayed Gratification index, and Aggregation Value are defined (Methods).
        *   **Control Experiments:** Traditional algorithms serve as controls for cell-view algorithms (Figs 3, 4, 5, 7). Homogeneous arrays serve as controls for chimeric arrays (Fig 8A, pink vs red lines). Experiments with 0 Frozen Cells serve as controls for experiments with Frozen Cells (Figs 5, 7). Sorting identical Algotypes serves as a negative control for aggregation (Fig 8B, light pink line).
        *   **Quantitative Analysis:** Behaviors are quantified using the defined metrics over 100 replicate simulations. Averages and standard deviations are considered (implied in plots). Statistical significance tests (Z-test, T-test) are applied to compare groups (e.g., traditional vs. cell-view efficiency, DG levels, aggregation significance), with p-values reported (Results section).
        *   **Robustness Demonstrated:** Sensitivity to the number of Frozen Cells is analyzed (Figs 5, 7). Behavior in different chimeric combinations is tested (Fig 8).
        *   **Reproducibility:** The provision of the GitHub code allows for reproducibility.
        *   **Limitations:** Validation is purely computational; physical realization is not performed. Analysis focuses on specific metrics; other emergent behaviors might exist but were not tested for. The DG definition/calculation could be slightly clearer.

---

#Key: [couzin_collective_2009]

# Collective cognition in animal groups

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews the principles of collective cognition in animal groups (e.g., swarming ants, schooling fish, flocking birds). It describes how local interactions among individuals, based on cues like position and motion of neighbors, scale up to coordinated group-level behaviors such as collective motion, decision-making (e.g., direction choice, consensus), and foraging. The purpose is to understand the relationship between individual behaviors and group properties, highlighting analogies with neuronal processes and cognitive science concepts like feedback, speed-accuracy trade-offs, memory (hysteresis), and quorum sensing. Key components are the individual animals within the group and their local interaction rules (repulsion, alignment, attraction). The system's function is adaptive collective response to environmental challenges (e.g., predation, resource finding, navigation).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Zone of Repulsion (zor) range | Conceptual | Length | Box 1, Fig I | Explicit | Medium | Conceptual parameter in models. |
        | Zone of Orientation (zoo) range | Conceptual | Length | Box 1, Fig I | Explicit | Medium | Conceptual parameter in models; varied to show transitions. |
        | Zone of Attraction (zoa) range | Conceptual | Length | Box 1, Fig I | Explicit | Medium | Conceptual parameter in models. |
        | Quorum threshold (Sticklebacks) | Qualitative ('more neighbours') | Individuals | Section: Feedback and the speed-accuracy trade-off | Explicit | Medium | Concept explicitly discussed, specific number not given universally. |
        | Quorum threshold (Ants - T. albipennis) | Density-dependent | Ants / Area | Section: Finding a new home | Explicit | Medium | Concept explicitly discussed, specific density not given universally. |

    *   **Note:** These parameters are primarily conceptual or illustrative as presented in the review, representing classes of parameters used in models or observed qualitatively in experiments cited. Precise values are often study-specific (in the cited literature) rather than universally defined in this review.

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable / Tunable

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Low (Hysteresis) / Potentially High (Stigmergy)
*   Units: States (Hysteresis) / Information Content (Stigmergy - qualitative)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: Probabilistic / Stochastic
*   Units: Probability / % (qualitative)

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Related to Pheromone Evaporation Rate
    *   Units: Concentration / time (qualitative)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content:
        *   **General Model (Box 1):** Individuals adjust movement based on neighbors within distinct spatial zones:
            *   Short-range repulsion (avoid collisions).
            *   Intermediate-range alignment (orient with neighbors).
            *   Long-range attraction (maintain cohesion).
        *   **Sticklebacks (Decision-making):** Non-linear response to neighbors' direction; largely ignore single neighbor, increased copying probability with a 'quorum' of neighbors turning.
        *   **Ants (Foraging - Stigmergy):** Deposit pheromones (strength potentially related to food quality/quantity); Probabilistically follow existing pheromone trails (bias towards stronger trails); Pheromones decay/evaporate over time.
        *   **Ants (Nest Selection - T. albipennis):** Scout assesses potential nest (assessment time inversely proportional to quality); Recruits nestmates via 'tandem running' (slow, one-to-one recruitment); Switches to faster 'carrying' recruitment upon detecting a quorum of ants at the new site.
        *   **Ants (Nest Activity - Box 2):** Inactive ants become active spontaneously (low probability) or via contact with active ants (excitation, temporal integration); Active ants can become inactive (tendency increases if insufficient reinforcement); Refractory period after becoming inactive.
        *   **Honeybees (Nest Selection):** Scouts perform waggle dance (duration proportional to site quality) to recruit others; Dance decays over time; Quorum sensing at potential site triggers rapid commitment/recruitment.
        *   **Informed Individuals (Leadership):** Informed individuals bias movement towards a preferred direction; Naive individuals follow local rules (alignment, attraction).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | General Model | Alignment | Range (zoo) | Variable (Conceptual) | Length | Box 1 | Explicit | Varied in model to show transitions. |
    | Sticklebacks | Quorum Response | Copying Probability | Non-linear function of # neighbors | Probability | Section: Feedback... | Explicit | Described qualitatively as non-linear. |
    | Ants (Foraging) | Pheromone Following | Following Probability | Function of trail concentration | Probability | Section: Collective cognition... | Explicit | Described qualitatively (bias to stronger trails). |
    | Ants (Nest Sel.) | Quorum Sensing | Quorum Threshold | Variable (depends on urgency) | Ants / Area | Section: Finding a new home | Explicit | Threshold concept explicit, value tunable. |
    | Honeybees | Waggle Dance | Dance Duration | Proportional to site quality | Time | Section: Finding a new home | Explicit | Proportional relationship stated. |

### **4.3 Global Order:**

    *   Content: Cohesive group structures (swarms, torus, polarized schools/flocks - Box 1); Coordinated collective movement; Consensus decisions (e.g., direction of travel, best food source, new nest site); Efficient foraging patterns (e.g., shortest paths); Propagating waves of turning/activity (e.g., predator avoidance, ant nest rhythms); Spatially sorted groups (mentioned briefly in Box 1 ref [10]).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Alignment | Tendency to align with neighbors | Range (zoo) | Variable (conceptual) | Length | Explicit | Key parameter varied in model. | Box 1 |
| Quorum | Non-linear response to neighbor number | Threshold | Qualitative ('a quorum') | Individuals | Explicit | Explicit concept, specific value varies. | Section: Feedback... |
| Pheromone Following | Bias towards stronger trails | Following Probability | f(Concentration) | Probability | Explicit | Qualitative description of mechanism. | Section: Collective cognition... |
| Tandem Recruitment | Slow one-to-one leading | Recruitment Rate | ~1/3 of carrying | Rate | Explicit | Compared to carrying rate. | Section: Finding a new home |
| Contact Activation | Excitation by active neighbors | Activation Threshold | Qualitative ('above a threshold') | Inputs/Time | Explicit | Analogous to neuronal threshold. | Box 2 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Polarity | Group alignment | Polarization Vector Magnitude | 0 (swarm) to ~1 (polarized) | Dimensionless | Explicit | Implied by descriptions of swarm vs polarized group. | Simulation (Box 1) | Box 1 |
| Consensus | Agreement on direction | % Choosing Majority | High (approaching 100%) | % | Explicit | Result of consensus model simulation. | Simulation (Fig 1a) | Fig 1a, Ref [31] |
| Trail Efficiency | Path selection | Path Length Chosen | Converges to shortest | Length | Explicit | Outcome of stigmergy models/experiments. | Experiment/Model | Section: Collective cognition... |
| Activity Rhythm | Synchronized activity | Period | ~20 | min | Explicit | Measured from ant colony activity. | Experiment (Box 2) | Box 2, Fig I |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes (by analogy)

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog / Neuromorphic / Other (Distributed Collective Computation)

### **5.3 Computational Primitive:**

    *   Content:
        *   **Information Integration/Averaging:** Combining noisy individual estimates or preferences (e.g., fish integrating personal/social info, pigeons averaging routes below conflict threshold).
        *   **Thresholding/Quorum Sensing:** Switching behavior based on a threshold number/density of neighbors exhibiting a certain state/action (e.g., fish copying, ants switching recruitment, bees committing to site).
        *   **Competition/Winner-Takes-All:** Selecting one option among alternatives through feedback dynamics (e.g., consensus decision above conflict threshold in groups/pigeons, selection of best/shortest ant trail via reinforcement/decay).
        *   **Signal Amplification:** Positive feedback enhances signals (e.g., turning waves, trail reinforcement).
        *   **Signal Damping/Negative Feedback:** Mechanisms limit amplification or decay signals (e.g., pheromone evaporation, ant refractory periods).

### **5.4 Embodied Computational Units**
    | Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Individual Reaction Time | Implicitly Fast | ms - s | General Biology | Implicit | Needed for coordination, but not quantified. |
        | Wave Propagation Speed | Qualitative ('rapid') | Distance / Time | Section: Amplification... | Explicit | Mentioned for turning waves. |
        | Decision Time (Speed-Accuracy Trade-off) | Variable | Time | Section: Feedback... | Explicit | Central concept discussed. |
        | Pheromone Decay Time | Variable (Tunable) | Time | Section: Collective cognition... | Explicit | Different volatilities give different timescales. |
        | Ant Nest Activity Rhythm Period | ~20 | min | Box 2, Fig I | Explicit | Measured period. |
        | Recruitment Time (Ants - Tandem) | Slow | Time | Section: Finding a new home | Explicit | Relative description. |
        | Recruitment Time (Ants - Carrying) | ~3x Faster than Tandem | Time | Section: Finding a new home | Explicit | Relative description. |
        | Bee Dance Duration | Proportional to Quality | Time | Section: Finding a new home | Explicit | Qualitative relationship. |

### **6.2 Active Inference:**

    *   Content: Unclear / Partial
        (1) *Prediction:* Implicitly, animals might predict neighbor movements based on local cues, but predictive internal models are not explicitly described. Ants using long-term trails might be seen as using a model of resource locations.
        (2) *Action Selection:* Animals clearly select actions (move, turn, follow trail, recruit) based on local information (neighbors, pheromones, quorum) and internal state/goals (find food, new nest, avoid predator). This action aims to achieve goals (maintain cohesion, reach resource), which *could* be framed as minimizing prediction error relative to a goal state, but isn't framed this way in the paper. Speed-accuracy trade-offs imply optimization.
        (3) *Internal Models:* Explicit internal models are not discussed. Pheromone trails act as an externalized environmental model. Hysteresis suggests a simple 'memory' of past state.
        Overall, while goal-directed behavior and adaptation exist, the formal structure of Active Inference (explicit predictive models, free energy minimization) is not invoked or demonstrated. The parallels drawn are more with general cognitive processes like decision-making and feedback control.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes
        1.  **Behavioral Tuning:** Groups adaptively tune their collective response (e.g., speed vs. accuracy) based on context (e.g., urgency in ant nest selection modifies quorum thresholds and recruitment speed).
        2.  **Learning (Stigmergy):** Ant colonies 'learn' the locations of better/closer food sources or efficient paths through the reinforcement and decay dynamics of pheromone trails. The trail network adapts over time based on foraging success.
        3.  **Rule Adaptation (Implicit):** While not the focus, individual animals can learn and adapt their responses over their lifetime (e.g., learning routes, predator recognition), which would influence collective outcomes. The paper mentions pigeons learning homing routes. Tandem running in ants allows followers to learn the route.

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content:
        *   **Feedback Adjustment:** Collective adaptation often involves tuning the balance between positive and negative feedback (e.g., trail reinforcement vs. evaporation; recruitment amplification vs. limited pool of scouts). Urgency can shift parameters like quorum thresholds (Ants).
        *   **Stigmergic Learning:** Environmental modification (pheromone deposition) coupled with probabilistic response (trail following) and decay leads to reinforcement of successful behaviors/paths (analogous to reinforcement learning or Hebbian reinforcement). Quality influences deposition/recruitment strength.
        *   **Individual Learning:** Route learning in pigeons and ants involves individual cognitive processes (e.g., landmark navigation), not detailed in this review but mentioned as relevant.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**</h3>

    *   Content: Coordinated Collective Motion (schooling, flocking, swarming); Collective Decision-Making (direction choice, site selection, foraging choices); Information Transfer and Amplification (e.g., predator detection waves); Collective Sensing (increased effective range); Efficient Collective Search/Exploitation (e.g., shortest path finding, focusing on best resources); Synchronization (e.g., ant activity cycles); Consensus Formation; Emergent Leadership.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The review cites validation methods from the primary literature (which are not fully detailed here). These include:
         *   **Computational Modeling/Simulation:** Used extensively to demonstrate how local rules generate global patterns (Box 1, Fig 1a). Model predictions are compared conceptually or quantitatively to real systems.
         *   **Experimental Observation & Quantification:** Field and lab studies observe and measure collective behaviors (e.g., fish schooling dynamics by Ward et al. [25, 26], pigeon navigation by Biro et al. [33], ant behavior by Pratt et al. [53, 58, 60]). Computer vision is mentioned for data collection [68-70].
         *   **Comparative Analysis:** Comparing behaviors across different species or contexts (e.g., ants vs. bees nest selection).
         *   **Analogy to Other Systems:** Comparing collective animal behavior to neural systems provides conceptual validation [Explicitly drawn analogy].
         Operational definitions are implicit in the models and experiments cited. Robustness is discussed as an emergent property validated by the distributed nature of control. Limitations are discussed, e.g., need for more kinematic data (Box 3).

---

#Key: [goh_alignment-induced_2025]

# Alignment-induced self-organization of autonomously steering microswimmers: Turbulence, clusters, vortices, and jets

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of N spherical microswimmers (squirmers) simulated in a three-dimensional fluid environment using the Multiparticle Collision Dynamics (MPC) technique. Each squirmer possesses self-propulsion (characterized by speed v0 and active stress β, differentiating pushers β<0 and pullers β>0) and autonomous self-steering capabilities. The steering mechanism allows squirmers to align their propulsion direction (e_i) towards the average orientation (e_aim,i) of neighboring squirmers within a defined sensing range (R_a), mimicking a Vicsek-like alignment interaction embedded in hydrodynamic interactions. The steering is achieved by adaptive non-axisymmetric surface flows (C11, C̃11) controlled by the alignment goal, with a maneuverability strength C0 (or dimensionless γ). The purpose is to study the emergent collective self-organization dynamics (e.g., turbulence, clustering, vortex formation, jets) arising from the interplay of hydrodynamic interactions, active stress (pusher/puller type), and local alignment steering in 3D. Components include spherical squirmers, the MPC fluid, the alignment sensing mechanism (neighbor averaging within R_a), the steering surface flow mechanism, and excluded-volume interactions (Lennard-Jones).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters fundamentally define the system's behavior and are systematically varied or specified. R_sq (squirmer radius) is set relative to MPC cell size `a` (R_sq = 3a, Appendix A.3), providing a length scale.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The energy input is implicitly the energy consumed by each squirmer to maintain its self-propulsion speed v0 against viscous drag and to generate the steering surface flows. This originates from an unspecified internal energy source analogous to metabolic processes in microorganisms or stored energy in microrobots. The model does not explicitly define an energy source or consumption rate.

### **2.2 Energy Transduction**

    *   Content: 1. Internal Energy -> Mechanical Work: Squirmers convert internal energy into mechanical work to generate tangential surface flows (Eqs. 1-2, defining u_theta, u_phi). 2. Mechanical Work -> Kinetic Energy (Fluid & Squirmers): The surface flows exert forces on the surrounding fluid, causing squirmer propulsion (translational KE), rotation (rotational KE due to steering), and fluid motion (fluid KE). Hydrodynamic interactions mediate energy transfer between squirmers via the fluid.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not analyze or quantify the efficiency of energy conversion (internal to kinetic) or propulsion. Metrics like hydrodynamic efficiency are not calculated.

### **2.4 Energy Dissipation**

    *   Content: Energy is dissipated primarily through viscous friction within the MPC fluid as squirmers move and stir the fluid (hydrodynamic interactions). This is inherent to the MPC simulation method which models a fluid with viscosity (given as η = 42.6 sqrt(m*k_B*T)/a in Appendix A.3). Thermal fluctuations, modeled by the thermostat in MPC (Appendix A.1) and contributing to rotational diffusion D_R, also represent energy exchange/dissipation with a thermal bath. The Lennard-Jones potential represents conservative interactions, not dissipation. Quantification of dissipation rates is not provided, but its presence is qualitatively High due to the viscous fluid environment at low Reynolds number.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content:
        1.  **Hydrodynamic Interaction:** Mediated by the MPC fluid. Squirmers generate flow fields via surface slip (Eqs. 1-2, determined by v0, β). These flows affect neighboring squirmers' translation and rotation. Squirmers also experience forces and torques from the flow generated by others. This is implicitly modeled by the MPC simulation coupling squirmers and fluid particles (Appendix A).
        2.  **Alignment Steering:** Each squirmer `i` calculates a target orientation `e_aim,i` based on the average orientation of neighbors `j` within distance `R_a` (Eq. 6). It then generates adaptive surface flows (non-axisymmetric terms in Eqs. 1-2, determined by `C_11`, `C̃_11` via Eqs. 3-4) that produce a torque causing rotation towards `e_aim,i` with an angular velocity `ω_i = C_0 * e_i × e_aim,i` (Eq. 5). The strength is controlled by C0 (or dimensionless γ).
        3.  **Steric Repulsion:** Short-range excluded volume interaction modeled by a shifted Lennard-Jones potential to prevent overlap (Eq. B1, Appendix B).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Hydro | Active Stress | β | -3, 3 | dimensionless | Sec II | Explicit | Defines pusher/puller flow field type |
    | Align | Alignment Strength | γ (or C0) | 0 - 8192 | dimensionless | Sec II, Eq 7 | Explicit | Controls rate of alignment steering |
    | Align | Sensing Range | R_a | 4 * R_sq | Length | Sec II | Explicit | Defines neighborhood for alignment |

### **4.3 Global Order:**

    *   Content: Global polar order does *not* emerge (Fig 1d). Instead, different types of dynamic, self-organized structures emerge depending on the squirmer type (β) and maneuverability (γ):
        *   **Pushers (β < 0):** Active turbulence characterized by nearly homogeneous density, Gaussian velocity distributions (Fig 5b), collective vortical flows (Fig 2a-II), and power-law decay in the energy spectrum E(k) ~ k^-ν (Fig 2e).
        *   **Pullers (β > 0):** Swarming dynamics characterized by strong density clustering (Fig 3a-I, 3b), non-Gaussian velocity/vorticity distributions with fat tails (Fig 5b, 5c), and the dynamic formation and dissolution of vortex rings and fluid jets (Fig 3a-II, Fig 4).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Align | Vicsek-like alignment steering | γ (Maneuverability) | 0 - 8192 | dimensionless | Explicit | Controls strength of explicit alignment attempts | Eq. 7, Sec II |
| Hydro | Hydrodynamic interaction via fluid | β (Active Stress) | -3, 3 | dimensionless | Explicit | Distinguishes pusher/puller flow fields influencing interactions | Sec II |
| Hydro | Hydrodynamic interaction via fluid | Pe (Péclet Number) | 32, 128 | dimensionless | Explicit | Relative strength of self-propulsion vs. thermal diffusion | Eq. 7, Sec II |
| Align | Vicsek-like alignment steering | R_a (Sensing Range) | 4 * R_sq | Length | Explicit | Defines spatial extent of alignment interaction | Sec II |
| All | Collective Dynamics | ρ (Packing Fraction) | 0.012 - 0.186 | dimensionless | Explicit | Influences frequency and strength of interactions | Sec II, Figs 2h, 3g/h |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| PolarOrder | Global Polar Order Parameter | Φ | ~0 - 0.1 (Wet), up to ~0.9 (Dry) | dimensionless | Explicit | Measures global alignment; shows HDIs destabilize order | Fig 1(d) | Eq. in Fig 1(d) caption |
| PusherTurb | Energy Spectrum Function (Fluid/Squirmer) | E(k) | Varies (see Fig) | Energy * Length | Explicit | Characterizes turbulent state, power-law decay E~k^-ν | Fig 2(e,g,h) | Fourier Transform of Vel. Field/Correlation | App E |
| PusherTurb | Velocity Correlation Function | C_sq(r), C_fl(r) | -0.05 to 1 | dimensionless | Explicit | Shows spatial structure of velocity field | Fig 2(b) | Eq. E1 / Inverse FT of E(k) | App E |
| PusherTurb | Mean Squared Displacement (MSD) | <(Δr)^2> | Varies (see Fig) | Length^2 | Explicit | Characterizes particle dynamics (ballistic/diffusive regimes) | Fig 2(d) | Particle Tracking | Text Sec III |
| PullerClust | Local Density Distribution | P(ρ_loc) | Bimodal/Broad (High γ) | Probability Density | Explicit | Characterizes density heterogeneity / clustering | Fig 3(b,g) | Voronoi Tessellation | Text Sec IV.A |
| PullerClust | Energy Spectrum Function (Fluid) | E_fl(k) | Varies (see Fig) | Energy * Length | Explicit | Shows fluid dynamics has power-law features, differs from squirmers | Fig 3(e,h) | Fourier Transform of Vel. Field | App E |
| PullerVortex | Velocity-Vorticity Cross-Correlation | C_vω(r) | Varies (see Fig) | Vel*Vorticity (or normalized) | Explicit | Characterizes coupling between flow and vorticity, differs pushers/pullers | Fig 6 | Eq. E7 | App E |
| PullerVortex | Velocity/Vorticity Comp. Distribution | P(v̄_α), P(ω̄_α) | Non-Gaussian (fat tails) | Probability Density | Explicit | Shows deviation from Gaussian typical of turbulence | Fig 5(b,c) | Histogram of Field Components | Text Sec V |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Rotational Diffusion Time (τ_R = 1/D_R) | ~24390 (Derived) | a * sqrt(m / k_B T) | App A.3 (for D_R) | Mixed | D_R value given, τ_R derived. Sets thermal reorientation timescale. |
        | Alignment Response Time (τ_Align ~ 1/C0) | Varies (γ=C0/DR) | (τ_R units) | Eq. 5, Eq. 7 | Implicit | Inverse of alignment strength C0 sets the timescale for intentional reorientation. Depends on γ. |
        | Collective Structure Lifetime (e.g., vortex ring) | ~0.1 / D_R (Example) | (τ_R units) | Fig 4(c) caption | Explicit | Example lifetime shown for a specific event. Statistical distributions not provided. |
        | Simulation Time Step (Δt = h) | 0.02 | a * sqrt(m / k_B T) | Appendix A.2.a | Explicit | Discretization time for dynamics integration. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content:
        *   **Pushers (β<0):** Exhibit collective motion characterized as **Active Turbulence**. This includes formation of transient vortices, enhanced velocity fluctuations (speed up compared to v0), nearly homogeneous density distribution, and specific statistical signatures like Gaussian velocity distributions and power-law energy spectra (E(k) ~ k^-ν). (Sec III, Fig 2)
        *   **Pullers (β>0):** Exhibit dynamic **Swarming and Clustering**. This involves the formation of dense, mobile clusters with complex morphology, significant density inhomogeneity, strong fluid flows (jets) generated by aligned clusters, and the transient formation of **Vortex Rings**. Velocity and vorticity distributions deviate significantly from Gaussian, showing fat tails. Dynamics are described as chaotic. (Sec IV, V, Figs 3, 4, 5)
        *   **Both:** Lack of global polar order despite local alignment rule. (Sec II, Fig 1d)

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation relies on large-scale mesoscale hydrodynamic simulations (MPC). Evidence includes:
        *   **Visualizations:** Snapshots and movies showing collective structures (Figs 2a, 3a, 4a, 4c, Movies S1-S6).
        *   **Statistical Analysis:** Calculation of standard metrics used in active matter and fluid dynamics:
            *   Global Polar Order Parameter (Fig 1d).
            *   Spatial Velocity Correlation Functions (Figs 2b, 3f, 6).
            *   Local Density Distributions (via Voronoi tessellation, Figs 2c, 3b, 3g).
            *   Mean Squared Displacement (MSD) (Figs 2d, 3c).
            *   Kinetic Energy Spectra (Figs 2e,g,h, 3e,h).
            *   Velocity and Vorticity Distributions (Fig 5b,c).
        *   **Parameter Dependence:** Demonstrating consistent emergence of behaviors across varied Pe, γ, ρ (Sec III, IV, Figs 2, 3).
        *   **Comparison:** Contrasting pusher vs. puller behavior, and wet vs. dry systems (Fig 1d).
        *   **Finite-Size Analysis:** Checking for convergence with system size (App H, Fig 10).
     * Limitations: Validation is purely computational; no experimental comparison is presented within this paper. Claims rely on the validity and accuracy of the MPC squirmer model.

---

#Key: [terryn_review_2021]

# A review on self-healing polymers for soft robotics

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: This review paper surveys the field of self-healing (SH) polymers and evaluates their potential for application in soft robotics. It classifies various SH mechanisms (extrinsic/intrinsic, autonomous/non-autonomous) and assesses them against criteria derived from soft robotics requirements (healing macroscopic damage, multiple healing cycles, property recovery, elastomeric behavior, reprocessability/recyclability). The purpose is to bridge the SH polymer and soft robotics fields by analyzing existing SH elastomers, identifying trade-offs (e.g., mechanical strength vs. healing conditions), and reviewing the state-of-the-art in healable soft robotic prototypes (grippers, actuators, muscles). Key components discussed are various polymer chemistries (e.g., Diels-Alder, disulphide exchange, hydrogen bonds, metal-ligand coordination) and soft robotic components (pneumatic actuators, grippers).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name        | Value                       | Units          | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :-------------------- | :-------------------------- | :------------- | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
    *   **Note:** The parameters listed are key performance indicators for the *class* of SH polymers reviewed, as summarized in Table 3. Values represent the range reported in the table. Data Reliability is "High (as cited)" because the review explicitly tabulates these values from primary literature.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: For non-autonomous self-healing, the primary energy input is typically thermal (heat) or electromagnetic (light, often UV or Visible). For autonomous healing, the energy input triggering healing is the mechanical energy of the damage event itself (e.g., breaking capsules) or ambient conditions (e.g., room temperature for some bond reformation). For soft robotic actuation discussed, energy inputs are pneumatic pressure, vacuum, electrical (for tendons, SMAs, DEAs), or thermal (for SMPs).
    *   Value: Variable (e.g., TH = 37-180 °C, Light λ = 254-365 nm)
    *   Units: °C, nm, J (implicit for mechanical damage)

### **2.2 Energy Transduction**

    *   Content: *Healing:* Thermal energy increases molecular mobility and drives reversible/exchange reactions (bond breaking/reformation). Light energy directly breaks/forms photoreversible bonds or initiates polymerization (in extrinsic systems). Mechanical energy ruptures capsules or breaks mechanoreversible bonds. *Actuation:* Pneumatic/hydraulic energy is transduced into mechanical work via deformation of elastomers. Electrical energy is transduced into heat (driving SMPs/SMAs) or electrostatic force (DEAs), then mechanical work.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The review does not provide quantitative data or qualitative discussion on the energy efficiency of the *healing processes* themselves (e.g., energy required per healed bond or unit area). Energy efficiency is mentioned briefly in the context of soft robot actuation challenges (viscoelastic losses), but not quantified for SH materials specifically.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation during healing primarily occurs as heat loss to the surroundings, especially during thermal activation. For light-activated healing, non-absorbed/scattered light is lost. In extrinsic systems, energy is consumed during polymerization reactions (exothermic). During soft robotic actuation using these materials, viscoelastic effects (hysteresis, creep, stress relaxation related to dynamic bonds) are mentioned as causing energy loss/dissipation (Section "Requirements of SH polymers for soft robotics", C4; Section "Limitations of artificial self-healing polymers"). Quantification is not provided. Assessment: Medium (qualitative, due to viscoelasticity and healing stimuli).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable (seconds to days for healing completion; potentially long-term for healed state/shape memory)
*    Units: s, min, h, days (Qualitative Descriptor: "Long-term" for the healed state stability)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: 69-100 (Healing Efficiency ησ, ηε)
*   Units: %

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Low / Slight decrease over cycles
    *   Units: % loss per cycle (qualitative)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation | Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------------- | :------------------------------------------------ | :------------------------- | :------ | :---------------------------------- | :------------ | :----------------: | :--------------------------------------------- |
    | Healing Eff ησ  | Recovery of ultimate stress after healing         | 69-100 (Range)             | %       | Attribute `fidelity_stress` of MemoryNode | Table 3       | Explicit          | Measures recovery fidelity of tensile strength. |
    | Healing Eff ηε  | Recovery of ultimate strain after healing         | 65-100 (Range)             | %       | Attribute `fidelity_strain` of MemoryNode | Table 3       | Explicit          | Measures recovery fidelity of stretchability.   |
    | Multi-Cycle Heal| Ability to heal repeatedly at the same location | Yes (Generally, C2)        | Binary  | Attribute `robustness_cycles` of MemoryNode | Text (C2, DA) | Mixed             | Explicitly stated as requirement/possible.      |

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip to Module 6.)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description    | Value            | Units      | Source    | Implicit/Explicit | Justification                                            |
        | :----------------------- | :--------------- | :--------- | :-------- | :----------------: | :------------------------------------------------------- |
        | Healing Time (Stimulated)| Variable (Table 3)| s, min, h  | Table 3   | Explicit          | Time required at elevated temp/stimulus for healing.      |
        | Healing Time (Autonomous)| Variable (Table 3)| min, h, d  | Table 3   | Explicit          | Time required at RT/ambient for healing.                 |
        | Healing Recovery Time    | Variable (Table 3)| h, d       | Table 3   | Explicit          | Time required after stimulus for full property recovery.   |
        | Actuation Response Time  | Qualitative      | s (implied)| Text      | Implicit          | Robots actuate, implying response times (e.g., ~1s for butterfly Fig 8C), but not systematically reviewed. |
    *   **Note:** Timescales are primarily related to the healing process, derived from Table 3. Actuation and relaxation times are mentioned qualitatively.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes (Limited)

### **7.2 Adaptation Mechanism:**

    *   Content: The primary adaptation mechanism is the reversal of damage through the activation of specific chemical pathways inherent to the material. For intrinsic SH polymers, this involves the dynamic breaking and reformation of reversible covalent bonds (e.g., Diels-Alder, disulphide exchange) or physical interactions (e.g., hydrogen bonds, metal-ligand coordination), typically triggered by stimuli like heat or light, or occurring autonomously. For extrinsic systems, damage triggers the release and reaction of encapsulated healing agents. This mechanism restores the material's structural integrity and mechanical properties. It is a pre-designed, chemically encoded response to damage, not a learning process that modifies behavior based on performance feedback.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors discussed are: 1) **Self-Healing:** The ability of the polymer material to repair damage (cuts, punctures, cracks) either autonomously or upon application of an external stimulus (heat, light), leading to partial or full recovery of mechanical properties. 2) **Soft Robotic Actuation:** The use of these elastomers as the deformable bodies or components (e.g., grippers, artificial muscles, bending actuators) in soft robots, enabling functions like gripping, manipulation, and locomotion, typically driven pneumatically, by tendons, or other methods suitable for soft materials (e.g., DEAs).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The review validates the described behaviors (self-healing, soft robotic actuation) by citing numerous primary research articles. *Self-healing* is typically validated experimentally via mechanical testing (tensile tests comparing pristine, damaged, and healed samples to calculate healing efficiency ησ, ηε - see Table 3 methodology) and sometimes spectroscopic/microscopic analysis to confirm bond reformation or crack closure. *Soft robotic actuation* is validated through demonstration of prototypes performing tasks (gripping, bending, locomotion - see Figs 6-8) and characterization of their performance (force output, range of motion, response time, albeit often qualitatively in the review). Control experiments implicitly involve comparing healed vs. unhealed damaged samples or functional vs. damaged robots. Reproducibility/reliability is assessed via multiple healing cycles or lifetime testing (mentioned for commercial gripper). Limitations often involve ideal lab conditions vs. real-world environments.

---

#Key: [ballester_attending_2024]

# Attending to Topological Spaces: The Cellular Transformer

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is the Cellular Transformer (CT), a neural network architecture designed to generalize graph-based transformers to operate on cell complexes (topological spaces generalizing graphs, including nodes, edges, and faces). It processes data represented as 'cochains' (signals associated with cells of different ranks) defined on these complexes. Its core components include: (1) Transformer layers adapted for cell complexes, (2) Novel cellular attention mechanisms (pairwise and general) utilizing incidence relations between cells (e.g., node-edge, edge-face), and (3) Topological positional encodings (BSPe, RWPe, TopoSlepiansPE) designed specifically for cell complexes. The purpose is to improve the predictive performance of models on tasks involving topological data by effectively capturing higher-order relationships and long-range dependencies inherent in the cell complex structure, outperforming standard graph-based methods without needing enhancements like virtual nodes or graph rewiring. It takes an annotated cell complex as input and outputs predictions (e.g., classification, regression).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name             | Value       | Units   | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------------------- | :---------: | :-----: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Values for Hidden Dimension and Attention Heads vary depending on the dataset (GCB/ogbg-molhiv/ZINC) as specified in Table 3. The maximum cell complex dimension considered is 2 (nodes, edges, faces), although the framework is claimed to be generalizable. Positional encoding type is a key selectable parameter.

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8.)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value        | Units   | Source              | Implicit/Explicit | Justification                          |
        | :-------------------- | :----------: | :-----: | :-----------------: | :----------------: | :------------------------------------- |
    *   **Note:** Training and warmup epochs vary by dataset (GCB/ogbg-molhiv/ZINC). The paper mentions the quadratic complexity of attention (Sec 6.1), implying computation time scales with input size, but doesn't provide specific time values for inference or individual operations.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is performing predictions on datasets structured as cell complexes. Specifically, it performs graph classification (on GCB dataset, measured by accuracy) and molecular property prediction (on ZINC dataset, measured by Mean Absolute Error - MAE; on ogbg-molhiv dataset, measured by Area Under the ROC Curve - AUC-ROC). The system learns representations of cells (nodes, edges, faces) and uses the attention mechanism to aggregate information across the complex to make these predictions.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The behavior (predictive performance) is validated through standard machine learning benchmark practices. This involves: (1) Using established public benchmark datasets (ZINC, ogbg-molhiv, GCB) with predefined train/validation/test splits (Sec 5). (2) Comparing performance against state-of-the-art baseline models (message passing networks, graph transformers, simplicial transformers) using standard metrics (Accuracy, MAE, AUC-ROC) (Table 1). (3) Reporting results on the held-out test set (Tables 1, 2, 4). (4) For GCB, reporting mean and standard deviation over multiple runs (5 random seeds) to assess stability (Sec 5). Limitations include single runs for ZINC/ogbg-molhiv due to computational constraints and the use of a potentially suboptimal graph lifting procedure (cycle filling) (Sec 5, 6.1). Claims of emergence in the complex systems sense are not made or validated.

---

#Key: [toyabe_experimental_2010]

# Experimental demonstration of information-to-energy conversion and validation of the generalized Jarzynski equality

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### 1.1 System Description
    *   Content: The system consists of a dimeric polystyrene bead (diameter 287 nm) attached to a glass surface via a linker molecule, allowing rotational Brownian motion in a buffer solution. Quadrant electrodes impose a 1 MHz elliptically rotating electric field, creating a tilted periodic potential (analogous to a spiral staircase) for the particle's angular position. A real-time feedback control system monitors the particle's angular position. If the particle is detected in a specific angular region 'S' (where potential energy before switching is higher than after) at time t=0, the phase of the potential is inverted after a feedback delay 'ε'. Otherwise, no action is taken. The cycle repeats with period τ=44ms. The purpose is to demonstrate information-to-energy conversion (Szilárd engine) by using information about the particle's position to selectively apply feedback, causing the particle to climb the potential against the gradient, gaining free energy exceeding the work done. It also aims to experimentally validate the generalized Jarzynski equality under feedback control.

### 1.2 Implementation Clarity

### 1.3 Key Parameters
        | Parameter Name            | Value             | Units      | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------------------ | :---------------: | :--------- | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
    *   **Note:** The table lists key parameters characterizing the system's implementation as described in the paper. Reliability is high as these are directly stated experimental parameters or measured quantities central to the setup.

## M2: Energy Flow

### 2.1 Energy Input
    *   Content: The primary energy inputs are: (1) Thermal energy from the environment (buffer solution at temperature T) driving the Brownian motion (kBT). (2) Electrical energy supplied to the quadrant electrodes (at 1 MHz) to generate the time-varying potential landscape. (3) Energy consumed by the measurement and feedback control system (camera, computer, etc.) - acknowledged as macroscopic and large but not quantified in the context of the microscopic system's energetics.
    *   Units: Joules (or kBT for thermal energy).

### 2.2 Energy Transduction
    *   Content: 1. Thermal energy from the bath is transduced into kinetic energy of the particle (rotational Brownian motion). 2. Electrical energy applied to the electrodes is transduced into potential energy for the particle via dielectrophoresis in the non-uniform, rotating field. 3. Information acquired by measurement (particle position) is used by the feedback controller to trigger a change in the electrical potential (phase inversion). This action performs work (W) on the particle (change in potential energy due to switching) and influences the particle's ability to extract energy from the thermal bath, leading to a net gain in free energy (ΔF) for the particle. Heat is exchanged with the thermal bath during the particle's movement and relaxation within the potential wells. Specifically, for small ε, the particle gains free energy (climbs potential) by absorbing heat from the environment, facilitated by the information-driven feedback.

### 2.3 Energy Efficiency
    *   Justification/Metrics: The paper quantifies the *information-to-energy conversion efficiency* at the microscopic level as <ΔF - W> / (kBT * I) = 28% for the shortest feedback delay (ε=1.1 ms). This efficiency measures how much of the acquired information (I) is converted into excess free energy gain (<ΔF - W>). However, the paper explicitly states that a "huge amount of energy was consumed for the information processing at the macroscopic level" (control system). This macroscopic energy cost makes the *overall* process extremely inefficient from an energy harvesting perspective. The score reflects the low overall efficiency when considering the controller, despite the non-zero microscopic conversion.

### 2.4 Energy Dissipation
    *   Content: 1. Viscous Drag: The rotating particle experiences viscous friction with the surrounding buffer solution, dissipating energy as heat into the thermal bath. This is inherent to Brownian motion. (Quantification: Related to rotational diffusion coefficient, not explicitly calculated as dissipated power). 2. Control System: The macroscopic measurement and control apparatus (computer, camera, electrode drivers) dissipates a significant amount of energy as heat (explicitly mentioned as "huge", but not quantified). 3. Switching Work: The work (W) done *on* the particle during potential switching is calculated. In cycles where ΔF < W, the excess work eventually dissipates as heat. Qualitatively, viscous drag is the primary microscopic dissipation mechanism. Macroscopic dissipation in the controller is high.

## M3: Memory

### 3.1 Memory Presence:
    *   Content: No

**(Conditional: M3.1 is "No", skip to Module 4.)**

### 3.2 Memory Type:

### 3.3 Memory Retention Time:

### 3.4 Memory Capacity (Optional - if applicable)

### 3.5 Readout Accuracy (Optional - if applicable)

### 3.6 Degradation Rate (Optional - if applicable)

### 3.7 Memory Operations Energy Cost (Optional - if applicable)
        | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
        | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### 3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)
        | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

## M4: Self-Organization and Emergent Order

### 4.1 Self-Organization Presence:
    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

### 4.2 Local Interaction Rules:

### 4.2.1 Local Interaction Parameters:
    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### 4.3 Global Order:

### 4.4 Predictability of Global Order:

### 4.5. Local Interaction Rules (for Self-Organization)
        | Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
        | :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### 4.6. Globally Emergent Order and Order Parameters
        | Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
        | :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### 4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity
        | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
        | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### 5.1 Embodied Computation Presence:
    *   Content: No

**(Conditional: M5.1 is "No", skip to Module 6.)**

### 5.2 Computation Type:

### 5.3 Computational Primitive:

### 5.4 Embodied Computational Units
        | Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
        | :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### 6.1 Timescales:
        | Timescale Description         | Value        | Units | Source             | Implicit/Explicit | Justification                                    |
        | :---------------------------- | :----------: | :---- | :----------------- | :---------------- | :----------------------------------------------- |
        | Feedback Cycle Period (τ)     | 44           | ms    | Main Text, Methods | Explicit          | Stated experimental parameter.                   |
        | Feedback Delay (ε)          | 1.1 - ~40    | ms    | Main Text, Figs    | Explicit          | Varied experimental parameter.                   |
        | Minimum Feedback Delay        | 1.1          | ms    | Main Text          | Explicit          | Stated system specification.                     |
        | Measurement Period            | 1.1          | ms    | Methods            | Explicit          | Camera frame period.                             |
        | Measurement Exposure Time     | 0.3          | ms    | Methods            | Explicit          | Camera setting.                                  |
        | Relaxation time in well       | ~10          | ms    | Main Text          | Explicit          | Stated estimate. τ > relaxation time.           |
        | Time to jump neighbours       | ~1           | s     | Main Text          | Explicit          | Stated estimate. τ < jump time.                    |
        | Time-reversed cycle period    | 220          | ms    | Main Text, Methods | Explicit          | Period used for measuring γ, ensures equilibrium. |

### 6.2 Active Inference:
    *   Content: No

## M7: Adaptation

### 7.1 Adaptive Plasticity Presence:
    *   Content: No

**(Conditional: M7.1 is "No", skip to Module 8.)**

### 7.2 Adaptation Mechanism:

## M8: Emergent Behaviors

### 8.1 Behavior Description:
    *   Content: The primary functional behavior is unidirectional rotation of the particle against the potential gradient (climbing the spiral staircase) when the feedback delay 'ε' is small. This results in a net gain of free energy (ΔF) greater than the work (W) done by the potential switching. For large 'ε', the behavior is reversed, with the particle moving down the potential gradient. The core behavior demonstrates controlled energy extraction from a thermal bath using information. A secondary observed behavior is the violation of the standard Jarzynski equality and the validation of the generalized version under feedback.

### 8.2 Behavior Robustness:

### 8.3 CT-GIN Emergent Behavior Validation
     *  Content: The main behavior (information-to-energy conversion, ΔF - W > 0 for small ε) is validated quantitatively by measuring the free energy difference ΔF (derived from potential measurements) and the work W (potential energy change during switching) per cycle, averaged over many cycles and particles (Fig 3d). The validation of the generalized Jarzynski equality <exp[-(ΔF-W)/kBT]> = γ is performed by measuring ΔF and W for individual cycles to compute the exponential average, and independently measuring the feedback efficacy γ using time-reversed protocols (Methods, Supplementary Fig S8). The equality is shown to hold within experimental error (<3% discrepancy for small ε) over a range of ε (Fig 4a, 4b). Control experiments implicitly exist via variation of ε (large ε shows expected behavior without net free energy gain). Reproducibility is suggested by averaging over seven particles. Limitations include potential systematic errors in measuring potentials/ΔF, deviations from equilibrium assumptions, and the finite number of cycles affecting convergence (acknowledged in Fig 4c discussion).

---

#Key: [savoie_amorphous_2023]

# Amorphous entangled active matter

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The study investigates the emergent mechanical properties of amorphous entangled systems using two primary models: 1) An *in silico* system of U-shaped, three-link active particles ("smarticles") simulated using multibody dynamics (ProjectChrono). Smarticles have two outer links (barbs) connected to a middle segment via rotational actuators (2 DOF). 2) A *living system* composed of California blackworms (*Lumbriculus variegatus*) which form entangled "blobs". The purpose is to understand how local interactions (particle shape changes, activity protocols, worm behavior influenced by dissolved oxygen) lead to global emergent properties like packing fraction, entanglement, tensile strength, and collective behaviors (casting, toppling, melting). The study compares simulation results (smarticle packing, entanglement, fracture strength under different activation protocols: external oscillation, shape-change, internal oscillation) with biological experiments (worm blob packing, stability, and tensile strength under varying dissolved oxygen levels).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters chosen represent key variables controlling the system's state and behavior in both simulation and experiment.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Energy is input into the smarticle system via: 1) Velocity-controlled actuators applying torque to smarticle barbs (internal oscillation, shape-change protocols). 2) A linear actuator shaking the container (external oscillation protocol). For the worm system, energy originates from metabolic processes, influenced by the availability of dissolved oxygen which affects worm activity levels.
    *   Value: Varies depending on protocol (See Fig 8). E.g., Shape-change ≈ 2-10 J; Internal Oscillation ≈ 0.05-0.4 J; External Oscillation ≈ 0.1-0.3 J (for simulations). Biological energy input not quantified.

### **2.2 Energy Transduction**

    *   Content: In simulations: Electrical/control energy is transduced into mechanical work by smarticle actuators (torque x angular displacement) or the container shaker (force x linear displacement). This work overcomes internal friction, particle-particle friction, particle-wall friction, and increases the potential/kinetic energy of the smarticles, leading to rearrangement, packing changes, and entanglement. In the biological system: Chemical energy (from metabolism dependent on DO) is transduced into mechanical energy via muscle contraction, leading to worm movement, disentanglement/entanglement, and changes in blob structure/rheology.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper quantifies the *total energy input* for different simulation protocols (Fig 8) and relates it to outcomes like packing (Fig 7) and entanglement (Fig 9). However, it does not define or calculate a thermodynamic efficiency (e.g., useful work output / energy input) for achieving these states. The focus is on energy cost comparison between protocols, not efficiency in a thermodynamic sense. Therefore, assigning a score is not possible based on the provided text.

### **2.4 Energy Dissipation**

    *   Content: In simulations: Energy is dissipated primarily through friction (particle-particle, particle-wall, internal actuator friction - mentioned implicitly via torque limits) and inelastic collisions. The relaxation phase allows kinetic energy to dissipate. In the biological system: Energy is dissipated as heat from metabolic processes, viscous drag during movement in water, and internal friction within/between worms. The paper does not quantify these mechanisms. Qualitative assessment: Dissipation is significant in both systems, allowing them to reach quasi-static states after activity.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Seconds to Minutes (Qualitative)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Low/Medium (Qualitative)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: **Simulation (Smarticles):** Interactions governed by multibody dynamics physics engine (ProjectChrono). Key rules include:
        *   Contact mechanics: Hertzian contact model (implied by physics engine use, though specific model not stated) determining normal and frictional forces upon collision. Friction coefficient (μ=0.4) is specified (Table 1 footnote).
        *   Actuation: Motors controlling barb angles are velocity-controlled (ω = 6 rad/s) with a maximum torque limit (τ_max, defined in Sec 2.1 based on lifting particle weight).
        *   Gravity: Constant downward force.
        *   Confinement: Interaction with cylindrical container walls (friction μ=0.4).
        *   Activity Protocols define sequences of actuation/external forcing (Sec 2.1, Fig 2c-e).
    **Biology (Worms):** Interactions governed by:
        *   Thigmotaxis: Tendency of worms to maximize body contact, leading to entanglement (Sec 2.2).
        *   Activity Level: Individual worm movement (undulation, tail waving) influenced by dissolved oxygen (DO) levels (low DO -> increased activity for respiration, high DO -> decreased activity). This affects entanglement/disentanglement dynamics (Sec 2.2, 3.2).
        *   Physical constraints: Worm body flexibility, friction, hydrodynamics (implicitly).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Sim_Contact | Particle-Particle/Wall Friction | μ | 0.4 | dimensionless | Table 1 footnote | Explicit | Value given. |
    | Sim_Actuation | Actuation velocity | ω | 6 | rad/s | Section 2.1, Table 1 | Explicit | Value given. |
    | Sim_Actuation | Max actuation torque | τ_max | Defined relative to particle weight | N m (implied) | Section 2.1 | Explicit | Definition given, value depends on l/w. |
    | Bio_Activity | Environmental Factor | Dissolved Oxygen (DO) | <2 (Low), >8 (High) | mg/L | Section 2.2 | Explicit | Thresholds given. |
    | Sim_External | Oscillation Acceleration | Γ | 2 | g | Table 1 | Explicit | Value given. |
    | Sim_External | Oscillation Frequency | f | 30 | Hz | Table 1 | Explicit | Value given. |

### **4.3 Global Order:**

    *   Content: The emergent global order is characterized by:
        *   Packing Fraction (ϕ): A measure of how densely the particles/worms fill the available volume (Figs 3, 4, 5, 6, 7 for smarticles; Fig 14 for worms).
        *   Average Entanglement Number (N): Quantification of the degree of interpenetration between smarticles (Fig 9). Analogous entanglement strength implied for worms based on DO (Sec 3.2, Fig 17).
        *   Macroscopic Structure/Stability: Qualitative description of the collective's shape and its stability after confinement removal (casting, toppling, melting behaviors) (Figs 10, 11 for smarticles; Fig 16 for worms).
        *   Mechanical Properties: Collective tensile strength measured via fracture force (Fig 13 for smarticles; Fig 17 for worms).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Sim_Contact | Particle-Particle/Wall Friction | μ | 0.4 | dimensionless | Explicit | Value given. | Table 1 |
| Sim_Actuation | Actuation velocity | ω | 6 | rad/s | Explicit | Value given. | Table 1 |
| Bio_Activity | Environmental Factor | Dissolved Oxygen (DO) | <2 (Low), >8 (High) | mg/L | Explicit | Thresholds defining low/high activity states given. | Sec 2.2 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Global_Packing_Sim | Packing Density (Smarticles) | ϕ | ~0.25 - 0.55 | dimensionless | Explicit | Measured from simulation volume. Varies with l/w and protocol. | Deposition, Activity (Ext. Osc, Shape Chg, Int. Osc) | Fig 3, 4, 5, 6, 7 |
| Global_Entanglement_Sim | Average Entanglements (Smarticles) | N | ~2.5 - 6 | count | Explicit | Measured via plane intersection definition. Varies with l/w and protocol. | Post-Activity | Fig 9 |
| Global_Strength_Sim | Tensile Fracture Force (Smarticles) | F/ (W_s * n) | ~0 - 0.05 | dimensionless | Explicit | Measured in simulation pull test. Varies with l/w and protocol. | Fracture Test | Fig 13 |
| Global_Packing_Bio | Blob Height / Centroid (Worms) | h_t / h_max | ~0.7 - 1.0 | dimensionless | Explicit | Measured from images. Varies with DO/time. | DO Depletion | Fig 14 |
| Global_Strength_Bio | Max Tensile Force (Worms) | F_max | ~10 - 90 | mN | Explicit | Measured with load cell. Varies with DO. | Tensile Test | Fig 17 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Simulation Time Step (dt) | 2E-4 | s | Table 1 | Explicit | Value given. |
        | Simulation Relaxation Period | 0.5 | s | Sec 2.1 | Explicit | Value given. |
        | Simulation External Oscillation Period (1/f) | ~0.033 | s | Table 1 | Explicit | Calculated from f=30Hz. |
        | Simulation Shape-Change Phase Duration | 1.5 + 1.5 = 3 | s | Sec 2.1 | Explicit | Time for outwards + inwards actuation. |
        | Simulation Internal Oscillation Duration | 5 | s | Sec 2.1 | Explicit | Value given. |
        | Simulation Fracture Test Duration | ~1-2 | s | Fig 13 (estimated from x-axis) | Implicit | Estimated from plot timescale. |
        | Biological Packing Test Duration | ~3500 | s | Fig 14 (x-axis) | Explicit | Duration shown in plot. |
        | Biological Stability Test Duration | ~1-2 | s | Fig 16 (x-axis) | Explicit | Duration shown in plot. |
        | Biological Tensile Test Duration | ~7 | s | Fig 17b (x-axis) | Explicit | Duration shown in plot. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main emergent behaviors observed are:
        *   **Collective Packing/Compaction:** Change in volume fraction (ϕ) under external or internal activity (Figs 3-7, 14).
        *   **Entanglement Formation/Modulation:** Development of physical interpenetration between constituents, quantified by average entanglement number (N) in simulations, and inferred qualitatively/mechanically in worms based on DO level (Figs 2b, 9, 17).
        *   **Structural Stability/Instability:** Behavior upon removal of confinement, including maintaining shape ("casting"), collapsing sideways ("toppling"), or particles detaching ("melting") depending on preparation (Figs 10, 11, 16).
        *   **Tensile Load Bearing/Fracture:** Ability of the entangled collective to resist pulling forces, characterized by fracture force (Figs 12, 13, 17).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Emergent behaviors are validated through:
         *   **Quantitative Measurements:** Packing fraction (ϕ) calculated from simulation volumes; average entanglement number (N) calculated using a specific geometric definition in simulations; force measurements using simulated hooks and real load cells; centroid tracking from images. (Explicit, Sec 3.1.1, 3.1.4, 3.1.7, 3.2.1, 3.2.3, relevant figures).
         *   **Qualitative Observation:** Visual analysis of simulation renders and experimental videos/images to classify behaviors like toppling, melting, and casting. (Explicit, Sec 3.1.5, 3.1.6, 3.2.2, Figs 10, 11, 16, ESI Videos).
         *   **Comparative Analysis:** Comparing simulation results across different protocols and aspect ratios; comparing simulation trends to biological experiments under analogous conditions (high/low activity). (Explicit, Sec 3.1, 3.2, Figs 7, 9, 13, discussions comparing simulations and experiments).
     *   **Reproducibility:** Averaging over multiple trials (simulations) and repetitions (experiments) provides some measure of reproducibility. (Explicit, mentioned in text/captions for Figs 8, 13, 14, 16, 17).

---

#Key: [koehler_how_2024]

# How Do Particles with Complex Interactions Self-Assemble?

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system models the self-assembly of identical hexagonal particles confined to a 2D triangular lattice. Particles interact directionally through their six faces (patches). The interaction energy `J_ab` between two neighboring particles depends on the specific pair of faces (a, b) that are in contact and the particles' relative orientations. There are 21 independent interaction energies, forming an "interaction map" `J`. The purpose is to investigate how the complexity arising from this large number of independent, potentially competing interactions (`J_ab` values, characterized globally by average affinity `μ` and asymmetry `σ`) influences the resulting equilibrium aggregate morphologies. The study uses Monte Carlo (MC) simulated annealing to find low-energy configurations for various interaction maps, followed by machine learning (ML) classification of the resulting morphologies (e.g., liquid, crystal, fiber, oligomer) and identification of key predictive features of the interaction map.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** The table lists key parameters defining the simulation setup and the interaction landscape explored. `J` represents a specific instance, while `μ` and `σ` characterize the distribution from which `J` elements are drawn.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The system operates in the canonical (NVT) ensemble, implying coupling to a thermal bath at a constant temperature `T`. The energy input is thermal energy from this bath, driving the stochastic particle movements and reorientations in the Monte Carlo simulation. The temperature sets the scale for energy fluctuations.
    *   Value: 1
    *   Units: `k_B*T` (normalized units)

### **2.2 Energy Transduction**

    *   Content: Thermal energy from the bath enables particles to overcome energy barriers and explore different configurations via proposed MC moves (translations, rotations, cluster moves). The change in system potential energy (`ΔE`, determined by the interaction map `J`) upon a proposed move dictates, along with temperature `T`, the probability of accepting the move via the Metropolis criterion. Energy is exchanged between the kinetic energy of particles (implicitly represented by the MC process) and the potential energy stored in inter-particle interactions (`J_ab`). The system evolves towards lower potential energy states (or higher entropy states at `T>0`), characteristic of equilibrium self-assembly.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The concept of energy efficiency is not applicable here. The simulation aims to find equilibrium (minimum free energy) configurations, not to perform work or convert energy from one form to another with a specific output goal.

### **2.4 Energy Dissipation**

    *   Content: In the context of the Metropolis MC simulation, energy is exchanged with the thermal bath. Moves that decrease the system's potential energy (`E' < E`) are always accepted, effectively dissipating the energy difference `(E - E')` into the bath. Moves that increase potential energy (`E' > E`) are accepted with probability `exp[-(E'-E)/(k_B*T)]`, representing energy absorption from the bath. Over the course of simulated annealing towards equilibrium, the net flow is typically dissipative as the system settles into lower energy states. However, the paper doesn't quantify dissipation rates or mechanisms beyond the inherent energy exchange with the thermostat in the MC algorithm. Assessment: Medium (inherent in MC), but not quantified.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: 1. **Potential Energy:** Pairs of adjacent particles interact via face-specific energies `J_ab`, where `a` and `b` are the indices (1-6) of the interacting faces, determined by the particles' relative positions and orientations. The total potential energy is the sum over all interacting pairs: `E = Σ N_ab * J_ab`, where `N_ab` is the number of `ab` contacts (Sec II, Eq A1). 2. **Dynamics (MC):** Particles attempt moves (single particle translation/rotation, cluster swap/rotation) chosen randomly. A move changing energy by `ΔE` is accepted with probability `min(1, exp(-ΔE / k_B*T))` (Metropolis criterion, Appendix A1). Interactions are short-range (nearest neighbors only).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Potential | Interaction energy between face a and b | `J_ab` | Continuous, sampled from Gaussian(μ, σ) | `k_B*T` | Sec II, Eq (1) | Explicit | Defines interaction strength/sign. |
    | Dynamics | Thermal energy scale | `k_B*T` | 1 (fixed for final equilibrium) | Energy (normalized) | Sec II | Explicit | Sets probability of accepting unfavorable moves. |
    | Dynamics | Fraction of cluster moves | `τ` | 0.05 (used in study) | Dimensionless | Appendix A1, Fig 13 | Explicit | Influences equilibration speed. |

### **4.3 Global Order:**

    *   Content: The system exhibits various emergent global morphologies characterized by dimensionality, size limitations, and degree of orientational order. These include: Monomer (gas), Oligomer (small, finite clusters), Fiber (1D chains), Gel (ramified, porous network), Sponge (porous, ordered 2D structure), Crystal (ordered, dense 2D structure), Polycrystal (multiple crystalline domains), Liquid (dense, disordered 2D structure).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Explicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Pair Interaction | Energy between contacting faces a, b | `J_ab` | Continuous, drawn from Gaussian(μ, σ) | `k_B*T` | Explicit | Defines energetic preference for specific contacts/orientations. | Sec II, Eq (1) |
| MC Dynamics | Thermal energy scale | `k_B*T` | 1 (typically) | Energy | Explicit | Controls thermal fluctuations enabling exploration. | Sec II |
| MC Dynamics | Acceptance probability for move with energy change ΔE | `P_accept` | `min(1, exp(-ΔE/k_B*T))` | Probability | Explicit | Governs transitions between states based on energy. | Appendix A1 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Morphology | Aggregate Category | Type | {Monomer, Oligomer, Fiber, Gel, Sponge, Crystal, Polycrystal, Liquid} | Categorical | Explicit | Visual/structural classification of emergent state. | ML Classification based on geometric features | Sec III, Fig 2c, Figs 16-23 |
| Geometry | Average aggregate size | Size | Variable | Particles | Explicit | Measures extent of aggregation. | Image analysis | Sec III |
| Geometry | Aggregate porosity | Porosity | Variable | Dimensionless | Explicit | Measures fraction of empty space within aggregate boundary. | Image analysis | Sec III |
| Geometry | Aggregate surface-to-volume ratio | S/V ratio | Variable | 1/length | Explicit | Measures aggregate compactness/dimensionality. | Image analysis | Sec III |
| Order | Local particle environment entropy | `s` | 0 - ~17 | Bits | Explicit | Quantifies local orientational disorder (low=ordered, high=disordered). | Calculation from configurations | Appendix B5, Fig 27 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | MC Step | 1 | Simulation time step | Appendix A1 | Explicit | Basic unit of simulation time. |
        | Steps per Temperature | 7200 * N_particles | MC Steps | Appendix A1 | Explicit | Duration spent at each temperature during annealing. |
        | Annealing Duration | 100 * (Steps per Temp) | MC Steps | Appendix A1 | Explicit | Total simulation time for annealing. |
        | Equilibration Time | > 7200 steps/particle/temp step | MC Steps | Appendix B2, Fig 12 | Explicit | Time needed to reach approximate equilibrium at a given T. |
    *   **Note:** These are simulation timescales, not intrinsic material timescales. The relationship between MC steps and real time depends on unspecified system-specific attempt frequencies and energy barriers.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior is the self-assembly of particles into distinct macroscopic morphologies. Depending on the specific set of local interaction energies (`J`), the system spontaneously forms structures classified as Monomer, Oligomer, Fiber, Gel, Sponge, Crystal, Polycrystal, or Liquid phase. These morphologies differ in their dimensionality (0D, 1D, 2D), size (finite vs. extended), porosity, and degree of translational and orientational order.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation relies on: 1) Monte Carlo simulations using simulated annealing to find low-energy (equilibrium) states (Sec II, Appendix A1). 2) Consistency checks showing results are independent of simulation details like annealing duration, particle density, and system size (Appendix B2, Figs 12, 14, 15). 3) Classification of morphologies using geometric features (size, porosity, S/V ratio) and visual inspection (Sec III). 4) High accuracy (>99%) of a machine learning classifier trained to predict the morphology category from interaction maps and geometric features, confirming the distinctness and predictability of the categories (Sec III, Fig 5, Appendix A2). 5) Analysis of local order entropy discriminating between dense phases (Appendix B5, Fig 27). Limitations include the 2D lattice constraint and reliance on equilibrium assumptions.

---

#Key: [gompper_2025_2025]

```markdown
# The 2025 Motile Active Matter Roadmap

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews the field of Motile Active Matter, defined as non-equilibrium systems composed of numerous autonomous self-propelled agents. These agents consume energy to move and interact, exhibiting collective behaviors across scales (nanomotors, cells, insects, etc.). Key characteristics include inherent non-equilibrium nature, directional sensing, information processing, adaptive motion, non-additive/non-reciprocal interactions, and self-organization in complex environments. The purpose of the roadmap is to review the state-of-the-art and outline future research directions, encompassing understanding fundamental principles and designing advanced synthetic active systems (materials, microbots). Components include the active agents (biological or synthetic), the environment (fluid, boundaries, chemical fields), energy sources, and interactions (hydrodynamic, steric, chemical, informational).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
    *   **Note:** As a review paper covering a broad field, specific quantitative parameters for a single implementation are not provided in the overview sections. Various parameters are discussed qualitatively or within specific contexts in the different roadmap sections (e.g., Peclet number, vision angle, maneuverability in Sec 14; rotation frequency in Sec 8; density, noise levels).

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The systems described rely on persistent energy consumption from various sources, including ambient energy, chemical energy (e.g., fuel decomposition for synthetic swimmers, metabolism for biological organisms), or external fields (light, magnetic, acoustic fields mentioned implicitly via references to different system types). Section 5 specifically focuses on chemical propulsion.

### **2.2 Energy Transduction**

    *   Content: Energy (chemical, light, etc.) is transduced into mechanical work, resulting in self-propulsion (motility) and potentially other activities like internal dynamics or signal generation. Specific mechanisms vary widely depending on the system (e.g., catalytic reactions driving phoretic flows, biological motors like flagella, magnetic actuation, light-induced activity).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Energy efficiency is not quantitatively discussed in the provided overview sections. Generally, microscale swimmers operate at low Reynolds numbers and are known to have low thermodynamic efficiency, but this is not stated or quantified in the excerpt.

### **2.4 Energy Dissipation**

    *   Content: Dissipation is implicitly required due to the non-equilibrium nature described (Abstract: "inherent non-equilibrium nature"). Mechanisms include viscous drag in the fluid environment, internal friction within agents, and potentially heat loss from chemical reactions or motor activity. Quantification is not provided. Assessment: High (expected for active matter systems).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: Various local rules are described across different sections:
        *   **Section 1:** Interactions with complex/porous media (geometric constraints, rheology), hydrodynamic interactions, chemotaxis, quorum sensing.
        *   **Section 2:** Collision-mediated interactions (mixing, depletion-like forces), actuation via swimmer-generated flows.
        *   **Section 3:** Interactions with quenched disorder (obstacles, random fields). Torque interactions. Metric vs. metric-free interactions.
        *   **Section 5:** Chemical interactions (phoresis, gradients).
        *   **Section 7:** Hydrodynamic interactions between spinning/chiral swimmers (Stokeslets, spin-orbit coupling).
        *   **Section 8:** Isotropic repulsion, polar/nematic alignment interactions.
        *   **Section 9:** Elastic couplings (active solids, metamaterials).
        *   **Section 11:** Adhesion logic, density-motility coupling via quorum sensing.
        *   **Section 13:** Non-reciprocal interactions (vision-cone based, cross-species/field couplings).
        *   **Section 14:** Vision-induced steering (attraction towards neighbors in vision cone), alignment steering (orientational alignment with neighbors). These are explicitly non-additive and vision-steering is non-reciprocal.
        *   **Section 15:** Feedback control defining effective interactions (perception-reaction delays). Thermo-osmotic/phoretic interactions.
        *   **Section 17:** Interactions underlying collective problem solving (e.g., recruitment trails, cooperative transport).
        *   **Section 19:** Mechanical interactions, potentially gene-expression influenced interactions.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :------------------: | :---: | :----------: | :----------------: | :------------: |
    *   **Note:** Specific parameters for local rules are not systematically listed or quantified for the entire field in this roadmap overview.

### **4.3 Global Order:**

    *   Content: Numerous emergent global orders/patterns are explicitly mentioned:
        *   Motility-Induced Phase Separation (MIPS) / Clustering (Sec 1, 8)
        *   Flocking / Swarming / Schooling (Intro, Sec 14)
        *   Rotating macrodroplets / Microflock patterns (Sec 8)
        *   Polar vortices / Active foams / Traveling bands / Waves (Sec 8, 13, 18)
        *   Active solids / Crystals / Odd elastic states (Sec 9)
        *   Patterned biofilms (Sec 11, 19)
        *   Periodic stripes / Segregation / Co-localization (Sec 11)
        *   Hydrodynamic bound states / Orbits (Sec 7)
        *   Jamming / Trapping (Sec 3)
        *   Self-organized transport / manipulation (Sec 2, 12)
        *   Hierarchical structures (Sec 15)
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Other (Specify: Collective Decision Making, Biological Signal Processing, Reinforcement Learning Policy Execution, Reservoir Computing). Possibly Analog for sensor integration pathways.

### **5.3 Computational Primitive:**

    *   Content: Varies greatly depending on the specific system and level of description. Examples implied include:
        *   **Thresholding:** Response activation based on stimulus level (e.g., chemotaxis, quorum sensing).
        *   **Signal Integration:** Combining inputs from multiple neighbours or sensors (e.g., vision cone integration in Sec 14).
        *   **Filtering:** Adaptive phototaxis acting as a bandpass filter (Sec 7).
        *   **Logic Gate:** Synthetic adhesion logic (Sec 11).
        *   **Policy Mapping (State-Action):** In RL-trained swarms (Sec 16).
        *   **Nonlinear Transformation:** Reservoir computing relies on the system's dynamics for transformation (Sec 15).
        *   **Gradient Calculation/Following:** Implicit in chemotaxis/phototaxis.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description        | Value | Units | Source        | Implicit/Explicit | Justification                                   |
        | :--------------------------- | :---: | :---: | :------------ | :----------------: | :---------------------------------------------- |
    *   **Note:** Specific values are not provided in the overview. The table lists types of relevant timescales mentioned across different sections.

### **6.2 Active Inference:**

    *   Content: Partial
        *   (1) *Prediction/Anticipation:* Implicit in pursuit strategies (Sec 14), problem-solving (Sec 17), and potentially in RL policies (Sec 16) that optimize future rewards. Adaptive phototaxis (Sec 7) implies anticipating periodic signals.
        *   (2) *Action Selection:* Agents actively "adjust their motion" (Abstract, Sec 14) based on sensed information to achieve goals (e.g., alignment, pursuit, foraging in Sec 16, problem-solving in Sec 17).
        *   (3) *Internal Models:* Implicit in cognitive processing (Sec 14, 17), learning (Sec 16), and adaptation (Sec 7, 10). Biological systems possess internal models via genetics and physiology (Sec 19).
        The framework is particularly suggested by the descriptions of cognitive agents performing tasks like pursuit or collective problem solving. However, the explicitness and complexity of the internal models and the minimization principle are not detailed enough across all described systems to confirm full active inference universally.
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** Measuring prediction error reduction rates in trained RL swarms (Sec 16); quantifying the correlation between sensory input and anticipatory motor response in phototaxis (Sec 7) or pursuit (Sec 14); analyzing information flow in sensorimotor loops using CT; assessing the complexity of learned policies (Sec 16) or evolved strategies (Sec 17) as proxies for internal model complexity.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: Several mechanisms are described or implied:
        *   **Feedback Control:** External feedback loops (often digital) modifying particle behavior based on sensed state (Sec 15, Sec 16).
        *   **Reinforcement Learning:** Agents' policies (mapping states to actions) are modified over time based on reward signals to improve task performance (Sec 16).
        *   **Biological Adaptation / Phenotypic Plasticity:** Changes in gene expression leading to altered morphology or behavior in response to environmental history or cues (Sec 19, implicitly Sec 6, 7, 11). Specific examples include tuning phototactic response (Sec 7).
        *   **Material Response:** Intrinsic changes in material properties or structure (e.g., reconfiguration of microswimmers/clusters - Sec 10).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The roadmap describes a wide array of collective/emergent behaviors:
        *   **Collective Motion:** Flocking, swarming, schooling, milling, active turbulence (Intro, Sec 8, 14, 18).
        *   **Pattern Formation:** MIPS, clustering, vortex formation, traveling waves/bands, stripe formation, active crystals/solids, biofilm structuring (Sec 1, 8, 9, 11, 13, 18, 19).
        *   **Transport & Manipulation:** Transport of passive objects, cargo delivery, microrobotic actuation, manipulation via active flows (Sec 2, 12, 15).
        *   **Environmental Interaction:** Navigation in complex/porous media, surface accumulation/entrapment, chemotaxis, phototaxis, rheotaxis (Sec 1, 3, 6, 7).
        *   **Task Performance:** Pursuit, foraging, collective problem solving, targeted drug delivery, self-assembly (Sec 2, 12, 14, 16, 17).
        *   **Mechanical Response:** Active elasticity, odd elasticity, self-healing/reconfiguration (Sec 9).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The roadmap validates claims of emergent behaviors primarily by citing existing literature (experimental, theoretical, computational studies - see references throughout). Specific sections reference figures from cited papers (e.g., Fig 1.1, 1.2, 3.1, 4.1, 7.1, 8.1, 9.1, 10.1, 11.1, 12.1, 13.1, 15.1, 16.1, 17.1, 18.1, 19.1). Validation methods inherent to the field include direct observation (microscopy), simulations (agent-based, continuum), and theoretical analysis (hydrodynamics, statistical mechanics, RG). The roadmap itself synthesizes these validated findings. Limitations of specific validations are generally not discussed in the overview.

---

#Key: [kamsma_iontronic_2023]

# Iontronic Neuromorphic Signaling with Conical Microfluidic Memristors

**Paper Type:** Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of conical microfluidic channels connecting two bulk reservoirs of an aqueous 1:1 electrolyte. An external voltage V(t) is applied across the channel. The conductance of these channels depends on the history of the applied voltage due to transient concentration polarization, making them function as memristors. The paper primarily models the physics of a single conical channel (using Poisson-Nernst-Planck-Stokes equations and an analytical approximation) and then proposes an iontronic circuit composed of three such oriented conical channels, batteries, and a capacitor, designed to mimic the Hodgkin-Huxley model of neuronal signaling. The purpose is to demonstrate that these conical memristors can be used as building blocks for neuromorphic circuits capable of generating neuronal features like action potentials and spike trains. Key components are the conical channel, aqueous electrolyte (water + ions), electrodes (implicit for applying V(t)), and, in the proposed circuit, additional cones, batteries, and a capacitor.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Characteristic Time (τ) | 4.8 | ms | Text p.3, Eq. 6 | Explicit | Medium | Calculated via Eq. 6 using L and D |

    *   **Note:** These parameters define the physical structure and transport properties of the single conical channel used in the core analysis. Data reliability is 'High' as these are stated parameters for the model/simulations, 'Medium' for τ as it's derived.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the externally applied electrical potential difference, V(t), across the conical channel (or the circuit). In the proposed circuit, batteries (E+, E-, Es) also provide potential differences.
    *   Value: Variable (e.g., ±1 V for single cone analysis, E values for circuit)
    *   Units: V (Volts)

### **2.2 Energy Transduction**

    *   Content: Electrical potential energy from V(t) drives ion movement (conduction, diffusion, advection - Nernst-Planck Eq. 3) and fluid flow (electro-osmosis - Stokes Eq. 4). This kinetic energy of ions and fluid is dissipated through viscous drag within the fluid and interactions with channel walls. Electrical energy is also converted into stored potential energy associated with non-equilibrium ion concentration gradients (concentration polarization). In the circuit, chemical energy from batteries is converted to electrical potential energy. Energy is stored in the capacitor's electric field.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss energy efficiency. The primary function described (memristance, neuromorphic signaling) relies on dissipative processes (ion transport through a resistive medium, viscous fluid flow). While energy is temporarily stored in concentration gradients and the capacitor, the overall operation appears highly dissipative, analogous to ionic currents in biological neurons which are not optimized for energy efficiency in the traditional work-output sense. No metrics for efficiency are provided. The score reflects the lack of focus on efficiency and the inherently dissipative nature of the underlying ion transport processes.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation occurs primarily through:
        1.  Joule heating: Due to ionic current (I) flowing through the electrolyte resistance (related to conductance g). (Qualitative: Significant, as current flow is central).
        2.  Viscous dissipation: Due to electro-osmotic fluid flow (u) within the channel, governed by fluid viscosity (η). (Qualitative: Present, mentioned as Stokesian advection and electro-osmotic flow Q(V), but likely less dominant than Joule heating unless flow is very strong).
        Dissipation is not explicitly quantified in the paper.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: τ ≈ L² / (12D) ≈ 4.8 (for standard parameters)
*    Units: ms (milliseconds)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Governed by 1/τ
    *   Units: s⁻¹

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Neuromorphic

### **5.3 Computational Primitive:**

    *   Content: Temporal Integration / State-dependent Resistance (Memristance). The basic operation performed by the material (cone+electrolyte) is the dynamic, leaky integration of the applied voltage history into its conductance state 'g', governed by dg/dt = (g∞[V(t)] - g)/τ. This state 'g' then acts as a variable resistor determining the output current I = gV.
    *   **Sub-Type (if applicable):** Leaky Integration

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Conductance Relaxation / Memory (τ) | 4.8 (std), varies with L² | ms | Eq. 6, p.3 | Explicit | Characteristic time derived and discussed. |
        | Input Voltage Frequency (f) | 40 (Fig 2), variable | Hz | Fig 2 | Explicit | Frequency used in example simulation. |
        | AP Duration (Circuit) | ~10-20 | ms | Fig 3d | Explicit | Estimated from plot. |
        | Spike Train Period (Circuit) | ~80-100 | ms | Fig 3f | Explicit | Estimated from plot (tunable). |
        | Fast Channel τ (Circuit) | 0.048 | ms | p.4 (derived from L=1µm) | Explicit | Calculated using Eq. 6 for circuit parameters. |
        | Slow Channel τ (Circuit) | 30 | ms | p.4 (derived from L=25µm) | Explicit | Calculated using Eq. 6 for circuit parameters. |
    *   **Note:** Multiple timescales are relevant: the intrinsic memory time τ of a single cone, the frequency of applied signals, and the emergent timescales of the proposed circuit (AP duration, spike period). Circuit channel timescales are also specified.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content:
        1.  **Single Cone:** Exhibits memristance (history-dependent conductance/resistance), specifically showing hysteresis in conductance-voltage and current-voltage plots (pinched hysteresis loop). This behavior stems from dynamic concentration polarization.
        2.  **Proposed Circuit:** Exhibits key features of neuronal signaling:
            *   All-or-none action potential (AP) generation in response to a supercritical current pulse stimulus.
            *   Generation of a spike train (sequence of APs) in response to a sustained supercritical stimulus current.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content:
        *   **Memristance/Hysteresis (Single Cone):** Validated through (1) Finite Element (FE) numerical simulations of the full PNPS equations (1-4) and (2) an Approximate Analytical (AA) model (Eqs. 5-8). The agreement between FE and AA results (shown in Fig. 2a, b, c) provides validation for the AA model and confirms the hysteretic behavior arising from concentration polarization dynamics. The pinched hysteresis loop (Fig. 2c) is presented as the hallmark validation. Comparison to experimental results in *similar* systems is mentioned qualitatively (p.3).
        *   **APs/Spike Trains (Circuit):** Validated through numerical solution of the proposed circuit equations (Eq. 9 coupled with Eq. 7 for each cone), using the validated AA model for cone conductance. The emergence of all-or-none APs (Fig. 3d, inset) and spike trains (Fig. 3f) is demonstrated through these simulations when specific stimulus protocols (Fig. 3c, e) are applied.
        *   **Limitations:** Validation is purely theoretical/computational. No experimental validation of *this specific system* or circuit is presented. Robustness/reproducibility under non-ideal conditions is not assessed.

---

#Key: [zhang_nanopore_2022]

# A nanopore interface for higher bandwidth DNA computing

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a multiplexable, sequencing-free readout method for DNA Strand Displacement (DSD) circuits using nanopore sensor array technology (Oxford Nanopore Technologies' MinION device). It enables real-time, kinetic measurement of DSD circuit activity by directly detecting barcoded ssDNA output strands. Components include: DSD circuits (input strands, gate complexes comprising output and bottom strands, fuel strands), 3'-biotinylated and barcoded ssDNA output strands, streptavidin, a MinION device with R9.4.1 flow cells, custom MinKNOW run script for voltage flipping, analysis pipeline (adapted from Cardozo et al., involving capture event isolation, filtering, feature extraction), and machine learning classifiers (Logistic Regression, RandomForest, CNN, ResNet-18) for barcode identification from raw ionic current signals. The purpose is to increase the output bandwidth and scalability of DSD circuit readout compared to traditional fluorescence spectroscopy, using an inexpensive and portable device.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Electrical potential difference applied across the nanopore membrane.
    *   Value: -180
    *   Units: mV

### **2.2 Energy Transduction**

    *   Content: Electrical potential energy drives the electrophoretic movement of negatively charged DNA molecules through the buffer solution towards the positively charged *cis* side of the membrane. When a DNA molecule enters the nanopore, it partially blocks the flow of ions (driven by the same electrical potential), converting the potential energy difference across the pore into a modulated ionic current. The binding of streptavidin to the biotin prevents full translocation, holding the DNA in the pore for sensing. Reversing the voltage ejects the strand.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss or provide metrics related to the energy efficiency of the nanopore readout process (e.g., energy consumed per classified strand). The focus is on readout bandwidth and accuracy.

### **2.4 Energy Dissipation**

    *   Content: Energy is dissipated primarily as heat due to the ionic current flowing through the resistance of the buffer solution and the nanopore itself (Joule heating). There may also be viscous drag on the DNA molecule during electrophoretic movement, though likely negligible compared to ionic current dissipation. Quantification is not provided. Qualitative assessment: likely low per pore, but potentially significant across the entire array.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Sampling Interval | 0.1 | ms | Methods (derived from 10kHz sampling freq) | Explicit | Sampling frequency is explicit. |
        | Voltage Flip Period | 10 | s | Fig. 1b caption, Methods (ref 34) | Explicit | Stated as 1/frequency. |
        | Minimum Capture Event Duration | 1 | ms | Methods | Explicit | Stated threshold for analysis. |
        | Analysis Window for Classification | 2 | s | Methods, Fig. 3c | Explicit | Stated window length used for CNN input. |
        | Kinetic Measurement Interval | 5 | min | Methods, Fig. 2c | Explicit | Stated interval for calculating TBC. |
        | Kinetic Experiment Duration | ~4 | hours | Methods | Explicit | Stated duration for single circuit kinetics. |
        | Titration Experiment Duration | 10-20 | min | Methods | Explicit | Stated duration range depending on concentration. |
        | Reaction Time to Steady State (Multiplexing/let-7) | 1-3 | hours | Methods | Explicit | Stated pre-incubation times. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary observable behavior is the generation of characteristic, sequence-dependent ionic current blockade signals when individual 3'-biotin-streptavidin modified ssDNA molecules are captured in the nanopores under an applied voltage. The system allows repeated capture and ejection via voltage flipping. These signals are then processed externally to classify the captured strand based on its barcode sequence, enabling multiplexed, real-time quantification of different DNA species (DSD outputs) in solution. Secondary behaviors include concentration-dependent capture frequency and distinct current levels for different barcodes.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary behavior (barcode classification) is validated through quantitative analysis of experimental data. Classification accuracy is assessed using confusion matrices on withheld test sets (Figs 3d, 4a, 4c, Supp Fig 13). Multiplexing capability is validated by experiments where specific circuits are activated and their corresponding barcodes are shown to be enriched compared to inactive or absent circuits (Figs 3e, 4d, 5c). Kinetic measurements are validated by comparison with traditional fluorescence methods (Fig 2c). Control experiments are used (e.g., no input samples, testing non-output strands, Supp Figs 1, 3, 4, 5, 6). Reproducibility is suggested by replicates (Figs 1d, 4d, 5c). Limitations include potential classifier dependency and observed crosstalk in one application (let-7).

---

#Key: [wehner_integrated_2016]

# An integrated design and fabrication strategy for entirely soft, autonomous robots

**Paper Type:** Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is an untethered, entirely soft robot ('octobot') featuring eight pneumatically actuated arms. Its components include a moulded elastomeric body, embedded fuel reservoirs (containing H2O2 monopropellant), platinum catalyst reaction chambers, pneumatic actuator networks, vent orifices, and a microfluidic logic controller fabricated using soft lithography. The robot's purpose is to demonstrate an integrated design and fabrication strategy for achieving autonomy in completely soft robots. It operates by catalytically decomposing the on-board fuel supply, generating pressurized gas that inflates actuator networks regulated by the microfluidic oscillator, resulting in alternating limb movement.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                         | Value                    | Units       | Source (Fig/Table/Section)   | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------------------------------- | :-----------------------: | :----------: | :---------------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Oscillator Design Flow Rate            | ~40                      | μl min⁻¹    | Main Text                    | Explicit          | Medium                          | Based on design/prior work ref 11 |
        | Operating Pressure (Actuation)         | ~50                      | kPa         | Main Text / Fig. 4b          | Explicit          | Medium                          | Inferred from actuation data      |
        | Actuator Hyperelastic Layer Thickness  | 1000                     | μm          | Main Text / Fig. 4b          | Explicit          | High                            | Stated choice                     |
        | Body Matrix Storage Modulus (G')       | ~10³ - 10⁴               | Pa          | Fig. 2b                      | Explicit          | High                            | Measured                          |
        | Fugitive Ink Storage Modulus (G')      | ~10⁴ - 10⁵               | Pa          | Fig. 2b                      | Explicit          | High                            | Measured                          |
        | Run Time                               | 4 - 8                    | minutes     | Main Text / Ext Data Fig. 8 | Explicit          | High                            | Observed                          |

    *   **Note:** Parameters selected represent key material, operational, and performance aspects.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is chemical potential energy stored in the on-board liquid fuel, specifically 50 wt% aqueous hydrogen peroxide (H2O2).
    *   Value: 1.44 (Energy Density)
    *   Units: kJ g⁻¹ (Energy Density)

### **2.2 Energy Transduction**

    *   Content: 1. Chemical potential energy (H2O2) is converted into chemical kinetic/thermal energy via catalytic decomposition (exothermic reaction: 2H₂O₂(l) → 2H₂O(l, g) + O₂(g)) in the platinum-laden reaction chambers. 2. This generates high-pressure gas (water vapor and oxygen). The chemical/thermal energy is transduced into pneumatic potential energy (pressurized gas). 3. The pneumatic potential energy is converted into mechanical work via the inflation and deformation (bending) of the soft actuators.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Efficiency is not explicitly quantified. However, based on the processes involved (exothermic reaction with heat loss, potential gas leakage, viscous losses in microfluidic channels, viscoelastic damping in actuators, energy lost through venting), the overall efficiency converting chemical energy to useful mechanical work is expected to be very low. The system prioritizes autonomy and softness over efficiency. Qualitative Assessment: Low.

### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip to Module 4.)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: 1. **Fluid Dynamics:** Pressure-driven flow (Navier-Stokes approx.) through micro/meso-scale channels, governed by channel geometry, fluid viscosity, and pressure gradients. Includes behavior of check valves (unidirectional flow) and pinch valves (occlusion based on pressure differentials) in the oscillator (Fig 3a, 3b). 2. **Reaction Kinetics:** Catalytic decomposition rate of H2O2 on Pt surface, dependent on fuel concentration, catalyst surface area, and temperature (implicitly). Produces gas leading to pressure increase. 3. **Elastic Mechanics:** Deformation of elastomeric actuators and reservoirs in response to internal pressure (pressure-volume relationship, bending mechanics based on geometry and material modulus differences) (Fig 4a, 4b). 4. **Mass Transport (Auto-evacuation):** Diffusion of water from fugitive ink through the PDMS matrix driven by concentration gradient and temperature (Extended Data Fig. 5).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID              | Description                        | Parameter Name             | Parameter Value Range | Units        | Data Source             | Implicit/Explicit | Justification                                    |
    | :------------------- | :--------------------------------- | :------------------------- | :-------------------- | :----------- | :---------------------- | :---------------- | :----------------------------------------------- |
    | Fluid Dynamics       | Oscillator Valve Operation       | Fuel Flow Rate (design)    | ~40                   | μl min⁻¹     | Main Text               | Explicit          | Stated design parameter based on ref 11          |
    | Fluid Dynamics       | Actuator Inflation/Venting       | Vent orifice width         | ~75                   | μm           | Main Text               | Explicit          | Stated tailored value                            |
    | Reaction Kinetics    | Fuel Decomposition             | Fuel Concentration         | 50                    | wt%          | Main Text               | Explicit          | Chosen concentration                             |
    | Elastic Mechanics    | Actuator Bending                 | Hyperelastic layer thickness | 1000                  | μm           | Main Text / Fig. 4b     | Explicit          | Chosen parameter affecting displacement/pressure |
    | Elastic Mechanics    | Matrix Behavior during Printing  | Body Matrix Yield Stress   | ~10 - ~100            | Pa           | Ext. Data Fig. 2c       | Explicit          | Measured property ensuring ink stability         |
    | Mass Transport       | Auto-evacuation                  | Temperature                | 90                    | °C           | Methods / Main Text     | Explicit          | Condition for auto-evacuation                  |

### **4.3 Global Order:**

    *   Content: The primary emergent global order (dynamic) is the rhythmic, alternating actuation of the two sets of four arms (red vs. blue states in Fig 4c, 4d). This macroscopic oscillation arises from the designed microfluidic logic and the coupled chemo-pneumatic processes. During fabrication, the global order is the patterned, monolithic robot structure itself, achieved via designed deposition and self-assembly (auto-evacuation).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID              | Description                     | Parameter                    | Value Range           | Units        | Implicit/Explicit | Justification                             | Source                  |
| :------------------- | :------------------------------ | :--------------------------- | :-------------------- | :----------- | :---------------- | :---------------------------------------- | :---------------------- |
| Mass Transport       | Water Diffusion (Auto-evac)   | Temperature                  | 90                    | °C           | Explicit          | Parameter controlling diffusion rate      | Methods / Main Text     |
| Fluid Dynamics       | Oscillator Valve Action         | Pressure Differential        | Variable              | Pa           | Implicit          | Valves act based on pressure differences  | Fig 3 / Ref 11          |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID      | Description                        | Parameter            | Value Range           | Units   | Implicit/Explicit | Justification                                     | Protocol        | Source                  |
| :--------------- | :--------------------------------- | :------------------- | :-------------------- | :------ | :---------------- | :------------------------------------------------ | :-------------- | :---------------------- |
| Dyn. Oscillation | Alternating Arm Actuation          | Oscillation Frequency | ~0.1 - 0.2 (estimate) | Hz      | Implicit          | Estimated from 4-8 min runtime & visual inspection | Observation     | Main Text / Ext Data Fig. 8 |
| Dyn. Oscillation | Actuator Angular Displacement      | Max Angle (θ)        | ~60                   | degrees | Explicit          | Maximum deflection achieved                       | Measurement     | Fig. 4b                 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type        | Description                                         | Predictability   | Yoneda Score | Metrics                 | Implicit/Explicit | Justification                                                                 | Source    |
    | :--------------- | :-------------------------------------------------- | :--------------- | :----------- | :---------------------- | :---------------- | :---------------------------------------------------------------------------- | :-------- |


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid

### **5.3 Computational Primitive:**

    *   Content: Fluidic Oscillation / Flow Switching. The basic operation is the alternating diversion of fluid flow between two output channels based on internal pressure feedback mediated by check and pinch valves. This constitutes a fluidic relaxation oscillator.

### **5.4 Embodied Computational Units**
| Unit ID            | Description                    | Processing Power | Energy/Operation | Freq/Resp. Time     | Bit-Depth | Data Source | Implicit/Explicit | Justification                                      |
| :----------------- | :----------------------------- | :--------------- | :--------------- | :------------------ | :-------- | :---------- | :---------------- | :------------------------------------------------- |

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description          | Value                    | Units          | Source                    | Implicit/Explicit | Justification                             |
        | :----------------------------- | :-----------------------: | :------------- | :------------------------ | :---------------- | :---------------------------------------- |
        | Oscillation Period (est.)      | 5 - 10                   | s              | Ext. Data Fig. 8 / Text | Implicit          | Inverse of estimated frequency (0.1-0.2 Hz) |
        | Total Operational Run Time     | 4 - 8                    | minutes        | Main Text / Ext Data Fig 8 | Explicit          | Observed duration of operation            |
        | Auto-evacuation Time           | 4                        | days           | Methods                   | Explicit          | Required time for fabrication step      |
        | Material Restructuring Time    | < 200                    | s              | Ext. Data Fig. 3          | Explicit          | Measured recovery time after shear        |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip to Module 8.)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary, directly observable functional behavior is the autonomous, rhythmic, alternating inflation and deflation (actuation) of two sets of four limbs. This results in a periodic change in the robot's shape. While designed for locomotion, the paper focuses on demonstrating the autonomous actuation cycle itself rather than effective movement across a surface.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary emergent behavior (autonomous oscillation/alternating actuation) is validated through: 1. Direct observation and video recording (Supp. Videos 5, 6; Fig 4d). 2. Tracking of dyed fuel flow through the transparent/translucent body, confirming the switching action of the controller and flow to alternate limb sets (Fig 4c, Ext. Data Fig. 6, Ext. Data Fig. 8). 3. Characterization of actuator displacement vs. pressure (Fig 4b), confirming the mechanical response underlying the behavior. Reproducibility is implicitly suggested by the fabrication of nearly 300 units to converge on the design. Control experiments (e.g., disabling catalyst, altering oscillator) are not explicitly described for validating the emergence mechanism itself, though component characterization supports it. Limitations include lack of quantitative analysis of oscillation frequency/stability over time or across different units.

---

#Key: [sagawa_thermodynamics_2013]

# Thermodynamics of Information Processing in Small Systems

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system analyzed is theoretical, focusing on "small systems" where thermal fluctuations are significant. The work investigates the thermodynamics of information processing within these systems, specifically exploring the interplay between information theory, thermodynamics, measurement theory, and nonequilibrium statistical mechanics. Key aspects include generalizing the second law of thermodynamics in the presence of feedback control (Maxwell's demon paradigm), determining minimum energy costs for measurement and information erasure (Landauer's principle), and extending nonequilibrium relations like the Jarzynski equality to incorporate feedback. The purpose is to establish fundamental principles governing information processing in microscopic thermodynamic systems, with potential applications in nanotechnology and nanomachines. Components involve theoretical constructs like heat baths, microscopic objects, feedback controllers (demons), and memory/information registers.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Information Content | Variable | bits (implied) | Foreword, ToC (Ch 3, 7, 9) | Mixed | Medium | Inferred from context of information theory |
        | Temperature | T (Variable) | K (implied) | Foreword (thermodynamics), ToC (Ch 5) | Implicit | Medium | Inferred from context of thermodynamics |

    *   **Note:** Parameters are high-level concepts central to the thesis topic, derived from the title, forewords, and table of contents. Specific numerical values are not available in the excerpt. Units are standard units implied by the physical quantities. Reliability is Medium as specific contexts aren't detailed.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Energy input primarily comes from heat exchange with thermal reservoirs (heat baths), and potentially work done on the system during control protocols.
    *   Units: J (Joules)

### **2.2 Energy Transduction**

    *   Content: Energy is transduced between thermal energy (heat), mechanical energy (work), and the energy associated with information content. Processes include: (1) Heat from bath potentially converted to work via feedback control (Maxwell's demon). (2) Work performed on the system during measurement or control protocols. (3) Work required to erase information (Landauer's principle), typically dissipated as heat. (4) Energy changes associated with manipulating the state of the small system under feedback.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The thesis investigates the *limits* of energy conversion and the *minimum* energy costs, relating efficiency to information gain (mutual information) via generalized second laws and fluctuation theorems (Foreword, Ch 6, 7, 9). However, no specific efficiency values or scores for a particular process are provided in the excerpt. The focus is on theoretical bounds. Efficiency is discussed qualitatively in terms of extracting work potentially violating the naive second law using information.

### **2.4 Energy Dissipation**

    *   Content: Dissipation is inherent in the nonequilibrium processes studied, particularly information erasure (Landauer's principle implies a minimum dissipated heat of kT ln2 per bit erased, Ch 7) and potentially during measurement (Ch 7) and feedback control cycles operating away from quasi-static limits (Ch 8, 9). Heat transferred to baths is the primary form of dissipation. Quantification is central to the thesis (e.g., relating entropy production to information) but not detailed in the excerpt.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**

*   Units: bits (implied)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Skipping M4.2-M4.7 as M4.1 is "No")**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Other (Thermodynamic Information Processing)

### **5.3 Computational Primitive:**

    *   Content: The fundamental operations analyzed are:
        1.  **Measurement:** Acquiring information about the system state (e.g., position in Szilard engine). Thermodynamics cost analyzed (Ch 7.3). Includes classical (Ch 3.3) and quantum (Ch 4.2) measurement.
        2.  **Information Erasure/Reset:** Returning a memory register to a standard state (e.g., setting a bit to 0). Thermodynamic cost analyzed (Landauer's principle, Ch 2.4, Ch 7.2).
        3.  **Feedback Operation:** Using measurement information to perform a state-dependent action/protocol on the system (e.g., applying a force, changing a potential based on measured position). Analyzed in context of generalized second laws and fluctuation theorems (Ch 6, Ch 9).
    *   **Sub-Type (if applicable):** Measurement (Classical/Quantum/Projection/POVM), Erasure (Bit reset), Feedback (Conditional Protocol Application).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
    *   **Note:** The study involves dynamics (classical, quantum, stochastic, feedback) implying relevant timescales, but the excerpt does not quantify them.

### **6.2 Active Inference:**

    *   Content: Partial

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Partial

**(Conditional: If M7.1 is "Yes/Partial", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism is measurement-feedback control. The system's dynamics or the protocol applied to it are altered based on the outcome of a measurement performed on the system. The information gained from measurement dictates the subsequent action. Example: In a Szilard engine (Ch 2.2, 6.3, 9.4.1), measuring the particle's side determines which side a piston compresses or extracts work from. The adaptation is the selection of the appropriate control protocol branch conditional on the measurement outcome.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: Key behaviors analyzed include:
        1.  **Work Extraction using Information:** Demonstrating how feedback control, fueled by information, can seemingly allow work extraction exceeding limits suggested by the naive second law (Maxwell's demon, Szilard engine discussed in Ch 2, 6).
        2.  **Thermodynamically Consistent Information Processing:** Quantifying the minimum energy cost (work/dissipation) required for fundamental information processing tasks like measurement and erasure (Landauer's principle, Ch 7).
        3.  **Generalized Nonequilibrium Dynamics:** Characterizing the statistical behavior of small systems under feedback control using generalized fluctuation theorems (like Jarzynski equality) that incorporate information content (mutual information) (Ch 9).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The validation methods are primarily theoretical derivations and mathematical proofs within the frameworks of statistical mechanics, information theory, and quantum mechanics (Ch 3-9). The generalized Jarzynski equality with feedback was reportedly verified experimentally (Supervisor's Foreword), lending empirical support to the theoretical findings, although the details of the experiment are not in this excerpt but referenced in the thesis. The behaviors (e.g., generalized second law, minimum energy costs, fluctuation theorems) emerge mathematically from the underlying system dynamics combined with information processing steps.

---

#Key: [liu_van_2016]

# Van der Waals heterostructures and devices

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The review describes Van der Waals heterostructures (vdWHs), which are synthetic materials created by vertically stacking different two-dimensional layered materials (2DLMs) such as graphene, boron nitride (BN), and transition metal dichalcogenides (TMDs). Components include individual 2DLMs (metals, semiconductors, insulators) and the interfaces between them, primarily governed by van der Waals forces. The purpose is to create novel materials with engineered electronic and optical properties by combining disparate 2DLMs without lattice matching constraints. These vdWHs are used to fabricate various electronic and optoelectronic devices like tunnelling transistors, barristors, photodetectors, solar cells, and LEDs with potentially unique functionalities or enhanced performance (e.g., high mobility, tunable bandgaps, strong light-matter interaction, gate-tunable properties). The review covers synthesis/assembly methods (exfoliation/restacking, CVD), resultant properties (interlayer coupling, moiré patterns, band alignment), and device applications.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | BN Bandgap | Large (~6 eV, inferred) | eV | Section: Two-dimensional layered materials | Mixed | Medium | Value inferred from description as "large-bandgap insulator" and common knowledge. |
        | Interlayer Distance (vdW gap, typical) | ~0.33 | nm | Section: Electronic devices (discussion on Rc) | Implicit | Medium | Value mentioned in context of graphene/2DSC gap, assumed typical. |

    *   **Note:** Listed parameters represent typical or specific highlighted values for key components and properties discussed in the review.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: For the described *devices*, energy input is primarily electrical (voltage bias applied across terminals for transistors, diodes, LEDs) or optical (photons for photodetectors, photovoltaics).

### **2.2 Energy Transduction**

    *   Content: Energy transduction mechanisms are device-specific:
        *   **Transistors:** Electrical energy (gate field) modulates electrical energy flow (channel current) via field effect (changing carrier density/barrier height). Mechanisms include thermionic emission over barriers (vertical transistors, contacts) and tunnelling (contacts, vertical tunnelling transistors).
        *   **Photodetectors/Photovoltaics:** Optical energy (photons) is converted to electrical energy (photocurrent/voltage) via photocarrier generation (electron-hole pairs), separation (driven by built-in or applied fields, band offsets), and transport/collection at electrodes. Interlayer exciton formation and dissociation are key steps in some vdWHs.
        *   **LEDs:** Electrical energy (injected electrons and holes) is converted to optical energy (photons) via radiative recombination of charge carriers (or excitons/trions) within the 2DLM layers. Tunnelling injection through barriers (e.g., BN) is also employed.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The review mentions high performance (e.g., high mobility, high frequency for transistors, high quantum efficiency for LEDs/photodetectors) for *specific* optimized devices, but does not provide a general efficiency assessment across all vdWH systems. Efficiency is highly dependent on the specific materials, structure, and device type. For example:
        *   LED Quantum Efficiency: Mentioned as ~10% for specific quantum well vdWH LEDs (Ref 16, 172), comparable to organic LEDs.
        *   Photodetector Internal Quantum Efficiency: Mentioned as >70% for specific thin WSe2 devices (Ref 169).
        *   Transistor efficiency is typically discussed via metrics like on/off ratio, transconductance, or speed (cutoff frequency), not power efficiency directly.
        A single score is not meaningful for such a broad class of materials/devices.

### **2.4 Energy Dissipation**

    *   Content: Dissipation mechanisms are implied but not generally quantified.
        *   **Joule Heating:** Resistive losses in contacts (Rc mentioned as a key challenge, Fig 4e) and channels contribute to heating.
        *   **Non-radiative Recombination:** In optoelectronic devices (LEDs, photodetectors), electron-hole recombination via defects or phonon emission dissipates energy as heat instead of light. Implicit in quantum efficiencies < 100%.
        *   **Scattering:** Charge carriers scattering off phonons, impurities, defects, and interfaces dissipates energy and limits mobility. Mentioned as substrate effects reduced by BN.
        *   **Leakage Currents:** Tunnelling through thin dielectrics (e.g., BN barriers) or leakage in reverse-biased diodes represents energy loss. Mentioned as an issue for thin insulators and vertical LEDs.
    *   Qualitative Assessment: Medium to High, depending on device quality, materials, and operating conditions. Contact resistance is highlighted as a major performance limitation (implying significant dissipation). Leakage is mentioned as a challenge for LEDs.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | MoS₂/WSe₂ Interlayer Hole Transfer Time | ~50 | fs | Section: Electronic and optoelectronic properties (Ref 87) | Explicit | Value cited explicitly. |
        | Graphene/WSe₂ Photodiode Response Time (thin WSe₂) | Picoseconds | ps | Section: Light-harvesting and detection devices (Ref 169) | Explicit | "Picosecond photoresponse" stated explicitly. |
        | Graphene Photodetector Intrinsic Response Time | Sub-picosecond (<1 ps) | ps | Section: Light-harvesting and detection devices (Ref 161 implicitly related) | Implicit | While Ref 161 is cited regarding response time, the sub-ps value is general knowledge for graphene photodetectors contextually implied. |
        | Few-layer MoS₂ Transistor Cut-off Frequency (fT) | 42 | GHz | Section: Electronic devices (Ref 91) | Explicit | Value cited explicitly. |
        | Few-layer MoS₂ Transistor Max Oscillation Frequency (fmax) | 50 | GHz | Section: Electronic devices (Ref 91) | Explicit | Value cited explicitly. |
        | Graphene-P3HT Vertical Transistor Frequency Response | Megahertz | MHz | Section: Electronic devices (Ref 129) | Explicit | Value cited explicitly. |
    *   **Note:** Timescales listed are primarily related to carrier dynamics and device operational speeds mentioned in the review.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors arise from combining different 2DLMs:
        *   **Enhanced Electronic Transport:** High carrier mobility in graphene/TMDs when placed on or encapsulated by BN (reduced scattering).
        *   **Tunable Electronic Properties:** Gate-tunable band alignment, work function (graphene contacts), bandgaps (bilayer graphene electric field, black phosphorus layers), Schottky barriers (graphene contacts). Interlayer coupling modifies electronic/optical spectra (Raman, PL shifts). Moiré superlattices induce new Dirac points and modify transport.
        *   **Novel Device Functions:** Tunnelling transport (through BN, thin TMDs), field-effect switching (planar and vertical FETs), barrier modulation (barristors), rectification (p-n junctions), photocurrent generation (photodiodes), electroluminescence (LEDs).
        *   **Specific Optoelectronic Phenomena:** Strong light-matter interaction, interlayer excitons/trions, gate-tunable photoresponse/emission.

### **8.2 Behavior Robustness:**

        *   **Environmental Stability:** Many 2DLMs (e.g., black phosphorus, perovskites) are air-sensitive. Encapsulation (e.g., with BN) is explicitly mentioned as a strategy to improve stability and performance under ambient conditions (Refs 23, 76, 77).
        *   **Contact Stability/Performance:** Contact resistance (Rc) is a major challenge, implying variability and potential degradation (Fig 4e). Metal deposition can damage 2DLMs (Ref 92). VdW contacts offer cleaner interfaces but may have higher Rc. Hybrid contacts aim to balance stability and performance. Doping for low Rc can have stability issues (Ref 115).
        *   **Fabrication Scalability/Reproducibility:** Exfoliation/stacking is inherently not scalable and has variability. CVD offers scalability but quality/uniformity can be challenging, especially for complex heterostructures.
        *   **Operational Robustness:** Vertical transistors made with brittle oxides are shown to be mechanically robust due to geometry (Ref 124).
        Overall, while specific strategies (encapsulation, contact engineering) improve robustness, inherent material stability, contact issues, and fabrication challenges limit the general robustness, leading to a moderate score. Quantitative robustness metrics are generally lacking in the review.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The review validates claims primarily through citing experimental results from referenced primary literature. Validation methods implied include:
         *   **Electrical Characterization:** Measuring I-V curves, transfer characteristics (Ids-Vg), mobility (field-effect, Hall), contact resistance (TLM), frequency response (fT, fmax) to demonstrate transistor behavior, rectification, barrier tuning. (Figs 4, 5).
         *   **Optical Spectroscopy:** Raman and Photoluminescence (PL) spectroscopy show shifts and new peaks indicating interlayer coupling and modified electronic structure (Fig 3). Absorption/emission spectra validate optoelectronic function.
         *   **Microscopy:** STM/STEM provides atomic-scale visualization of interfaces and moiré patterns (Fig 2b, c, d). Scanning photocurrent mapping visualizes spatial photoresponse (Fig 7d).
         *   **Device Performance Metrics:** Quantifying on/off ratios, gain, quantum efficiency, responsivity, emission brightness validates device function.
     * Reproducibility is implied by multiple groups reporting similar phenomena, but variability (especially related to fabrication) is a known challenge in the field, acknowledged in the review (e.g., regarding scalability and contact resistance). Robustness is validated in specific cases like encapsulated devices performing well in air.
     * Limitations: As a review, it relies on the validation presented in the cited primary sources. It doesn't perform new control experiments.

---

#Key: [rothemund_folding_2006]

# Folding DNA to create nanoscale shapes and patterns

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is "scaffolded DNA origami," a method for creating arbitrary two-dimensional nanoscale shapes and patterned structures. It utilizes a long, single-stranded DNA molecule (the scaffold, typically M13mp18 viral DNA, ~7 kilobases) and numerous short synthetic oligonucleotide "staple strands" (~200-250 staples, typically 16-49 mers). The purpose is bottom-up nanofabrication. The system works through self-assembly: the scaffold and staple strands are mixed and annealed (heated and slowly cooled). The staples bind to specific complementary regions on the scaffold, folding it into a pre-designed shape based on the arrangement of DNA double helices and crossovers. The method allows for programming complex patterns (like pixels) onto the surface by selectively using labelled staples. Structures are typically ~100 nm in diameter with ~6 nm spatial resolution. Individual structures can be programmed to assemble into larger lattices or defined complexes.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name         | Value                      | Units   | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :--------------------- | :-------------------------: | :------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Inter-helix Gap        | ~1 (for 1.5-turn spacing)  | nm      | Sect: Design, Sect: Folding M13 | Explicit (Inferred from measurements) | Medium | Derived from AFM aspect ratios and consistency |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input is thermal energy applied during the annealing process. This involves heating the mixture of scaffold and staple strands and then slowly cooling it.
    *   Value: 95 down to 20
    *   Units: °C (temperature range)

### **2.2 Energy Transduction**

    *   Content: Thermal energy provides the activation energy needed for DNA strands to overcome kinetic barriers, explore different conformations, and find their thermodynamically favorable state. Specifically, high temperatures denature any initial secondary structures and allow staples to bind to the scaffold. As the temperature slowly decreases, Watson-Crick base pairing between staples and the scaffold occurs, releasing energy (enthalpy of hybridization) and increasing entropy (due to counter-ion release), driving the system towards the minimum free energy state corresponding to the folded origami structure. The potential energy stored in the specific base pairings dictates the final folded shape.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide information to quantify the thermodynamic efficiency of the self-assembly process (e.g., comparing the free energy change of folding to the total thermal energy input/output during annealing). While the process leads to high yields of well-formed structures under optimal conditions (implying thermodynamic favorability), efficiency metrics are not discussed. A qualitative assessment would be speculative.

### **2.4 Energy Dissipation**

    *   Content: The primary energy dissipation mechanism is heat transfer to the surrounding environment (thermal bath/solvent) during the cooling phase of annealing. Energy released during the exothermic formation of base pairs is dissipated as heat. Viscous dissipation within the solvent as strands move and reconfigure also occurs but is likely negligible compared to heat transfer. The paper does not quantify these aspects. Qualitative assessment: High (relative to the energy stored in the final structure, as the annealing process involves significant heating and cooling of the bulk solution).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rule is Watson-Crick base pairing (A with T, G with C) between the single-stranded DNA scaffold and the complementary sequences on the short staple strands. Staples are designed to bind specific segments of the scaffold. A secondary rule involves the formation of antiparallel crossovers, where strands switch between adjacent helices at specific points determined by the design, stabilized by base stacking interactions. The design aims to minimize twist strain, effectively encoding preferred relative orientations and distances between helices based on the number of base pairs between crossovers (e.g., 16 bp for 1.5 turns). Staples hybridize to the scaffold, effectively crosslinking different parts of the scaffold strand according to the pre-defined folding path. Merging staples creates longer binding domains, increasing binding energy and specificity.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID                    | Description                      | Parameter Name      | Parameter Value Range | Units    | Data Source       | Implicit/Explicit | Justification                                           |
    | :------------------------- | :------------------------------- | :------------------ | :-------------------- | :------- | :---------------- | :---------------- | :------------------------------------------------------ |
    | Watson-Crick Pairing       | Binding between staple & scaffold | Binding Energy      | Sequence-dependent    | kJ/mol   | Implicit          | Implicit          | Standard DNA thermodynamics, not quantified per staple. |
    | Watson-Crick Pairing       | Binding between staple & scaffold | Staple Length       | 16-49 (examples)      | nt       | Fig 1c, 1e, Sec Design | Explicit          | Minimum/maximum not strictly defined, but examples given. |
    | Crossover Formation        | Inter-helix connections          | Crossover Spacing   | 1.5, 2.5 (examples)   | turns    | Fig 1a, Sec Design | Explicit          | Specific values used in designs shown.                   |
    | Crossover Formation        | Inter-helix connections          | Bases between Xovers | 16 (for 1.5 turns)  | bp       | Sect: Design      | Explicit          | Example value given.                                    |
    | Staple Design Constraint   | Binding domains                  | Merged Staple Length| 32 (example)          | nt       | Fig 1e, Sec Folding | Explicit          | Example value given for merged staples.                |

### **4.3 Global Order:**

    *   Content: The emergent global order consists of specific, predefined two-dimensional shapes (e.g., square, rectangle, star, disk with holes, triangle) formed by the folded DNA scaffold held together by staples. This includes structures with controlled surface patterns (pixels created by labelled staples, forming words or images) and higher-order assemblies like periodic lattices or finite complexes (hexamer of triangles) formed by programmed interactions between individual origami structures. The order is characterized by the specific arrangement of parallel DNA helices and the lattice of crossovers.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID             | Description                            | Parameter         | Value Range              | Units   | Implicit/Explicit | Justification                                     | Source                               |
| :------------------ | :------------------------------------- | :---------------- | :----------------------- | :------ | :---------------- | :------------------------------------------------ | :----------------------------------- |
| Hybridization       | Staple binding to scaffold             | Sequence          | Defined by design        | String  | Explicit          | Staple sequences are designed for specific binding | Abstract, Sect Design, Figs 1b, 1c |
| Crossover Stability | Connection between helices           | Spacing (turns)   | 1.5, 2.5 (examples)      | Turns   | Explicit          | Controls inter-helix distance and stability     | Sect Design, Sect Folding, Fig 2     |
| Crossover Stability | Connection between helices           | Bases btw Xovers  | 16 (for 1.5 turns)       | bp      | Explicit          | Balances twist strain                           | Sect Design                          |
| Binding Specificity | Correct staple binding               | Staple Length     | 16-49 (examples)         | nt      | Explicit          | Longer staples generally increase specificity     | Fig 1c, 1e, Sect Design              |
| Assembly Trigger    | Initiation of folding process          | Temperature       | 95 -> 20                 | °C      | Explicit          | Annealing profile drives assembly               | Sect Folding M13mp18                 |
| Stoichiometry       | Relative concentration of components | Staple:Scaffold Ratio | 1.5:1 to 100:1 (tested) | Unitless| Explicit          | Affects yield and defect rate                 | Sect Folding M13mp18                 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID     | Description                      | Parameter         | Value Range        | Units    | Implicit/Explicit | Justification                                         | Protocol   | Source                |
| :-------------- | :------------------------------- | :---------------- | :----------------- | :------- | :---------------- | :---------------------------------------------------- | :--------- | :-------------------- |
| Shape Fidelity  | Conformity to designed shape     | % Well-formed     | 11 - 91            | %        | Explicit          | Percentage of observed structures matching design     | AFM        | Sect Folding M13mp18  |
| Structure Size  | Overall dimensions               | Diameter/Width    | ~100 / 33-94       | nm       | Explicit          | Measured size of final structures                   | AFM        | Abstract, Fig 1a, 2a  |
| Pattern Fidelity| Correctness of pixel pattern     | % Visualized Pixels | ~94                | %        | Explicit          | Percentage of '1' pixels correctly appearing in AFM | AFM        | Sect Patterning       |
| Pixel Size      | Dimensions of pattern elements   | Width x Height    | ~5.4 x 6 (theory)  | nm       | Explicit          | Expected pixel dimensions                           | Sect Patterning       |
| Pixel Spacing   | Distance between pattern elements| 2 Widths / 1 Height | 11.5 / 6.6         | nm       | Explicit          | Measured distances between '1' pixels               | AFM        | Sect Patterning       |
| Complex Stoich. | Correct assembly of multi-units  | Hexamer Yield     | ~2                 | %        | Explicit          | Yield of specific 6-triangle complex                | AFM        | Sect Patterning       |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type           | Description                                  | Predictability | Yoneda Score | Metrics                                                                 | Implicit/Explicit | Justification                                                                                                                                     | Source       |
    | :------------------ | :------------------------------------------- | :------------- | :----------- | :---------------------------------------------------------------------- | :---------------- | :------------------------------------------------------------------------------------------------------------------------------------------------ | :----------- |
    *   **Metrics:** Yield percentage, structural fidelity comparison (AFM vs. design), missing pixel rate, feature size/spacing accuracy.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description   | Value     | Units   | Source                 | Implicit/Explicit | Justification                                         |
        | :---------------------- | :-------- | :------ | :--------------------- | :---------------- | :---------------------------------------------------- |
        | Annealing Duration      | ~2        | hours   | Sect: Folding M13mp18  | Explicit          | Explicitly stated duration for the cooling phase.    |
        | AFM Imaging Time        | "two days per structure" | days | Sect: Discussion | Explicit | Time stated for acquiring high-resolution images. |
        | Design Time (Structure) | ~1 week   | week    | Sect: Discussion       | Explicit          | Author's estimate for designing a new structure.      |
        | Design Time (Program)   | 3 months  | months  | Sect: Discussion       | Explicit          | Time spent developing the design software.            |
        | Synthesis Time          | ~1 week   | week    | Sect: Discussion       | Explicit          | Time for commercial oligonucleotide synthesis.         |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is programmed self-assembly into specific nanoscale shapes (squares, stars, disks, triangles) with optional surface patterns (pixels representing images/words). A secondary behavior is the programmed or spontaneous assembly of these individual units into larger structures (chains via stacking, designed lattices, finite complexes like hexamers). Unintended behaviors like specific deformations (hourglass shape) or fragmentation are also observed.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary validation method for the emergent shapes and patterns is Atomic Force Microscopy (AFM) imaging (Figs 2, 3). This directly visualizes the resulting nanostructures, allowing comparison with the intended designs in terms of shape, dimensions, crossover patterns, and pixel patterns. Quantitative analysis includes measuring yields (% well-formed structures), aspect ratios, inter-pixel distances, and defect rates (% missing pixels). Control experiments related to robustness include varying staple stoichiometry and observing the effect on yield/structure integrity. Reproducibility is implied by the presentation of multiple examples and quantitative yield data. Limitations include potential AFM artifacts (stretching, tip-induced damage creating holes) that can complicate defect analysis.

---

#Key: [kiefer_psychophysical_2020]

# Psychophysical identity and free energy

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper describes a theoretical framework proposing an identity between thermodynamic free energy (TFE) A and variational free energy (VFE) F in biological systems, particularly the brain. It explores the implications of VFE minimization (as outlined by the Free Energy Principle - FEP) being implemented directly through TFE minimization. The system *is* the biological entity (e.g., brain) performing variational Bayesian inference. Its components are neurons and their dynamics. Its purpose is to model perception, action, and learning as processes that minimize the discrepancy between an internal generative model and sensory inputs, arguing this minimization corresponds to a physical energy minimization process. This requires a specific type of neuronal encoding, suggested to be a stochastic population code. It connects this idea to mind-brain identity theses and Gestalt psychophysical isomorphism.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Temperature (T) | Assumed T=1 for VFE-TFE analogy; biological range for actual TFE | Kelvin (K) or Dimensionless (for analogy) | Sec 2.1, Sec 2.3 | Mixed | Medium (Analogy value explicit, biological value implicit) | Value T=1 used explicitly for formal analogy, biological T is implicit context. |

    *   **Note:** The paper is theoretical. Parameters listed are key concepts defined mathematically. VFE has units related to information (like nats or bits, though often treated as dimensionless energy in analogies), while TFE has physical energy units. The T=1 assumption is specific to simplifying the VFE-TFE *formal analogy*.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: For the biological system (e.g., brain) considered, the ultimate energy input is metabolic (e.g., glucose, ATP). The paper also discusses sensory inputs perturbing the system from steady state, which can be seen as informational "energy" in the VFE context or physical energy perturbing the TFE state. The paper focuses more on the internal energy dynamics related to VFE/TFE minimization rather than the primary metabolic source.

### **2.2 Energy Transduction**

    *   Content: The core thesis involves the transduction between information processing (VFE minimization) and physical processes (TFE minimization). Sensory input (physical energy/information) perturbs the system. This perturbation increases VFE. The system then acts (physically, requiring metabolic energy) or updates its internal model (physically, requiring metabolic energy for neuronal activity) to minimize VFE. The paper argues this VFE minimization *is* a process of TFE minimization (§3.2, §4.1), implying a transduction where informational "potential" (surprise, prediction error) is converted into physical work/state change driven by thermodynamic potentials. The paper discusses how internal energy U (potential and kinetic energy of neuronal states) and entropy S contribute to TFE (A = U - TS) and how these relate to VFE components (expected energy under Q and entropy of Q) (§2.1, Eq 2.3, §2.2, §2.3). Specifically, it relates the energy term E(v,z) = -log P(v,z) in VFE to the physical potential energy driving neuronal dynamics under the identity thesis.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper argues that minimizing VFE is computationally/statistically efficient and proposes this corresponds to thermodynamic efficiency ("statistical and thermodynamic efficiency go hand-in-hand", §3.2). It mentions minimizing energy expenditure as a goal for material computation in the context of the background field (§2.3 discussion on Helmholtz free energy, §3.2 discussion on belief updates). However, no quantitative measure of the efficiency of the proposed brain mechanism is provided. The link to metabolic efficiency is mentioned via reference [45].

### **2.4 Energy Dissipation**

    *   Content: The paper mentions heat dissipation implicitly in the context of thermodynamic free energy (A = U - TS, where TS relates to heat) and non-equilibrium steady states (§2.1, §2.3, §2.4). It references Parr et al. [18] regarding the relationship between heat dissipated and surprisal/changes in free energy (§3.2, footnote 16). Maintaining non-equilibrium steady state requires ongoing energy input to counter dissipation to the environment (§2.4). However, specific mechanisms or quantitative values for energy dissipation in the brain related to VFE minimization are not provided within this paper.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding with M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term
*    Units: Qualitative Descriptor

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", proceeding with M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The paper doesn't specify the *exact* local interaction rules but alludes to several possibilities based on the models discussed:
        1.  **Hopfield-like Dynamics:** Neurons update state based on weighted inputs from connected neurons to minimize a global energy function (§2.2). Rule: Update state `s_i` based on `sum(w_ij * s_j)`.
        2.  **Predictive Coding:** Neurons representing predictions interact with neurons representing prediction errors based on top-down (generative) and bottom-up (recognition) connections (§3.1, citing [5, 25]). Rules involve passing prediction errors up and predictions down, adjusting states to minimize error (VFE).
        3.  **Stochastic Population Code Dynamics:** Probability of neuronal firing related to membrane potential via sigmoid function (§2.2, citing [31]), potentially following Boltzmann distribution (Eq 2.4) under specific conditions (§2.2, §3.1). Interaction rules would involve synaptic weighting and integration leading to stochastic firing.
        4.  **Wake-Sleep Algorithm Dynamics:** Units' activities driven by recognition weights (bottom-up) or generative weights (top-down) depending on algorithm phase (§3.1).
        The paper argues for a stochastic encoding (§3.1) where the *thermodynamic* interactions (leading to TFE minimization) *are* the *computational* interactions (VFE minimization).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID                  | Description                               | Parameter Name          | Parameter Value Range | Units          | Data Source                             | Implicit/Explicit | Justification                                           |
    | :----------------------- | :---------------------------------------- | :---------------------- | :-------------------- | :------------- | :-------------------------------------- | :---------------- | :------------------------------------------------------ |

### **4.3 Global Order:**

    *   Content: The emergent global order discussed is the formation of an effective internal generative model of the environment, encoded in the system's structure (e.g., synaptic weights) and dynamics. This allows the system to maintain non-equilibrium steady state (homeostasis) and perform accurate perception and action by minimizing VFE (§1, §2.4). Another form of order is the system settling into low-energy states corresponding to coherent interpretations or memories (as in Hopfield nets, §2.2). The paper also mentions the system approaches non-equilibrium steady state via TFE minimization ([9], §1).
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID                  | Description                               | Parameter      | Value Range | Units        | Implicit/Explicit | Justification                                           | Source                             |
| :----------------------- | :---------------------------------------- | :------------- | :---------- | :----------- | :---------------- | :------------------------------------------------------ | :--------------------------------- |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID        | Description                                       | Parameter           | Value Range | Units      | Implicit/Explicit | Justification                                                                    | Protocol                       | Source                 |
| :----------------- | :------------------------------------------------ | :------------------ | :---------- | :--------- | :---------------- | :------------------------------------------------------------------------------- | :----------------------------- | :--------------------- |
| VFE                | Variational Free Energy                           | F or VFE            | Minimized   | Nats (Info) | Explicit          | Central quantity the system aims to minimize.                                    | Bayesian Inference             | Eq 2.1, 2.3, 3.2       |
| TFE                | Thermodynamic Free Energy                         | A                   | Minimized   | Joules     | Explicit          | Proposed to be identical to or tracked by VFE.                                   | Statistical Mechanics          | §2.1, §4.1             |
| KL Divergence      | Difference between Recognition and Generative Post. | D<sub>KL</sub>(Q\|\|P) | Minimized   | Nats (Info) | Explicit          | Minimized when VFE is maximized (holding evidence fixed).                        | Variational Inference          | Eq 2.2                 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type                      | Description                                                                     | Predictability   | Yoneda Score | Metrics                                    | Implicit/Explicit | Justification                                                                                               | Source   |
    | :----------------------------- | :------------------------------------------------------------------------------ | :--------------- | :----------- | :----------------------------------------- | :---------------- | :---------------------------------------------------------------------------------------------------------- | :------- |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", proceeding with M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Neuromorphic / Stochastic / Analog / Bayesian Inference Engine

### **5.3 Computational Primitive:**

    *   Content: The fundamental computation is **Variational Bayesian Inference**, aiming to approximate a posterior distribution P(z|v) by optimizing an approximate distribution Q(z|v) to minimize VFE (or maximize negative VFE, F). This involves calculating expectations under Q, evaluating probabilities under P, calculating entropy of Q (see Eq 2.1, 2.3, 3.2). At a lower level, this relies on neuronal operations like:
        *   **Weighted Summation:** Integrating synaptic inputs (implied by neuronal models like Hopfield, §2.2).
        *   **Non-linear Activation:** Applying a sigmoid function or similar to determine firing probability (§2.2).
        *   **Stochastic Sampling:** Generating activity according to a probability distribution (Boltzmann or otherwise, §3.1).
        *   **Error Calculation:** Comparing predictions and actual inputs (in predictive coding context, §3.1).
    *   **Sub-Type (if applicable):** Bayesian Inference (Variational Approximation)

### **5.4 Embodied Computational Units**
| Unit ID | Description                                   | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification                             |
| :------ | :-------------------------------------------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :---------------------------------------- |

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description           | Value                  | Units               | Source                | Implicit/Explicit | Justification                                                                 |
        | :------------------------------ | :--------------------- | :------------------ | :-------------------- | :---------------- | :---------------------------------------------------------------------------- |
        | Ontogenetic/Phylogenetic        | Lifetime / Generations | Years / Generations | §2.4                  | Explicit          | Explicitly mentioned timescales for VFE minimization.                         |
    *   **Note:** The paper discusses processes occurring across a vast range of timescales, from fast neural dynamics to evolutionary adaptation, but provides no specific values.

### **6.2 Active Inference:**

    *   Content: Yes
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:**
        *   **Prediction Error Reduction Rate:** (`MetricNode` attribute) Quantify the rate of change of VFE or KL divergence (Eq 2.2) over time during a task. CT-GIN: Track `VFE` attribute of `SystemStateNode` over `TemporalEvolutionEdge`.
        *   **Model Update Magnitude:** (`MetricNode` attribute) Measure the change in parameters (θ) of the `GenerativeModelNode` per unit time or per prediction error signal. CT-GIN: Track attributes of `GenerativeModelNode` linked via `AdaptationEdge`.
        *   **Action Selection ProbabilityBias:** (`MetricNode` attribute) Measure the probability of selecting actions that lead to sensory inputs predicted by the generative model (high P(v|h)). CT-GIN: Analyze transition probabilities between `ActionNode`s conditional on `SystemStateNode` (representing expected VFE).
        *   **Anticipation Timescale:** (`MetricNode` attribute) Measure how far into the future the system's actions seem to be planned based on minimizing predicted VFE. CT-GIN: Analyze sequences of `ActionNode` selections using temporal graph algorithms.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", proceeding with M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The primary mechanism described is **Variational Free Energy (VFE) Minimization** via gradient descent (§1). The system adjusts its internal parameters (θ, representing synaptic weights or similar in the brain) to minimize the difference between its predictions (from the generative model P) and the actual sensory data (represented via the recognition density Q). This minimizes KL divergence (Eq 2.2) and maximizes model evidence (or a lower bound on it). Specific algorithms mentioned as potential implementations include:
        *   **Wake-Sleep Algorithm:** Alternating phases update generative and recognition models (§3.1).
        *   **Predictive Coding:** Hierarchical message passing minimizes prediction error (§3.1).
        *   **Expectation-Maximization (E-M):** Iterative estimation of hidden states (E-step, inference via Q) and model parameters (M-step, updating P) (§2.4 footnote, §3.2).
        The paper argues this process is physically realized by the system minimizing its thermodynamic free energy (§3.2). The learning rule adjusts parameters θ to minimize VFE, effectively performing Bayesian inference/learning. E.g., `Δθ ∝ -∂VFE/∂θ`.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behaviors resulting from the described framework (FEP/VFE minimization) are:
        1.  **Perception:** Inferring the hidden causes (H or z) of sensory data (v) by optimizing the recognition density Q(z|v) (§1, §2.1).
        2.  **Learning:** Updating the parameters (θ) of the internal generative model P(v,z) to better account for experienced sensory data (§1, §2.1).
        3.  **Action:** Interacting with the environment to selectively sample sensory data that confirms the generative model's predictions (i.e., minimize prediction error/VFE) (§1, §2.1, footnote 14).
        4.  **Homeostasis / Self-Maintenance:** Maintaining the organism's physiological states within viable bounds (phenotypic states), framed as staying close to non-equilibrium steady state by minimizing surprisal/VFE (§1, §2.4).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

---

#Key: [esplandiu_radial_2022]

# From radial to unidirectional water pumping in zeta-potential modulated Nafion nanostructures

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a self-driven micropump based on the cation-exchanger polymer Nafion. It utilizes the ion-exchange capability of Nafion when immersed in salt solutions to generate chemical gradients and local electric fields, which in turn trigger interfacial electroosmotic flows. The purpose is to pump fluids autonomously, fueled by salt gradients. Components include patterned Nafion structures (discs or strips), deactivated Nafion regions (modified by e-beam lithography), and potentially adjacent strips of materials with different zeta potentials (e.g., Al2O3, SiO2) fabricated on a substrate (e.g., Si wafer, polycarbonate). The system can be configured to produce either radial fluid flow (using Nafion discs) or unidirectional flow (using arrays of alternating strips: deactivated Nafion/Nafion/Al2O3 or SiO2/Nafion/Al2O3). The pump operates using various salts (e.g., LiCl, NaCl, CdCl2) as fuel, including contaminant ions like Cd2+, suggesting potential for water remediation.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Max Radial Velocity (Initial, 1e-4 M LiCl) | ~30 | µm/s | Fig 1d,e / Results | Explicit | Medium | Averaged particle tracking |
        | Avg Unidirectional Velocity | >2 - 3 | µm/s | Fig 3c, 4c / Results | Explicit | Medium | Averaged particle tracking |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the chemical potential difference associated with the concentration gradient of ions established during the ion-exchange process between the protonated Nafion and the surrounding salt solution. The salt itself acts as the fuel.

### **2.2 Energy Transduction**

    *   Content: 1. Chemical potential energy stored in ion gradients is released via ion exchange (H+ out, Cation+ in). 2. Due to unequal diffusion coefficients of the exchanged ions (e.g., H+ vs Li+), a self-generated electric field (E) is established near the Nafion interface (Eq. 2). This transduces chemical gradient energy into electrical potential energy. 3. The tangential component of this electric field acts on mobile counterions in the electrical double layer adjacent to the charged surfaces (Nafion, deactivated Nafion, Al2O3, SiO2). 4. This electrophoretic motion of ions drags the surrounding fluid via viscous coupling, resulting in electroosmotic flow (kinetic energy). The main transduction pathway is Chemical Gradient -> Electric Field -> Fluid Kinetic Energy.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any quantitative measure of energy efficiency (e.g., chemical energy input vs. useful fluid pumping work). Such self-powered micro/nanoscale systems based on phoretic effects are generally known to have very low thermodynamic efficiencies, often converting only a tiny fraction of the available chemical energy into directed motion due to dominant dissipative processes like diffusion and viscous drag. The score reflects this general understanding (Low efficiency).

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include: 1. Irreversible ion diffusion down concentration gradients. 2. Viscous drag within the fluid as it flows (conversion of kinetic energy to heat). 3. Joule heating associated with ion currents driven by the self-generated electric field (likely minor). 4. Energy associated with the ion exchange process itself (enthalpy/entropy changes). Quantification is not provided. Qualitative Assessment: High, dominated by diffusion and viscous effects intrinsic to microscale fluid dynamics and phoretic phenomena.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: ~45 minutes (operational timescale)
*    Units: minutes

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 2.4 x 10⁻³
*   Units: mol H+/m² (or ~ 4 x 10³ mol H+/m³)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Variable (Qualitative: High initially, decreases over time)
    *   Units: µm/s per minute (or related units)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Initial Pumping Response | < seconds | s | Implicit | Implicit | Flow observed quickly after immersion (movies referenced). |
        | Velocity Decay (Memory) | 10s - 1000s | s | Fig 1e, Results | Explicit | Time range over which velocity significantly decreases. |
        | Operational Lifetime | > 45 | min | Results | Explicit | Duration pump remains active before exhaustion. |
        | Regeneration Time | 6 | h | Supp Fig 2b / Results | Explicit | Time required to reset the protonated state. |
        | Ion Diffusion/Exchange | Microseconds - Seconds | µs - s | Implicit | Implicit | Characteristic time for ions to diffuse across boundary layers and exchange. Governs initial response and local field generation. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors are fluid pumping driven by self-generated electroosmotic flows. Depending on the geometric configuration of the Nafion and surrounding materials, two main flow patterns emerge: 1. **Radial Pumping:** Fluid flows radially inward towards a central Nafion disc near the surface, then moves upward and outward, forming convection rolls. 2. **Unidirectional Pumping:** Fluid flows predominantly in one direction along an array of patterned strips with alternating zeta potentials (e.g., negative/Nafion/positive), where the asymmetric zeta potential landscape rectifies the local flows generated by the Nafion strips. A secondary behavior is the selective capture of cations (e.g., Cd2+) from the solution, which fuels the pumping.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent flow behavior (radial and unidirectional pumping) are validated through: 1. **Experimental Observation:** Direct visualization and tracking of fluorescent polystyrene tracer particles using optical microscopy. Particle trajectories and velocities are quantified (Figs 1b-e, 3b-c, 4b-c, Supp Movies). 2. **Numerical Simulations:** Finite element simulations modeling the ion exchange, Nernst-Planck ion transport, electric field generation (Eq 2), and resulting electroosmotic fluid flow (Eq 3, Supp Figs 3-7, Fig 5) qualitatively reproduce the observed flow patterns (convection rolls for radial, net directional flow for strips) and support the proposed mechanism based on ion exchange and zeta potential modulation. Control experiments (absence of salt showing only Brownian motion, Supp Movie 7) confirm the salt-driven nature. Reproducibility is implied by averaging over multiple particles/trajectories and showing standard deviations.

---

#Key: [zhang_logic_2022]

# Logic operations with active topological defects

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system utilizes topological defects (+1/2 defects) in active nematic liquid crystals (specifically modeled based on actin-based 2D nematics) as information carriers. By spatially patterning the activity (extensile active stress, α) using simulated light-sensitive motors, the formation, self-propulsion, and trajectories of these defects are controlled. The components are the active nematic liquid crystal (described by Q-tensor and velocity field u), surfaces imposing anchoring conditions (e.g., normal/homeotropic), and predefined spatial patterns of high and low activity (`α(r)`). The purpose is to demonstrate that these controlled defect dynamics can be used to perform logic operations analogous to electronic circuits, such as gating, tunneling, and amplification, enabling computation within the active soft material itself.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Elastic Constant (L) | 0.1 (Sim Units); ~1 (pN) | Simulation Units; pN | Methods; Results | Explicit | High (Sim Input); Medium (Mapping) | Mapping based on typical LC values |
        | Viscosity (η) | 1/3 (Sim Units); ~0.1 (Pa·s) | Simulation Units; Pa·s | Methods; Results | Explicit | High (Sim Input); Medium (Mapping) | Mapping based on typical LC values |
        | Channel Width | 50 - 80 | Simulation Units; ~µm | Fig 1, 3, 4, 5 Captions; Results | Explicit | High (Simulation Geometry) | Mapping based on ξ ≈ 1 µm |
        | Retention Time (Memory) | ~1106 | Simulation Time Units (τ); ~619 (s) | Results; Fig 3 Discussion | Explicit | Medium (Derived from Simulation) | Time for -1/2 defect passage |

    *   **Note:** Simulation units are dimensionless based on the chosen characteristic scales. Mapping to SI units depends on the assumed characteristic length (ξ ≈ 1 µm) and time (τ ≈ 0.56 s).

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the chemical energy (e.g., ATP hydrolysis for actin/myosin systems) that drives the activity of the nematic constituents, modeled phenomenologically as an active stress `Πa = -αQ`. This stress term injects energy locally into the system, converting chemical energy into mechanical work (flow and deformation).
    *   Value: Not explicitly quantified in Joules or Watts. Represented by the activity parameter `α`.
    *   Units: Simulation units for `α` (related to stress/energy density).

### **2.2 Energy Transduction**

    *   Content: Chemical energy is transduced into mechanical energy. The active stress (`Πa = -αQ`) generated by the constituents (e.g., motor proteins walking on filaments) exerts forces on the liquid crystal fluid, inducing hydrodynamic flows (velocity field `u`) and changes in the nematic order (Q-tensor). This mechanical energy drives the self-propulsion of +1/2 defects and generates collective dynamics.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify energy efficiency. Biological active matter systems like actin/myosin are known to be highly inefficient thermodynamically, converting only a small fraction of chemical energy into useful mechanical work. The simulations focus on the dynamics resulting from the active stress, not the efficiency of its generation or utilization for computation. Score is low based on general knowledge of such systems.

### **2.4 Energy Dissipation**

    *   Content: Energy is dissipated primarily through viscous friction within the liquid crystal fluid (represented by the viscosity `η` in the stress tensor `Πp` and the damping term `−νu` in Eq. 2) and rotational viscosity (`Γ` in Eq. 1) associated with the relaxation of the director field. These terms convert mechanical energy (kinetic energy of flow, elastic energy of deformation) into heat. Quantification is not provided, but dissipation is inherent in the governing equations. Assessment: High, typical for fluid systems.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: ~1106
*    Units: Simulation Time Units (τ); (~619 seconds)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: ~1
*   Units: state (per junction)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: High (qualitative)

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: ~1 / (1106 τ)
    *   Units: τ⁻¹ (state loss per simulation time unit)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are defined by the coupled partial differential equations governing the system:
        1.  **Beris-Edwards Equation (Eq. 1):** Governs the evolution of the nematic order parameter Q-tensor (`∂Q/∂t + u · ∇Q - S = ΓH`). This includes advection by flow (`u · ∇Q`), flow-alignment/tumbling (`S`), and relaxation towards minimum free energy (`H = - (δF/δQ - (I/3)Tr(δF/δQ))`). The free energy functional `F` includes terms for bulk nematic order and elastic distortions (`∇Q`).
        2.  **Generalized Navier-Stokes Equation (Eq. 2):** Governs the fluid velocity `u` (`ρ(∂u/∂t + u · ∇)u = ∇ · Π - νu`). The stress tensor `Π` includes passive contributions (`Πp` - viscous and elastic stresses) and the crucial active stress term `Πa = -α(r)Q` which links local order `Q` and local activity level `α(r)` to fluid forcing.
        These equations describe how local velocity, velocity gradients, order parameter, order parameter gradients, and activity levels interact to determine the system's evolution.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Eq. 1   | Nematic Evolution | Rotational Viscosity (rel. to Γ) | Γ = 0.1 | Sim. Units | Methods | Explicit | Explicitly stated parameter value. |
    | Eq. 1   | Nematic Evolution | Flow-aligning parameter (ξ) | ξ = 0.7 | Dimensionless | Methods | Explicit | Explicitly stated parameter value. |
    | Eq. 1/F | Elasticity   | Elastic Constant (L) | L = 0.1 | Sim. Units | Methods | Explicit | Explicitly stated parameter value. |
    | Eq. 2   | Fluid Dynamics | Isotropic Viscosity (η) | η = 1/3 | Sim. Units | Methods | Explicit | Explicitly stated parameter value. |
    | Eq. 2   | Fluid Dynamics | Damping Parameter (ν) | ν = 0.01 | Sim. Units | Methods | Explicit | Explicitly stated parameter value. |
    | Eq. 2   | Active Stress | Activity Parameter (α) | 0 - 0.022 | Sim. Units | Methods/Results | Explicit | Explicitly stated range used. |

### **4.3 Global Order:**

    *   Content: The global order that emerges includes:
        1.  **Defect Trajectories:** Specific paths followed by +1/2 defects guided by the activity patterns (e.g., moving along stripes, turning at junctions, Fig 1).
        2.  **Director Field Configurations:** The overall arrangement of the nematic director field, especially the persistent state changes at junctions after defect passage (Fig 3 discussion).
        3.  **Defect Arrangements:** Specific configurations like the two +1/2 defects in the passive circle surrounded by active nematic (Fig 2B), mimicking homeotropic anchoring effects.
        4.  **Logic States:** The functional state of the designed structures (e.g., gate open/closed, amplification occurring).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Active Propulsion | Force on defect due to activity | α | 0.001 - 0.022 | Sim. Units | Explicit | Governs defect motion speed/direction related to local activity. | Methods/Results |
| Elastic Interaction | Force/torque due to director field distortion | L | 0.1 | Sim. Units | Explicit | Governs defect-defect and defect-boundary interactions based on minimizing elastic energy. | Methods |
| Hydrodynamic Interaction | Flow induced by defect motion/activity | η, ν | 1/3, 0.01 | Sim. Units | Explicit | Governs how defects influence each other via the fluid medium. | Methods |
| Boundary Interaction | Anchoring/Confinement | Anchoring Strength (W), Geometry | W ≥ 5e-7 (effective); Channel Widths 50-80 | N/m; Sim. Units | Mixed | Explicit boundary conditions (homeotropic, no-slip) and geometry. Effective anchoring strength W inferred from behavior (Fig 2). | Methods/Results/Fig 2 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| GO1 | Defect Trajectory | Defect Position (x,y) | Varies | Sim. Units | Explicit | Tracks movement over time. | Simulation Tracking | Fig 1, 3, 4D |
| GO3 | Defect Tunneling | Effective Passage Speed | High (relative to diffusion/drift) | Sim. Units/τ | Mixed | Calculated from simulation time/distance (Fig 4D), compared to non-tunneling case. | Simulation Timing | Fig 4 |
| GO4 | Defect Amplification | Amplification Factor | 3 (in example) | Dimensionless | Explicit | Count of output defects per input defect. | Simulation Observation | Fig 5 |
| GO5 | Effective Anchoring | Director Angle at Interface | Near Normal | Radians/Degrees | Explicit | Measured from director field configuration. | Simulation Analysis | Fig 2C |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 7. Rubric: Score reflects how completely the observed global behavior (Category B - emergent dynamics) can be predicted or explained solely by the local interaction rules and initial/boundary conditions (Category A - local physics) using the functor (simulation/physical laws). 0 = No relation; 5 = Qualitative agreement, some divergence; 7 = Quantitative agreement in demonstrated cases, potential deviations under noise/complexity; 10 = Perfect, provable mapping. Here, simulations show good mapping for presented cases, but robustness/universality isn't fully explored.
    *   **Metrics:** Consistency of simulation output (defect trajectories, logic function success) across runs (implied, not shown), comparison of simulated dynamics with expected behavior based on local forces (e.g., defect following pattern).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Physical Computation

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operations demonstrated are:
        1.  **Defect Guiding/Transport Control:** Steering a +1/2 defect along a predefined path using an activity pattern (Fig 1). This acts like a wire.
        2.  **Defect Gating:** Blocking or allowing the passage of a +1/2 defect based on the state of a junction (altered by a previous defect, Fig 3) or the presence/absence of a "tunneling" pattern (Fig 4B/C). This resembles a transistor or switch.
        3.  **Defect Tunneling:** Facilitating rapid transport across a passive gap via nucleation/annihilation cycles induced by a patterned high-activity region (Fig 4A, 4B). This is an effective speed-up mechanism.
        4.  **Defect Amplification:** Generating multiple output +1/2 defects from a single input +1/2 defect interacting with specific channel geometries and activity patterns (Fig 5). This resembles signal amplification.
    *   **Sub-Type (if applicable):** (e.g., Gating: Defect-blocking; Amplification: Defect-multiplication)

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Characteristic Time Unit | ~0.56 | s | Results | Explicit | Explicit mapping provided. |
        | Defect passage across junction (stripe pattern) | ~140 | τ (~78 s) | Fig 1D, Fig 3 | Explicit | Read from plot/text. |
        | Defect passage across junction (cross pattern) | ~70-140 | τ (~39-78 s) | Fig 1D | Explicit | Read from plot. |
        | Defect tunneling across triangular pattern | ~5 | τ (~2.8 s) | Fig 4D | Explicit | Read from plot/text. |
        | Defect gating operation (incl. tunneling) | ~20 | τ (~11 s) | Fig 4 Discussion | Explicit | Stated in text. |
        | Defect passage without tunneling (comparison) | ~100 | τ (~56 s) | Fig 4D | Explicit | Read from plot/text. |
        | Memory Retention / Erasure Time (-1/2 defect) | ~1106 | τ (~619 s) | Fig 3 Discussion | Explicit | Stated in text. |
        | Amplification Process Time | ~160 | τ (~90 s) | Fig 5 Caption/Visual | Mixed | Estimated from figure sequence timing. |
    *   **Note:** τ represents the simulation time unit. Conversion to seconds uses the paper's mapping τ ≈ 0.56 s.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors are logic-like operations performed using controlled topological defect dynamics:
        *   **Controlled Defect Transport:** Guiding +1/2 defects along specified paths defined by activity patterns.
        *   **Defect Gating:** Allowing or blocking defect passage through junctions based on the junction's history (previous defect passage) or temporary activation of a control pattern.
        *   **Defect Tunneling:** Rapid effective transport of a +1/2 defect across a barrier (passive region) mediated by activity-induced defect pair nucleation and annihilation.
        *   **Defect Amplification:** Generation of multiple output +1/2 defects resulting from the passage of a single input +1/2 defect through a specifically designed structure with activity patterns.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation is entirely based on hydrodynamic simulations using the described model (Beris-Edwards/Navier-Stokes). Specific designs for activity patterns and channel geometries are implemented, and the resulting defect dynamics are simulated over time. The emergent behaviors (gating, tunneling, amplification) are validated by observing the simulation outcomes, such as tracking defect trajectories (Fig 1, Fig 4D), observing the blocking effect (Fig 3, movie S4), timing the passage (Fig 4D), and counting defects (Fig 5). Control experiments (e.g., Fig 4C showing blocking without the tunneling pattern) are included. Reliability and reproducibility are implied by showing consistent outcomes but not statistically quantified. Limitations include the idealized nature of simulations (no thermal noise mentioned, perfect patterns) and lack of experimental verification within this paper (though refs 29, 30 are cited for related experimental confirmation of guiding).

---

#Key: [manzano_thermodynamics_2024]

# Thermodynamics of Computations with Absolute Irreversibility, Unidirectional Transitions, and Stochastic Computation Times

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### 1.1 System Description
    *   Content: The paper presents a theoretical framework based on stochastic thermodynamics and martingale theory to analyze the thermodynamic costs (specifically intrinsic mismatch cost, related to entropy production) of computational processes. It focuses on generic Markovian processes, particularly Discrete-Time Markov Chains (DTMCs), that model computations. These computations can feature characteristics common in real-world computers and theoretical models like Deterministic Finite Automata (DFAs), including stochastic stopping/halting times, unidirectional transitions (breaking local detailed balance), and absolute irreversibility (due to restricted initial states, like a designated start state). The purpose is to derive universal fluctuation relations and second-law-like inequalities bounding dissipation in such computations, illustrated with DFAs processing bit strings. Components are abstract computational states (x), transition probabilities (P(xt+1|xt)), initial distributions (ρ0), and time (t, τ, T).

### 1.2 Implementation Clarity

### 1.3 Key Parameters
        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### 2.1 Energy Input
    *   Content: The paper focuses on abstract thermodynamic costs (entropy production, mismatch cost) derived from information processing, not physical energy input to a material system. It mentions underlying physical processes (e.g., CTMC driven by AC current) but doesn't specify or quantify a primary energy source for the computations analyzed. The framework aims to be independent of specific physical implementation details.

### 2.2 Energy Transduction
    *   Content: The paper does not describe physical energy transduction mechanisms within a material. It discusses the *computation* itself (state transitions in a DTMC) as the process under thermodynamic scrutiny. The link made is that any physical implementation of these computational transitions (e.g., manipulating electrical signals, DNA structures) incurs thermodynamic costs, specifically entropy production, lower-bounded by the calculated intrinsic mismatch cost. The transduction is abstract: information state change -> thermodynamic cost.

### 2.3 Energy Efficiency
    *   Justification/Metrics: The paper does not calculate or discuss the overall energy efficiency of a specific computational implementation. It focuses on establishing *lower bounds* on dissipation (intrinsic mismatch cost, related to entropy production), which is related to inefficiency, but doesn't provide a metric for total efficiency. The goal is to find the unavoidable minimum cost, not the total efficiency achieved.

### 2.4 Energy Dissipation
    *   Content: The central theme is quantifying a fundamental component of dissipation, the "intrinsic mismatch cost" Σ(T), derived from the DTMC model of computation. This cost is shown to be a lower bound on the physical entropy production (dissipation) of *any* periodic physical process implementing the DTMC (Sec. II.C, Appendix C). Key results are fluctuation relations (Eq. 3, 35) and inequalities (Eq. 4, 6, 39, 41, 43, 44) bounding the average intrinsic mismatch cost hΣ(T)i. The paper quantifies this specific, information-theoretic component of dissipation, acknowledging it's part of the total physical dissipation (which also includes residual costs, ignored here - Appendix A). Specific values depend on the computational model (DFA parameters, τ, T). For the DFA example with stationary prior, hΣ(T)i = (p0-2)ln(p0) for τ≥2 (Eq. 63).

## M3: Memory

### 3.1 Memory Presence:
    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding with M3.2 and M3.3.)**

### 3.2 Memory Type:

### 3.3 Memory Retention Time:
*   Value: Up to τ (limit time) or T (stopping time)
*    Units: Time steps / Iterations

### 3.4 Memory Capacity (Optional - if applicable)
*  Value: N (Number of states)
*   Units: Dimensionless (distinct states)

### 3.5 Readout Accuracy (Optional - if applicable)
*   Value: 100% (within the model)
*   Units: %

### 3.6 Degradation Rate (Optional - if applicable)
    *   Value: 0 (within the model)

### 3.7 Memory Operations Energy Cost (Optional - if applicable)
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### 3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### 4.1 Self-Organization Presence:
    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### 5.1 Embodied Computation Presence:
    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### 6.1 Timescales:
        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | DTMC Iteration Step | 1 (by definition) | Dimensionless Time Steps | Sec II.A | Explicit | Basic unit of time evolution in the DTMC model. |
        | Limit Time (τ) | Variable (fixed per analysis) | Time steps / Iterations | Sec. I.B | Explicit | Maximum duration considered for a computation. |
        | Stopping Time (T) | Stochastic Variable (≤ τ) | Time steps / Iterations | Sec. I.B, Eq. (28) | Explicit | Time when a specific condition is met (e.g., reaching accept state). |
        | Relaxation Time (to π) | ≥ 2 (for DFA example) | Time steps / Iterations | Eq. (56) | Explicit (Example) | Time to reach the stationary distribution (in the specific DFA example). |
    *   **Note:** The fundamental timescale is the discrete iteration step of the DTMC. Other relevant timescales are the process duration limits (τ) and the stochastic actual duration (T).

### 6.2 Active Inference:
    *   Content: No

## M7: Adaptation

### 7.1 Adaptive Plasticity Presence:
    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### 8.1 Behavior Description:
    *   Content: The primary behavior is the execution of a computational task, modeled as a trajectory through states of a DTMC. Specific examples include a DFA processing an input string to determine acceptance (e.g., checking if a binary number is divisible by four or three). The behavior is the sequence of state transitions driven by the input and the computational rules, culminating potentially at a stopping time T based on reaching a certain state (e.g., an accept state). Other behaviors analyzed are related to the statistics of stopping times and acceptance probabilities (Sec VI).

### 8.2 Behavior Robustness:

### 8.3 CT-GIN Emergent Behavior Validation
     *  Content: The behaviors described (computation execution, string acceptance) are inherent results of the defined DTMC/DFA rules, not emergent in the sense of arising unexpectedly from complex local interactions. Validation relies on the mathematical correctness of the DTMC model and the derivation of thermodynamic quantities and statistical properties (e.g., acceptance probability bounds in Sec. VI). Numerical simulations (Sec. V) validate the analytical results for the DFA examples under the specified model assumptions (i.i.d. or Markovian input sources). There is no claim or validation of emergent properties beyond the direct consequences of the model definition.

---

#Key: [fair_digital_2007]

# Digital microfluidics: is a true lab-on-a-chip possible?

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is Electrowetting-on-Dielectric (EWD) based digital microfluidics. It manipulates discrete liquid droplets on a 2D array of electrodes coated with a dielectric and hydrophobic layer. Droplets are typically sandwiched between this electrode array plate and a top plate (often a ground plane or passive plate), separated by a gap usually filled with an immiscible fluid like silicone oil. Applying voltages sequentially to adjacent electrodes modulates the interfacial tension, creating pressure gradients that move, merge, split, mix, or dispense droplets. The system's purpose is to serve as a programmable, reconfigurable, and potentially reusable platform ("lab-on-a-chip") for automating and integrating various biochemical protocols (e.g., assays, PCR, sequencing) at small scales (nL to μL). Components include the electrode array substrate, dielectric layer (e.g., Parylene C), hydrophobic coating (e.g., Teflon AF), spacers, top plate, filler fluid (e.g., silicone oil), and the liquid droplets being manipulated. The system is controlled electronically, allowing software-defined protocols.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Values represent ranges discussed or shown in examples within the review. Reliability is considered 'High' as these are typical experimental parameters explicitly stated.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is electrical, supplied by an external voltage source to the control electrodes. This can be DC or AC voltage.
    *   Value: Nanowatts–microwatts per transfer
    *   Units: W

### **2.2 Energy Transduction**

    *   Content: Electrical energy is transduced into electrostatic energy stored in the capacitor formed by the droplet, dielectric, and electrode. The applied electric field modifies the solid-liquid interfacial tension (electrowetting effect, Lippman-Young equation). This change in interfacial tension creates a pressure gradient across the droplet when voltage is applied non-uniformly (e.g., activating an adjacent electrode), which is then transduced into kinetic energy as the droplet moves.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper explicitly states the system is "Extremely energy efficient: Nanowatts–microwatts of power per transfer" (Sec 1.2). This suggests high efficiency in terms of energy per operation. However, a precise thermodynamic efficiency (work done on droplet vs. electrical energy input) is not calculated. The score reflects the low power consumption claim but acknowledges the lack of a formal efficiency metric.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation mechanisms include:
        1.  **Capacitive Charging/Discharging:** Although DC current is blocked, AC actuation involves capacitive currents (explicitly mentioned in Sec 1.2). Energy is dissipated during charging/discharging cycles (I²R losses in electrodes/liquid if not purely capacitive, dielectric losses). Qualitative assessment: potentially Medium, frequency-dependent.
        2.  **Viscous Drag:** Friction between the moving droplet and the surrounding filler fluid (e.g., silicone oil) and potentially between the droplet and the surfaces (though minimized by oil). Qualitative assessment: Medium, dependent on viscosity, speed, geometry.
        3.  **Contact Angle Hysteresis:** Energy is lost overcoming the pinning forces associated with contact angle hysteresis during droplet motion. Qualitative assessment: Medium, depends on surface properties and liquids.
        4.  **Heat Generation:** Minimal ohmic heating is claimed (Sec 1.2), but capacitive currents and viscous drag will generate some heat. Qualitative assessment: Low.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Droplet Transport Rate / Switching Frequency | up to ~666 (for 3nl), 5-25 (typical assay range) | Hz | Sec 2.1.1, Fig 11, Fig 39 | Explicit | Values explicitly stated or derivable from velocity/pitch. |
        | Droplet Transport Velocity | up to ~250 (25 cm/s) | mm/s | Sec 2.1.1 | Explicit | Value explicitly stated. |
        | Mixing Time (Active, 2x4 array) | ~2.8 | s | Sec 2.1.4, Fig 26 | Explicit | Value explicitly stated. |
        | Mixing Time (Passive Diffusion Estimate) | ~1000x slower than active operations | cycles (relative) | Sec 2.1.4 | Explicit | Relative comparison explicitly stated. |
        | Dispensing Rate (Pressure-assisted) | 8 - 120 | droplets/min | Sec 2.1.3, Fig 23 | Explicit | Range explicitly stated. |
        | Capacitance Transient Time (Entrained Oil) | ~2 - 6 | s | Sec 2.1.1, Fig 8 | Explicit | Derived from text description of Fig 8. |
        | PCR Denaturation Time (Droplet) | ~3 | s | Sec 2.2.4 | Explicit | Value explicitly stated. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The system performs fundamental fluidic operations on discrete droplets:
        *   **Transport:** Moving droplets between locations on the electrode array (Sec 2.1.1).
        *   **Merging:** Combining two or more droplets into one (Sec 2.1.5 implicitly, essential for mixing).
        *   **Mixing:** Homogenizing the contents of a merged droplet, often actively by shuttling it across electrodes (Sec 2.1.4).
        *   **Splitting:** Dividing one droplet into two or more smaller droplets (Sec 2.1.5).
        *   **Dispensing:** Creating droplets of a defined volume from a larger reservoir on-chip (Sec 2.1.3).
        *   **Storage:** Holding droplets in reservoirs or on inactive electrodes (Sec 2.1.3).
        *   **Fluidic I/O:** Loading liquids onto the chip and potentially removing them (Sec 2.1.2).
        These elemental operations are combined to implement more complex fluidic functions like sample dilution, purification, assays, PCR protocols, and sequencing steps (Sec 2.2).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper validates the described behaviors (fluidic operations) through experimental results presented primarily via:
        *   Time-lapse photographs/sequences of droplet operations (e.g., Dispensing Fig 15, Splitting Fig 30, PCR setup Fig 40 insert).
        *   Quantitative measurements of operational parameters (e.g., Transport rates vs voltage Fig 7, Fig 11; Mixing times vs frequency/array size Fig 25, Fig 26; Dispensed volume variability Fig 21, Fig 22; Assay absorbance Fig 38; PCR amplification curves Fig 40, Fig 41).
        *   Demonstration of integrated applications (e.g., Glucose assay Fig 38, MALDI interface Fig 33, PCR Fig 40/41, Pyrosequencing concept Fig 43).
        Control experiments are implicitly used (e.g., comparing transport in oil vs air Fig 7, comparing mixing methods Fig 25/26, PCR controls Fig 40). Reproducibility is explicitly discussed and teilweise quantified (e.g., dispensing CV, assay CV). Limitations are acknowledged (see M8.2). The validation focuses on demonstrating the feasibility and characteristics of programmed operations, not on validating emergent phenomena in the sense of M4.1.

---

#Key: [crepaldi_experimental_2023]

# Experimental Demonstration of In-Memory Computing in a Ferrofluid System

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is an experimental setup demonstrating in-memory computing capabilities using a ferrofluid (FF), specifically a Fe3O4 water-based colloid. The FF itself acts as the computational and memory medium. Key components include the FF sample contained in a vial, two ports (implemented using exposed feedlines of SMA connectors) for programming and readout, a vector network analyzer (VNA) for RF readout (measuring S-parameters, converted to impedance), a DC bias generator for programming (applying quasi-DC voltage signals), two bias tees to decouple RF and DC signals, and a personal computer for controlling the experiment and acquiring data. The purpose is to demonstrate that a ferrofluid can perform electrical analogue computing, exhibit memristive behavior (including short and long-term memory/plasticity), and execute tasks like digit classification using in-memory computing and physical reservoir computing (PRC) approaches, leveraging the complex dynamics of the nanoparticle suspension under electrical stimuli.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** ZC values are derived from summing impedance magnitudes over the frequency range.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The system uses two primary energy inputs:
        1.  Electrical energy from the DC generator for programming the ferrofluid state (applying quasi-DC voltage `V_P`).
        2.  Electrical energy (RF signal) from the VNA for reading the ferrofluid state.
    *   Value: DC: up to +/-10V available, typically +/-3.8V used; RF: -3 dBm
    *   Units: DC: Volts (V); RF: dBm

### **2.2 Energy Transduction**

    *   Content: The primary energy transduction involves:
        1.  **Programming:** Input DC electrical energy is transduced into work done on the ferrofluid nanoparticles, overcoming viscous forces and potentially altering electrostatic/magnetic interactions. This changes the spatial configuration and potentially the orientation/aggregation state of the Fe3O4 nanoparticles suspended in the water-based solvent, likely influenced by surfactant molecule polarization. This reconfiguration manifests as a change in the bulk electrical impedance of the fluid between the ports.
        2.  **Readout:** Input RF electrical energy probes the impedance state of the ferrofluid. Energy is partially reflected and transmitted, measured by the VNA as S-parameters, which are then mathematically converted to impedance (`Z_C`). The RF energy interacts with the nanoparticle configuration established during programming.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper states the power required for the in-memory computing scheme (Fig 3) is below 200 μW (Sec 5), which is claimed to be low-power. However, this is the *overall system power* for a specific task and doesn't represent the efficiency of the core transduction (DC energy to stored memory state) or computation itself. The underlying mechanism involves moving nanoparticles in a viscous fluid and potentially overcoming electrostatic forces, which is likely inefficient compared to solid-state devices. The macroscopic volume (5 mL) contains ~10^18 particles, suggesting significant energy might be needed per computational/memory state change compared to nanoscale devices. No explicit efficiency metric (e.g., energy per state transition, energy per FLOP equivalent) is provided. The low score reflects the likely low intrinsic efficiency of the physical process and the lack of quantitative data specifically on computational/memory efficiency.

### **2.4 Energy Dissipation**

    *   Content: Potential dissipation mechanisms include:
        1.  **Resistive Heating (Joule Heating):** Electrical current flowing through the ferrofluid (which has finite resistance implied by impedance measurements) during both DC programming and RF readout will cause heating. (Qualitative: Medium, given applied voltages and observed impedance).
        2.  **Viscous Damping:** Movement of nanoparticles through the solvent during reconfiguration dissipates energy due to fluid viscosity. (Qualitative: Medium/High, inherent to liquid-state operation).
        3.  **Dielectric Losses:** At RF frequencies, energy loss associated with the dielectric properties of the fluid and surfactant layers. (Qualitative: Low/Medium, typical for RF measurements).
        4.  **Electrolysis:** Mentioned as a possibility at excessive voltages (Sec 5), consuming energy to produce gas (H2/O2). (Qualitative: Low under normal operation, potential failure mode).
        Quantification is not provided in the excerpt.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Analog (Demonstrated N=16 distinct states)
*   Units: States (or potentially bits if resolution quantified)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Skipping M4.2-M4.7 as M4.1 is "No")**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Neuromorphic/Reservoir Computing/Analog

### **5.3 Computational Primitive:**

    *   Content: The paper demonstrates two main computational approaches:
        1.  **Weighted Sequence Matching / Temporal Pattern Classification:** In the custom in-memory scheme (Sec 4.1, Fig 3), the ferrofluid's state (impedance `Z_C22`) evolves in response to a time-serialized input sequence (digit pixels mapped to voltage pulses with varying durations/weights). The final impedance state effectively classifies the input sequence based on pre-defined weighting (matching). This acts as a form of temporal pattern recognition or sequence processing.
        2.  **Non-linear Temporal Transformation (Reservoir Computing):** In the PRC approach (Sec 4.2, Fig 4), the ferrofluid acts as a reservoir whose complex, fading memory dynamics perform a non-linear transformation of the input time series (serialized digit pixels) into a higher-dimensional state space (represented by the 64 impedance values `Z_C22` recorded over time). This transformed state is then linearly classified by a trained readout layer (NN). The primitive here is the complex, history-dependent, non-linear mapping of input sequence to internal state trajectory.
    *   **Sub-Type (if applicable):** Pattern Classification, Non-linear Temporal Mapping.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | DC Programming Pulse Width (Memory Write) | Variable (up to seconds demonstrated) | s | Fig 2 | Explicit | Value range used in Fig 2 experiments. |
        | DC Programming Pulse Width (Classification Weights) | 0.5, 4.5 (Fig 3); 2 (Fig 4) | s | Fig 3, Sec 4.2 | Explicit | Specific values used in experiments. |
        | Hysteresis Sweep Step Duration | 1 | s | Sec 2 | Explicit | Explicitly stated parameter. |
        | Memory Retention/Fading | Seconds to Days | s | Fig 2C, Sec 2, Sec 4.1 (ref S7) | Mixed | Qualitative description (fading, long/short term) + inference from experiments/refs. |
        | Digit Classification Time (PRC) | 64 pixels * 2 s/pixel = 128 | s | Sec 4.2 | Explicit | Calculation based on stated parameters. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism of adaptation appears to be the history-dependent reconfiguration of the Fe3O4 nanoparticles within the water-based solvent, driven by the applied DC electric fields. The paper suggests nanoparticles are subject to electromagnetic forces altering their state (location, potentially aggregation/orientation), and that the polarizability of surfactant molecules might play a role (Sec 5). The system's impedance (`Z_C`) reflects this configuration. Applying DC bias changes the configuration, and this change persists to some degree (memory/plasticity), influencing future impedance measurements and responses to subsequent stimuli. This is analogous to synaptic plasticity where connection strength changes with activity, but here the physical basis is the collective state of nanoparticles in the fluid. The driving force is the history of applied DC voltage signals. It's a form of material plasticity where the electrical history molds the conductive pathways or dielectric properties of the colloid.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors demonstrated are:
        1.  **Memristive Switching / Memory Storage:** The system exhibits history-dependent resistance/impedance changes characteristic of memristors, allowing it to store information (analog or multi-level) encoded in its impedance state based on past electrical stimuli (voltage pulse width/amplitude). Includes short-term plasticity (hysteresis changes) and longer-term (fading) storage.
        2.  **In-Memory Computation (Pattern Classification):** The system performs digit classification by processing serialized input sequences directly within the ferrofluid medium. This is achieved either through a custom weighted sequence matching scheme or via Physical Reservoir Computing (PRC) where the fluid's dynamics transform the input for an external classifier.
        3.  **Self-Healing (Claimed):** The system is claimed to recover full functionality after disturbances like excessive voltage (causing electrolysis), due to its liquid nature.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of memory and computation are validated through quantitative experimental results:
        *   **Memory:** Hysteresis loops measured (Fig 1C), demonstrating state retention. Storage of N=16 distinct states shown with mean/variance plots during hold phase (Fig 2B, 2C), validating controllable state setting and short-term retention. Fading mentioned qualitatively.
        *   **Computation (In-Memory Matching):** Impedance evolution shown for different digits and weightings (Fig 3C-E), demonstrating differential response. Final impedance values used for classification, achieving 90% accuracy (Fig 3F) on a custom 8x8 dataset (details in S5).
        *   **Computation (PRC):** Classification accuracy of 90.6% achieved on a validation set (300 digits) using a trained NN readout layer on the reservoir's output (64 impedance values), demonstrated via confusion map (Fig 4B). Dataset details in S11.
        *   **Robustness/Self-Healing:** Claims made in Sec 5 refer to S12 for validation (not provided). Functional variability (chaos, drift) acknowledged and addressed with specific reset protocol for PRC (Sec 4.2, Fig 4A).
        Limitations: Validation relies partly on supplementary info. Robustness claims lack direct experimental validation *in the excerpt*. Emergence aspect is less rigorously defined/validated; behaviors result from complex dynamics but are heavily programmed/driven by external signals.

---

#Key: [fuchslin_evolving_2006]

# Evolving inductive generalization via genetic self-assembly

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system uses a genetic algorithm to evolve Self-Assembling Logic Blocks (SLBs) capable of forming scalable digital circuits, specifically multipliers and ALUs. Each SLB has genetically encoded logic functions (combinatorial logic, MUXs, pass-through) and recognition sites (plugs/sockets) determining how they assemble on a 2D grid (simulated substrate). The genome also encodes a set of test vectors used in fitness evaluation via tournament selection. The purpose is to demonstrate that genetic encoding of self-assembling components enables the evolution of inductive generalization, allowing circuits evolved on small examples to scale correctly to arbitrarily large problem instances (e.g., n x n-bit multipliers).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Selected parameters highlight population, genome complexity, and key evolutionary dynamics variables discussed extensively in the results.

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term (Encoded Design); Generational (Test Vectors)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 6-10 (SLB types); 16-32 (Test Vector size)
*   Units: SLB types; Problem instances

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: 0 (Encoded Design); Related to r_TV (Test Vectors)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rule is the matching between recognition sites (plugs and sockets) on adjacent edges of SLBs. An SLB can attach to the growing structure if its left and upper edges match the exposed plugs/sockets of the existing SLBs to its left and above it. Matching requires specific plug/socket types to align, unless a site is "promiscuous" (no plug/socket defined), which allows matching with anything. Ambiguities (multiple SLB types matching) are resolved first by highest binding energy (number of non-promiscuous matches) and then by genomic order. If no evolved SLB matches, a default SLB (always matches due to promiscuous sockets) is used. Optional edge self-assembly follows similar rules. (Sec 2.1, Sec 2.2, Appendix).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Match | Recognition site matching | Length of recognition pattern per edge (l_rec) | 2 | Sites | Table 1 | Explicit | Parameter stated. |
    | Match | Recognition site matching | Plug/Socket types per site | 2 (encoded by 1 bit) | Types | Appendix | Explicit | Details provided in Appendix. |
    | Match | Recognition site matching | Promiscuity encoding | 1 bit (presence/absence of plug/socket) | Binary | Appendix | Explicit | Details provided in Appendix. |
    | Ambiguity | Binding energy | Binding energy unit | 1 per exact match | Energy units (arbitrary) | Sec 2.2, Appendix | Explicit | Mechanism described. |
    | Ambiguity | Tie-breaking | Genomic order | Max distance from genome start | Index | Sec 2.2 | Explicit | Rule stated. |

### **4.3 Global Order:**

    *   Content: The emergent global order is a 2D tiled array forming a functional digital logic circuit (e.g., n x n-bit multiplier, ALU). This structure exhibits spatial patterns determined by the types of SLBs placed at each location, governed by the self-assembly rules. Successful evolution results in patterns that implement the target logic function correctly and scalably. Examples show regular and repeating patterns for multipliers and ALUs (Fig 6).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Explicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Match | Recognition site matching | l_rec | 2 | Sites | Explicit | Parameter stated. | Table 1 |
| Match | Plug/Socket types | Bit encoding | 1 | bit | Explicit | Details in Appendix. | Appendix |
| Match | Promiscuity | Bit encoding | 1 | bit | Explicit | Details in Appendix. | Appendix |
| Ambiguity | Binding energy calc. | Num exact matches | Integer >= 0 | Count | Explicit | Mechanism described. | Sec 2.2 |
| Ambiguity | Tie-breaking | Genomic order | Integer index | Index | Explicit | Rule stated. | Sec 2.2 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Structure | Overall Circuit Layout | Circuit Size (s_board x s_board) | e.g., 24x24, 32x32, 48x48 | Tiles | Explicit | Board size determines overall size. | Sec 2.3 |
| Performance | Logical Correctness | Fitness Score (f) | Real number >= 0 | Score units | Explicit | Fitness function quantifies correctness. | Sec 2.3, Fig 7 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | Assembly | Local matching rule to global circuit structure | 1.0 (Deterministic Simulation) | 8 | Deterministic algorithm, Circuit connectivity | Implicit | The deterministic nature ensures perfect mapping fidelity *within the simulation*. Score reflects high fidelity but acknowledges it's a simulation construct. | Sec 2.2 |
    | Function | Local SLB logic to global circuit function (e.g., multiplication) | High (for evolved scalable circuits) | 7 | Circuit simulation, Logical analysis, Fitness score | Mixed | Scalability suggests high fidelity mapping, tested via simulation/analysis. Score reflects success but potential for unseen errors in infinite cases. | Sec 2.2, Sec 2.3, Sec 3.1 |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Digital

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operations are performed by the individual SLBs. Each SLB output is a function of its four inputs. The primitive operations available within an SLB include:
        1.  Arbitrary 4-input Boolean function (via a 4-bit function generator)
        2.  Multiplexing (selecting one input or ground)
        3.  Direct routing (pass-through connection)
        4.  Constant output (ground/zero)
        The specific function for each output is genetically encoded and evolvable, with a bias towards simpler functions like routing (Sec 2.1, Appendix, Fig 3b).
    *   **Sub-Type (if applicable):** Combinatorial Logic (4-input LUT), Multiplexer (4-to-1), Routing, Constant Output.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Evolutionary Generation | 1 | Generation | Table 1, Sec 3.1 | Explicit | Simulation proceeds in generations. |
        | Total Evolution Time | ~100,000 (Typical) | Generations | Table 1 | Explicit | Typical run length parameter. |
        | Total Evolution Time (CPU) | < 24 | Hours (on PC) | Sec 3.1 | Explicit | Mentioned computational cost. |
    *   **Note:** The primary timescales relate to the evolutionary simulation process. Circuit operation time is not modeled.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes (at the evolutionary level) / No (at the individual circuit level post-assembly)

**(Conditional: If M7.1 is "No" at the individual circuit level, skip to Module 8. If "Yes" at the evolutionary level, include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism is a Genetic Algorithm operating on a population of genomes. Adaptation is driven by:
        1.  **Variation:** Random mutations (single bit flips in logic/recognition sites, switching logic function types), gene duplication (copying SLB genes), structured mutations (twin changes in recognition sites). (Sec 2.4)
        2.  **Selection:** Pair-wise tournament selection based on fitness. Fitness is evaluated by running an individual's circuit on its opponent's co-evolved test vector. The winner replaces the loser (with mutation). (Sec 1, Sec 2.3, Fig 1e, Fig 2)
        3.  **Inheritance:** Genomes of successful individuals are passed on (with variation) to the next generation.
        This constitutes an evolutionary adaptation mechanism operating on the population's genetic makeup over generations.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behaviors are:
        1.  **Self-Assembly:** The SLBs spontaneously (driven by local rules in simulation) assemble into a 2D tiled structure.
        2.  **Digital Computation:** The assembled structure functions as a digital logic circuit, performing operations like n x n-bit multiplication or ALU functions (addition, XOR, AND, OR based on selector bits).
        3.  **Scalability/Inductive Generalization:** The key emergent behavior highlighted is that circuits evolved using limited examples (small n) and test vectors correctly perform the computation for much larger problem instances (larger n) due to the evolved self-assembly rules capturing the general logic structure.

### **8.2 Behavior Robustness:**

        *   **Strengths:** The deterministic simulation ensures perfect reproducibility given the same genome and parameters. Scalability implies robustness to problem size changes. Some robustness to mutation rates (Fig 9, 10) and test vector randomness (Fig 8a) in the evolutionary *process* is shown. Preliminary tests with non-deterministic assembly (errors, ambiguous matching) suggest the *evolutionary outcome* can be robust (Sec 4).
        *   **Weaknesses:** The paper primarily uses deterministic assembly; robustness to physical noise, component failures, environmental variations, or errors *during* assembly in a real system is largely untested/unquantified beyond the brief mention in Sec 4. Robustness of the *final circuit's computation* to input noise or internal faults is not discussed. The score reflects robustness within the simulation context but acknowledges lack of testing against realistic physical perturbations.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content:
         *   **Self-Assembly:** Validated by simulation; the algorithm deterministically produces a tiled structure based on local rules (Sec 2.2, Fig 6c).
         *   **Digital Computation:** Validated by simulating the evolved circuit's logic on test inputs and comparing outputs against correct results. Fitness function quantifies correctness (Sec 2.3, Fig 7). Exhaustive testing for small input sizes (e.g., up to 8x8 bits) mentioned (Sec 2.2, Sec 3.1). Specific evolved SLB logic is analyzed (Fig 6a, 6d).
         *   **Scalability/Inductive Generalization:** Validated by: 1) Testing circuits evolved on smaller problems (e.g., 4x4 or 8x8 bit multiplication) on larger instances (16x16, 32x32) via simulation (Sec 2.3). 2) Analyzing the logic of evolved SLBs and the assembled pattern to show inductive properties mathematically/logically ("analyzing the inductive properties of the assembled pattern", Sec 2.2; "scalability...shown by an analysis of their internal logic", Sec 3.1). The "inductive hill" (Fig 9) provides statistical evidence from the evolutionary process.

---

#Key: [falk_learning_2023]

# Learning to learn by using nonequilibrium training protocols for adaptable materials

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### 1.1 System Description

    *   Content: The paper proposes a computational training methodology ("nonequilibrium training protocol") to design adaptable synthetic materials. The method involves periodically switching between incompatible target functionalities (G1, G2) during an optimization process, using the partially optimized design parameters from one function as the initial conditions for the next. This process aims to find regions in the high-dimensional design parameter space where minimal changes are required to switch between functions. The method acts as a "wrapper" around existing optimization algorithms. The system components are the material model (simulated elastic networks or heteropolymers), the design parameters (e.g., bond stiffnesses, monomer affinities), the optimization algorithm (e.g., gradient descent, CMA-ES, local bond pruning), and the oscillating target functions/goals. The purpose is to achieve adaptability – the ability to switch between incompatible functions with minimal parameter changes – as an emergent property of the training sequence. This is demonstrated in three simulated contexts: a) elastic networks trained for switchable allostery, b) elastic networks trained for switchable Poisson's ratio using irreversible bond removal, and c) heteropolymers trained for folding into distinct structures.

### 1.2 Implementation Clarity


### 1.3 Key Parameters

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters characterize the implementation of the oscillatory training protocol in the different examples. Reliability is high as these are explicitly stated values defining the simulation/training setup.

## M2: Energy Flow

### 2.1 Energy Input

    *   Content: The primary energy input is computational work performed by the optimization algorithm (gradient descent, CMA-ES, etc.) to adjust the design parameters. Within the simulations, potential energy functions (elastic spring energy, monomer interaction potentials via Morse potential) define the system's energy landscape, which the optimization navigates. For polymer folding, thermal energy (kT) is included via Langevin dynamics. There is no physical energy input to a real device.

### 2.2 Energy Transduction

    *   Content: Energy transformation primarily occurs within the simulated physics. For elastic networks, applied strain energy is stored as potential energy in deformed springs, leading to forces and displacements (allostery, Poisson's ratio). For polymers, thermal energy (kinetic) interacts with potential energy (monomer affinities, bending stiffness, excluded volume) leading to conformational changes (folding). The optimization algorithm uses information derived from these modeled energies (or related properties like displacement modes) via a cost function to guide changes in design parameters (transducing cost function value into parameter updates).

### 2.3 Energy Efficiency

    *   Justification/Metrics: The paper does not quantify the energy efficiency of the computational training process or the resulting simulated materials in terms of physical energy conversion. Adaptability might imply lower energy barriers for switching between states (discussed qualitatively for polymers, Fig 5), but this is not framed as efficiency.

### 2.4 Energy Dissipation

    *   Content: In the polymer folding simulations using Langevin dynamics, energy is dissipated implicitly through the friction/damping term mimicking interaction with a solvent (thermal bath). The magnitude is related to temperature (kT) but not explicitly quantified as a dissipation rate in the excerpt. The elastic network simulations using gradient descent or bond pruning appear quasi-static or energy-conserving in their core mechanics (dynamics matrix analysis, removing bonds based on static strain), although the optimization process itself is dissipative computationally.

## M3: Memory

### 3.1 Memory Presence:

    *   Content: Yes

**(Conditional: M3.1 is "Yes", continue)**

### 3.2 Memory Type:**


### 3.3 Memory Retention Time:**

*   Value: τ (Oscillation Period)
*    Units: training steps or bonds removed

### 3.4 Memory Capacity (Optional - if applicable)

*  Value: High (e.g., 66 for Polymer, 83 for Network)
*   Units: Number of tunable parameters (dimensionality of design space)

### 3.5 Readout Accuracy (Optional - if applicable)

*   Value: 100% (within numerical precision)
*   Units: %

### 3.6 Degradation Rate (Optional - if applicable)
    *   Value: 100% erasure/overwrite potential per `tau` steps
    *   Units: % per `tau` steps

### 3.7 Memory Operations Energy Cost (Optional - if applicable)
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### 3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### 4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", continue)**

### 4.2 Local Interaction Rules:**

    *   Content: The local rules are the update steps of the specific optimization algorithm used in each case:
        1.  **Allostery:** Gradient descent on a cost function related to the dynamical matrix (details in SI). Update rule adjusts bond stiffnesses `k_i` based on the gradient: `k_i(t+1) = k_i(t) - learning_rate * d(Cost)/d(k_i)`.
        2.  **Poisson's Ratio:** Irreversible bond removal. At each step, apply deformation, measure strain `ε_j` in each bond `j`, remove the bond with maximum strain: `Remove bond j where ε_j = max(ε)`. This rule uses local stress/strain information.
        3.  **Folding:** Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES). This involves maintaining a population of affinity matrices `A`, evaluating their yield (negative cost), and updating the distribution (mean and covariance matrix) from which new candidate solutions are sampled, biased towards higher-yield regions. The rule is the complex update procedure of CMA-ES itself.
        These rules are applied locally in time (at each step) based on the current state (parameters K or A) and the current goal (G1 or G2).
    * **Implicit/Explicit**: Mixed

### 4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :-------------------- | :---: | :----------: | :----------------: | :------------: |

### 4.3 Global Order:**

    *   Content: The primary emergent global order is the configuration of design parameters (K or A) residing in a specific "adaptable" region of the high-dimensional design space. This region is characterized by the property that pairs of parameter sets (K1, K2 or A1, A2) exist within it, where each set performs one of the target functions (G1 or G2) well, and the distance (e.g., number of significantly different parameters) between the sets in the pair is minimal. Subsidiary emergent orders are the specific physical mechanisms enabling this adaptability, such as the formation of coherent displacement units in elastic networks or the development of specific kinetic barriers (nucleation control) in the folding energy landscape.
    * **Implicit/Explicit**: Explicit

### 4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Explicit

### 4.5. Local Interaction Rules (for Self-Organization)
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| All | Oscillation Period | τ | 5, 20, 50, 100, 200 | steps/bonds | Explicit | Explicitly varied parameter. | Figs 2-4 |

### 4.6. Globally Emergent Order and Order Parameters
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| 1 | Adaptability | Fraction Changed Bonds/Interactions | ~0.05 - 0.6 (Networks), ~0.05 - 0.3 (Polymer) | Fraction | Explicit | Primary metric for adaptability. | Oscillatory Training | Figs 2D, 3D, 4D |
| 2 | Allostery Perf. | Cost Function Value | ~0 to ~0.5 | Arbitrary units | Explicit | Metric for achieving G1/G2. | Oscillatory Training | Fig 2C |
| 3 | Poisson Perf. | Poisson's Ratio (ν) | >0.75 (G1), <-0.75 (G2) | Dimensionless | Explicit | Target goals. | Oscillatory Training | Fig 3A, C |
| 4 | Folding Perf. | Yield in time T | > 0.7 required; values ~0.7-0.8 | % | Explicit | Metric for achieving G1/G2. | Oscillatory Training | Results, SI Fig 2 |
| 5 | Coherent Motion | Displacement Overlap | Higher for adaptable pairs | Arbitrary units | Explicit (SI) | Physical mechanism metric. | Oscillatory Training | SI Fig 1 |
| 6 | Nucleation Control | Off-Target Energy | Lower for adaptable pairs (~ -10kT) | kT | Explicit | Physical mechanism metric. | Oscillatory Training | Fig 5B |

### 4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 0

## M5: Computation

### 5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip to Module 6)**

### 5.2 Computation Type:**


### 5.3 Computational Primitive:**


### 5.4 Embodied Computational Units
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### 6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Training Oscillation Period (τ) | 5, 20, 50, 100, 200 | steps/bonds removed | Figs 2, 3, 4 | Explicit | Key parameter of the method. |
        | Polymer Folding Time Limit | 500 | simulation time units | Results: Heteropolymer Folding | Explicit | Time constraint for yield calculation. |
    *   **Note:** The crucial timescale is `tau`, controlling the non-equilibrium aspect of the training.

### 6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### 7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", include M7.2)**

### 7.2 Adaptation Mechanism:**

    *   Content: The mechanism driving the emergence of adaptability is the iterative application of an optimization algorithm (gradient descent, bond pruning, CMA-ES) under periodically switching target functions (G1 -> G2 -> G1...). The "learning" occurs as the parameter vector is pushed back and forth between regions satisfying G1 and G2. Over many cycles, if successful, the process selects for parameter vectors that lie in regions where the G1 and G2 solutions are close together. The feedback is provided by the cost function (or equivalent objective like strain minimization) relative to the *currently active* goal. The change is implemented by the specific update rule of the chosen optimization algorithm. It resembles aspects of curriculum learning or dealing with changing tasks, selecting for generalizability/transferability between the specific tasks G1 and G2.

## M8: Emergent Behaviors

### 8.1 Behavior Description:**

    *   Content: The primary emergent behavior is **adaptability**: the ability of the designed material (parameter set) to switch between two distinct, incompatible functions (G1 and G2) with minimal changes to its design parameters. The specific functions G1/G2 demonstrated are:
        1.  Opposing allosteric responses (extensile vs. contractile strain at target for same source strain) in elastic networks.
        2.  Opposing bulk deformation responses (Poisson's ratio ν > 0.75 vs. ν < -0.75) in elastic networks.
        3.  Folding into distinct target structures (clockwise spiral vs. counter-clockwise antispiral) for heteropolymers within a finite time.

### 8.2 Behavior Robustness:**


### 8.3 CT-GIN Emergent Behavior Validation

     *  Content: Claims of emergent adaptability are validated through computational simulations.
        1.  **Operational Definition:** Adaptability is quantified as the fraction of design parameters (bond stiffnesses or affinities) that need to change significantly (above a threshold) to switch between optimized solutions for G1 and G2 (Figs 2D, 3D, 4D).
        2.  **Control Experiments:** Results from oscillatory training are compared against standard optimization for single fixed goals ("no oscillation"), showing significantly higher parameter changes required for switching in the latter case (Figs 2B, 3B, 4C).
        3.  **Quantitative Analysis:** Performance (cost function, yield) is tracked during training (Figs 2C, 3C, 4B). The trade-off between yield and adaptability (parameter similarity) is quantified across different oscillation frequencies (Fig 6).
        4.  **Mechanism Analysis:** Physical mechanisms underlying adaptability (coherent motions, nucleation barriers) are identified and analyzed (Fig 5, SI Fig 1).
        5.  **Reproducibility:** Multiple simulation runs (e.g., n=500, n=62) with random initial conditions are performed to assess statistical significance and yield.
        6.  **Limitations:** Validation is purely computational; experimental verification is needed. Robustness assessment is limited (mainly bond pruning for Poisson's ratio).

---

#Key: [thuruthel_soft_2019]

# Soft robot perception using embedded soft sensors and recurrent neural networks

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of a pneumatically actuated planar soft finger made of silicone elastomer (Dragon Skin 20) with embedded soft resistive strain sensors made from carbon nanotube-impregnated polydimethylsiloxane (cPDMS). The system's purpose is to perceive its own kinematic state (proprioception) and estimate external contact forces using these embedded sensors and a machine learning model (Long Short-Term Memory - LSTM recurrent neural network). An external motion capture system and a load cell provide ground truth data for training the LSTM. The system aims to achieve real-time, model-free multimodal sensing (kinematics and force) for soft robots, drawing inspiration from biological sensory systems with redundant and unstructured sensor architectures. Components include the soft actuator (finger), embedded cPDMS sensors, pneumatic control system, LSTM network, motion capture system (ground truth), and load cell (ground truth). It demonstrates kinematic estimation during free motion and contact, as well as indirect external force sensing.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Dimensions for sensors varied; typical/example values are listed. Pressure is the command range. Sampling frequency is limited by the LCR meter setup.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is pressurized air supplied to the pneumatic actuator channels. Electrical energy is also required for the sensor readout (LCR meter), control board, LSTM computation, and motion capture system. However, the primary driver of the robot's physical action is pneumatic pressure.
    *   Value: 0 - 3.5 (pressure range)

### **2.2 Energy Transduction**

    *   Content: 1. Pneumatic energy (pressure) is transduced into mechanical work, causing deformation (bending) of the soft actuator. 2. Mechanical deformation (strain) of the actuator is transduced into changes in electrical impedance (resistance and reactance) by the embedded cPDMS sensors. 3. Electrical signals (impedance) from sensors, along with pressure command signals, are transduced into digital data processed by the LSTM. 4. The LSTM computation transduces input data into estimated kinematic (position) and kinetic (force) outputs. Electrical energy also powers the LCR meter, control electronics, and computational hardware.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any quantitative metrics for energy efficiency (e.g., work output vs. pneumatic energy input, computational efficiency). Soft pneumatic actuators are generally known to be inefficient compared to traditional rigid robots due to material deformation losses and air compressibility. The computational aspect (LSTM) also consumes energy. Therefore, the efficiency is assessed as qualitatively Low.

### **2.4 Energy Dissipation**

    *   Content: Dissipation mechanisms include: 1. Viscoelastic damping within the soft elastomer material during deformation (heat loss). 2. Internal friction within the actuator material and sensor material. 3. Air leakage (though minimized by sealing). 4. Resistance heating in sensor wires and potentially within the cPDMS material during current flow for impedance measurement (likely minimal). 5. Heat generated by the control electronics and computational hardware performing LSTM calculations. Quantification is not provided. Qualitative assessment: Material damping is likely significant (Medium/High); Computational dissipation depends on hardware (Unknown); Other losses likely Lower.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceed with M3.2 and M3.3.)**

### **3.2 Memory Type:**

    *   **Material Memory (Viscoelasticity/Hysteresis):** This is inherent to the soft materials (cPDMS, elastomer). It's a passive, relatively short-term memory exhibiting decay (creep recovery) and path dependence (hysteresis). It's not actively written or read in a controlled digital sense. Score contribution: ~2/10 (low capacity, passive, decaying).
    *   **Computational Memory (LSTM State):** This is an active memory embedded in the structure and weights of the trained LSTM network. It captures temporal dependencies learned from training data to predict future states (kinematics/force) based on past sensor readings and control inputs. It is adaptive (through training) and actively used for computation. This memory is crucial for compensating for the material's non-ideal behavior (drift). Score contribution: ~6/10 (learned, actively used, compensates for material limits, capacity depends on network size).
    *   **Overall:** The combined system leverages the computational memory to overcome the limitations of the passive material memory. The overall score reflects the functional memory achieved through the LSTM, which is essential for the system's performance, but it's still a learned mapping rather than a versatile, re-writable high-fidelity memory system. Retention depends on the LSTM's learned dynamics, capacity on network size/complexity. Read-out accuracy is reflected in the prediction errors.

### **3.3 Memory Retention Time:**

*   Value: Short-term (Material); Dependent on task/training (Computational)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No (Computation is performed by an external LSTM network, not intrinsically by the material properties themselves).

**(Conditional: If M5.1 is "No", skip to Module 6.)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Sensor Sampling Interval | 0.1 | s | Materials and Methods | Explicit | Inverse of 10 Hz sampling rate. |
        | Pressure Control Update Period | 1 | s | Materials and Methods | Explicit | Random reference pressures varied every second. |
        | Low-level PD Control Rate | 0.001 | s | Materials and Methods | Explicit | Low-level controller runs at 1000 Hz. |
        | Training Duration | ~50 | min | Results | Explicit | Calculated from 10Hz sampling rate and number of samples needed (table S1 implies ~30000 samples). |

    *   **Note:** Different processes operate on different timescales, from milliseconds (low-level control) to seconds (actuation commands, prediction lag) to minutes (training, potentially drift).

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes (Computational Adaptation during Training) / No (Material Adaptation during Operation)

**(Conditional: If M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is supervised learning applied to the LSTM network during the training phase. Specifically, the network parameters (weights and biases) are optimized using the Adam algorithm (a stochastic gradient descent optimization method) to minimize the error between the network's predictions (kinematics y(t) or force F(t)) and the ground truth values obtained from the motion capture system or load cell. Input data consists of sensor readings sd(t), reference pressure (t), and the LSTM's internal state c(t). This process adjusts the LSTM's internal representation to capture the dynamics of the soft robot and sensor system from the training examples. Regularization (L2 and dropout) is used to prevent overfitting and improve generalization. This adaptation occurs offline before deployment; the network is static during testing/operation.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors are: 1. **Kinematic State Estimation (Proprioception):** Predicting the Cartesian coordinates (tip position y(t)) of the soft actuator based on embedded sensor readings (sd(t)) and actuation commands ((t)), including adaptation to changes in kinematics due to external contact. 2. **External Force Estimation:** Indirectly estimating the magnitude of external forces (F(t)) applied to the actuator (specifically at the tip in the main experiment) using the same sensor and actuation inputs. 3. **Contact Detection (Implicit):** The system implicitly detects contact through its effect on sensor readings, allowing the kinematic model to adjust (as demonstrated by testing with tip and mid contact). 4. **Graceful Degradation:** Maintaining functional performance (though potentially degraded) in kinematic estimation even when some sensor inputs are lost, due to sensor redundancy and the learned model.

### **8.2 Behavior Robustness:**

        *   **Robustness to Sensor Nonlinearities/Drift:** Explicitly stated as an advantage of the ML approach using LSTM to handle time-variant behavior (compensated by computational memory). Performance is maintained despite these sensor issues.
        *   **Robustness to Contact:** The kinematic model performs consistently across no-contact, tip-contact, and mid-contact scenarios (Fig. 2), unlike the commercial flex sensor which degrades significantly upon contact.
        *   **Robustness to Sensor Failure (Graceful Degradation):** Simulation results (Fig. 6, Fig. 7) show that the system can maintain reasonable prediction accuracy even with the virtual removal of one or two sensors, especially in the no-contact case, due to redundancy. Performance degrades more significantly with contact when sensors are removed.
        *   **Limitations:** Robustness to variations in fabrication, environmental changes (temperature, humidity), or untrained contact locations/types is not explicitly tested. The 10 Hz sampling rate limits robustness to fast dynamic movements. The phase lag could be an issue for real-time control applications.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The behaviors (kinematic/force estimation) are validated through quantitative comparison with ground truth data from external systems (motion capture, load cell). Performance metrics include mean error (e.g., mm for position, N or % range for force) presented in figures (Fig. 2, Fig. 4, Fig. 5) and tables (table S1). Control experiments comparing cPDMS sensors with commercial flex sensors provide further validation of the approach's advantages, particularly under contact (Fig. 2, Fig. S2, S3, S4). Graceful degradation is validated through simulations involving virtual sensor removal (Fig. 6, Fig. 7). Reproducibility is implied by the description of methods, though not explicitly tested across multiple builds. The claim of "perception" emerges from the successful mapping of low-level sensor signals to higher-level state information (kinematics, force), enabled by the ML model. Limitations include testing primarily on planar motion and specific contact scenarios/locations.

---

#Key: [sasaki_large-scale_2016]

# Large-scale self-organization of reconfigurable topological defect networks in nematic liquid crystals

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of a nematic liquid crystal (NLC, specifically CCN-mn) doped with an ionic impurity (tetrabutylammonium benzoate, TBABE), confined between two parallel glass plates coated with indium-tin-oxide (ITO) electrodes. The electrodes are coated with an amorphous fluorinated polymer (CYTOP) to induce homeotropic alignment. Applying an AC voltage across the electrodes induces reorientation of the NLC director and, crucially, leads to a periodic density modulation of ions accumulated at the insulating CYTOP interface. This results in the large-scale self-organization of a tunable and reconfigurable two-dimensional square array of umbilical topological defects (+1 and -1 strength) in the NLC bulk. The system's primary purpose is to demonstrate a method for stabilizing and controlling large networks of topological defects in NLCs without relying on pre-patterned surfaces, primarily driven by ionic effects coupled with the NLC's response to the AC field. The defect array spacing is tunable via voltage, frequency, and cell thickness. Manipulation is possible via optical tweezers (laser heating) and mechanical shear flow.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | NLC Cell Thickness (d) | 2.5 - 24+ | μm | Fig 3a, Fig 8a caption | Explicit | High | Standard Interferometry (Methods) |
        | Alignment Layer Thickness (ls) | ~10 - 120 | nm | Fig 3a legend, Methods | Explicit | Medium | Spectroscopic Ellipsometry (Methods) |

    *   **Note:** Values represent typical ranges used or observed in the experiments. NLC properties (dielectric anisotropy, elastic constants) and CYTOP properties (resistivity, dielectric constant) are mentioned qualitatively or used in theoretical modeling (Fig 10 caption).

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the external AC electrical voltage applied across the ITO electrodes of the NLC cell.
    *   Value: Variable (e.g., V0 = 10-40 V, f = 50-1000 Hz)
    *   Units: Volts (Amplitude), Hertz (Frequency)

### **2.2 Energy Transduction**

    *   Content: Electrical energy input drives several coupled processes:
        1.  **Dielectric Response:** The electric field interacts with the NLC's dielectric anisotropy (negative), causing director reorientation (tilting away from the field direction above the Freedericksz threshold). This converts electrical energy into mechanical potential energy stored in the NLC's elastic deformation (Frank elastic energy).
        2.  **Ionic Transport:** The electric field drives the migration of impurity ions (TBABE).
        3.  **Field Screening/Charge Accumulation:** Due to the insulating nature of the CYTOP layers, ions accumulate near the NLC-CYTOP interface, creating surface charges (σs). This leads to a significant voltage drop across the thin CYTOP layers, especially at low frequencies, effectively screening the electric field within the bulk NLC. This couples electrical energy to charge separation and interfacial potential energy.
        4.  **Coupled Electro-Ionic-Elastic Effects:** The spatial modulation of the screened electric field (hypothesized to arise from instabilities related to ion distribution) interacts with the NLC director field, leading to the formation and stabilization of the periodic defect structures (G, S states). Energy is transduced between electrical, ionic potential, and elastic deformation energy forms in a coupled, self-organizing manner.
        5.  **Optical Tweezing (Secondary Input):** Laser energy (1064 nm) is absorbed, causing local heating, which converts optical energy to thermal energy, leading to a local phase transition to isotropic or reduced order, erasing defects and allowing re-organization upon cooling (thermal energy -> elastic energy).
        6.  **Shear Flow (Secondary Input):** Mechanical energy input via shear flow couples to the director field (flow alignment), converting mechanical energy to elastic deformation energy, disrupting the grid structure.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any quantitative assessment of energy efficiency for the pattern formation process. The primary focus is on the phenomenon and its control, not its energetic cost. Qualitatively, the process likely involves significant dissipative losses (ionic conduction, dielectric loss, viscous dissipation in NLC reorientation), suggesting low efficiency in terms of converting electrical input energy into stored elastic energy of the defect array. Assigning a score without data would be speculative.

### **2.4 Energy Dissipation**

    *   Content: Several dissipation mechanisms are present:
        1.  **Ionic Conduction:** Movement of ions through the NLC bulk represents electrical energy dissipated as heat (Joule heating), related to the NLC conductivity (σLC). This is frequency-dependent and significant, especially at lower frequencies where ion movement is more pronounced. (Qualitatively High, based on the strong frequency dependence of Vth related to conductivity).
        2.  **Dielectric Loss:** AC field interaction with the dielectric material (NLC and CYTOP) leads to dielectric relaxation losses, dissipated as heat. (Qualitatively Medium/Low compared to ionic conduction at low frequencies).
        3.  **Viscous Dissipation:** Reorientation of the NLC director involves internal friction (rotational viscosity, γ1), dissipating energy as heat. (Qualitatively Medium).
        4.  **Defect Annihilation/Movement:** Dynamics of defects (e.g., during formation, reconfiguration, or dislocation motion, Fig 9b) involve viscous dissipation. (Qualitatively Low overall energy cost, but locally significant).
        5.  **Laser Heating:** Energy from the optical tweezers is dissipated primarily as heat. (Relevant only during manipulation).
        6.  **Shear Flow:** Mechanical energy input is dissipated via NLC viscosity during flow. (Relevant only during shear).
        Quantitative values are not provided in the paper. The theoretical model implicitly includes conductivity (σLC, σs) and permittivity (εLC, εs), which relate to dissipation.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local rules governing the self-organization involve a complex interplay described theoretically in the Discussion and Methods (Eqs 4-27, Fig 10):
        1.  **NLC Director Dynamics:** Governed by minimizing the total free energy (Frank elastic energy Eq. 9 + electric field contribution Eq. 10) subject to boundary conditions (homeotropic anchoring) and external fields. Torque balance (Eq. 8) describes reorientation dynamics, influenced by dielectric anisotropy (εa < 0) and elastic constants (K1, K3). Director tends to align perpendicular to the field above threshold.
        2.  **Ion Dynamics:** Governed by drift in the electric field and diffusion (diffusion neglected in the model for simplicity). Governed by Poisson equation (Eq. 4) and charge conservation (Eq. 5, leading to Eq. 11 using complex conductivity). Anisotropic conductivity (σa) is considered.
        3.  **Field Screening:** Ions accumulate at the insulating CYTOP interface (high resistivity ρs > 10^15 Ω·m, low conductivity σs, Eq. 13). This creates a surface charge density (σs) that screens the electric field in the NLC bulk, especially at low frequencies. The voltage drop across NLC (VLC) and CYTOP (Vs) depends on complex conductivities (Eq. 11-16, Fig 3b, Fig 10a).
        4.  **Coupling:** The spatially modulated director field (Eq. 1-2) couples to the ion distribution and electric potential (Eq. 3, 20, 22). Conversely, the modulated potential/ion distribution influences the director field stability (Eq. 17, 18, 24, 26). The model suggests that the grid state (G) emerges due to this coupling when a perturbation with a non-zero wavevector (q≠0) becomes the minimum energy state (Fig 10b,c).
        5.  **Topological Constraints:** Degeneracy in the azimuthal angle of the director tilt leads to the formation of umbilical defects (s=±1).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | 1 | NLC Elasticity | K1 (Splay) | 4.5 (Model) | pN | Fig 10 caption | Explicit (Model) | Value used in theoretical model. |
    | 1 | NLC Elasticity | K3 (Bend) | 8.5 (Model) | pN | Fig 10 caption | Explicit (Model) | Value used in theoretical model. |
    | 1 | NLC Dielectric | ε∥ | 4 (Model) | - (ε0) | Fig 10 caption | Explicit (Model) | Value used in theoretical model. |
    | 1 | NLC Dielectric | ε⊥ | 11 (Model) | - (ε0) | Fig 10 caption | Explicit (Model) | Value used in theoretical model. |
    | 2, 3 | NLC Conductivity | σ∥ | 1.5 - 2.6 x 10^-6 (Model) | S/m (Ω⁻¹m⁻¹) | Fig 10 caption | Explicit (Model) | Value used in theoretical model, noted as depending on ion concentration. |
    | 2, 3 | NLC Conductivity | σ⊥ | 1.2 - 2.1 x 10^-6 (Model) | S/m (Ω⁻¹m⁻¹) | Fig 10 caption | Explicit (Model) | Value used in theoretical model, noted as depending on ion concentration. |
    | 3 | Insulator Dielectric | εs | 2 (Model) | - (ε0) | Fig 10 caption | Explicit (Model) | Value used in theoretical model (Typical CYTOP). |
    | 3 | Insulator Conductivity | σs | 10^-15 (Model) | S/m (Ω⁻¹m⁻¹) | Fig 10 caption | Explicit (Model) | Value used in theoretical model (Typical CYTOP). |

### **4.3 Global Order:**

    *   Content: The primary emergent global order is a large-scale, two-dimensional square array (lattice) of alternating +1 and -1 umbilical topological defects (the "Grid-like" or 'G' state). A related "Striped" ('S') state with one-dimensional order also emerges under different conditions. These ordered states can form large single domains, potentially on the mm or cm scale with manipulation.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| 1 | NLC Director Response to E-Field | Dielectric Anisotropy (εa = ε∥ - ε⊥) | -7 (Model) | - (ε0) | Explicit (Model) | Calculated from model ε∥, ε⊥. Drives reorientation. | Fig 10 Caption |
| 3 | Field Screening by Insulator | CYTOP Resistivity (ρs = 1/σs) | > 10^15 (Typical) | Ω·m | Explicit (Text) | High resistivity causes ion buildup & voltage drop. Crucial for low-freq Vth increase. | Discussion, Fig 10 cap. |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| G1 | Grid State Lattice Spacing (a) | Lattice Constant | ~5 - 200 | μm | Explicit | Measured distance between adjacent umbilics. Tunable. | Polarization Microscopy | Fig 8a |
| G2 | Defect Density (Grid) | ρ_defects | ~25 - 10^4 | defects/mm² | Implicit | Calculated as ~1/a². Tunable via 'a'. | Derived from G1 | Fig 8a |
| G3 | Domain Size (Single Crystal) | L_domain | ~100 μm - mm | μm / mm | Explicit | Size of uniform G-state area achieved. | Polarization Microscopy | Fig 4c, 5d |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | Local Rules -> Global State | Mapping from AC parameters (V0, f) and material properties (d, ls, ε, σ) to the observed macroscopic state (H, G, S, U). | High (State Type), Med-High (Spacing) | 7 | State Diagram Fit (Fig 2b vs Fig 10c), Spacing vs d (Fig 8a), Spacing vs V0 (Fig 8c). | Mixed | State type well predicted. Spacing predictable but sensitive. Achieving perfect single crystal state less predictable. Yoneda score reflects good but not perfect mapping fidelity. | Fig 2b, 8a, 8c, 10c |
    | Local Defect Config -> Optical Output | Mapping from the local director configuration around defects to the observed pattern under polarized light. | High | 9 | Micrograph patterns (Fig 4c-g) match expected texture for ±1 umbilics. | Explicit | Standard LC optics, well understood. | Fig 4 |
    | Local Interactions -> Dislocation Dynamics | Mapping from local elastic forces/stress to the movement of dislocations in the defect lattice. | Medium | 6 | Observed dislocation glide/climb (Fig 9b) qualitatively consistent with minimizing elastic energy/defect density, but precise dynamics not modeled. | Mixed | Observation is explicit, underlying force mapping is implicit. | Fig 9b |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 7. (Rubric: 0=No clear link; 3=Qualitative link; 5=Quantitative link for some aspects; 7=Quantitative link for main aspects, minor deviations; 9=Near-perfect quantitative mapping; 10=Perfect mapping). The model captures the phase boundaries and spacing dependence well, but doesn't fully predict domain structure or dynamics without fitting/simplifications.
    *   **Metrics:** State transition voltages/frequencies (Vth, f_transition), Defect lattice spacing (a), Domain size (L_domain), Dislocation velocity.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | AC Field Period | 1 / f (~1 - 20) | ms | Fig 2b | Explicit | Inverse of applied AC frequency (f ~ 50-1000 Hz). |
        | NLC Director Relaxation | ~ms - s | ms / s | Implicit (General LC knowledge) | Implicit | Typical timescale for NLC reorientation (depends on viscosity, elastic constants, length scales). Mentioned as slower than AC period in Methods. |
        | Ion Drift/Relaxation | ~1 / f_low (~ms - s) | ms / s | Implicit | Related to the low-frequency Vth increase. Depends on ion mobility, distance. Related to charge relaxation time τ = ε/σ. | Fig 3a, Discussion |
        | Pattern Formation Time | ~seconds - few hundred seconds | s | Fig 4 caption, Text p. 4 | Explicit | Time required for G state to form and fill the area upon changing parameters. |
        | Dislocation Motion Time | ~tens of seconds | s | Fig 9b caption | Explicit | Time observed for dislocations to move across the field of view (~100s μm). |
        | Laser-Induced Relaxation | ~seconds | s | Fig 5, Sup. Movie 2 | Explicit | Time for director to recover and pattern to reform after laser spot removal. |
        | Shear-Induced Relaxation | ~seconds | s | Fig 7b, Sup. Movie 6 | Explicit | Time for G state to recover after shear stops. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is the voltage- and frequency-controlled self-organization of nematic liquid crystal director fields into distinct macroscopic patterns: Homeotropic (H), Grid-like square array of ±1 umbilical defects (G), Striped defect array (S), and disordered Umbilical texture (U). Key behaviors include:
        1.  **State Transitions:** Reversible switching between H, G, S, U states by tuning AC voltage amplitude (V0) and frequency (f) (Fig 2b).
        2.  **Tunable Pattern Formation:** Formation of the G state with a lattice spacing controllable by cell thickness (d) and voltage (V0) (Fig 8a, c).
        3.  **Large-Area Domain Formation:** Spontaneous or guided (edge effect, laser scanning) formation of large, single-domain defect arrays (Fig 4, 5).
        4.  **Reconfiguration:** Transformation between G and S states (Fig 6, Sup. Movie 3/4) and local modification (creation/annihilation of defects) using optical tweezers (Fig 5, 6, Sup. Movie 2).
        5.  **Response to Flow:** Transition from G to S state under shear flow and recovery upon flow cessation (Fig 7, Sup. Movie 5/6).
        6.  **Dislocation Dynamics:** Formation and movement of dislocations within the defect lattice under induced stress (change in V0/f) (Fig 9).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behavior (self-organized G and S states) are primarily validated through:
        1.  **Direct Observation:** Polarization light microscopy provides visual evidence of the distinct H, G, S, U textures (Fig 2a, 4, 5, 6, 7, 8, 9). The formation process is observed (Sup. Movie 1).
        2.  **Parameter Mapping:** Systematic mapping of observed states onto the V0-f parameter space yields a reproducible state diagram (Fig 2b), confirming the link between external parameters and emergent state.
        3.  **Structural Characterization:** Analysis of micrographs (rotation of polarizers, insertion of waveplates - Fig 4c-g) and fluorescence confocal polarizing microscopy (Sup. Fig 1) confirms the square lattice structure and the ±1 nature of the umbilical defects in the G state (Fig 4h).
        4.  **Tunability Confirmation:** Experiments varying cell thickness (Fig 8a) and voltage (Fig 8c) demonstrate the predicted tunability of the grid spacing.
        5.  **Theoretical Modeling:** A theoretical model based on coupled NLC elasticity, electrostatics, and ion transport predicts the transition from H to G/U states and replicates the frequency dependence of the threshold voltage (Fig 10), supporting the proposed physical mechanism for emergence.
        6.  **Manipulation Response:** Observation of predictable responses to laser manipulation (Fig 5, 6) and shear flow (Fig 7) further validates the nature of the structures.
        *Limitations:* While the emergence is well-documented, the precise instability mechanism leading to the specific wavelength selection (grid spacing) is primarily addressed through the linear stability analysis model, not direct measurement of all local field/ion modulations. Reproducibility across different labs or minor variations in materials might exist but isn't discussed.

---

#Key: [zhang_soft_2018]

# Soft mechanical metamaterials with unusual swelling behavior and tunable stress-strain curves

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a soft mechanical metamaterial designed to exhibit controllable swelling/shrinking behavior and tunable stress-strain curves upon hydration/dehydration. It consists of horseshoe-shaped composite microstructures arranged in a periodic 2D triangular lattice. Each microstructure is a sandwich of a passive supporting layer (digital polymeric material, RGD8530), an active hydrogel layer (SUP705), and a thin encapsulation layer (elastomer, TangoBlackPlus). The purpose is to achieve large magnitude, potentially anisotropic, negative or positive swelling, and tunable mechanical responses (e.g., J-shaped stress-strain curves) potentially for applications in tissue engineering, soft robotics, biosensing, and flexible displays. The swelling/shrinking behavior arises from the hydrogel expanding/contracting upon water absorption/evaporation, which, due to the sandwich structure's designed offset, induces bending deformation in the microstructures, leading to macroscopic network expansion or contraction.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters significantly influence the swelling behavior and mechanical response as discussed extensively in the results section, particularly around Figure 2. Values represent the range explored in the study. Reliability is high as these are either design inputs or directly measured material properties.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input driving the deformation (swelling/shrinking) is the chemical potential difference between the water (or solvent) and the hydrogel material (SUP705). This drives the absorption or desorption of water molecules. For dehydration, thermal energy input from the drying oven (75°C) drives water evaporation.

### **2.2 Energy Transduction**

    *   Content: 1. **Swelling:** Chemical potential energy drives water absorption into the hydrogel (SUP705). This increases the volume of the hydrogel (chemical -> mechanical energy storage/volume work). 2. **Bending:** Due to the constraint from the stiffer supporting layer (RGD8530) and the designed offset in the sandwich structure, the hydrogel's expansion is converted into bending deformation of the composite horseshoe microstructure (mechanical energy stored in hydrogel -> elastic strain energy, primarily bending, in the composite beam). 3. **Network Deformation:** The bending of individual microstructures alters their end-to-end distance, causing macroscopic deformation (shrinkage or expansion) of the entire lattice network (elastic strain energy in beams -> macroscopic mechanical work/deformation). During dehydration, the process reverses (thermal energy drives evaporation -> hydrogel shrinks -> bending relaxes -> network recovers).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss or quantify energy efficiency. The system primarily uses energy input (chemical potential change, heat) to induce shape changes, not necessarily to perform useful external work in an energetically efficient manner typical of machines. Defining efficiency would require specifying an output work task. As a shape-changing material, efficiency is not a primary metric discussed.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation mechanisms likely include: 1. **Viscoelastic losses:** within the hydrogel (SUP705), elastomer (TangoBlackPlus), and potentially the supporting polymer (RGD8530) during deformation cycles (swelling/deswelling, mechanical loading). 2. **Internal friction:** at interfaces between layers, although the encapsulation layer aims to maintain good bonding. 3. **Heat generation:** due to viscous flow of water within the hydrogel during swelling/deswelling. 4. **Energy loss during dehydration:** Thermal energy is supplied to evaporate water, which is then lost to the environment. The paper does not quantify these losses; mechanical tests were done quasi-statically to minimize viscoelastic effects (Methods). Reversibility plots (e.g., Fig 1D) show some hysteresis, indicating dissipation (~5% residual strain after one cycle). Qualitative Assessment: Medium (due to inherent polymer viscoelasticity and swelling/deswelling dynamics).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Hydration Time (to saturation/contact) | ~45 | min | Fig. 1C, 1D | Explicit | Time given for maximum shrinkage state. |
        | Dehydration Time (near complete recovery) | ~35-40 | min | Fig. 1C, 1D | Explicit | Time given for near-initial state recovery. |
        | Mechanical Testing Loading Rate | 2 | mm/min | Methods | Explicit | Rate used to ensure quasi-static conditions. |
        | Deswelling due to Evaporation (in air, room temp) | Negligible | %/hour | Fig. S14, Methods | Explicit | Stated as negligible within the ~1 hour testing time. |
    *   **Note:** The key dynamics are related to water transport (diffusion) into/out of the hydrogel, governing the swelling/deswelling timescales.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors are: 1. **Large Negative Isotropic Swelling:** The material network shrinks significantly (up to ~ -47% linear strain, ~70% area change) upon hydration due to the conversion of hydrogel expansion into microstructure bending (Fig 1C, 1D, 2A, 2F). 2. **Large Positive Isotropic Swelling:** Achieved with different microstructure designs (e.g., negative arc angle), resulting in expansion up to ~98% linear strain (Fig 2A, 2D). 3. **Anisotropic Swelling:** By designing heterogeneous unit cells (varying geometry or hydrogel placement), the material exhibits different swelling ratios along different directions, including unusual behaviors like shrinking in one direction while expanding in the perpendicular direction (Fig 3). 4. **Tunable Stress-Strain Curves:** The macroscopic mechanical response, particularly the stress-strain curve under uniaxial tension, can be tuned by controlling the hydration level. This includes transitioning from a linear response (dry state) to a J-shaped non-linear response (hydrated state), with tunable stiffness and critical strain (Fig 1E, 1F, Fig 4).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The "unusual" swelling behaviors (negative, anisotropic, shrinking/expanding) are validated through: 1. **Experimental Observation:** Fabricated samples were hydrated/dehydrated, and their dimensional changes were measured photographically and quantified as effective strains (e.g., Fig 1C, 1D, Fig 2D-F, Fig 3). Mechanical testing provided stress-strain curves (Fig 1E, Fig 4). 2. **Computational Modeling (FEA):** Finite Element Analysis was used extensively to predict the deformation configurations, strain distributions (Fig 1C, Fig S3, Fig S4), swelling strains (Fig 2A, Fig 3), and stress-strain curves (Fig 1F, Fig 4). 3. **Analytical Modeling:** A simplified composite beam theory model was developed to predict the change in curvature and resulting effective strain (Eqs. 1-3, Fig 2A), capturing key trends. Quantitative agreement between experiments, FEA, and (where applicable) the analytical model provides strong validation for the observed behaviors being direct consequences of the designed structure and material properties. Control is implicit; behavior is compared against known hydrogel swelling and conventional materials. Reproducibility is implied rather than explicitly proven with statistical analysis. The behaviors, while unusual compared to natural materials, are engineered and predicted by mechanics models, thus not "emergent" in the sense of being unpredictably complex outcomes of simple rules.

---

#Key: [millan_topology_2025]

# Topology shapes dynamics of higher-order networks

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper presents a theoretical framework and reviews recent developments concerning the dynamics of higher-order networks, specifically focusing on simplicial and cell complexes. It explores how network topology, particularly higher-order structures (simplices like edges, triangles, tetrahedra), shapes dynamical processes defined upon them. Key components include nodes, edges, and higher-order simplices, along with dynamical variables ("topological signals") assigned to these components (e.g., phases on edges or triangles). The purpose is to understand the interplay between higher-order topology and non-linear dynamics, moving beyond traditional node-centric network analysis. It introduces concepts like the Hodge Laplacian and the Topological Dirac operator as tools to model these dynamics, highlighting phenomena such as topological synchronization, topological pattern formation, and triadic percolation.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters are fundamental to the theoretical framework discussed, not from a single experimental implementation. Values are often symbolic or depend on the specific network/model.

## M2: Energy Flow
*   **Note:** This paper focuses on abstract mathematical and dynamical properties of networks, not physical systems with explicit energy sources or detailed thermodynamics. Energy concepts are generally absent or highly abstract (e.g., Hamiltonian in Box 2).

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable / Long-term (for topology)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content:
        *   **Topological Kuramoto Model:** Local interaction rule is defined by Eq (1) (Box 2). Each n-simplex oscillator's phase (φ<sub>α</sub>) evolves based on its intrinsic frequency (ω<sub>α</sub>) and coupling terms involving the sine of phase differences with adjacent (n-1) and (n+1) simplices, mediated by boundary operators (B<sub>[n]</sub>, B<sub>[n+1]</sub>). Rule: `dφ<sub>α</sub>/dt = ω<sub>α</sub> - σ * Sum_neighbors(sin(phase_difference))`.
        *   **Topological Global Synchronization:** Interaction rules involve coupling identical oscillators (potentially chaotic) based on topological adjacency, aiming for a state where all oscillator states are identical. The specific coupling function isn't given in the excerpt, but it depends on topological operators (likely Hodge Laplacians) [41].
        *   **Topological Pattern Formation:** Local rules involve reaction-diffusion dynamics defined on topological signals (node, edge, etc.) coupled via operators like the Hodge Laplacian or Dirac operator [46, 47]. Specific reaction terms depend on the model (e.g., activator-inhibitor).
        *   **Triadic Percolation:** Local rules are a two-step algorithm: (i) nodes are active if part of the giant component formed by active edges, (ii) active nodes regulate adjacent structural edges (activate/deactivate) based on predefined triadic interactions [48, 49].
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Kuramoto | Intrinsic Frequency | ω<sub>α</sub> | Real numbers | rad/s (or dimensionless) | Box 2, Eq(1) | Explicit | Distribution typically specified (e.g., Gaussian) |
    | Triadic Percolation | Control Parameter | (Generic) p | Varies | Dimensionless | Fig 5c,d,e | Explicit | Controls transition/dynamics |
    | Pattern Formation | Diffusion Coefficients | D<sub>u</sub>, D<sub>v</sub> (example) | > 0 | space<sup>2</sup>/time | Sec "The Topological Dirac operator..." [46, 47] | Implicit | Specific values depend on model, not given in excerpt. |
    | Pattern Formation | Reaction Rates | k<sub>i</sub> (example) | > 0 | 1/time, etc. | Sec "The Topological Dirac operator..." [46, 47] | Implicit | Specific values depend on model, not given in excerpt. |

### **4.3 Global Order:**

    *   Content:
        *   **Synchronization:** Global phase/frequency locking (Kuramoto) or state identity (Global Sync). Synchronization state can be localized on topological holes (n-dimensional cycles).
        *   **Pattern Formation:** Emergence of spatially heterogeneous patterns (e.g., Turing patterns) in topological signals defined on nodes, edges, etc.
        *   **Percolation:** Emergence of a giant connected component spanning the network. In triadic percolation, this component can be dynamic, exhibiting periodic or chaotic fluctuations in size and topology.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Kuramoto | Phase coupling via boundary operators | σ, ω<sub>α</sub> | See 4.2.1 | Varies | Explicit | Parameters defined in Box 2, Eq (1) | Box 2 |
| Pattern Form. | Reaction-Diffusion on signals | Diffusion coeffs, Reaction rates | > 0 | Varies | Implicit | Generic description, specific params depend on cited models | Sec "The Topological Dirac operator..." |
| Global Sync | Oscillator coupling via topology | Coupling strength (implicit) | > 0 | Varies | Implicit | Mechanism described qualitatively, specific func/params from cites | Sec "Topology shapes dynamics..." |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Sync-1 | Kuramoto Synchronization | Global Order Param (X<sup>±</sup>) | 0 to 1 | Dimensionless | Explicit | Defined in Box 2 | Calculation from phases | Box 2 |
| Sync-2 | Kuramoto Synchronization | Local Order Param (X<sub>o</sub>) | 0 to 1 | Dimensionless | Explicit | Defined in Box 2 caption | Calculation from phases | Box 2, Fig 3 |
| Pattern-1 | Topological Patterns | Pattern Wavelength/Amplitude | > 0 | Length / Signal Units | Implicit | Characteristic of patterns, not quantified generically | Analysis of signal distribution | Sec "The Topological Dirac operator..." |
| Perk-1 | Percolation | Giant Component Size (S) | 0 to N<sub>0</sub> | Nodes | Explicit | Standard percolation order parameter | Network analysis | Sec "Topology is dynamical", Fig 5 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Skipping M5.2-5.4 as M5.1 is "No")**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Oscillator Period (Intrinsic) | 2π/ω<sub>α</sub> | time | Box 2 (Kuramoto) | Explicit | Derived from intrinsic frequency |
        | Synchronization Time | Variable | time | Fig 3, Sec "Topology shapes dynamics..." | Implicit | Timescale for system to reach sync state; model/parameter dependent |
        | Pattern Formation Time | Variable | time | Sec "The Topological Dirac operator..." | Implicit | Timescale for patterns to emerge/stabilize; model/parameter dependent |
        | Triadic Percolation Dynamics | Variable | time steps | Fig 5e, Sec "Topology is dynamical" | Explicit | System evolves in discrete or continuous time; can be periodic or chaotic |
        | Relaxation Times | Variable | time | General Dynamics | Implicit | Characteristic times to reach steady states or attractors; model dependent |
    *   **Note:** Specific values are generally not provided as the paper covers a range of phenomena. Timescales depend heavily on the specific model, parameters, and network structure.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes (Partial)

### **7.2 Adaptation Mechanism:**

    *   Content: In triadic percolation, the adaptation mechanism is rule-based structural change. Active nodes (part of the current giant component) either enhance or inhibit the "active" status of structural edges they participate in regulating, according to predefined signed triadic interaction rules. This changes the set of active edges, thus altering the network topology (specifically, the giant component) for the next time step. This is a form of feedback where the dynamical state (node activity defined by global structure) modifies the structure itself.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content:
        *   **Topological Synchronization:** Oscillators on nodes/edges/simplices synchronize their phases or states, potentially localized on topological holes.
        *   **Topological Pattern Formation:** Spatially non-uniform patterns emerge in signals defined on topological elements (nodes, edges, etc.).
        *   **Triadic Percolation:** A giant connected component forms dynamically, potentially exhibiting periodic or chaotic fluctuations in size and topology over time.
        *   **Higher-Order Diffusion / Random Walks:** Transport processes constrained by higher-order topology, potentially separating into irrotational/solenoidal components.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper primarily validates claims through theoretical analysis, mathematical derivations (e.g., using Hodge Laplacians, Dirac operators, stability analysis implicitly referenced), and computational simulations (implied by figures like Fig 3, Fig 4, Fig 5). For instance, the link between Betti numbers and synchronization is a theoretical result. The chaotic dynamics in triadic percolation (Fig 5d,e) are likely validated via simulations. The paper cites numerous external references [e.g., 40, 41, 46, 47, 48, 49, 85] which contain detailed validations (mathematical proofs, simulation results) for specific models. Within the excerpt itself, validation is primarily conceptual and illustrative.

---

#Key: [hanczyc_fatty_2007]

# Fatty Acid Chemistry at the Oil−Water Interface:  Self-Propelled Oil Droplets

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of oil droplets (either 0.5 M oleic anhydride in nitrobenzene or pure oleic anhydride) introduced into an aqueous solution containing oleate micelles (10 mM oleate at pH 11). The purpose is to investigate autonomous movement driven by fatty acid chemistry at the oil-water interface. The oil droplets exhibit sustained movement, internal convection, shedding of lipid material (forming vesicles), and directional movement in response to externally imposed pH gradients (chemotaxis). The movement is initiated by the hydrolysis of oleic anhydride at the oil-water interface, which produces oleic acid. This reaction, coupled with Marangoni effects driven by surface tension gradients (likely influenced by the reaction products and self-generated pH gradients), creates internal convection and propels the droplet. A positive feedback loop is proposed where convection brings fresh precursor to the interface, sustaining the reaction and motion.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name           | Value             | Units        | Source (Fig/Table/Section)   | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :----------------------- | :---------------: | :----------: | :-------------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
    *   **Note:** Room temperature is mentioned implicitly by stating the hydrolysis occurs at room temperature. Other parameters like droplet velocity or convection speed vary and are qualitatively described or shown in time-lapse figures rather than given as fixed values.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the chemical potential energy released during the hydrolysis of oleic anhydride into oleic acid at the oil-water interface. The reaction is: Oleic Anhydride + H₂O → 2 Oleic Acid. This is an exothermic reaction, especially upon subsequent deprotonation of the acid at high pH.

### **2.2 Energy Transduction**

    *   Content: Chemical potential energy from hydrolysis is transduced into mechanical energy driving fluid flow (internal convection and external droplet motion) and interfacial energy changes. The mechanism involves the creation of reaction products (oleic acid/oleate) at the interface, leading to local changes in interfacial tension and potentially pH. These gradients in interfacial tension drive Marangoni flow along the interface, which in turn drives convection within the droplet and propels the droplet through the surrounding fluid. Some energy is also used in the formation of new interfaces (vesicles). Heat generated might also contribute via thermocapillary effects, though this is mentioned as a possibility rather than the primary driver.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any quantification of energy input (e.g., reaction enthalpy consumed per unit time) or useful work output (e.g., force x velocity, kinetic energy of fluid). Efficiency cannot be calculated or reliably estimated. Qualitatively, much energy is likely dissipated as heat and through viscous drag.

### **2.4 Energy Dissipation**

    *   Content: Energy is primarily dissipated through viscous friction within the convecting oil droplet, viscous drag as the droplet moves through the aqueous medium, and heat loss to the surroundings (implied from the exothermic nature of the reaction/neutralization). The formation and shedding of vesicles represent another pathway where energy associated with interface creation is effectively 'lost' from the droplet's propulsion system. Quantification is not provided. Assessment: Likely High dissipation due to viscous effects in fluid motion.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes
        1.  **Internal Convection:** The ordered, sustained flow pattern within the droplet emerges spontaneously from the interplay of reaction, diffusion, and Marangoni stresses at the interface. It's not imposed externally.
        2.  **Internal Structures:** The formation of aqueous structures (likely reverse micelles or water pockets containing calcein) within the oil phase is a spontaneous process driven by surfactant behavior and water ingress.
        3.  **Self-Generated pH Gradient:** The spatial pattern of pH around the moving droplet emerges from the localized reaction and subsequent transport/diffusion processes.
        4.  **Vesicle Formation:** The aggregation of shed lipid material into multilamellar vesicles is a classic example of self-assembly driven by amphiphile thermodynamics.
        5.  **Sustained Motion:** The directional movement itself is an emergent property arising from the breaking of symmetry and the coupling of the processes above.

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content:
        1.  **Hydrolysis:** Oleic anhydride at the oil-water interface reacts with water to form oleic acid (rate likely dependent on local precursor concentration, water availability, and pH). `Oleic Anhydride + H₂O → 2 Oleic Acid` (at interface).
        2.  **Deprotonation:** Oleic acid produced is deprotonated at high pH, forming oleate ions and consuming OH⁻ (or releasing H⁺ effectively lowering local pH). `Oleic Acid + OH⁻ ⇌ Oleate⁻ + H₂O`.
        3.  **Interfacial Tension Variation:** Local interfacial tension depends on the concentration of surfactant (oleate) and potentially pH and temperature. Increased oleate concentration generally lowers interfacial tension.
        4.  **Marangoni Stress:** Gradients in interfacial tension (`∇γ`) exert a tangential stress on the interface, driving fluid flow proportional to the gradient. Flow occurs from regions of low tension to high tension.
        5.  **Convection Coupling:** Interfacial flow drives bulk fluid flow (convection) within the droplet and potentially in the external medium, transporting reactants (oleic anhydride from interior) and products (oleate/H⁺ away).
        6.  **Diffusion:** Reactants, products, and heat diffuse according to Fick's and Fourier's laws, respectively.
        7.  **Surfactant Adsorption/Desorption:** Oleate molecules adsorb to and desorb from the interface, influencing local tension. Dynamics depend on bulk/interfacial concentrations and kinetics. Possible formation of reverse micelles within the oil phase also involves surfactant interactions.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID             | Description                      | Parameter Name         | Parameter Value Range | Units      | Data Source          | Implicit/Explicit   | Justification                                     |
    | :------------------ | :------------------------------- | :--------------------- | :-------------------- | :--------- | :------------------- | :------------------ | :------------------------------------------------ |
    | 2 (Deprotonation)   | Acid-Base Equilibrium            | pKa (Oleic Acid)       | ~8.5                  | pH units   | Ref 12 / Literature  | Implicit          | pKa value stated as ~8.5 from Ref 12.           |

### **4.3 Global Order:**

    *   Content: The primary emergent global order is the **sustained, directional locomotion** of the oil droplet, coupled with a persistent **internal convection cell** (toroidal flow pattern inferred from Fig 3 and 4E) and a stable, trailing **pH gradient** and stream of shed **vesicular structures**. The droplet transitions from a symmetric, quiescent state to an asymmetric, motile state.
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID             | Description                      | Parameter             | Value Range | Units             | Implicit/Explicit   | Justification                                      | Source               |
| :------------------ | :------------------------------- | :-------------------- | :---------- | :---------------- | :------------------ | :------------------------------------------------- | :------------------- |
| 7 (Surfactant Dyn)  | Adsorption/Desorption/Micelles   | Surfactant Conc.      | 10 mM (bulk)| mM                | Explicit (bulk)   | Bulk concentration given, dynamics discussed.    | Exp. Section       |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID   | Description              | Parameter        | Value Range | Units       | Implicit/Explicit   | Justification                                               | Protocol                 | Source        |
| :------------ | :----------------------- | :--------------- | :---------- | :-------- | :------------------ | :---------------------------------------------------------- | :----------------------- | :------------ |
| Chem. Gradient| pH Gradient Magnitude    | ΔpH              | ~4 (est.)   | pH units  | Mixed             | Low pH zone observed (Fig 2C-E), estimated diff in text. | Fluorescence Microscopy | Fig 2C-E, Text|

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type         | Description                                             | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification                                                                            | Source        |
    | :---------------- | :------------------------------------------------------ | :------------- | :----------- | :------ | :---------------- | :--------------------------------------------------------------------------------------- | :------------ |


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description        | Value              | Units        | Source     | Implicit/Explicit   | Justification                                                      |
        | :--------------------------- | :----------------: | :----------: | :--------- | :------------------ | :----------------------------------------------------------------- |
        | Initial Structure Formation  | ~20                | s            | Fig 2A-B   | Mixed             | Explicit images at 5s and 25s show significant structure formation. |
        | Initiation of Movement       | Seconds to minutes | s - min      | Text       | Mixed             | "Initially...episodically...Once...in phase...began to move".    |
        | Frame Interval (Motion Seq)  | 2                  | s            | Fig 4      | Explicit          | Explicitly stated in Fig 4 caption.                                |
        | Frame Interval (pH Trail)    | ~1                 | s            | Fig 2C-E   | Mixed             | Inferred from time stamps (0s, 10s, 11s).                          |
        | Frame Interval (Phase Trans) | 0.2                | s            | Fig 6      | Explicit          | Explicitly stated in Fig 6 caption.                                |
        | Duration of Movement         | Minutes to hours   | min - h      | Text       | Mixed             | "stopped moving but internal movement continued for many minutes more", "remains after many hours". |
        | Internal Convection Duration | Minutes (post motion)| min        | Text       | Explicit          | "...internal movement continued for many minutes more."              |
        | Reaction Completion Time     | Hours ?            | h            | Text       | Mixed             | "hydrolysis often occurs long after...stop moving", "pure anhydride...complete consumption". |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main emergent behaviors are:
        1.  **Autonomous Locomotion:** Sustained, self-propelled movement of the oil droplet through the aqueous phase.
        2.  **Internal Convection:** Organized, continuous flow patterns within the oil droplet, coupled to the motion.
        3.  **Chemotaxis-like Gradient Following:** Directional movement in response to externally imposed or self-generated pH gradients (moving towards higher pH, away from lower pH).
        4.  **Vesicle Production/Shedding:** Continuous release of lipid material, which self-assembles into vesicles, often forming a trailing structure.
        5.  **Phase Transition (Pure Anhydride):** Transformation of large pure oleic anhydride droplets into aggregate structures under certain conditions.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors (motion, convection, gradients, vesicles) are primarily validated through direct microscopic observation (Phase Contrast, Fluorescence, DIC - Figs 2, 4, 6). Time-lapse imaging demonstrates the dynamics (Figs 2C-E, 4A-D, 6). The use of fluorescent dyes (calcein, fluorescein) helps visualize internal structures (Fig 4E) and pH gradients (Fig 2C-E). Control experiments (pure nitrobenzene, oleic acid instead of anhydride, different fatty acids, different pH) are used to establish necessary conditions (hydrolysis reaction, liquid surfactant state) and support the proposed mechanisms (Figs 5). Size exclusion chromatography confirms vesicle formation (Fig 2H-I). While observations are clear, quantitative analysis of the dynamics (e.g., flow fields, precise gradient profiles, correlation metrics) is limited. Reproducibility is implied but not explicitly quantified (e.g., number of runs, statistical analysis of velocities).

---

#Key: [mcconnell_effects_1959]

# The effects of regeneration upon retention of a conditioned response in the planarian.

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is the planarian flatworm, *Dugesia dorotocephala*, subjected to a classical conditioning paradigm. The purpose is to investigate whether a conditioned response (CR), established through pairing light (Conditioned Stimulus, CS) with electric shock (Unconditioned Stimulus, US), is retained after the planarian is cut in half and allowed to regenerate into two complete organisms. The components include the planaria, aquaria for housing, a conditioning apparatus (plastic trough with electrodes and light source), and the experimental procedure involving conditioning, cutting, regeneration, and retesting. The system *does* demonstrate learning (acquisition of a CR) and memory retention through regeneration.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name           | Value                        | Units         | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :----------------------- | :---------------------------: | :-----------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy inputs related to the *experimental manipulation* are electrical energy for the Unconditioned Stimulus (US - shock) and light energy for the Conditioned Stimulus (CS). The biological system (planarian) also relies on metabolic energy derived from feeding (not detailed in the excerpt) for survival, movement, regeneration, and neural activity.

### **2.2 Energy Transduction**

    *   Content: Electrical energy from batteries is transduced via electrodes into ionic current flow through the water, stimulating the planarian (presumably nerve/muscle cells). Light energy from bulbs is transduced by the planarian's photoreceptors (eyespots) into biochemical signals and neural impulses. Metabolic energy is transduced through biochemical pathways to fuel all biological processes including movement (ciliary action), neural firing, protein synthesis for regeneration, etc. The conditioning process involves transduction of sensory signals into changes in neural circuitry (learning/memory formation).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Extremely low efficiency in the context of material intelligence. The focus is on behavioral response, not energy conversion efficiency. Biological systems are inherently inefficient in converting external stimuli energy into specific computational or behavioral outputs compared to engineered systems. Metabolic energy efficiency for regeneration or learning is not discussed or measured.

### **2.4 Energy Dissipation**

    *   Content: Electrical energy dissipates as heat due to the resistance of the water and the planarian's tissues. Light energy is absorbed and partially dissipated as heat. Significant energy is dissipated as heat through metabolic processes required for maintaining life, movement, regeneration, and neural activity (entropy production). These are not quantified. Qualitative assessment: High (inherent in biological systems).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: ~4
*    Units: weeks (Qualitative Descriptor: Long-term relative to experiment duration)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: High (qualitative)

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Low (qualitative)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Observer Reliability | Consistency in identifying the CR | 85-100% agreement | % | `ReadoutEdge` attribute: `observer_agreement`: High | Results (Table 4) | Explicit | Explicitly measured and reported. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The paper does not explicitly describe the local rules governing regeneration. These would involve complex cascades of gene expression, cell signaling pathways (e.g., Wnt, BMP, FGF), cell adhesion dynamics, and cell-matrix interactions that orchestrate cell fate decisions, patterning, and morphogenesis. These rules are intrinsic to the planarian's biology.
    * **Implicit/Explicit**: Implicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The globally emergent order is a complete, morphologically normal, and behaviorally functional planarian organism (*Dugesia dorotocephala*), regenerated from either a head or tail fragment. This includes the reformation of all necessary tissues and organs, including the central nervous system (cephalic ganglia, nerve cords) and sensory structures (eyespots).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description            | Parameter         | Value Range | Units | Implicit/Explicit | Justification                      | Protocol | Source |
| :---------- | :--------------------- | :---------------- | :---------- | :---: | :----------------: | :---------------------------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Skipping M5.2-5.4 as M5.1 is "No")**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description    | Value | Units | Source           | Implicit/Explicit | Justification                                    |
        | :----------------------- | :---: | :---: | :--------------- | :----------------: | :----------------------------------------------- |
        | CS Presentation (Light)  | 2     | s     | Method-Procedure | Explicit          | Stated duration of light alone.                  |
        | US Presentation (Overlap)| 1     | s     | Method-Procedure | Explicit          | Stated duration of light+shock.                  |
        | Intertrial Interval (Avg)| ~84   | s     | Results          | Explicit          | Average time between trials reported.            |
        | Behavioral Response (CR) | < 2   | s     | Method-Procedure | Implicit          | Response must occur within the 2s CS period.      |
        | Single Session Duration  | Max ~1.5 | hours| Method-Procedure | Implicit          | Max 50 trials * avg ~84s/trial = ~70 mins.       |
        | Regeneration Time        | ~4    | weeks | Method-Procedure | Explicit          | Stated duration allowed for regeneration/recovery.|
        | Memory Retention         | >=4   | weeks | Results          | Explicit          | Memory demonstrated after 4 weeks.               |
        | Learning Acquisition     | 83-325| trials| Results (Table 1,3)| Explicit          | Varies between individuals.                      |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism is classical conditioning (associative learning). The paper suggests the cephalic ganglia ("brain") are necessary for *acquisition* but possibly not for *retention*. The surprising retention in regenerated tails (which initially lack ganglia) leads the authors to speculate that the memory storage mechanism might involve structural changes throughout the nervous system, or potentially non-neural mechanisms (the paper doesn't specify, but later work in the field suggested RNA). The exact molecular/cellular mechanism of plasticity is not determined in this paper.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior studied is the Conditioned Response (CR) elicited by the light stimulus (CS) after conditioning. This CR consists of observable motor actions: a sharp turn of the cephalic region, a longitudinal contraction of the body, or a combination where the head turns and the tail contracts. The Unconditioned Response (UR) to shock is a longitudinal contraction. Basic locomotion (gliding) is also described.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The existence and retention of the Conditioned Response (CR) behavior were validated through several means:
        1.  **Operational Definition:** The CR was operationally defined by specific observable motor patterns (turning, contraction) occurring during the CS presentation period (Results).
        2.  **Control Groups:** A Regeneration Control (RC) group (cut/regenerated without prior training) controlled for sensitization effects of cutting/regeneration. A Time Control (TC) group (trained, rested uncut) controlled for simple forgetting over time (Method, Results). Both controls showed significantly different results from the experimental group (E) where expected (RC took longer to train than E retest; TC showed savings similar to E retest).
        3.  **Quantitative Analysis:** Statistical tests (t-tests) were used to compare trials-to-criterion between groups and conditions, demonstrating significant savings (p < .01) in the experimental group (Results).
        4.  **Reliability:** An inter-observer reliability study confirmed that the CR could be consistently identified (Table 4, Results).
        *Limitations:* The study doesn't fully elucidate the *mechanism* underlying the behavior's emergence or retention (especially in tails). The interpretation relies on behavioral data.

---

#Key: [monzel_energetics_2024]

# The energetics of cellular life transitions

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is the biological cell undergoing identity transitions, specifically focusing on the interplay between cellular bioenergetics (primarily mitochondrial oxidative phosphorylation - OxPhos) and stress signaling pathways (specifically the Integrated Stress Response - ISR). The paper hypothesizes that cellular transitions (like differentiation) are energetically costly and that the ISR acts as an energetic checkpoint, potentially preventing energetically compromised cells (e.g., with OxPhos defects) from undertaking these costly transitions. Key components discussed include mitochondria, the OxPhos system, NADH/NAD+ ratio (as a sensor of reductive stress), the ISR pathway components (eIF2, ATF4, CHOP, GDF15, FGF21), and cellular states (stem-like, transitional, differentiated). The purpose is to propose a perspective on how cellular energy status, monitored and signaled via pathways like the ISR, governs major cell fate decisions during development and potentially disease. It specifically examines the hypothesis using the example of mouse lung alveolar epithelial cell (AT2 to AT1) transition. It does *not* describe an engineered material system but rather biological processes viewed through an energetic and signaling lens.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is chemical energy derived from metabolic substrates (e.g., glucose, fatty acids, amino acids) that provide electrons (hydrocarbons) for oxidative phosphorylation.

### **2.2 Energy Transduction**

    *   Content: The main energy transformation discussed is mitochondrial oxidative phosphorylation (OxPhos). This process transduces chemical energy stored in electron carriers (like NADH, derived from substrate metabolism) into a proton-motive force (transmembrane electrochemical gradient) across the inner mitochondrial membrane. This gradient's potential energy is then used by ATP synthase to phosphorylate ADP to ATP, converting electrochemical potential energy into the chemical energy of ATP's phosphate bond. The paper also implicitly discusses glycolysis as an alternative (less efficient) ATP source when OxPhos is defective.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper explicitly states that OxPhos defects increase the "cost of living," causing hypermetabolism and reduced lifespan in cells [57]. This implies significantly reduced efficiency of energy transduction (more substrate consumed for the same amount of useful work/ATP or maintenance). While OxPhos itself is relatively efficient under normal conditions, the focus here is on the *inefficiency* caused by defects, leading to stress. No specific efficiency percentage is given for the defective state in the excerpt. Score reflects the described inefficiency under stress/defect conditions. Qualitative Assessment: Low (in defective cells).

### **2.4 Energy Dissipation**

    *   Content: The paper implies increased energy dissipation in cells with OxPhos defects through the concept of "hypermetabolism" [57, 58] and the increased "cost of living." This suggests that energy derived from substrates is dissipated less efficiently, likely as heat, due to compensatory mechanisms or inefficiencies in alternative pathways when OxPhos is impaired. The stress responses themselves (e.g., ISR activation, protein synthesis/secretion like GDF15) are also described as energetically costly, representing another form of energy expenditure/dissipation from the cell's perspective, diverting energy from baseline maintenance or transitions. Specific mechanisms like proton leak or heat production are not detailed in the excerpt. Qualitative Assessment: High (in defective/stressed cells).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The primary local rule focused on is the activation of the Integrated Stress Response (ISR) pathway within a cell based on specific intracellular conditions. The paper highlights:
        1.  **Trigger:** Elevated NADH/NAD+ ratio (reductive stress), often resulting from mitochondrial OxPhos defects (specifically complex I deficiency in the example). Other triggers like nutrient deprivation, misfolded proteins are also mentioned generally.
        2.  **Mechanism:** Stress sensing leads to phosphorylation of eIF2, signaling through ATF4/ATF5/CHOP to upregulate downstream genes (e.g., GDF15, FGF21, ~120 others mentioned in Han et al.).
        3.  **Consequence:** Inhibition of cell fate transition (acting as a checkpoint), secretion of signaling molecules (GDF15, FGF21) for systemic communication (e.g., to the brain).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The emergent global order is correctly differentiated lung tissue capable of respiration, involving the presence and correct spatial arrangement of terminally differentiated AT1 cells derived from AT2 precursors. The alternative (disordered) state is developmental failure with an accumulation of cells in a "transitional" state, leading to non-functional lungs and organismal death [6]. A related global order discussed is organismal energy homeostasis, influenced by ISR signaling via GDF15 to the brain.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | Local ISR -> Global Tissue State | How intracellular ISR activation state determines the overall tissue differentiation status (e.g., functional lung vs. blocked transition). | High (in model [6]) | 6 | Cell type ratios, Survival Rate | Mixed | Predictability explicitly shown in ref [6]. Yoneda score estimated based on clear link but biological complexity. | Section: The ISR in mouse lung development |
    | Local ISR -> Systemic Signaling | How cellular ISR activation (specifically GDF15 secretion) influences organism-level physiology via brain signaling. | Medium/Unclear (Complex physiology) | 4 | Circulating GDF15 levels, Downstream HPA axis markers (Corticosterone [90]) | Mixed | Link explicitly proposed, but predictability/fidelity of full systemic response less clear from excerpt. Yoneda score reflects proposed link but lack of full system characterization. | Section: The ISR as a brain–body signaling mechanism |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 5 (Average estimate. Rubric: 0=No relation, 3=Correlation proposed, 5=Mechanism proposed/partially shown, 7=Mechanism quantitatively linked, 10=Fully predictable isomorphic mapping). The ISR->Tissue link is better characterized (Score 6) than ISR->Systemic (Score 4) in the provided text.
    *   **Metrics:** Cell type ratios determined by scRNA-seq or histology, organism survival rate, circulating hormone levels (GDF15, Corticosterone).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid (Biochemical signaling networks often involve analog signal transduction - concentrations, phosphorylation levels - but can have threshold-like effects and downstream digital-like gene expression outputs). Could also be considered Neuromorphic in a broad sense (stress integration, signaling cascade).

### **5.3 Computational Primitive:**

    *   Content: The most basic operation appears to be **Signal Integration and Thresholding**. The ISR pathway integrates various stress signals (e.g., reductive stress via NADH/NAD+, misfolded proteins, nutrient deprivation). This integrated signal likely needs to cross a threshold to trigger the full downstream response (eIF2 phosphorylation, ATF4 activation, differentiation block). The upstream sensing of NADH/NAD+ could be considered a form of **Sensing/Transduction**. The downstream effects involve **Gene Expression Regulation**.
    *   **Sub-Type (if applicable):** Thresholding (Stress level required for ISR activation), Signal Integration (Convergence of different stress signals on pathway).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Developmental Transition (Human) | Years (1-15) | years | Section: Life transitions cost energy; ref [5] | Explicit | Explicitly stated period of high energy expenditure linked to development. |
        | ISR State Persistence (Chronic) | Weeks/Months+ (in disease models) | months | Section: Physiological roles of the ISR; Ref [6] (mice lived up to 25 months) | Mixed | Implied by chronic nature of defects and long-term consequences [6], explicit mention of "chronicity". |
        | Organism Lifespan (OxPhos Defect - Cell Culture) | Reduced (relative) | time | Section: Mitochondria, energy,...; ref [57] | Explicit | Explicitly stated finding from ref [57]. |
    *   **Note:** Biological timescales are highly variable and context-dependent. Values here are estimates or qualitative descriptions based on the text.

### **6.2 Active Inference:**

    *   Content: Unclear/Partial

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism is the activation of the Integrated Stress Response (ISR) signaling pathway. It is driven by feedback from the cell's internal state (e.g., elevated NADH/NAD+ ratio indicating reductive stress/OxPhos dysfunction). Specific molecular events include phosphorylation of eIF2, leading to preferential translation of transcription factors like ATF4 and ATF5. These factors then drive the expression of a suite of genes aimed at mitigating the stress and promoting survival. This includes genes involved in amino acid metabolism, antioxidant response, and signaling molecules like GDF15 and FGF21. The net effect relevant to the paper's focus is the alteration of cell behavior, notably the inhibition of energetically costly processes like terminal differentiation, acting as a survival strategy. It's a programmed cellular stress response pathway.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors discussed are:
        1.  **Cell Fate Transition / Differentiation:** The process of a cell changing from one type to another (e.g., AT2 to AT1).
        2.  **Maintenance of Cellular Integrity/Homeostasis:** Baseline processes keeping the cell alive.
        3.  **Stress Response Activation (ISR):** The specific signaling and gene expression changes triggered by stressors.
        4.  **Inhibition of Differentiation:** The specific outcome of pathological ISR activation in the developmental context discussed.
        5.  **Systemic Signaling:** Secretion of factors like GDF15 to communicate cellular stress to the rest of the organism (e.g., brain).
        6.  **Hypermetabolism:** Increased energy expenditure observed in cells/organisms with OxPhos defects.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper primarily relies on citing experimental work, particularly Han et al. [6], to validate its claims about emergent behavior (differentiation failure). Methods used in the cited work likely include:
        *   **Genetic manipulation:** Cell-type-specific knockout of Ndufs2.
        *   **Single-cell RNA sequencing (scRNA-seq):** To map cellular states and identify transitional populations and ISR gene expression (Fig 2a refers to scRNA-seq).
        *   **Pharmacological inhibition/activation:** Use of small molecules to modulate OxPhos or ISR.
        *   **Metabolic measurements:** Assessing NADH/NAD+ ratio (implied).
        *   **Phenotypic analysis:** Assessing lung structure (histology) and organism survival.
        *   Control experiments (e.g., wild-type mice, rescue experiments like Ndi1 overexpression) are crucial parts of the cited study [6].
     The validation seems robust for the specific model system (mouse lung development with complex I defect). Limitations include potential differences in other cell types/species and complexity of in vivo physiology beyond the specific defect.

---

#Key: [lee_shape_2022]

# Shape memory in self-adapting colloidal crystals

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of large (>100 µm) body-centred cubic (bcc) colloidal crystals self-assembled from gold nanoparticles (AuNPs, 5 nm or 10 nm) functionalized with specific DNA sequences (Programmable Atom Equivalents or PAEs). The DNA linkages allow the crystals to undergo substantial anisotropic deformation (wrinkling, creasing) upon dehydration and rapidly (< seconds) recover their original well-faceted rhombic dodecahedron morphology and internal nanoscale order upon rehydration. This shape memory behaviour is accompanied by reversible changes in optical properties, transitioning from near-perfect broadband absorption in the intact/recovered state to significantly increased reflection in the deformed state. The purpose is to study and demonstrate shape memory and reversible optical properties in DNA-engineered colloidal crystals, leveraging the flexibility and programmability of DNA bonds.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Recovery Time (Rehydration) | seconds | s | Abstract, Fig. 1c,d | Explicit | Medium | Qualitative observation |
        | Unit Cell Parameter (Intact, 5nm PAE) | ~221-227 | Å | Fig. 3a, Extended Data Fig. 7a | Explicit | High | SXRD |
        | Unit Cell Parameter (Dehydrated, 5nm PAE) | ~164 | Å | Fig. 3a, Extended Data Fig. 7a | Explicit | High | SXRD |

    *   **Note:** Recovery time is explicitly stated as "seconds" but not precisely quantified. Unit cell parameters show slight variations between measurements.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: 1. Thermal energy gradient (cooling) for initial crystal self-assembly. 2. Chemical potential gradient change associated with solvent (water) removal (dehydration) and addition (rehydration), driving deformation and recovery. Surface tension forces during drying also contribute mechanical energy input. 3. Photon energy for optical measurements and characterization (microscopy, SXRD). 4. Mechanical energy input during AFM nanoindentation. Primary drivers for shape change are related to hydration state changes.

### **2.2 Energy Transduction**

    *   Content: 1. **Assembly:** Thermal energy removal drives DNA hybridization (chemical bond energy release, entropy change), leading to ordered structure formation (potential energy minimization). 2. **Deformation:** Removal of water (change in chemical potential, work done by surface tension) leads to DNA conformational changes (distortion/dissociation), resulting in stored elastic energy (mechanical strain) within the deformed crystal lattice. 3. **Recovery:** Addition of water (chemical potential change) allows DNA rehybridization and conformational relaxation, releasing stored elastic energy as the crystal returns to its lower-energy crystalline state. 4. **Optical:** Changes in crystal structure (lattice parameter, inhomogeneity) alter the interaction with incident photons (photon energy), modifying absorption and reflection (conversion to plasmonic excitation/heat or scattering). 5. **Mechanical (AFM):** Applied mechanical energy deforms the crystal lattice (elastic/plastic deformation), measured as force-displacement.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Energy efficiency is not quantified in the paper for any process (assembly, deformation, recovery). The recovery process appears qualitatively efficient in terms of restoring the original shape, but thermodynamic efficiency is not discussed. FDTD simulations calculate optical absorption/reflection but not overall energy balance or efficiency.

### **2.4 Energy Dissipation**

    *   Content: 1. **Assembly:** Heat released during DNA hybridization. 2. **Dehydration:** Latent heat of vaporization during water evaporation. Potential energy loss due to irreversible plastic deformation or damage if deformation is too severe (though the focus is on reversible deformation). 3. **Recovery:** Heat released during DNA rehybridization. Viscous dissipation within the hydrated structure during shape change. 4. **Optical:** Energy absorbed by AuNPs converted to heat via plasmon decay. 5. **Mechanical (AFM):** Hysteresis in force-displacement curves would indicate energy dissipation (viscoelasticity, plasticity), though not shown or discussed. Dissipation is not quantified. Qualitatively, evaporation is likely a major dissipation pathway during dehydration.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**

    The memory is not easily re-writable into arbitrary shapes; it's a return to a pre-defined ground state. The low score reflects the limited capacity and lack of arbitrary writability compared to high-fidelity memory systems.

### **3.3 Memory Retention Time:**

*   Value: Variable (state-dependent)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: ~2 (Qualitative)
*   Units: distinct states

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: High (Qualitative)

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Solidity | Ratio of area to convex hull area; measure of overall shape regularity | ~0.98 (Intact/Rehydrated), 0.83-0.96 (Dehydrated) | Dimensionless | `MemoryReadoutMetricNode` | Fig. 2a | Explicit | Measures morphological recovery |
    | Convexity | Ratio of convex hull perimeter to real perimeter; measure of boundary texture | ~0.98 (Intact/Rehydrated), 0.83-0.96 (Dehydrated) | Dimensionless | `MemoryReadoutMetricNode` | Fig. 2a | Explicit | Measures morphological recovery |
    | Average Curvature | Mean curvature along crystal boundary | ~2.53e-3 (Intact), ~2.60e-3 (Rehydrated) | rad/unit length | `MemoryReadoutMetricNode` | Fig. 2b | Explicit | Measures morphological recovery |
    | Cyclic Stability | Ability to undergo repeated deformation/recovery cycles | Maintained integrity for >= 6 cycles (some crystals) | Cycles | `MemoryNode` attribute: `robustness` | Supplementary Video 1 | Explicit | Qualitative assessment of robustness |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rule is the specific hybridization between complementary DNA sticky ends (A-type 'AAGGAA' and B-type 'TTCCTT') attached to the AuNPs. These interactions are thermodynamically driven and temperature-dependent (melting temperature Tm). The design of these sequences, along with particle size and DNA shell density/length, favors the formation of the non-close-packed bcc structure over other possibilities (like fcc). Linker strands mediate interactions. Interactions occur in a specific buffer environment (0.5 M NaCl, 0.01 M phosphate buffer, 0.01 wt% SDS). Rules also include excluded volume effects between the nanoparticle cores and DNA shells.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | ExclVol | Excluded Volume | PAE Core Diameter (5 or 10 nm) + DNA shell (~6 nm radius) | ~17 or ~22 nm | nm | Fig. 1a, Methods | Mixed | Core size explicit, DNA length explicit, overall excluded volume diameter is inferred. |

### **4.3 Global Order:**

    *   Content: The globally emergent order is a large (>100 µm), well-faceted single crystal with a body-centred cubic (bcc) lattice structure (Im-3m space group). The crystal habit is typically rhombic dodecahedron.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| DNA-Hyb | Temperature dependence of hybridization | Cooling Rate | 0.1°C/5min to 0.1°C/120min | °C/min | Explicit | Controls kinetics/quality of assembly | Methods, Extended Data Fig. 2 |
| IonicStr | Effect of salt concentration on DNA stability/screening | NaCl Concentration | 0.5 | M | Explicit | Standard condition for DNA assembly | Methods |
| PAE-Conc | Particle concentration effect on kinetics/equilibrium | PAE Concentration | 200 (5nm), 50 (10nm) | nM | Explicit | Affects collision frequency | Methods |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Lattice | Lattice constant (intact, 5nm PAE) | a | ~221-227 | Å | Explicit | Measured via SXRD | SXRD | Fig. 3a, Extended Data Fig. 7a |
| Lattice | Lattice constant (intact, 10nm PAE) | a | ~274-286 | Å | Explicit | Measured via SAXS/SXRD | SAXS/SXRD | Extended Data Fig. 4b, Extended Data Fig. 7b |
| Morphology | Crystal size | Diameter/Length | >100 | µm | Explicit | Measured via microscopy | Optical Microscopy | Abstract, Fig. 1, Extended Data Fig. 2 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip to Module 6.)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Crystallization (Slow Cooling) | 5 - 120 | min per 0.1°C | Methods | Explicit | Cooling rate defines assembly time. Total time depends on temp range. |
        | Recovery (Rehydration) | <~5 (Order of magnitude) | s | Abstract, Fig. 1c, d | Explicit | Explicitly stated as "within seconds". |
        | MD Simulation Timestep | 0.002 | dimensionless τ (simulation units) | Methods | Explicit | Timescale relevant only to simulation, not physical system directly. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip to Module 8.)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior is macroscopic shape memory: large-scale, reversible deformation (anisotropic wrinkling/creasing upon dehydration) and rapid recovery (return to original faceted morphology within seconds upon rehydration). Associated emergent behaviors include significant, reversible changes in optical properties (from high broadband absorption to increased reflection upon deformation) and characteristic mechanical properties (softness, measured by reduced modulus). Unusual SXRD features (lines, fringes) in the deformed state are also noted as behaviors related to the internal structural changes.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors are validated through multiple experimental techniques:
        *   **Shape Memory (Deformation/Recovery):** Directly observed and quantified using optical microscopy (Fig. 1c-f, Fig. 4a), including quantitative shape analysis (solidity, convexity, curvature - Fig. 2a,b) comparing intact, dehydrated, and rehydrated states. Cyclic behavior shown in Supplementary Video 1.
        *   **Internal Structure Changes:** Probed using single-crystal X-ray diffraction (SXRD) across the cycle, showing loss and recovery of higher-order reflections and changes in unit cell parameters (Fig. 3a, Extended Data Figs. 6, 7). Unusual diffraction features (lines, fringes) in the deformed state are analyzed (Fig. 3b,c).
        *   **Mechanical Properties:** Quantified using AFM nanoindentation to measure reduced modulus (Fig. 2c). Differences based on PAE size explored via MD simulations (Fig. 2d-g).
        *   **Optical Properties:** Measured using optical microscopy (bright/dark field) and spectroscopy, showing changes in absorption/reflection (Fig. 4b,c, Extended Data Fig. 9). Validated and interpreted using FDTD simulations and transfer matrix method (Fig. 4d-f).
        *   **Limitations:** Cyclic stability not exhaustively quantified. Precise influence of drying rate on deformation not fully mapped. MD simulations use simplified models. FDTD assumes periodicity even in deformed state calculation.

---

#Key: [nsamela_colloidal_2023]

# Colloidal Active Matter Mimics the Behavior of Biological Microorganisms—An Overview

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: This review paper summarizes recent progress in the development and understanding of colloidal active matter systems (specifically artificial microswimmers like Janus colloids, bimetallic rods, helical swimmers, catalytic microtubes) designed to mimic the behaviors of biological microorganisms (like bacteria, algae, sperm). The purpose is to compare the swimming mechanisms, fuel dependence, environmental interactions (walls, viscosity), and tactic responses (chemo-, rheo-, magneto-, photo-, thermo-, gravi-taxis) of both biological and artificial microswimmers to gain insights into the underlying physical principles and guide the engineering of synthetic analogues. The paper focuses on individual swimmer phenomena, with a brief outlook on collective behaviors. Key components discussed are the microswimmers themselves (various biological examples and artificial designs, often Janus particles or catalytic motors), the surrounding fluid medium, fuel sources (e.g., H2O2, nutrients), and environmental features (boundaries, gradients).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters are general characteristics discussed in the review, drawing from multiple cited studies. Specific values are context-dependent and found in the original research papers cited.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Energy input varies depending on the system. For biological systems: metabolic conversion of environmental resources (e.g., nutrients, sunlight for photosynthesis). For artificial systems: chemical energy from fuel decomposition (e.g., H2O2 catalysis), light energy (photocatalysis, light-induced gradients), magnetic fields (external actuation), electrical fields. Sec 2.2 explicitly mentions fuel dependence for artificial systems (e.g., H2O2, noble ions) and nutrients/light for biological ones. Sec 2.4 mentions light, magnetic fields, chemical gradients, temperature gradients as stimuli implicitly providing energy or directing motion.

### **2.2 Energy Transduction**

    *   Content: Biological: Complex metabolic pathways convert chemical energy (or light) into mechanical work via molecular motors (e.g., rotating flagella, beating cilia). Artificial: Chemical energy is often converted into fluid flow via phoretic mechanisms (e.g., diffusiophoresis, electrophoresis, thermophoresis) due to asymmetric reactions or material properties (Janus particles), generating thrust. Light energy can drive photocatalysis leading to phoretic propulsion or induce thermal gradients (thermophoresis). Magnetic/electric fields directly exert forces/torques on appropriately designed swimmers (e.g., magnetic helices, rods) converting field energy into kinetic energy. Section 2.1 discusses fluid flows generated, implying mechanical energy output. Section 2.2 discusses fuel converting to speed. Taxis sections (2.4) describe conversion of stimulus energy/gradient into directed motion. Figure 3 illustrates the interplay of fuel, propulsion, and fluid dynamics.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The review explicitly states (Sec 2.2) that efficiency calculations for artificial systems exist but are difficult to quantify accurately (especially power input) and generally yield very low values (citing [17, 20, 23]). It contrasts this with biological nanomotors (protein motors) which typically show very low energy consumption (implying higher efficiency, though not quantified for whole organisms). Given the mention of typically "very low" efficiency for artificial systems, the score is low. No specific efficiency values are provided in the text excerpt.

### **2.4 Energy Dissipation**

    *   Content: The primary dissipation mechanism is viscous drag from the surrounding fluid, inherent to low Reynolds number swimming. Heat dissipation occurs during chemical reactions (especially catalytic ones) and potentially light absorption (photothermal effects). Section 2.1 discusses drag forces balancing propulsive forces. Section 2.3.2 discusses increased drag in viscous media. Section 2.4.6 mentions thermophoresis driven by thermal gradients caused by reactions/light, implying heat generation. Quantification is not provided in the review text. Assessment: High (viscous dissipation dominates at low Re).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: Specific local rules mentioned or implied:
        1.  **Hydrodynamic Interactions:** Swimmers generate flow fields (pusher/puller, Sec 2.1, Fig 2) which affect nearby swimmers or boundaries. Near walls, hydrodynamic interactions cause torques leading to alignment and wall following (Sec 2.3.1, Fig 5a).
        2.  **Chemical Interactions:** Active colloids consume fuel and release products, creating local concentration gradients (phoretic fields, Sec 2.1, Fig 3). These gradients can cause chemotactic interactions between particles or influence behavior near boundaries (fuel depletion/product accumulation).
        3.  **Steric Interactions:** Particles physically cannot overlap. Implicit in confinement studies (Sec 2.3.1).
        4.  **Tactic Responses:** Particles reorient/move based on local gradients (viscosity, flow, magnetic field, chemical, light, temperature, gravity) as described in Sec 2.4.
        5.  **Alignment Rules (Collective):** Mentioned in outlook (Sec 3) via Vicsek model reference - particles tend to align velocity with neighbors.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: Global orders discussed or implied:
        1.  **Boundary Accumulation/Following:** Swimmers accumulate and move parallel to surfaces (Sec 2.3.1, Fig 5).
        2.  **Circular Trajectories (Near Wall):** Bacteria swimming near surfaces exhibit circular paths (Sec 2.3.1, Fig 5b).
        3.  **Upstream Swimming (Rheotaxis):** Alignment and motion against flow (Sec 2.4.2).
        4.  **Tactic Migration:** Directed movement of swimmers towards/away from stimulus source (Sec 2.4).
        5.  **Gear Rotation:** Coordinated motion of self-assembled swimmers driving a micro-gear (Sec 2.3.1, Fig 5h).
        6.  **Flocking/Swarming:** Ordered collective motion (mentioned in outlook, Sec 3).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| 1 | Hydrodynamic Wall Interaction | Distance to wall | >0 | µm | Explicit | Determines strength of wall-induced torques/forces. | Sec 2.3.1 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| 1 | Wall Accumulation/Following | Surface Coverage | 0-1 (or density) | swimmers/µm² | Implicit | Quantifies accumulation. | Microscopy | Sec 2.3.1 |
| 1 | Wall Accumulation/Following | Alignment Angle | 0-90 | degrees | Explicit | Angle relative to wall normal. | Microscopy | Sec 2.3.1, Fig 5c,f |
| 2 | Circular Trajectory | Radius of Curvature | Varies | µm | Explicit | Characterizes circular path near wall. | Microscopy | Sec 2.3.1 |
| 3 | Rheotaxis | Upstream Velocity Component | Varies | µm/s | Implicit | Measure of alignment against flow. | Microscopy | Sec 2.4.2 |
| 4 | Taxis | Chemotactic Index / Drift Velocity | Varies | Dimensionless or µm/s | Explicit | Quantifies bias in motion along gradient. | Microscopy / Tracking | Sec 2.4.4 |
| 6 | Flocking (Collective) | Polarization (Order Parameter) | 0-1 | Dimensionless | Explicit | Average alignment of velocities. | Simulation / Microscopy | Sec 3 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Brownian Reorientation Time (τR) | Depends on size/viscosity | s | Implicit | Time for orientation to randomize via Brownian motion. Relevant baseline. | Basic Colloid Physics |
        | Flagellar Beat Period (Biological) | ms range (typical) | ms | Implicit | Timescale of biological propulsion mechanism. | General Knowledge / Sec 2.1 |
        | Chemotactic Response Time (Bacteria "run/tumble") | ~ seconds | s | Explicit (Concept) | Timescale for changing direction based on sensed gradient change over time. | Sec 2.4.4, Fig 7a |
        | Fuel Depletion Time | Varies (depends on conc./reaction rate) | s or min | Implicit | Time until fuel runs out, affecting speed. | Sec 2.2 |
        | Gradient Establishment Time (Experimental) | Varies (depends on setup) | s or min | Implicit | Time needed to set up stable gradients for taxis studies. | Sec 2.4.4 |

    *   **Note:** Timescales are mostly implicit or order-of-magnitude based on general knowledge related to the phenomena discussed, except for the run/tumble concept which is more explicit.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main behaviors described are:
        1.  **Self-Propulsion:** Autonomous movement powered by local energy conversion (chemical, light, etc.).
        2.  **Taxis:** Directed motion in response to external stimuli gradients (chemo-, rheo-, magneto-, photo-, thermo-, gravi-taxis). Both positive (towards) and negative (away from) taxis are discussed.
        3.  **Boundary Interaction:** Specific behaviors near surfaces, including accumulation, alignment, wall-following, and circular motion.
        4.  **Environmental Response:** Changes in motility (speed, direction) due to environmental factors like viscosity or fuel concentration.
        5.  **Collective Motion (Outlook):** Mention of flocking, swarming, quorum sensing analogues, predator-prey dynamics.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The review relies on citing experimental observations (microscopy, particle tracking) and theoretical/computational modeling (hydrodynamic simulations, kinetic models) from the primary literature to validate the described behaviors. For instance:
        *   Wall following/orientation: Cites experimental observation and modeling ([31, 33, 34, 36, 40]). Fig 5 shows experimental images and models.
        *   Rheotaxis: Cites microfluidic experiments and modeling ([86, 87, 90, 98]). Fig 6 shows experimental setups/results.
        *   Chemotaxis: Cites microfluidic assays, stopped-flow techniques, and simulations ([17, 114, 115, 123]). Fig 7 shows schematics and results.
        *   Flow Fields (Pusher/Puller): Validated by hydrodynamic theory and observations ([Sec 2.1]). Fig 2 illustrates theoretical flow fields.
        Reproducibility is mentioned anecdotally (e.g., wall interactions). Limitations are sometimes acknowledged (e.g., difficulty establishing gradients, passive vs active mechanisms in rheo/chemotaxis).

---

#Key: [xia_dynamic_2022]

# Dynamic morphological transformations in soft architected materials via buckling instability encoded heterogeneous magnetization

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of soft architected materials fabricated from silicone elastomers doped with ferromagnetic particles (NdFeB). These magneto-elastomers exhibit swelling when exposed to organic solvents (e.g., toluene) and deswelling in others (e.g., ethanol). When constrained (e.g., attached to a substrate) and swelled, the structures undergo buckling instability, forming predictable 3D shapes (e.g., wavy patterns). These buckled structures are then magnetized using a strong pulse magnetic field (~2T), encoding a heterogeneous magnetization profile corresponding to the buckled shape. After returning to the initial undeformed state (by deswelling), the encoded magnetization allows the structures (strips, cellular lattices) to undergo controllable, dynamic, and anisotropic morphological transformations when subjected to external magnetic fields (strength, direction, gradient) and/or solvent environments. The purpose is to create morphable structures for applications like fluid manipulation, particle trapping, biomedical analysis, and soft robotics, harnessing buckling instability for magnetization encoding and achieving dynamic control.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters listed are key to defining the material system and its actuation. Values are explicitly stated in the text or figures. Reliability is high as these are directly specified material properties or experimental conditions.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Two primary energy inputs: 1) Chemical potential energy difference driving solvent diffusion (swelling/deswelling), leading to elastic energy storage and buckling. 2) Magnetic field energy used for actuation (applying torques via external fields) and initial magnetization (strong pulse field).
    *   Value: Magnetization: ~2 T pulse. Actuation: e.g., 20-150 mT (Fig. 4), 9 mT (robot). Solvent: Chemical potential gradient (not quantified).
    *   Units: T (Tesla), mT (milliTesla). Chemical potential (J/mol or similar, not specified).

### **2.2 Energy Transduction**

    *   Content: 1) Chemical Energy to Elastic Energy: Solvent diffusion causes elastomer swelling, constrained geometry leads to compressive stress, storing elastic energy until buckling occurs, converting elastic energy into mechanical deformation (change in shape). 2) Magnetic Energy to Mechanical Energy: External magnetic fields exert torques on the embedded magnetic domains (heterogeneous magnetization profile), causing the elastomer structure to deform (bend, twist, change shape). This mechanical work can then be transferred to a surrounding fluid (pumping, mixing, propulsion). 3) Pulse Magnetic Field Energy to Stored Magnetic Potential Energy: The initial high magnetic field aligns magnetic domains in the buckled state, storing potential energy in the material's persistent magnetization profile.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Efficiency is likely low. Swelling/deswelling involves dissipative diffusion processes. Magnetic actuation involves overcoming internal elastic forces and potentially significant viscous dissipation when interacting with fluids (e.g., glycerol, silicone oil used in experiments). The paper compares pumping performance (Supplementary Note 2, Table 1) showing it outperforms some devices, suggesting reasonable efficiency *relative* to other soft pumps, but absolute thermodynamic efficiency from input field energy to useful fluid work is likely low. No direct quantification of magnetic-to-mechanical or chemical-to-mechanical efficiency is provided. Score reflects potential for some useful work output despite inherent dissipation in soft, wet systems.

### **2.4 Energy Dissipation**

    *   Content: 1) Viscoelastic dissipation within the elastomer during deformation cycles. 2) Viscous dissipation in the surrounding fluid (e.g., glycerol, silicone oil, water) due to the structure's movement (significant in fluid manipulation/propulsion applications). 3) Heat generation during magnetization/demagnetization cycles (hysteresis, though likely minor during low-field actuation). 4) Frictional losses if surfaces rub. 5) Energy loss during solvent diffusion (entropy changes, mixing). Main dissipation mechanisms during operation are likely elastomer viscoelasticity and fluid viscosity. Qualitative assessment: Medium to High, especially during dynamic actuation in viscous fluids. No quantification provided.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rule governing buckling is continuum mechanics for hyperelastic materials under compression and geometric constraint. Solvent diffusion increases local volume (swelling strain). Substrate constraint prevents expansion at the base, inducing compressive stress (σ) in the material above. When σ exceeds a critical buckling stress (σ_crit), determined by material modulus (E), geometry (thickness T, height H, length L, width W), and boundary conditions, the structure deforms out-of-plane. The specific shape (wavelength λ, amplitude A) minimizes the total elastic energy (bending + stretching). Equations derived from Foppl-von Karman plate theory (Fig 3, Supp. Note 1, Eqs S1-S18) describe the relationship between geometry, stress, and the resulting pattern (e.g., λ ~ T^(1/2) H^(1/2), A related to post-buckling strain). For cellular structures, junction constraints act as boundary conditions influencing buckling modes (Fig 5).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Buckling | Foppl-von Karman derived | Young's Modulus (E) | 67 (Sim) | kPa | Section: Simulation methods | Explicit | Value used in simulation. |
    | Buckling | Foppl-von Karman derived | Aspect Ratio (L/H) | ~1.5 - ~12 (Exp/Theory) | dimensionless | Fig 3f, g | Explicit | Range explored in figures. |
    | Buckling | Foppl-von Karman derived | Critical Membrane Force (tc) | Varies with E, T, H | N/m | Fig 3f | Explicit | Parameter in theoretical model. |
    | Buckling | Foppl-von Karman derived | Thickness (T) | Not specified, varied implicitly in Fig 2d | mm | Fig 2d | Explicit | Varied experimentally. |
    | Buckling | Foppl-von Karman derived | Height (H) | 1.6 - 5.0 | mm | Fig 2c,d,f | Explicit | Range explored experimentally. |

### **4.3 Global Order:**

    *   Content: The emergent global order is the specific, often periodic, 3D morphology adopted by the structure in the buckled state. Examples include: sinusoidal wavy patterns along single strips (Fig 2, 3), ordered buckling patterns in square lattices (Fig 5d), hexagonal lattices (Fig 5e), and patterns influenced by defects or connectivity (Fig 5a, g, h). The order is characterized by wavelength (λ) and amplitude (A) or specific deformation modes (Wn).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| P1 | Buckled Wavelength (strip) | λ | ~4 - ~15 | mm | Explicit | Measured from experiments/theory. | Microscopy/Image Analysis | Fig 2b,c,d, Fig 3d |
| P2 | Buckled Amplitude (strip) | A | ~0.5 - ~3.5 | mm | Explicit | Measured from experiments/theory. | Microscopy/Image Analysis | Fig 2b,c,d, Fig 3d |
| P3 | Deformation Mode (strip/plate) | Wn | 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5 | dimensionless | Explicit | Defined/observed modes. | Microscopy/Image Analysis | Fig 3b,f,g, Fig 5b |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**



## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Solvent Swelling/Stabilization | ~20 | min | Supp Fig 4 | Explicit | Time for deformation to stabilize in toluene. |
        | Magnetic Actuation Frequency (Fluid Flow) | 1 - 5 | Hz | Fig 6a, b, Supp Fig 17 | Explicit | Frequency of rotating magnet for PIV. |
        | Magnetic Actuation Frequency (Robot) | 12 | Hz | Section: Fab & Actuation of Robots | Explicit | Frequency of Helmholtz coil field. |

    *   **Note:** Lists key timescales relevant to the system's dynamic behavior as reported or implied.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors demonstrated are: 1) Dynamic Shape Morphing: Controlled, anisotropic, and reversible changes in 3D geometry (wavy patterns, lattice deformations, twisting, collapse) in response to magnetic fields and/or solvents. 2) Fluid Manipulation: Generating directional fluid flow (pumping), mixing, and vortex generation via dynamic shape changes of structures immersed in fluid (demonstrated with PIV). 3) Droplet Transport: Moving liquid droplets on a surface using the induced fluid flows from underlying actuated structures. 4) Particle Manipulation: Selective trapping and sorting of microparticles using magnetically reconfigurable lattice structures. 5) Enhanced Detection: Collection and concentration of aerosol droplets via induced flow for enhanced fluorescence detection. 6) Untethered Locomotion: Propulsion of a soft robot at an air-water interface using oscillating magnetic fields to drive wave-like motion of the encoded structure.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of behavior (shape morphing, fluid flow, etc.) are primarily validated through experimental observation and characterization using: 1) Microscopy/imaging to observe shape changes (Figs 2, 4, 5, 7). 2) Magneto-optical sensor (MagView) to confirm magnetization patterns (Fig 2a). 3) Particle Image Velocimetry (PIV) to quantify induced fluid flow fields (Fig 6, Supp Figs 17, 18, 19, 20). 4) Videos demonstrating dynamic processes (droplet manipulation, particle sorting, robot motion - Supp Movies). 5) Quantitative measurements of parameters like wavelength, amplitude, phase shift, particle sizes, fluorescence intensity (Figs 2, 3, 4b, 5b,c, 7a,b). 6) Finite Element Analysis (ABAQUS, COMSOL) and analytical modeling (Foppl-von Karman) provide theoretical validation and mechanistic understanding, showing agreement with experiments (Figs 2e,f, 3, 4c, 5a,d). Reproducibility is suggested by n=3 trials for some quantitative data. Limitations include small sample sizes for statistical robustness and lack of long-term cycling/fatigue tests.

---

#Key: [march-pons_honeybee-like_2024]

# Honeybee-like collective decision making in a kilobot swarm

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of a swarm of Kilobots (up to 35 robots) programmed to mimic the nest-site selection process of honeybees within a circular arena (radius R=20cm). The Kilobots implement a mathematical model based on List et al. [10], where each robot can be in one of three states: uncommitted (searching), committed to site 1 (lower quality), or committed to site 2 (higher quality). Committed bots "dance" (advertise) their site for a duration proportional to the site's perceived quality (`q_j`), moving as Persistent Random Walkers (PRWs). Uncommitted bots remain stationary and assess the opinions of nearby "dancing" bots within a limited communication range (`r_int` ≈ 7cm) over a time step (`Δt`). An uncommitted bot `i` decides to commit to site `j` at time `t+1` based on a probability `p_j,t+1 = (1-λ)π_j + λf_j,t`, where `π_j` is the intrinsic self-discovery probability for site `j`, `λ` is the interdependence parameter (weight given to social information), and `f_j,t` is the *locally observed* fraction of neighbors dancing for site `j`. The purpose is to investigate how factors like interdependence (`λ`), swarm density, robot motion, and communication limitations affect the swarm's ability to reach consensus on the best quality site, comparing physical experiments and simulations to mean-field predictions.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameter `q` represents the duration of advertisement in discrete time steps of the model/simulation logic loop.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The paper does not explicitly discuss the energy source for the Kilobots. Kilobots are powered by onboard batteries (rechargeable Li-Po). The relevant energy input for the described *dynamics* is implicit in the computational execution of the algorithm and the physical actuation (vibration motors for movement, LEDs, IR communication).

### **2.2 Energy Transduction**

    *   Content: The paper does not detail energy transduction. Implicitly, electrical energy from the battery is transduced into:
        1.  Mechanical energy (vibrations) for locomotion (PRW).
        2.  Electromagnetic energy (infrared light) for communication.
        3.  Electromagnetic energy (visible light) for state indication (LED).
        4.  Thermal energy (heat) due to computational processing and motor/LED operation (dissipation).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper provides no information on the energy efficiency of the Kilobots or the decision-making process. Efficiency is not a focus of this study. Qualitatively, Kilobots are known to be low-power, but efficiency metrics are absent.

### **2.4 Energy Dissipation**

    *   Content: The paper does not quantify energy dissipation. Implicit dissipation mechanisms include:
        1.  Heat from the microcontroller during computation.
        2.  Heat from the vibration motors during locomotion.
        3.  Heat from the IR and RGB LEDs.
        4.  Friction between Kilobot legs and the arena surface during movement.
        Quantification is not possible from the text. Qualitatively likely Low per bot, but cumulative for the swarm.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: `q_j` * (duration of one model time step)
*    Units: time units (e.g., seconds)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 3 primary states (0, 1, 2) + timer value
*   Units: States + Integer


### **3.5 Readout Accuracy (Optional - if applicable)**



### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content:
        1.  **State Update (Uncommitted Bots):** An uncommitted bot `i` gathers information about the states (`s_k,t`) of neighboring bots `k` within its communication radius (`r_int` ≈ 7cm) during a time step (`Δt`). It calculates the local fraction `f_j,t` of neighbors committed to site `j`. It then commits to site `j` with probability `p_j,t+1 = (1-λ)π_j + λf_j,t` (Eq. 1). If it commits, its state becomes `s_i,t+1 = j` and dance duration `d_i,t+1 = q_j`.
        2.  **State Update (Committed Bots):** A bot `i` committed to site `j` remains committed (`s_i,t+1 = j`) but decreases its dance duration (`d_i,t+1 = d_i,t - 1`) until `d_i,t` reaches zero, at which point it becomes uncommitted (`s_i,t+1 = 0`).
        3.  **Movement:** Committed bots perform a Persistent Random Walk (PRW). Uncommitted bots remain stationary. (Sec IV.A, Appendix A2)
        4.  **Communication:** Bots broadcast their state (`s_i,t`) via IR. Bots receive messages from neighbors within `r_int`. (Sec III.A)
        5.  **Physical Interaction:** Implicit excluded volume interactions occur due to Kilobot size (diameter ≈ 3.3cm).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | 1       | State Update Prob. | λ (Interdependence) | 0 - 0.9 | dimensionless | Eq. 1, Fig 4 | Explicit | Parameter varied in study |
    | 1       | State Update Prob. | π_j (Self-Discovery) | e.g., 0.2, 0.3, 0.4 | dimensionless | Eq. 1, Fig 2, 3 | Explicit | Parameter varied in study |
    | 2       | Dance Duration | q_j (Site Quality) | 7, 10 | time steps | Sec IV.A | Explicit | Fixed parameter in study |
    | 4       | Communication | r_int (Interaction Radius) | ~7 (Exp), 3-12 (Sim) | cm | Sec III.A, Fig 7 | Explicit | Parameter described/varied |
    | 1, 4    | Information Gathering | Δt (Sensing Window) | 800 / ~8.24 | loops / s | Sec IV.A, Fig 5 | Explicit | Parameter used in study |
    | 3       | Movement | PRW Parameters (Speed, Turn Rate/Angle) | Not specified | variés | Appendix A2 | Mixed | PRW mentioned explicitly, parameters implicit |

### **4.3 Global Order:**

    *   Content: The emergent global order is the collective consensus state, characterized by the stationary distribution of opinions across the swarm. Specifically, it is quantified by the average proportions of bots in each state (`<f_0>`, `<f_1>`, `<f_2>`) and the consensus parameter `Q = <f_2> - 2<f_1>`. A state of strong consensus (`Q > 0`) for the high-quality option (site 2) represents the desired emergent order. The structure of the time-integrated communication network (specifically, its percolation status and giant component size) is another emergent structural order.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| 1 | State Update Probability | λ | 0 - 0.9 | dimensionless | Explicit | Parameter swept | Eq.1, Fig 4 |
| 1 | State Update Probability | π_j | e.g., 0.2-0.4 | dimensionless | Explicit | Parameter set | Eq.1, Fig 2 |
| 2 | Dance Duration Update | d_i,t | Decreases by 1 | step^-1 | Explicit | Model definition | Sec II.A |
| 3 | Movement Rule | Movement Type | PRW / Stationary | categorical | Explicit | Model definition | Sec IV.A |
| 4 | Communication Rule | r_int | ~7 (Exp) | cm | Explicit | Experimental setup | Sec III.A |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| 1 | Opinion Distribution | f_j | 0 - 1 | dimensionless | Explicit | Measured observable | Image Analysis / Simulation output | Fig 2, 4 |
| 2 | Consensus Metric | Q | Approx -0.7 to 0.8 | dimensionless | Explicit | Calculated observable | Q = f2 - 2*f1 | Eq. 2, Fig 4 |
| 3 | Network Connectivity | <S_max>/N | 0 - 1 | dimensionless | Explicit | Calculated observable | Cluster analysis | Fig 5b |
| 4 | Network Connectivity | <S> | 1 - ~50 | bots | Explicit | Calculated observable | Cluster analysis | Fig 5a |
| 5 | Network Connectivity | Percolation Threshold η* | Varies (e.g., ~0.3 for N=35, Δt=800) | dimensionless | Explicit | Calculated observable | Analysis of <S> vs η | Sec V.B, Fig 8e,f |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Hybrid / Stochastic Analog

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operation performed *by each agent* is the stochastic state update based on local sampling and weighted preference.
        Mathematically: `s_i,t+1 = Sample( P(s | f_0,t, f_1,t, f_2,t, π_0, π_1, π_2, λ) )`
        where P is derived from Eq. 1 (and normalization). This involves:
        1. **Local Sensing/Sampling:** Estimating `f_j,t` from neighbors within `r_int`.
        2. **Weighted Averaging/Integration:** Combining self-discovery (`π_j`) and social influence (`f_j,t`) using weight `λ` via Eq. 1.
        3. **Stochastic Thresholding/Selection:** Choosing the next state based on the calculated probabilities `p_j,t+1`.
    *   **Sub-Type (if applicable):** Stochastic Weighted Opinion Update.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Algorithm Step / Sensing Window (Δt) | ~8.24 | s | Sec IV.A, Appendix A2 | Explicit | Duration of one loop iteration provided |
        | Memory Retention / Dance Duration (Site 1) | ~58 | s | Calculated from q1=7, Δt=8.24s | Mixed | Based on explicit q1 and Δt |
        | Memory Retention / Dance Duration (Site 2) | ~82 | s | Calculated from q2=10, Δt=8.24s | Mixed | Based on explicit q2 and Δt |
        | PRW Straight Segment | ~3.8 | s | Appendix A2 | Explicit | Average duration given |
        | PRW Turn Duration | ~2.8 or ~5.8 | s | Appendix A2 | Explicit | Turn durations given |
        | Consensus Convergence Time | ~50 * Δt ≈ 412 (Transient); varies | s | Fig 2a | Mixed | Visual estimate from graph; depends on parameters |
        | Network Integration Time | 0 - 800 loops (explored) | loops / s | Fig 5 | Explicit | Parameter varied in percolation analysis |

    *   **Note:** Convergence time to steady state is parameter-dependent (Sec IV.A).

### **6.2 Active Inference:**

    *   Content: No
        1.  *Prediction*: Agents don't predict future states of neighbors or the environment.
        2.  *Action selection to minimize prediction error*: Actions (committing, moving) are based on Eq. 1 and PRW rules, not on minimizing a prediction error or surprise relative to an internal model.
        3.  *Internal models*: Agents don't possess or update an internal model of the environment or other agents' behavior beyond the simple rules they follow.
        The system reaches consensus through reinforcement dynamics (higher quality sites get advertised longer, increasing `f_j,t`), but this doesn't equate to active inference's generative model framework.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Partial

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is the dynamic restructuring of the communication network topology via the Persistent Random Walk (PRW) of committed agents. Over a time window (`Δt`), moving agents encounter different neighbors than they would in a static configuration. This increases the effective number of interaction partners and facilitates information propagation across the swarm, lowering the effective percolation threshold (Fig 5). This improved information flow allows the swarm to better integrate opinions and converge towards the globally optimal consensus, overcoming limitations of purely local communication in sparse or static settings. The adaptation is physical/structural (changing network edges over time) driven by agent mobility, not by changes in internal agent parameters or rules.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior is **collective decision-making**, specifically achieving **consensus** on one of two options with differing qualities (`q1`, `q2`) and discovery probabilities (`π1`, `π2`). This involves the swarm dynamically converging to a state where a majority (ideally a strong majority, `Q > 0`) of agents advocate for the higher-quality option (site 2), even when it might be harder to discover initially (asymmetric `π`). The strength and accuracy of this consensus depend on system parameters (`λ`), agent density, and mobility. A related emergent behavior is the formation of a **percolating communication network** due to agent mobility integrated over time.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behavior (consensus, network percolation) are validated through:
        1.  **Physical Experiments:** Using N=5-35 Kilobots in a physical arena, tracking states over time (Sec III, IV.A, Fig 2, 4, 8). Multiple repetitions (5 per condition for N=35) provide statistical validation.
        2.  **Agent-Based Simulations (KILOMBO):** Emulating the Kilobot dynamics with larger N (up to 492) and more repetitions (50-100) to confirm experimental trends and explore parameter space (Sec III.B, IV, V, Fig 4, 5, 8).
        3.  **Simplified Model Simulations:** Mean-field (fully connected) and quenched (static network) simulations provide theoretical baselines for comparison (Sec II.A, II.B, IV.B, Fig 3, 4, 7, 8).
        4.  **Quantitative Analysis:** Measuring order parameters (`f_j`, `Q`), network properties (`<S>`, `<S_max>`, `P(k)`, `η*`), and their dependence on system parameters (`λ`, `π_j`, N, `r_int`, `Δt`) (Sec IV, V, Fig 2-8).
        5.  **Comparison Across Models:** Showing agreement/disagreement between experiments, KILOMBO, quenched, and mean-field results helps isolate the effects of mobility and limited communication (Fig 4, 8).
        Limitations include the limited number of physical robot experiments and potential simulator-reality gaps (though KILOMBO is shown to match well).

---

#Key: [stern_training_2024]

# Training self-learning circuits for power-efficient solutions

**Paper Type:** Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of self-learning electrical circuits, specifically resistor networks where edge conductances are the learning degrees of freedom (DOF) and node voltages are the physical DOF. The system learns desired functions (e.g., regression, classification, allostery tasks) from examples using coupled learning, a local contrastive learning rule that modifies conductances based on the difference between clamped (target-nudged) and free states. The purpose is to demonstrate and analyze methods for training these circuits to find power-efficient solutions by minimizing both learning error and power consumption, using appropriate initialization and modified learning rules. The system aims to provide an analog hardware platform for low-energy machine learning.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters related to the core learning process and system scale are listed. Simulation parameters are generally explicitly stated, while experimental values sometimes represent effective or specific implementation choices.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Electrical energy supplied to the circuit, primarily through applied voltages or currents at input nodes/edges during the free and clamped phases of learning and during inference (free phase).
    *   Units: Volts (V), Amperes (A)

### **2.2 Energy Transduction**

    *   Content: Electrical energy is transduced into dissipated power (heat) within the resistive elements (conductances) of the network. The amount of power dissipated depends on the node voltages (physical DOF) and the edge conductances (learning DOF), according to Ohm's law and Joule heating (P = Σ kᵢΔVᵢ² / 2). During learning, information derived from energy differences (Contrast C = η⁻¹[Pᶜ - Pᶠ]) is used to modify the conductances (transducing system state information into structural change).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper's central theme is improving power efficiency. It demonstrates significant potential for reduction compared to standard digital ML (stated as 2-5 orders of magnitude potential for neuromorphic hardware in general, Intro) and shows methods (initialization, modified learning rules) to further reduce power consumption in the analog circuit itself (e.g., Fig 1e, Fig 2b show reductions, Fig 4 shows ~50% saving). The efficiency is relative; the circuits are analog and inherently consume less than digital inference. The score reflects the demonstrated improvement potential and inherent analog advantage, but acknowledges dissipation is still present. Metrics: Free Power (Pᶠ), Training Energy (E). Units: [k][V]², [τ_epoch][k][V]². Specific values depend on system scale and task.

### **2.4 Energy Dissipation**

    *   Content: The primary dissipation mechanism is Joule heating in the variable resistors. The total dissipated power is explicitly defined as P = 1/2 Σ kᵢΔVᵢ², where kᵢ are conductances and ΔVᵢ are voltage drops across edges. The paper focuses on minimizing this dissipated power ('free power', Pᶠ) during inference and potentially reducing the integrated power (Training Energy, E) during training. The magnitude depends on the conductance state (k) and the applied inputs (which determine V). Lower conductances generally lead to lower power dissipation for the same relative task performance (Sec II B).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*    Units: Seconds (s) (or Qualitative Descriptor: "Assumed Stable" (Sim), "Short-to-Medium Term" (Exp, inferred))

### **3.4 Memory Capacity (Optional - if applicable)**

*   Units: Number of edges * log2(precision levels) (Bits, potentially)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Units: Mean Squared Error (MSE) units (e.g., V²) or classification accuracy (%)

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Noise Tolerance | Effect of noise on learning/error floor | L ~ 10⁻⁶ (Sim with noise δ=10⁻³) | V² | `MemoryNode` attribute | Appendix B / Sec III A | Explicit | Simulation results explicitly show an error floor due to noise. |
    | Conductance Bounds | Min/Max conductance limits | k_min=10⁻³, k_max=10¹ (Sim) | [k] | `MemoryNode` attribute | Sec II B | Explicit | Simulation bounds are explicitly stated. |
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No



## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog / Neuromorphic

### **5.3 Computational Primitive:**

    *   Content: Weighted summation / Linear transformation (physically realized). The core computation performed by the resistor network during inference is solving a linear system defined by Kirchhoff's laws, where node voltages are determined by input voltages and the conductance matrix (k). For specific input/output nodes, this results in a linear transformation from input voltages/currents to output voltages/currents, effectively a distributed analog matrix-vector multiplication or linear function evaluation. The learning process tunes the "weights" (conductances) to approximate a target function (e.g., regression ΔV_o = A * ΔV_i).
    *   **Sub-Type (if applicable):** Distributed Analog Linear Algebra

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Learning Rate Timescale (α⁻¹) | Variable (Sim), 24 (Exp) | Epochs (Sim), ms (Exp) | Sec II B, Sec III A | Explicit | Defines the characteristic time for conductance changes per epoch/step. |
        | Training Time (T) | Variable (~10³-10⁵ epochs for L<10⁻⁴) | Epochs | Fig 1c | Explicit | Time needed to reach a target error level during training. |
        | λ Update Timescale (ρ) | 1 (Sim) | Epochs | Sec IV | Explicit | Timescale for dynamical control of power minimization amplitude. |
        | Experimental Run Time | 20 | s | Sec III A | Explicit | Duration of a single experimental learning run. |
    *   **Note:** Key timescales relate to the learning process dynamics and the physical circuit response.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is coupled learning, a form of contrastive learning. It modifies the learning degrees of freedom (conductances kᵢ) proportional to the negative partial derivative of a contrast function C (Eq 1: C ≡ η⁻¹[Pᶜ - Pᶠ], or Eq 8 for power minimization: Cλ ≡ η⁻¹[Pᶜ - Pᶠ + λPᶠ]). The learning rule is local: k̇ᵢ = -α ∂C/∂kᵢ (or ∂Cλ/∂kᵢ). This rule adjusts conductances based on locally available information related to the difference in power dissipation between the free state (system's natural response) and the clamped state (system nudged towards the target response). It effectively performs gradient descent on the contrast function, which is linked to minimizing the task error L.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior is the system's ability to perform specific computational tasks (inference) after training. Examples studied include linear regression (approximating ΔV_o = A * ΔV_i), classification (Iris dataset mentioned), and node allostery (achieving specific output voltages at target nodes given input voltages at source nodes). This functional behavior (e.g., correctly predicting outputs for inputs) emerges from the collective state of the adapted conductances governed by the local learning rule applied during training.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behavior (task performance) are validated through:
        1.  **Simulations:** Quantitative measurement of task error (L, MSE) drop during training (e.g., Fig 1b, Fig 5a). Comparison of network output to target values for regression/allostery. Classification accuracy measured on training/test sets (Appendix C, Fig 7c). Generalization tested by evaluating performance on unseen test data (Appendix A, Fig 5c/d).
        2.  **Experiments:** Measurement of error (L) drop during training on hardware (Fig 3a). Comparison of final learned performance (error vs power trade-off, Fig 3c).
        The validation relies on standard ML performance metrics (error, accuracy) applied to the physical system. Reproducibility is demonstrated by averaging over multiple realizations (simulations) or experiments. Limitations include the relatively small scale of networks tested and the focus on specific task types.

---

#Key: [fuchslin_morphological_2013]

# Morphological Computation and Morphological Control: Steps Toward a Formal Theory and Applications

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper describes and aims to formalize the concepts of Morphological Computation (MC) and Morphological Control (MCon). MC is defined as the exploitation of a physical system's shape, material properties, and dynamics for computational purposes. MCon is the application of MC to control tasks. The core idea is outsourcing computation/control to the physical "body" (e.g., robot body, chemical system) rather than relying solely on a separate (typically digital) controller. Components involve a physical system (body/plant) with inherent dynamics (shape, materials, physics) and potentially a simple control unit that influences/selects these dynamics (e.g., by setting parameters, choosing initial states/basins of attraction). The purpose is to achieve more efficient (time, power, memory, cost) computation or control by leveraging the physics, potentially trading off universality and precision for efficiency and robustness. Case studies illustrate applications in robotics (tensairity support system), chemistry (self-assembly of microreactors), and medicine (modeling tumor response).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Diffusion Timescale (t_diam) (Case 2 Estimate) | ~0.4 (for d=1 μm) | s | Sec 2.4 (Eq 2) | Explicit (Calculation based on formula) | Low (Illustrative estimate) | Formula provided |

    *   **Note:** Parameters listed are key elements from the formal definition (S, M, j) and specific illustrative examples from the text/case studies. The formal definition parameters define the system abstractly. Case study parameters provide concrete examples but are specific to those instances.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The paper explicitly states that the dynamical systems considered are usually dissipative and take up energy (Sec 2.1). The specific energy source depends on the implementation. Case 1 (Tensairity): Pressurized air. Case 2 (Self-assembly): Chemical potential (implicit in bonding/diffusion), thermal energy (kT). Case 3 (Oncology): Ionizing radiation, heat (hyperthermia). General: Abstract control signals could also be considered an informational input requiring energy indirectly.

### **2.2 Energy Transduction**

    *   Content: Energy is transduced via the physical dynamics of the system. Case 1: Pneumatic pressure energy -> mechanical work (actuation, support), dissipated via material damping/friction. Case 2: Chemical potential energy -> structural organization (self-assembly), kinetic energy (diffusion), dissipated via viscosity/heat. Thermal energy (kT) drives diffusion. Case 3: Radiation energy -> chemical changes (DNA damage, radicals) -> biological response/cell death/repair. Heat energy -> increased molecular motion, protein denaturation, altered reaction rates. General: Control signal energy -> changes in system parameters (e.g., internal tension, valve states) -> altered dynamics. These transformations perform the morphological computation/control.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper *claims* MC provides efficiency gains (time, memory, power, space, resources - Sec 1, 2.3, 4) but provides *no quantitative data or metrics* on energy efficiency for any specific MC system or in general. The main argument is avoiding the cost of *encoding/simulating* physics (Sec 2.3). While intuitively plausible, this is not demonstrated quantitatively in terms of energy. Qualitative assessment: Potentially High (compared to simulating the physics), but requires quantification.

### **2.4 Energy Dissipation**

    *   Content: Explicitly mentioned that systems are usually dissipative (Sec 2.1). Mechanisms depend on context: Case 1: Friction, material damping in soft structures. Case 2: Viscous drag during diffusion, heat released during bond formation/breaking (implicit). Case 3: Heat dissipation, entropy increase during biochemical reactions. The formalism (Sec 2.2) doesn't explicitly model dissipation, focusing on state transitions and terminations. Quantification is absent. Qualitative Assessment: Present and significant (as systems are dissipative and often reach attractors/steady states).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip to Module 4.)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: Case Study 3.2: 1) Diffusion of chemtainers above a substrate. 2) Non-selective, weak binding to the substrate. 3) Reversible, selective binding between adjacent chemtainers based on matching linkers (e.g., DNA hybridization). 4) Release probability depends on the number of bound neighbors (tuned so 6 neighbors make release improbable). 5) Diffusion of monomers/oligomers between chemtainers. 6) Catalysis of specific linker formation/breaking within different chemtainer types. General (Attractors): Abstractly defined by the transition function `f(s, t)` which represents the physics/dynamics governing state evolution from local state `s`. For robots/tensairity, this includes classical mechanics (forces, constraints, material properties like elasticity/friction). For biological examples (skiing, walking), includes biomechanics and neuromuscular control signals modulating parameters like joint stiffness/muscle tension.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: Case Study 3.2: A 2D structure of chemtainers with defined neighborhood correlations (spatially heterogeneous but not necessarily crystalline, see Fig 6 right). The functional global order is a reaction environment biased towards specific chemical pathways. General (Attractors): Stable dynamical patterns like limit cycles (oscillatory movements, gaits), fixed points (stable postures). The "attractor landscape" itself (structure of basins and attractors) represents emergent global order.
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| CS3.2-Transport | Diffusion of components | Diffusion Coefficient (D) | >0 | m^2/s | Implicit | Diffusion mentioned, value depends on context | Sec 2.4, 3.2.2 |
| CS3.2-Reversibility | Bonds between chemtainers can break | Binding Energy / kT | Tuned | Dimensionless | Explicit | Reversibility and energy tuning mentioned | Sec 3.2.2 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| CS3.2-Function | Biased reaction yield | Yield % (e.g., GS-04) | ~1-10% | % | Explicit | Measured outcome | Computational Simulation | Sec 3.2.2, Fig 7 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid
    *   The term "analogue computation" is used in the keywords.

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operation described by the formalism (Sec 2.2.1) is the mapping of an initial state `x` to a stable termination set `T` (e.g., fixed point, limit cycle) via the system's functional dynamics `dG(x) = T`. This acts as a form of state classification or attractor selection. The paper also mentions more complex computations achievable by post-processing the system state (Sec 2.1, item 3) and cites Hauser et al. [10] demonstrating that compliant bodies coupled with feed-forward networks can approximate time-invariant filters with fading memory, suggesting filtering or function approximation as potential primitives in specific implementations. Case study 3.2 uses self-assembly to implicitly perform quality control and bias reaction pathways, suggesting implicit logic/control primitives.
    *   **Sub-Type (if applicable):** Attractor Selection / State Classification; Filtering (via post-processing/specific design); Implicit Logic/Control (in self-assembly).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Diffusion Time (t_diam, example) | ~0.4 (for d=1 μm) | s | Sec 2.4 (Eq 2) | Explicit | Example calculation provided. |
    *   **Note:** Specific timescales are generally not quantified except for the diffusion example. The formalism uses continuous time. Case studies imply various timescales relevant to their domains.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: Mechanisms mentioned or used include:
        1.  **Parameter Tuning:** Externally changing system parameters `j` in the programmable dynamical system `C=(S,M,(f_j)_{j∈J})` to alter the dynamics `f_j` and thus the input-output map `d_{C,j}` (Formalism Sec 2.2.1, Examples in Sec 2.1 like adjusting springs). This is adaptation via external intervention based on desired behavior.
        2.  **Attractor Landscape Reshaping:** Using control signals to modify the system's dynamical structure (e.g., number, shape, location of attractors and basins), effectively changing the available repertoire of stable behaviors (Conceptual Sec 2.1, Fig 2). Mechanism driven by control inputs altering physical parameters (e.g., muscle tension, internal pressure).
        3.  **Evolutionary Algorithm:** Used in Case Study 3.2 (Sec 3.2.2, Fig 7) to optimize chemtainer properties (linkers, functionality) based on simulation outcomes (yield). This involves selection based on performance over generations.
        4.  **Learning (Mentioned):** Mentioned in context of brain control interacting with body dynamics (Sec 2.1) and potential future work (Sec 4), but specific learning algorithms within the MC framework are not detailed, apart from citing work combining compliant bodies with feed-forward neural networks [10, 11] which implies potential for standard ML techniques.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors emerging from MC/MCon systems discussed are:
        1.  **Stable Movement/Control Patterns:** Generation of stable gaits (passive walkers), oscillatory movements (general attractors, Fig 2), posture control/support (Tensairity system, Case 3.1), potentially complex trajectories via attractor switching/reshaping (Sec 2.1).
        2.  **Self-Assembly/Structural Organization:** Formation of specific structures (e.g., 2D microreactor network in Case 3.2) from components interacting via local rules.
        3.  **Biased Chemical Synthesis:** Increased yield of a specific target molecule (GS-04) due to the spatially heterogeneous environment created by self-assembly (Case 3.2).
        4.  **Computational Function Mapping:** Implementing a specific input-output function `B: N -> N` via the system dynamics and coding/decoding functions (Formalism Sec 2.2.1). Approximation of filters [10]. Performing logical operations [21].
        5.  **Physiological/Population Dynamics:** Modeling complex biological responses like tumor cell survival fractions under treatment (Case 3.3), characterized by emergent parameters like dose equivalent (G).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation methods depend on the case study. Case 3.1 (Tensairity): Experimental prototype built and tested, measuring joint angle trajectories under load, demonstrating behavior via physical realization and feedback control (Fig 3, 4). Case 3.2 (Self-assembly): In silico simulation using an evolutionary algorithm to optimize parameters and measure yield, validating emergent functional behavior (increased yield, Fig 7) via computational experiments. Case 3.3 (Oncology): Validation via fitting model equations (e.g., G-IR model) to existing experimental data (tumor spheroid survival fractions, Fig 8), demonstrating the model's ability to reproduce observed emergent population dynamics. General Concepts: Validated conceptually through illustrative examples (Fig 1, 2) and by reference to established results in related fields (robotics, dynamical systems, cited work like [10, 21, 23]). Limitations: Robustness claims are mostly qualitative. Scalability of simulated self-assembly to physical realization not shown. Predictive power of oncology models beyond fitting existing data needs further validation.

---

#Key: [babcock_ultraviolet_2024]

# Ultraviolet Superradiance from Mega-Networks of Tryptophan in Biological Architectures

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of large networks ('mega-networks') of Tryptophan (Trp) amino acid residues arranged in specific biological architectures, including individual tubulin dimers (TuD), microtubules (MTs), centrioles (built from MT triplets), and MT bundles mimicking those in neuronal axons. The study investigates the collective quantum optical response, specifically ultraviolet (UV) superradiance, arising from the interaction of UV-excited Trp transition dipoles within these hierarchically organized structures. The purpose is to understand how these collective effects influence fluorescence properties (like quantum yield, QY) and to explore potential implications for cellular signaling, control, and photoprotection. The analysis uses theoretical modeling (effective Hamiltonian in the single-excitation limit) and experimental fluorescence spectroscopy (QY measurements).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Lists key parameters characterizing the system's implementation. Reliability depends on whether it's directly measured in this study vs. cited/calculated.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Ultraviolet (UV) light used for photoexcitation of Tryptophan residues. Specific wavelengths of 280 nm (peak absorption) and 295 nm were used in experiments.
    *   Units: nm (wavelength)

### **2.2 Energy Transduction**

    *   Content: UV photons are absorbed by individual Trp residues, exciting them electronically (specifically the 1La transition). In the networks, these local excitations couple via dipole-dipole interactions, forming collective excitonic states. Energy is then transduced into emitted photons via radiative decay (fluorescence). This radiative decay can be enhanced for certain collective states (superradiance) or suppressed (subradiance). Energy is also lost via non-radiative pathways, including internal conversion (IC), intersystem crossing (ISC), quenching, and photochemical reactions, collectively represented by Γnr.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The primary metric related to efficiency discussed is the fluorescence Quantum Yield (QY = Γ / (Γ + Γnr)). The paper measures QY for Trp (12.4%), TuD (10.6%), and MTs (17.6%, average). The QY represents the efficiency of converting absorbed photons into emitted photons. The enhancement in QY for MTs compared to TuD suggests increased radiative efficiency due to collective effects. However, an overall energy efficiency score (0-10) is subjective without a specific task definition. The QY values are relatively low (<20%), indicating significant non-radiative losses.

### **2.4 Energy Dissipation**

    *   Content: Energy is primarily dissipated through non-radiative decay channels (Γnr), which include internal conversion (ΓIC), intersystem crossing (ΓISC), and quenching/photochemical reactions (Γreact). These processes convert electronic excitation energy into heat (vibrational energy within the molecule or transferred to the solvent/protein environment) or chemical energy. The non-radiative decay rate for a single Trp in buffer (γnr) is calculated as ≈ 0.0193 cm⁻¹, significantly larger than the radiative rate γ = 0.00273 cm⁻¹. This indicates that non-radiative dissipation is the dominant decay pathway for single Trp molecules. The paper assumes Γnr is constant per Trp even in networks, but acknowledges this is a simplifying assumption. Static disorder (fluctuations in Trp excitation energies) also effectively broadens states and can redistribute oscillator strength, impacting energy pathways, but isn't a direct dissipation mechanism itself in this model.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial

**(Conditional: M4.1 is "Partial", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rule governing the emergent optical behavior (superradiance) is the electromagnetic dipole-dipole interaction between pairs of UV-excited Trp transition dipoles. This interaction is mediated by the electromagnetic field and depends on the relative positions and orientations of the dipoles, and the distance between them. These interactions are captured in the off-diagonal elements of the effective Hamiltonian (Eq. S2/S3 in SI, mentioned in main text), which determine the collective excitonic states and their radiative properties (energies Eⱼ and decay rates Γⱼ). The rules governing the initial self-assembly of the protein structures are not detailed.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :-------------------: | :---: | :----------: | :----------------: | :------------: |
    | Dipole-Dipole | Interaction Strength | Coupling Constant (Vᵢⱼ) | ~60 (typical estimate mentioned in Discussion) | cm⁻¹ | Discussion | Explicit | Qualitative estimate provided. |
    | Dipole-Dipole | Geometry Dependence | Dipole Positions (rᵢ) | Varies (from PDB) | nm | Materials & Methods | Explicit | Coordinates extracted from PDB files. |
    | Dipole-Dipole | Geometry Dependence | Dipole Orientations (μ̂ᵢ) | Varies (from PDB) | Unitless | Materials & Methods | Explicit | Orientations extracted from PDB files. |

### **4.3 Global Order:**

    *   Content: The global order refers to the specific hierarchical architectures formed by the Trp networks: linear/quasi-linear arrays within TuD, helical-cylindrical lattices in MTs, cartwheel-like arrangements of MT triplets in centrioles, and hexagonal bundles of MTs in model axons. At the quantum optical level, the emergent order is the formation of collective excitonic states (eigenstates of the Hamiltonian), characterized by specific energies (Eⱼ) and decay rates (Γⱼ), including highly superradiant (large Γⱼ) and subradiant (small Γⱼ) states.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Dipole-Dipole | Electromagnetic Interaction | Coupling Strength | ~60 (typical) | cm⁻¹ | Explicit | Estimate mentioned in Discussion. | Discussion |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Superradiance | Max collective decay rate enhancement | max(Γⱼ/γ) | 1 to ~7000 | Unitless | Explicit | Calculated max eigenvalue ratio. | Diagonalization (Fig 5, 6) | Fig 5, 6 |
| Quantum Yield | Fluorescence Efficiency | QY<sub>th</sub> | 0.12 to ~0.14 | Unitless | Explicit | Calculated thermal average QY. | Eq 1, Thermal Avg. | Fig 3, 4 |
| Structural | MT/Centriole/Bundle Geometry | Size (Length/Diameter) | nm to μm | nm | Explicit | Defined by PDB models. | Materials & Methods, Fig 1 | Fig 1, 3, 5, 6 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
     | Structure-to-Optical | Mapping local Trp arrangement to collective excitonic states (Eⱼ, Γⱼ) | High (Score 8 in M4.4) | 7 | max(Γⱼ/γ), QY<sub>th</sub>, Spectrum | Mixed | See M4.4. Score reflects predictability within model, acknowledges model limits. | M4.4, Figs 3, 5, 6 |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 7. Rubric: Measures how well the global properties (collective optical states) can be predicted/reconstructed solely from the local properties (individual Trp dipoles) and their interaction rules (Hamiltonian). Score 0=No prediction possible; 5=Qualitative trends predictable; 7=Quantitative prediction with known model assumptions/limitations; 10=Perfect quantitative prediction capturing all aspects. Here, the model provides good quantitative prediction of key features (superradiance, QY trends) based on local structure/interactions, but relies on approximations (single excitation, ignoring some environmental factors).
    *   **Metrics:** Superradiance enhancement factor (max(Γⱼ/γ)), Thermally averaged Quantum Yield (QY<sub>th</sub>), Spectral line shapes/positions (Eⱼ, Γⱼ distribution).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Trp Radiative Lifetime (Single) | 1.9 | ns | Materials & Methods (ref 28) | Explicit | Cited value. |
        | Superradiant State Lifetime (Brightest) | Hundreds | fs | Abstract | Explicit | Stated in abstract. |
        | Subradiant State Lifetime (Darkest) | Tens | s | Abstract | Explicit | Stated in abstract. |
    *   **Note:** Captures the wide range of decay timescales involved, from femtoseconds for superradiant states to seconds for subradiant states.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behaviors described are:
        1.  **Superradiance:** A collective quantum optical effect where the radiative decay rate (Γⱼ) of certain excitonic states is significantly enhanced (max(Γⱼ/γ) >> 1) compared to the decay rate (γ) of an individual Trp molecule, leading to faster, intense light emission.
        2.  **Subradiance:** The complementary effect where other excitonic states have significantly suppressed radiative decay rates (Γⱼ/γ << 1), leading to long lifetimes.
        3.  **Enhanced Fluorescence Quantum Yield (QY):** The overall efficiency of light emission (QY) increases as Trp networks become larger and more organized (e.g., from TuD to MTs), particularly when superradiant states are thermally accessible near the lowest energy state.

### **8.2 Behavior Robustness:**

        *   **Superradiance:** Found to be robust to increasing system size (saturates rather than diminishes for large structures, Figs 5, 6). It is shown to decrease with static disorder (W), but significant enhancement persists even at disorder strengths comparable to thermal energy (W=200 cm⁻¹, Fig 5 bottom). Cooperative robustness (larger systems are more resilient to disorder) is observed for centrioles (Fig 5 bottom).
        *   **QY Enhancement:** Found to be remarkably robust to both thermal effects (calculated at room temp, Fig 3) and static disorder (Fig 4). Even with disorder strong enough to significantly reduce the peak superradiance factor, the thermally averaged QY enhancement persists largely unaffected, especially when the superradiant state is near the bottom of the band.
        *   The score reflects these demonstrated robustness features, balanced by the fact that extreme disorder would eventually quench the effects.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content:
         *   **Superradiance/Subradiance:** Validated theoretically/computationally via diagonalization of the effective Hamiltonian derived from quantum optics principles (Eq. S3). Predictions shown in Figures 3, 5, 6, S3, S5, S6. Consistency checks with scaling analysis performed.
         *   **Enhanced QY:**
             *   **Theoretical Validation:** Predicted computationally by calculating the thermally averaged QY (Eq. 1) using the eigenvalues from the Hamiltonian, including effects of thermal population and static disorder (Figs 3, 4).
             *   **Experimental Validation:** Measured using steady-state fluorescence spectroscopy. QY of MTs compared to Trp and TuD using a standard reference method (Eq. 2). Statistically significant increase in QY from TuD to MTs observed (Table 1), qualitatively confirming the trend predicted theoretically (Fig 3a). Scattering corrections and control experiments (excitation at 295 nm) were performed to improve reliability.
         *   **Limitations:** Experimental validation only performed up to MT level, not larger assemblies (centrioles, bundles). Direct measurement of fs lifetimes for superradiance not performed. Theoretical model uses approximations (single-excitation, constant Γnr).

---

#Key: [wool_self-healing_2008]

# Self-healing materials: a review

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: This paper reviews the field of self-healing materials, focusing primarily on polymers and their composites. It examines the history, evolution, mechanisms, and design principles of various self-repair systems, including passive healing (e.g., using solvents or reactive fluids in hollow fibers), autonomic healing (e.g., microencapsulated healing agents with catalysts), ballistic self-repair (resulting from high-velocity impact energy), and healing based on polymer interface dynamics (crack/craze healing, interdiffusion). The purpose is to provide an understanding of the kinetics and damage reversal processes necessary to impart self-healing characteristics and guide future material design for enhanced lifetime and safety. Components vary depending on the system discussed (e.g., polymer matrix, reinforcing fibers, hollow fibers, microcapsules, healing agents like epoxy or DCPD, catalysts like Grubb's catalyst, metallic particles, specific polymers like HPP, EMMA).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These are representative parameters discussed in the review; specific values are often cited for particular examples within the reviewed literature. Data Reliability is often 'Medium' or 'Low' as they are cited from other works or used conceptually within the review's framework.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Energy input typically comes from the event causing damage (e.g., mechanical stress/strain leading to fracture, ballistic impact kinetic energy, thermal energy). Some systems require subsequent energy input for healing (e.g., thermal energy for welding above Tg, chemical potential energy released during polymerization of healing agent). Ballistic healing explicitly uses impact energy. Shear thickening relies on kinetic energy from deformation. Thermal re-mending uses applied heat.
    *   Units: J (or related units like stress Pa, temperature K)

### **2.2 Energy Transduction**

    *   Content:
        1.  **Damage Input -> Material Deformation/Fracture:** Input energy (mechanical, kinetic) is transduced into elastic/plastic deformation, heat, and surface energy (creating crack surfaces).
        2.  **Ballistic Impact -> Thermal Energy:** Kinetic energy of projectile is dissipated as heat, potentially melting the polymer (Sec 1.4, Eq 1).
        3.  **Fracture -> Chemical Reaction:** Crack propagation ruptures microcapsules/fibers, releasing potential chemical energy stored in monomers/reactants (Sec 1.2, 1.3). Mechanical energy triggers release.
        4.  **Chemical Potential -> Thermal/Bond Energy:** Polymerization/reaction of healing agent converts chemical potential energy into heat (exothermic potential mentioned in Sec 1.2) and the energy of newly formed chemical bonds closing the crack.
        5.  **Applied Heat -> Increased Molecular Mobility:** Thermal energy increases kinetic energy of polymer segments, enabling diffusion and chain entanglement across interfaces (Sec 1.1, 2.1-2.5).
        6.  **High Strain Rate -> Phase Transition (STF):** Kinetic energy from impact/shear is transduced into structural ordering (liquid to solid-like transition) in shear-thickening fluids (Sec 1.4).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The review does not provide quantitative data on the overall energy efficiency of the *healing process* itself (e.g., energy input for damage vs. energy effectively used for repair bond formation or toughness restoration). It discusses fracture energy recovery (GIC or KIC), which relates to mechanical performance restoration, not thermodynamic efficiency. Ballistic healing provides a minimal energy balance (Eq 1) but this estimates temperature rise, not efficiency of healing conversion. Efficiency is highly system-dependent and not a focus of this review. Qualitative assessment: Likely low to medium, as energy is lost to heat, viscoelastic dissipation, incomplete reactions, etc.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation occurs during both damage and healing.
        *   **Damage:** Viscoelastic dissipation (heat), plastic deformation, sound emission, creation of new surface area (fracture energy). In ballistic impact, significant kinetic energy is dissipated as heat, plastic work, and projectile deformation (Sec 1.4).
        *   **Healing (Molecular):** Frictional losses during chain diffusion/reptation (viscous flow).
        *   **Healing (Reactive):** Heat loss from exothermic polymerization reactions (mentioned as possibility Sec 1.2). Energy lost to side reactions or incomplete curing. Evaporation of solvents if used.
        *   **General:** Heat transfer to surroundings.
        Quantification is not provided in the review, but dissipation mechanisms are inherent to the processes described. Qualitative assessment: High dissipation during damage (especially ballistic); Medium-High dissipation during reactive healing (exothermicity); Low-Medium dissipation during diffusion-based healing (viscous losses over time).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes
        *   HPP recovers its high elasticity after resting, indicating memory of the original structure (Sec 1.1, Fig 3).
        *   Crack healing studies (Kausch) show surfaces 'remember' how to rejoin when conditions (temp > Tg) are met (Sec 1.1).
        *   The "twinkling fractal theory" implicitly involves memory in the context of physical aging below Tg, where the non-equilibrium glassy structure slowly evolves towards equilibrium (Sec 4.1).

### **3.2 Memory Type:**

    *   **Retention:** Varies greatly. Structural memory in HPP seems high under rest (Fig 3). Diffusion-based healing memory depends on weld time/temperature (Sec 2.5, 3.5). Reactive healing creates a potentially permanent (but potentially imperfect) healed state (Sec 1.3). Ballistic healing is immediate but may not be perfect. Below Tg memory relates to slow relaxation (Sec 4.1). Rating: Low-Medium (highly variable).
    *   **Capacity:** Primarily represents a single 'undamaged' state to return to, or a spectrum of partially healed states. Not multiple distinct, addressable states. Rating: Low.
    *   **Read-out accuracy:** Measured by recovery of mechanical properties (e.g., toughness, modulus, fatigue life). Recovery is often partial (e.g., 75-80% KIC in Fig 6), indicating imperfect readout/restoration. Rating: Low-Medium.

### **3.3 Memory Retention Time:**

*   Value: Highly variable
*    Units: s, min, h (or Qualitative Descriptor: e.g., "Short-term", "Long-term", "Permanent-like")
    *   HPP: minutes to hours at room temp for significant recovery (Fig 3).
    *   Diffusion healing: scales with M or M^3, potentially very long, temperature-dependent (Sec 2.5, 3.5).
    *   Reactive healing: Time for reaction completion (e.g., 24h mentioned for microcapsule toughness test - Fig 6 caption), then potentially permanent chemical bonds.
    *   Ballistic healing: near-instantaneous resealing (Sec 1.4).
    *   Glassy relaxation: very long timescales below Tg (Sec 4.1).
    Qualitative: Can range from seconds (ballistic) to effectively permanent (fully reacted chemical heal).

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Low (Generally 1-2 states)
*   Units: distinct states

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: Variable, often < 100%
*   Units: % (of original property recovery)

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes (in specific contexts)
        *   **Yes:** The formation of specific microstructures during processing (e.g., stacked lamellae in HPP via stress-crystallization - Sec 1.1), the rearrangement of polymer chains during interface healing (diffusion leading to entanglement network - Sec 2.5, Fig 10), and potentially the dynamic twinkling fractal structure near Tg (Sec 4.1, Fig 16) can be seen as forms of self-organization driven by local thermodynamic and kinetic factors, leading to emergent structures/states without explicit global templating of the final healed structure. Nanoparticle migration to crack tips (Balazs simulation reference - Sec 1.3) is also a form of self-organization. Shear thickening involves particle organization under flow (Sec 1.4).
        *   **No:** The overall placement of microcapsules or hollow fibers is typically designed/engineered, not self-organized during operation (though their distribution might be random). The *healing reaction itself* is a designed chemical process, not self-organization in the structural sense.

### **4.2 Local Interaction Rules:**

    *   Content:
        *   **Polymer Diffusion:** Reptation dynamics govern chain movement along a tube, driven by thermal energy. Interactions are local segment movements constrained by entanglements. Scaling laws relate movement to time and molecular weight (L ~ t^1/2 M^-1/2) (Sec 2.5, Table 1).
        *   **Phase Separation/Crystallization:** Thermodynamic principles drive phase separation or crystallization based on minimizing free energy (e.g., stress-induced crystallization in HPP - Sec 1.1). Local chain packing/alignment rules apply.
        *   **Twinkling Fractal Theory:** Local atomic oscillators transition between 'solid' (x < xc) and 'liquid' (x > xc) states based on thermal energy relative to the anharmonic potential U(x) (Fig 15, Eq 36). Local dynamic equilibrium exists. (Sec 4.1).
        *   **Shear Thickening:** Hydrodynamic and frictional forces between particles increase dramatically at high shear rates, leading to jamming/percolation of rigidity (Sec 1.4).
        *   **Nanoparticle Migration (Simulation):** Implicitly involves interaction potentials between nanoparticles and polymer matrix/crack tip, leading to migration down a potential gradient (Sec 1.3).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Polymer Diffusion | Reptation Dynamics | Reptation time (τ) | Scales as M^3 or M | s | Sec 2.5 | Explicit | Time for chain renewal |
    | Polymer Diffusion | Reptation Dynamics | Entanglement MW (Mc) | Polymer specific (e.g., Table 2) | g/mol | Sec 2.5, Table 2 | Explicit | Threshold for entanglement |
    | Twinkling Fractal | Oscillator Transition | Critical expansion (xc) | ~1/3a (Morse) | Dimensionless | Sec 4.1 | Explicit | Lindemann-like criterion |
    | Twinkling Fractal | Oscillator Transition | Bond dissociation energy (D0) | Material specific (~3 kcal/mol example) | J or kcal/mol | Sec 4.1 | Explicit | Energy scale of potential |
    | Shear Thickening | Particle Jamming | Critical Shear Rate (gc) | Fluid/particle specific | s^-1 | Sec 4.2 | Explicit (concept) | Threshold for thickening |

### **4.3 Global Order:**

    *   Content:
        *   **Healed Interface:** Formation of an entangled polymer network across the initial crack plane (Sec 2.5, Fig 10).
        *   **HPP Structure:** Stacked lamellar morphology perpendicular to extrusion direction (Sec 1.1, Fig 2).
        *   **Twinkling Fractal State:** A percolating, dynamic fractal cluster of 'solid' regions within a 'liquid' matrix near Tg (Sec 4.1, Fig 16).
        *   **Shear Thickened State:** Percolated network providing rigidity in STFs (Sec 1.4).
        *   **Nanoparticle Aggregation (Simulation):** Concentration of nanoparticles at crack tips (Sec 1.3).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

        *   **Polymer Interface Healing:** Predictability is relatively high, governed by well-studied diffusion laws (Table 1, Sec 2.5, 3.5). Strength development follows predictable scaling (t^1/4 or t^1/2). Predictability depends on knowing material parameters (M, Mc, Tg, D0) and processing conditions (T, t, P).
        *   **HPP Structure:** Reasonably predictable based on processing conditions (stress-crystallization).
        *   **Twinkling Fractal:** This is a theoretical model; its quantitative predictability for real glass structure evolution is still under investigation. The *concept* predicts trends (e.g., Tg scaling, Cp change).
        *   **Shear Thickening:** Phenomenon is predictable (occurs above gc), but precise structure and stress response can be complex.
        *   **Nanoparticle Migration (Simulation):** Predictable within the simulation model based on defined parameters.
        Overall score reflects good predictability for established mechanisms like diffusion, but lower predictability for newer theories or complex phenomena like STF details.
    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Polymer Diffusion | Reptation Dynamics | Reptation time (τ) | Scales as M^3 or M | s | Explicit | Time for chain renewal | Sec 2.5 |
| Polymer Diffusion | Reptation Dynamics | Entanglement MW (Mc) | Polymer specific | g/mol | Explicit | Threshold for entanglement | Sec 2.5, Table 2 |
| Twinkling Fractal | Oscillator Transition | Critical expansion (xc) | ~1/3a (Morse) | Dimensionless | Explicit | Lindemann-like criterion | Sec 4.1 |
| Twinkling Fractal | Oscillator Transition | Bond dissociation energy (D0) | Material specific | J or kcal/mol | Explicit | Energy scale of potential | Sec 4.1 |
| Shear Thickening | Particle Jamming | Critical Shear Rate (gc) | Fluid/particle specific | s^-1 | Explicit (concept) | Threshold for thickening | Sec 4.2 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Healed Interface | Entanglement density | Bridge density P(t) | Scales t^1/2 M^-3/2 | mol/m^2 | Explicit | Measures connections | Theory/Exp | Sec 2.5, Table 1 |
| Healed Interface | Mechanical Strength | Fracture Toughness KIC / GIC | ~0 - Virgin Value | MPa m^1/2 / J/m^2 | Explicit | Measures global property | Fracture test | Sec 1.3, Fig 6, Sec 3.4, Fig 13 |
| Twinkling Fractal | Solid Fraction | Solid fraction Ps(T) | Pc - 1 | Dimensionless | Explicit | Order parameter for glass transition | Theory (Eq 36) | Sec 4.1 |
| Shear Thickened | Viscosity / Modulus | Apparent Viscosity η | Increases sharply | Pa·s | Explicit | Macroscopic signature | Rheometry | Sec 1.4 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: Sections M5.2-5.4 skipped as M5.1 is "No")**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Crack Propagation Rate (Fatigue da/dN) | Variable (e.g., Fig 7) | m/cycle | Sec 1.3, Fig 7 | Explicit | Rate of damage accumulation |
        | Polymer Reptation Time (τ) | Scales M^3 or M | s | Sec 2.5, Table 1 | Explicit | Molecular relaxation/diffusion |
        | Interface Healing Time (t*) | Scales M | s | Sec 3.5 (Eq 25) | Explicit | Time to achieve full strength |
        | Reaction Time (Chemical Heal) | Variable (e.g., 24h used in test) | s, h | Fig 6 caption | Explicit | Time for chemical cure |
        | Ballistic Penetration/Healing | << 1 (estimated) | s | Sec 1.4 | Explicit (qualitative) | Duration of impact event |
        | Glassy Relaxation (Aging) | Very long | s, h, years | Sec 4.1 | Implicit | Structural relaxation below Tg |
        | HPP Recovery Time | 10 - 1000 | min | Fig 3 | Explicit | Time for structural relaxation/healing |

    *   **Note:** Various timescales related to damage, healing, molecular motion, and relaxation are discussed across different systems.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes (Limited)
        *   **Healing as Adaptation:** The healing process itself can be viewed as a form of adaptation where the material structure changes (repairs) in response to experience (damage), leading to improved performance (restored strength/toughness). This is a persistent change.
        *   **Fatigue Healing:** The observation that healing can retard fatigue crack growth (Sec 1.3, Fig 7) suggests the material adapts its crack propagation behavior due to the healing process occurring concurrently with damage.
        *   **Physical Aging:** Glassy materials below Tg undergo structural relaxation (aging), changing their properties over time in response to their thermal history (Sec 4.1), which is a form of adaptive plasticity.
        However, this adaptation is generally restorative (returning to an original state) or passive relaxation, rather than learning a new function or optimizing behavior in a reinforcement learning sense.

### **7.2 Adaptation Mechanism:**

    *   Content:
        *   **Damage-Triggered Repair:** Damage (mechanical, thermal) acts as the stimulus. Adaptation involves physical/chemical changes restoring structural integrity. Mechanisms include: polymer chain interdiffusion and entanglement (driven by thermodynamics above Tg or with solvent), chemical reactions (polymerization, crosslinking triggered by released agents), melt flow and solidification (ballistic healing), particle rearrangement (shear thickening relaxation), structural relaxation (HPP void closure, glassy aging). Feedback is implicit: reduced stress concentration after healing might slow further damage.
        *   **No explicit learning rules (Hebbian, reinforcement etc.) are described.** Adaptation is primarily governed by physical chemistry and thermodynamics responding to the damage event or thermal history.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior described is **Self-Healing**: the autonomous restoration of material integrity and mechanical properties (strength, toughness, fatigue resistance) following damage. Specific manifestations include:
        *   Crack closure and strength recovery (general).
        *   Void filling/sealing (microcapsules, hollow fibers, ballistic).
        *   Recovery of elastic properties (HPP).
        *   Retardation of fatigue crack growth.
        *   Reversible hardening/softening (Shear Thickening Fluids, HPP).
        *   Thermally reversible network formation (Diels-Alder chemistry example).

### **8.2 Behavior Robustness:**

        *   **Limitations:** Healing efficiency is often <100% (Sec 1.3, 3.4). Fatigue healing may be significant but doesn't fully restore virgin material behavior (Sec 1.3, Fig 7). Reactive systems (capsules/fibers) are typically limited to a single healing event at a given location (though vascular systems aim to address this - Sec 5). Healing may depend strongly on conditions (temperature, pressure, time, presence of contaminants). Ballistic healing fails at low temperatures (Sec 1.4). Thermoset repair is shown to be difficult (Sec 3.4).
        *   **Strengths:** Some systems demonstrate multiple healing cycles (vascular - Sec 5, Diels-Alder - Sec 5). HPP shows good recovery over cycles (Fig 3). Shear thickening offers repeatable response.
        Overall score reflects potential but significant limitations in efficiency, repeatability (for some systems), and environmental dependence.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The review cites experimental results validating self-healing behavior:
        *   **Mechanical Testing:** Fracture toughness (KIC, GIC) measurements before and after healing using standard tests (e.g., TDCB, CT specimens) (Sec 1.3, Fig 6; Sec 3.4, Fig 13). Cyclic stress-strain tests showing recovery (HPP, Fig 3). Fiber pull-out tests for interface repair (Sec 1.2). Pressure burst tests for ballistic healing (Sec 1.4).
        *   **Fatigue Testing:** Crack growth rate (da/dN) measurements showing retardation or recovery (Sec 1.3, Fig 7).
        *   **Microscopy:** SEM used to visualize damage (cracks, voids) and morphology (HPP structure, Fig 2). Optical imaging of healed cracks/surfaces (Fig 8, Fig 18).
        *   **Spectroscopy/Chemical Analysis:** Not explicitly detailed for validation in this excerpt, but implied for understanding diffusion (DSIMS, neutron reflectivity cited - Sec 2.5).

---

#Key: [milana_morphological_2022]

# Morphological Control of Cilia-Inspired Asymmetric Movements Using Nonlinear Soft Inflatable Actuators:

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of soft, inflatable bending actuators interconnected via passive flow restrictors, driven by a single pressure input. The actuators exhibit nonlinear pressure-volume (PV) curves with snap-through instabilities. The system's purpose is to demonstrate "morphological control" – harnessing the passive mechanical and fluidic characteristics (nonlinearity, flow restriction) to achieve complex, sequenced motions mimicking biological cilia (spatially asymmetric stroke for a single cilium, metachronal wave for an array) using simplified control (single pressure line). Components include: 1) Nonlinear bending actuators (inner latex balloon, outer slit PVC shell, luer locks), 2) Passive flow restrictors (narrow tubes or restricted luer locks), 3) Pressure source (proportional valve), 4) Fluid (water/air). The system *does* convert a single pressure input into sequenced bending motions.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                  | Value                                  | Units         | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------------------------ | :------------------------------------- | :------------ | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Peak/Valley Pressures (NM-low) | ~0.2 / ~0.1 (approx from Fig 2A)      | MPa           | Figure 2A                 | Explicit          | Medium                          | Visual estimation from graph      |

    *   **Note:** Values for Peak/Valley Pressures are approximate readings from Fig 2A. Operating pressure range is taken from spatial asymmetry and metachrony experiments.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is pressurized fluid (air/water) supplied by an external pressure source (proportional valve connected to a pressure supply). The system converts potential energy stored in the pressurized fluid.

### **2.2 Energy Transduction**

    *   Content: 1. Potential Energy (Pressurized Fluid) -> Kinetic Energy (Fluid Flow): Pressure difference drives fluid flow through restrictors and into actuators. 2. Kinetic/Potential Energy (Fluid) -> Elastic Potential Energy (Actuator Material) + Work Done (Bending): Fluid entering the actuator inflates the latex balloon against the PVC shell, storing elastic energy in the deformed materials and performing mechanical work to bend the actuator. The snap-through instability involves a rapid conversion of stored elastic energy and fluid potential energy into kinetic energy of the actuator structure and fluid, followed by reaching a new equilibrium state.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Low. The paper does not quantify efficiency. However, significant energy losses are expected due to: 1) Viscous dissipation in the narrow flow restrictors (explicitly modeled using Darcy-Weisbach). 2) Material hysteresis/damping in the soft actuators during inflation/deflation cycles. 3) The snap-through itself is an irreversible process involving rapid energy release. The primary goal is controllled motion sequence, not energy efficiency. Qualitative assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: 1. Viscous Losses in Flow Restrictors: Modeled by Darcy-Weisbach law (Eq. 3/4), dependent on flow rate, fluid properties, and restrictor geometry (diameter, length). Quantified in simulations via pressure drop Δp. Assessment: Potentially High, especially in the metachrony setup with long, narrow tubes. 2. Material Damping/Hysteresis: Energy lost as heat during cyclic deformation of the latex and PVC materials. Not explicitly quantified, but inherent in soft materials. Assessment: Medium (expected for elastomers under cyclic load). 3. Snap-Through Instability: Rapid, irreversible transition involves energy dissipation (acoustic, thermal). Not quantified. Assessment: Low to Medium per snap event, depends on system dynamics. 4. Fluid Viscosity within Actuators: Minor compared to restrictors. Not quantified. Assessment: Low.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Short-term / Dynamic

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Low (e.g., ~2 states per NM actuator)
*   Units: States

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Hybrid (Analog/Digital aspects)

### **5.3 Computational Primitive:**

    *   Content: Thresholding & Sequencing. The core operation is the triggering of a state change (snap-through bending) when the internal pressure reaches a specific threshold (peak/valley pressure of the PV curve). By connecting actuators with flow restrictors, the system implements sequencing based on the time delays introduced by fluid flow and the different pressure thresholds (implicitly, if actuators differ, or dynamically due to pressure drops). It acts like a fluidic, physically embodied state machine where transitions are governed by pressure thresholds and flow dynamics.
    *   **Sub-Type (if applicable):** Thresholding: Pressure-triggered instability; Sequencing: Flow-delay based state transition.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description             | Value                       | Units   | Source                            | Implicit/Explicit | Justification                                    |
        | :-------------------------------- | :-------------------------- | :------ | :-------------------------------- | :----------------: | :----------------------------------------------- |
        | Input Pressure Ramp Rate (Config 2/3) | 5                           | kPa/s   | Results/Discussion (Spatial)      | Explicit          | Stated rate for quasi-static tests.              |
        | Input Pressure Ramp Time (Config 4) | 1                           | s       | Results/Discussion (Spatial)      | Explicit          | Stated inflation time.                           |
        | Input Pressure Dwell Time (Config 4) | 12                          | s       | Results/Discussion (Spatial)      | Explicit          | Stated dwell time.                               |
        | Input Pressure Deflation Rate (Config 4) | 60                          | kPa/s   | Results/Discussion (Spatial)      | Explicit          | Stated deflation rate.                           |
        | Single Cilium Cycle Time (Config 4 Exp) | 18                          | s       | Results/Discussion (Spatial)      | Explicit          | Mentioned beat time for flow estimate calc.    |
        | Metachronal Wave Cycle Time (Exp)  | ~25 (approx from Fig 4D)    | s       | Figure 4D                         | Explicit          | Visual estimation from pressure profile.         |
        | Metachronal Wave Speed (Inflation) | 23                          | mm/s    | Results/Discussion (Metachronal) | Explicit          | Calculated from experiments.                     |
        | Metachronal Wave Speed (Deflation) | 53                          | mm/s    | Results/Discussion (Metachronal) | Explicit          | Calculated from experiments.                     |
        | Actuator Response/Snap Time        | Likely << 1s (not specified) | s       | Implicit                          | Implicit          | Snap-through is typically fast; not quantified. |

    *   **Note:** Actuator snap time is inferred to be fast relative to the overall cycle times.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: 1. **Sequenced Bending:** Individual actuators bend in a discrete, pre-programmed sequence determined by their nonlinear PV curves and connection topology (e.g., via flow restrictors). 2. **Spatially Asymmetric Stroke (Single Cilium):** A system of two stacked actuators generates a different path during the inflation (forward) and deflation (backward) phases due to the sequenced snapping and hysteresis, mimicking a biological cilium's non-reciprocal motion required for low Reynolds number propulsion. Quantified by the swept area. 3. **Metachronal Wave (Cilia Array):** A linear array of identical actuators connected serially with flow restrictors generates a wave-like progression of bending motion along the array, mimicking the phase-shifted motion of biological cilia arrays. Quantified by wave speed and phase shift.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The emergent behaviors (asymmetric stroke, metachronal wave) are validated through: 1) **Experimental Demonstration:** Fabricated prototypes are actuated, and their motion is recorded and quantified (tip tracking for swept area - Fig 3F, curvature tracking for wave - Fig 4D/E, Supplementary Movies S1-S4, S6). 2) **Numerical Simulation:** Lumped parameter fluidic models (Eq 1-5) incorporating experimentally measured PV/KV curves (Fig 2A) are used to simulate the system dynamics. 3) **Comparison:** Experimental results are directly compared with simulation predictions (e.g., swept area in Fig 3F, curvature/displacement in Fig 4D/E). Good agreement is reported, validating the underlying model and the emergence of the described behaviors from the designed physics. Limitations: Only a small number of actuators tested (2 for single cilium, 4 for array); robustness to variations or noise not systematically tested experimentally.

---

#Key: [esplandiu_unraveling_2018]

# Unraveling the Operational Mechanisms of Chemically Propelled Motors with Micropumps

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper describes chemically propelled micropumps, which are immobilized counterparts of self-propelled micro/nanomotors. These pumps utilize self-generated physicochemical gradients from chemical reactions (primarily redox decomposition of fuels like H2O2 or water splitting) at engineered interfaces (e.g., bimetallic Au/Pt or semiconductor/metal Si/Pt structures) to drive fluid flow (electro-osmosis or diffusio-osmosis). The components are typically patterned metallic (Au, Pt) or semiconductor (Si) discs/surfaces fabricated using techniques like electron-beam lithography or stencils. The purpose is to use these well-defined, stationary pump systems as platforms to experimentally and theoretically investigate the fundamental chemomechanical actuation mechanisms (e.g., mapping chemical gradients, electric fields, fluid flow) relevant to their mobile swimmer counterparts, aiming for better understanding, control, and optimization of micro/nanomotor propulsion. Specific systems studied include Au/Pt pumps in H2O2 and light-activated Si/Pt or Si/Au pumps in water or H2O2.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Substrate ζ potential (ζw, Au/Pt pump) | -33 | mV | Section 2.1 / Eq 4 | Explicit (calculated) | Medium | Calculated from E(r), Vf(r), and Eq 4 |
        | Electric Field Strength (Si/Pt pump, water) | 80 | V/m | Section 3.1 | Explicit | Medium | Likely derived from tracer particle velocities |

    *   **Note:** These are representative key parameters mentioned. ζw is calculated within the paper. Electric field strength is stated but the derivation method (likely from tracer velocities via Eq 2) is implicit. Data reliability is based on whether it's a direct fabrication parameter, a measured property, or a derived quantity.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is chemical potential energy stored in the fuel (e.g., H2O2 decomposition: 2H2O2 -> 2H2O + O2, ΔG < 0) or electrochemical potential differences arising from redox reactions. For light-activated pumps (Si/Pt, Si/Au), visible light provides the energy to generate electron-hole pairs in the semiconductor (e.g., Si), driving photoelectrochemical reactions (water splitting or enhanced H2O2 decomposition).
    *   Units: J/mol (for chemical); W/m² (for light intensity)

### **2.2 Energy Transduction**

    *   Content:
        1.  **Chemical to Electrical:** Redox reactions (e.g., H2O2 decomposition at Au/Pt, photoelectrochemical reactions at Si/Pt) occurring spatially separated on the pump surface generate ionic currents (e.g., proton current from anode to cathode) and establish an electric field in the electrolyte near the surface.
        2.  **Chemical to Concentration Gradient:** Decomposition of fuel (e.g., H2O2) creates concentration gradients of reactants, products (e.g., O2), and ionic species (e.g., H+).
        3.  **Electrical to Kinetic (Fluid Flow):** The self-generated electric field acts on the ions within the electrical double layer (EDL) at the pump/substrate surface, inducing fluid motion via electro-osmosis. The direction depends on the field direction and the surface zeta potential (ζ).
        4.  **Concentration Gradient to Kinetic (Fluid Flow):** Gradients of solutes (neutral or ionic) interact with the surface, leading to pressure imbalances within the EDL or differential ion diffusion, driving fluid flow via diffusio-osmosis.
        5.  **Photon to Electrical (Photoactivated Pumps):** Light absorption in the semiconductor (Si) generates electron-hole pairs. Holes drive oxidation at the semiconductor interface, while electrons are injected into the metal (Pt/Au) to drive reduction, establishing the electrochemical potential difference.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any quantitative measure of energy efficiency (e.g., chemical energy input vs. kinetic energy output of the fluid). Chemomechanical energy conversion at this scale is typically very inefficient (often << 1%). The focus is on mechanism understanding, not efficiency optimization. Score is low based on general knowledge of these systems.

### **2.4 Energy Dissipation**

    *   Content: Energy is primarily dissipated as heat due to:
        1.  Viscous friction within the fluid as it flows.
        2.  Joule heating associated with the ionic currents flowing through the electrolyte resistance.
        3.  Thermodynamic irreversibilities in the chemical reactions (though the reactions themselves can be exothermic or endothermic, entropy production occurs).
        Quantification is not provided. Qualitative assessment: High, given the low efficiency (M2.3).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Fluid Flow Velocity (Au/Pt) | ~6 | μm/s | Section 3.1 | Explicit | Velocity is given, implies a timescale for particle transport over a distance. |
        | Fluid Flow Velocity (Si/Pt, water) | ~9 | μm/s | Section 3.1 | Explicit | Velocity is given, implies a timescale for particle transport over a distance. |
        | Light Response (Switching) | "Fast" | Qualitative | Section 3.1 | Explicit | Stated as "fast switchable capabilities". |
    *   **Note:** The paper primarily characterizes steady-state or quasi-steady-state behavior (flow fields, gradients). Dynamic timescales like how quickly the flow establishes or responds to concentration changes are not explicitly quantified, except for the qualitative mention of fast light switching. Velocities imply transport timescales.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is the generation of directed fluid flow (pumping) near the patterned surface. This is driven by self-generated gradients (electric potential, solute concentration) resulting from chemical or photoelectrochemical reactions at the asymmetric material interface. Specific behaviors include electro-osmotic flow and diffusio-osmotic flow, with direction and magnitude influenced by materials, fuel, surface chemistry, roughness, and external stimuli (light). Associated behaviors include the transport and manipulation of tracer particles within this flow field (attraction, repulsion, complex trajectories). For swimmers (discussed contextually), the behavior is self-propulsion.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary behavior (fluid pumping) is validated experimentally through:
        1.  **Tracer Particle Velocimetry:** Tracking differently charged particles allows quantification of fluid velocity and electric field strength (Section 2.1, Fig 4b-d, Eq 1-3). This directly measures the consequence of the pumping action.
        2.  **Fluorescence Microscopy:** Mapping pH gradients using indicators provides evidence for the location and direction of redox reactions driving the flow (Section 2.1, Fig 4f).
        3.  **Control Experiments:** Comparing treated vs. untreated surfaces (Fig 5), different materials (Au/Pt vs Si/Pt), different fuels (water vs H2O2), different roughness (smooth vs rough Pt, Fig 7), and varying light intensity (Fig 6g,h, Fig 9b) helps isolate the factors controlling the behavior. Pumps with insulating layers (Fig 6b) serve as negative controls.
        4.  **Numerical Simulations:** Complementary simulations reproduce experimental observations (electric field, fluid flow profiles, Fig 4c,d) and explore parameter space, validating the proposed physical mechanisms (Section 2.1).
        Reproducibility seems implied through consistent reporting across studies, but statistical analysis of run-to-run variation is not presented in this Account. Emergence in the strict sense (complex behavior from simple local rules without central control) is not claimed or validated; rather, the behavior is shown to arise deterministically from the designed structure and physicochemical laws.

---

#Key: [chen_integrated_2025]

# Integrated sensing and communication based on space-time-coding metasurfaces

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a Space-Time-Coding Metasurface (STCM) designed for Integrated Sensing and Communication (ISAC). It uses a programmable metasurface composed of unit cells (with PIN diodes) controlled by an FPGA to manipulate incoming electromagnetic (EM) waves. The system simultaneously controls the fundamental frequency (carrier) wave for communication (beam steering towards a user) and generates spatially distributed harmonics for sensing (estimating Direction of Arrival - DOA of targets or the transmitter). Key components include the STCM (16x16 unit cells, 2-bit control), PIN diodes, FPGA for control, RF signal source (transmitter), receiving antennas (for harmonics and communication signal), a USRP for signal processing/generation, and optionally an Artificial Neural Network (ANN) or sensing matrix algorithm for DOA estimation. Its purpose is to demonstrate a hardware platform that integrates sensing and communication functions cost-effectively, eliminating the need for separate sensors and potentially dedicated control links. Two specific coding strategies are proposed: "adjustable partitioning" (splitting the metasurface for dedicated sensing/communication tasks) and "full-aperture" (using the entire metasurface for both via STC matrix design).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters listed are key specifications of the demonstrated STCM system. Reliability is high as they are stated design/experimental parameters.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the incoming electromagnetic wave (microwave frequency) illuminating the metasurface, originating from a transmitting antenna (ANT1 connected to USRP via mixer). Additional energy input is required for the FPGA and biasing the PIN diodes to control the metasurface states.
    *   Value: Not specified for EM wave power; Control power not specified.

### **2.2 Energy Transduction**

    *   Content: 1. Incoming EM wave energy is reflected by the metasurface. 2. The phase of the reflected wave at each unit cell (or column) is modulated based on the applied STC matrix codes implemented via PIN diode switching (electrical control energy transduced to change EM properties - reflection phase). 3. This space-time modulation converts energy from the fundamental frequency (f_c) into harmonic frequencies (f_c + ν*f_0). 4. The spatial distribution of energy at different frequencies (fundamental and harmonics) is controlled (beamforming/steering). Electrical energy is consumed by the PIN diodes during switching and to maintain states, and by the FPGA/USRP for control and processing.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify the overall energy efficiency of the ISAC process or the metasurface reflection/conversion efficiency. However, metasurfaces, especially reflective ones with diode losses, are generally not highly efficient. Energy is lost in diodes, imperfect reflection, and significant power is spread into multiple desired/undesired harmonics and sidelobes. The primary goal stated is reduced hardware cost and integration, not necessarily high energy efficiency compared to dedicated systems. The score reflects low expected efficiency for the overall system (RF and control), particularly the harmonic generation process. Qualitative assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include: 1. Resistive losses in the PIN diodes when switched ON. 2. Insertion loss/reflection loss from the metasurface structure itself (dielectric and conductor losses). 3. Energy scattered into unwanted directions/sidelobes or unmodulated specular reflection. 4. Energy converted into unwanted harmonic frequencies. 5. Power consumption by the FPGA and control circuitry. 6. Power consumption by the USRP and mixers. These are not quantified in the paper. Qualitative assessment: Medium to High (due to active components, harmonic generation, and control overhead).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Partial

**(Conditional: M5.1 is "Partial", including M5.2-5.4 for the embodied part)**

### **5.2 Computation Type:**

    *   Content: Analog

### **5.3 Computational Primitive:**

    *   Content: Wavefront Transformation (including phase shifting, spatial filtering/beamforming, frequency mixing/harmonic generation). The basic operation is the local phase shift applied by each unit cell (or column) to the incident wave according to the time-varying code, contributing to the overall far-field pattern described by Equations 1 and 2.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | EM Wave Frequency (f_c) | 10.3 | GHz | Experimental verification section | Explicit | Stated operating frequency. |
        | STCM Modulation Frequency (f_0) | e.g., 2 | MHz | Fig 5 caption text | Explicit | Stated modulation frequency for specific experiment. Variable in principle. |
        | Signal Bandwidth (QPSK Example) | ~0.5+ | Mbps (Symbol Rate) | Fig 5 text | Explicit | Stated symbol rate implies minimum bandwidth. |

    *   **Note:** Timescales cover the EM wave, control modulation, device physics, and system operation.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: 1. **Beam Steering:** Control of the propagation direction of the fundamental frequency EM wave for communication. 2. **Harmonic Generation & Spatial Distribution:** Conversion of fundamental frequency energy into multiple harmonic frequencies with controllable, distinct spatial radiation patterns for sensing. 3. **Integrated Sensing and Communication (ISAC):** Simultaneous execution of beam steering for communication and harmonic generation/analysis for DOA sensing using the same hardware platform. 4. **Self-Adaptive Operation:** Real-time adjustment of the communication beam direction based on the DOA estimated from the harmonics.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims are validated through experimental measurements presented in Figures 4, 5, and 6.
        *   **Beam Steering:** Measured far-field patterns at the fundamental frequency for different coding sequences confirm directional control (Fig 2b theory, Fig 4c, Fig 5a measured).
        *   **Harmonic Generation/Distribution:** Measured far-field patterns (Fig 4d-f, Fig 5b-c) and received spectra (Fig 5d-e) confirm the generation and spatial separation of harmonics.
        *   **DOA Estimation:** ANN-based estimation from measured harmonic amplitudes shows accuracy within 3° across a range of angles (Fig 5f).
        *   **ISAC & Self-Adaptation:** System-level tests (Fig 6) demonstrate successful image transmission recovery when the STCM actively steers the beam based on the estimated DOA, compared to failure when inactive, across various transmitter positions (angles).
        *   **Reproducibility:** Experimental results are presented, implying reproducibility, though statistical analysis of multiple runs is not shown. Theoretical calculations (Fig 2) align with experimental trends.
        *   **Limitations:** Validation primarily in controlled lab/anechoic chamber settings (Fig 4a) or specific indoor/outdoor setups (Fig 6a, Supp Note 11). Scalability to larger arrays or different frequency bands not shown. Multi-target sensing validation not detailed in main text. Robustness to interference not explicitly tested.

---

#Key: [santoli_nanomacro-integrated_2010]

# Nano/Macro-Integrated Neuromorphic Sensorimotor Automata—A Bioinspired Nanoscale Field-Theoretic Approach to Motility

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is a theoretical concept called the "Huyghens Engine" (HE), proposed as a universal information processing (IP) engine for designing bioinspired sensorimotor automata capable of autonomous motility (walking, swimming, flying). It operates based on Huygens' principles of wave propagation and coupled oscillator synchronization. Components include: synchronizing sensors to transduce environmental stimuli (e.g., fluid flow) into wave patterns, an IP unit utilizing Generalized Quantum Holography (GQH) based on Lie group geometry (specifically Heisenberg group G) for processing these patterns via phase conjugation, and actuators (e.g., wings, fins) whose shape and dynamics are adjusted based on the processed information to interact with the environment. The purpose is to achieve holistic, bioinspired sensorimotor behavior by resonantly coupling internal nano-fields with external fields (aerodynamic, hydrodynamic, gravitational), potentially via Heisenberg's molecular field, moving beyond purely logical AI approaches.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters are fundamental to the theoretical models (oscillator dynamics, QFT) proposed but are not given specific values for a concrete implementation. Reliability is Low as they are part of a theoretical model, not measured or specified values for a device.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input considered is the energy from the environment impinging on the automaton, such as aerodynamic/hydrodynamic flows or gravitational fields, which act as stimuli driving the system's sensors (oscillators). The paper also implicitly requires energy to sustain oscillations and perform information processing, although it emphasizes minimizing this internal consumption.

### **2.2 Energy Transduction**

    *   Content: Energy flows from the external environment (e.g., kinetic energy of fluid) to mechanical oscillations in sensors. Synchronization processes aggregate this energy/information into coherent wave patterns. This information is processed via GQH (potentially involving transformations between energy levels/states). Phase conjugation maps the processed information to control signals for actuators (e.g., wings/fins), converting internal state energy/information back into mechanical work on the environment (motility). The QFT section suggests transduction mechanisms involving molecular potential energy landscapes.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper explicitly states that phase gating in GQH allows for "very low power consumption per bit of information," suggesting high efficiency for the *information processing* stage. However, the overall efficiency of the entire sensor-IP-actuator loop involving chaotic dynamics, synchronization, and mechanical actuation against environmental forces (like drag) is not quantified and likely involves significant losses (e.g., through dissipative chaos mentioned). The score reflects potentially high IP efficiency but unknown and possibly lower overall system efficiency. Qualitative assessment: High (IP), Medium/Low (Overall).

### **2.4 Energy Dissipation**

    *   Content: The paper explicitly mentions dissipation: 1) "irreversible nanochaotic processes" used to build codes via Information Compression (IC) (Section 4, point 1). 2) "dissipative nanochaos" in the context of synchronization in the transient regime (Section 4). Standard physical dissipation mechanisms like friction/viscosity during actuation (swimming/flying) and potential heat loss during computation/oscillation are implicitly present but not discussed. Quantification is absent. Qualitative assessment: High (during chaotic code generation), Present but Unquantified (Actuation, other processes).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceed with M3.2-M3.8 where applicable.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: 1. **Oscillator Dynamics:** Individual oscillators `x_k` follow dynamics described by `f(x_k)`. 2. **External Forcing:** Oscillators are forced by external periodic signals `εp(x,t)` (Eq. 1). 3. **Mutual Coupling:** Oscillators interact mutually, e.g., `dx_1/dt = f_1(x_1) + εp_1(x_1, x_2, t)` (Eq. 2). 4. **Phase Coupling (Lattice):** Phase dynamics in a lattice are governed by nearest-neighbor interactions `dφ_k/dt = ω_k + εq(φ_{k-1}-φ_k) + εq(φ_{k+1}-φ_k)` (Eq. 3). 5. **Resonant Coupling:** Interaction between internal and external fields occurs via adaptive resonant coupling in GQH (Section 4). 6. **Molecular Interactions (QFT):** At the nanoscale, interactions are governed by the inter-molecular potential `V(x-y)` in the Heisenberg Molecular Field Lagrangian (Section 5).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The global order emerging from local interactions includes: 1) **Synchronized States:** Perfect synchronization, phase locking, or clustering in coupled oscillator systems (Section 4). 2) **Coherent Flows/Waves:** Spatially extended patterns like plane waves in continuous systems or synchronized coherent oscillations representing sensory data (Section 4). 3) **Neural Images:** Patterns formed via GQH representing perception/action information (Section 4). 4) **Adaptive Structures:** Self-organized plasticity in the "neural net" connections via GQH (Section 4). 5) **Potentially structured matter fields** at the nanoscale via QFT interactions (Section 5).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 2. (Rubric: 0=No mention of relevant concepts; 2=Mentions related mathematical structures like topology/geometry/fibre bundles but no explicit Yoneda application; 5=Uses categorical language suggesting local-global consistency; 8=Explicitly attempts Yoneda mapping; 10=Rigorous proof of Yoneda embedding properties). The paper uses Lie groups, geometry, topology, fibre bundles, gauge potentials (Section 4), which are structures often studied using category theory, hinting at potential connections relevant to Yoneda (local behavior determining global properties). However, the embedding itself is not discussed or utilized.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Hybrid (Analog/Neuromorphic/Wave-based/Geometric)

### **5.3 Computational Primitive:**

    *   Content: Several primitives are involved: 1. **Synchronization Detection/Phase Comparison:** Implicit in oscillator coupling and GQH. 2. **Resonant Coupling Identification:** Core mechanism for perception-action link. 3. **Phase Conjugation:** Used in GQH to map perception to action. 4. **Geometric Mapping (exp/log):** GQH performs encoding/decoding between Lie algebra and Lie group `exp: Lie(G) -> G`, `log: G -> Lie(G)`. 5. **Information Compression (via Chaos):** Generation of minimal length algorithms from dynamics. 6. **Potential Calculation:** Implicit in QFT description (interaction term).
    *   **Sub-Type (if applicable):** Geometric Mapping: exp/log on Heisenberg Group; Phase Manipulation: Conjugation.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
    *   **Note:** The paper discusses dynamic processes but provides no quantitative values for these timescales.

### **6.2 Active Inference:**

    *   Content: Partial
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:**
        *   **Prediction Error Metric:** Quantify the mismatch between the sensory input (environmental state) and the system's internal representation or generated action (e.g., using correlation, mutual information, or a defined error function related to GQH). Track its reduction over time during adaptation. [`MetricNode` attribute: `PredictionErrorReductionRate`]
        *   **Adaptation Timescale:** Measure the time required for the system to adapt its resonant couplings or actuator dynamics in response to environmental changes. [`TemporalNode` attribute: `AdaptationTimescale`]
        *   **Model Complexity:** If the internal GQH representation or the oscillator network structure can be quantified (e.g., using topological measures, information content), track its evolution during learning/adaptation. [`ComplexityNode` attribute: `ModelComplexity`]
        *   **Goal Achievement Rate:** Quantify the success rate or efficiency of motility (e.g., speed, stability) as a measure of how well the system achieves its implicit goal. [`BehaviorNode` attribute: `GoalAchievementRate`]

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The primary mechanism described is "adaptive resonant coupling" within the GQH framework. This implies that the system adjusts its internal parameters or connections (e.g., oscillator frequencies, coupling strengths, 'neural net' connections represented topologically) to achieve or maintain resonance with relevant environmental signals. This adaptation provides "plasticity". The paper also links adaptation and learning needs to the redundancy/degeneracy available in a gauge potential description of the system's 'connections' (fibre bundle topology), suggesting changes in these connections as the mechanism. The specific rules or algorithms governing this adaptation (e.g., how resonance mismatch drives changes) are not detailed.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behavior targeted is autonomous sensorimotor activity, specifically **motility** (walking, swimming, flying) in macro/microrobots. This is achieved through a holistic perception-action loop where the automaton senses environmental fields (aerodynamic, hydrodynamic), processes this information via HE/GQH, and actuates (e.g., adjusts wing/fin shape/motion) to interact appropriately with the environment. Examples discussed include interpretations of insect flight aerodynamics (unsteady state mechanisms like delayed stall, rotational circulation, wake capture) and aquatic locomotion via shape changes driven by motoneuron activity images. The behavior is fundamentally about coherent, adaptive interaction with environmental fields.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper does *not* present experimental validation of the proposed HE mechanism generating motility. Instead, it *interprets* existing experimental findings on insect flight (Ellington et al. 1996, Dickinson et al. 1999) and aquatic locomotion (Daniel 1995) *through the lens* of the HE concept (Section 4). It argues that phenomena like unsteady aerodynamics (delayed stall, rotational circulation, wake capture) are consistent with HE principles (e.g., involving BI, extralogical processes, holistic structure-function). This serves as plausibility arguments, not direct validation of the HE model itself producing these behaviors. No control experiments or quantitative comparisons between HE predictions and experimental data are provided.

---

#Key: [kamsma_chemically_2024]

# Chemically Regulated Conical Channel Synapse for Neuromorphic and Sensing Applications

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a single, azimuthally symmetric conical microfluidic channel connecting two reservoirs containing an aqueous electrolyte. The channel wall has functionalized surface groups that can undergo displacement reactions (specifically, A3- displacing B2- from binding sites SA- or SB). The electrolyte contains a 1:1 background electrolyte (e.g., KCl) and low concentrations of trivalent (A3-) and divalent (B2-) reactant anions. The system's behavior is governed by continuum transport equations (Poisson-Nernst-Planck-Stokes, PNPS) coupled with Langmuir kinetics describing the surface reactions. Its purpose is to emulate multiple distinct chemically regulated synaptic features (STP, STD, LTP, LTD, FDP, chemical-electrical coincidence detection similar to NMDA receptor function/Hebbian learning) using a single, experimentally accessible channel, bridging fluidic iontronics with neuromorphic computing and sensing. It functions by coupling fast voltage-induced volumetric salt accumulation/depletion (concentration polarization) with slower, non-linear surface charge modulation via the displacement reaction.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These are key parameters defining the physical system and reaction kinetics studied computationally. Reliability is high as they are defined inputs to the model.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is electrical, provided by an externally applied voltage V(t) across the channel connecting the two reservoirs. Chemical energy is also implicitly involved via concentration gradients (imposed or voltage-induced) and the surface reactions, but the main driver for the dynamic effects explored is the electrical field.
    *   Value: e.g., 1.2 (for pulses) | Variable (for V(t))
    *   Units: V (Volts)

### **2.2 Energy Transduction**

    *   Content: Electrical energy is transduced into:
        1.  **Ionic Kinetic Energy:** Ions move due to the electric field (electromigration/Ohmic conduction) and concentration gradients (diffusion).
        2.  **Fluid Kinetic Energy:** The electric field acts on the net charge in the diffuse layer, driving electroosmotic flow (EOF) (electric body force term in Eq. 6).
        3.  **Chemical Potential Energy:** The electric field drives ions against concentration gradients, leading to accumulation/depletion (concentration polarization), storing energy in these gradients.
        4.  **Surface Chemical Energy:** Changes in local ion concentrations near the wall, driven by the applied voltage, shift the equilibrium of the Langmuir surface reaction (Eq. 2), changing the stored chemical energy associated with the bound/unbound state of surface groups and the associated surface charge density.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss or quantify the energy efficiency of the system for its neuromorphic functions (e.g., energy per synaptic event emulation). The focus is on demonstrating the phenomena. Efficiency would depend on factors like input voltage, pulse duration, ion mobility, and channel resistance, which change dynamically. Qualitatively, iontronic systems are often considered potentially low-power, but no specific metrics are given here.

### **2.4 Energy Dissipation**

    *   Content: Energy is dissipated primarily through:
        1.  **Joule Heating:** Due to ionic current flowing through the electrolyte resistance (related to Ohmic conduction in Eq. 5).
        2.  **Viscous Dissipation:** Due to fluid flow (EOF) within the channel (related to the η∇²u term in Eq. 6).
        3.  **Reaction Enthalpy Changes:** While likely small given the low concentrations, the surface binding/unbinding reactions (Eq. 1) will have associated enthalpy changes, leading to heat release or absorption.
        Quantification is not provided. Qualitative assessment: Joule heating is likely significant during voltage pulses due to ion current. Viscous dissipation depends on flow velocity. Reaction enthalpy is likely minor.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Seconds to ~10 minutes (estimated)
*    Units: s / min

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: 0.1 - 0.2 (Initial rate for LTP)
    *   Units: % / s

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Skipping M4.2-M4.7 as M4.1 is "No")**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog / Neuromorphic

### **5.3 Computational Primitive:**

    *   Content: The fundamental operations demonstrated are:
        1.  **Non-linear Signal Integration/Modulation:** The channel conductance (output) changes non-linearly based on the history and characteristics (amplitude, frequency, duration) of electrical and chemical input signals. This underlies STP, LTP, LTD.
        2.  **Frequency Filtering:** The system exhibits frequency-dependent plasticity (FDP), acting like a filter where different input frequencies lead to different long-term conductance states (Fig 3). This integrates history over time.
        3.  **Coincidence Detection (AND-like Logic):** A significant LTP response requires the near-simultaneous presence of both a chemical stimulus (reactant ion release) and an electrical stimulus (voltage pulse), acting like an AND gate (Fig 4a,b).
    *   **Sub-Type (if applicable):** Filtering: Frequency-dependent; Logic Gate: AND-like (chemical-electrical).

### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Concentration Polarization (τ) | ~5.6 | ms | Text (p. 3), calculated from L²/12D | Explicit | Value derived from explicitly stated parameters L and D. |
        | Voltage Pulse Duration (STP/LTP/LTD) | 0.5 | s | Text (p. 3, Fig 2), Text (p. 4, Fig 4) | Explicit | Value stated for specific simulations. |
        | Voltage Pulse Duration (FDP) | 3 | ms | Text (p. 3, Fig 3) | Explicit | Value stated for FDP simulations. |
        | Surface Charging (during pulse) | < 0.5 | s | Fig 2b | Implicit | Inferred from Fig 2b showing charge reaches plateau within the 0.5s pulse. Rate is "fast". |
        | Surface Discharging (post-pulse) | Seconds to Minutes | s, min | Fig 2b, Text (p. 3) | Mixed | Fig 2b shows slow decay. Text estimates up to ~10 min based on extrapolation. Rate is "slow". |
        | LTP Initial Decay Rate | 0.1-0.2 | %/s | Text (p. 3) | Explicit | Explicitly stated based on Fig 2c. |

    *   **Note:** Multiple relevant timescales exist, corresponding to fast ion transport/concentration changes and slower surface reaction/charge dynamics.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism involves the coupling of fast, voltage-induced changes in local ion concentrations near the channel wall with the slower, non-linear kinetics of a surface displacement reaction governed by Langmuir kinetics (Eq. 2).
        1.  **Stimulus:** Electrical pulses (voltage V(t)) and/or changes in reactant ion concentration (chemical signal Δρ<sub>i</sub>(t)).
        2.  **Internal Change:** Applied voltage drives concentration polarization (changes in ρ<sub>A</sub>, ρ<sub>B</sub> near the wall). This shift in local concentrations alters the rates of the forward and backward surface reactions (Eq. 2).
        3.  **State Modification:** The imbalance in reaction rates leads to a net change in the surface charge density σ(x,t). Due to the nature of the reaction and Coulombic effects, charging during a positive voltage pulse can be fast, while discharging after the pulse is slow.
        4.  **Behavioral Change:** The altered surface charge density σ modifies the channel's overall ionic conductance g(t). This change persists as long as the surface charge differs significantly from its initial equilibrium state.
        This mechanism allows electrical and chemical signals, along with their timing and frequency, to induce persistent changes in the system's conductance, mimicking synaptic plasticity and Hebbian learning (via coincidence detection).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behaviors are analogous to biological synaptic functions:
        1.  **Short-Term Potentiation (STP):** Transient increase in conductance during/immediately after a voltage pulse due to ion accumulation.
        2.  **Short-Term Depression (STD):** (Mentioned by analogy, not explicitly shown but implied as the opposite of STP). Transient decrease in conductance.
        3.  **Long-Term Potentiation (LTP):** Persistent increase in conductance following specific stimuli (e.g., positive voltage pulse, or combined chemical+electrical pulse) due to slow surface discharging.
        4.  **Long-Term Depression (LTD):** Persistent decrease in conductance following specific stimuli (e.g., positive voltage pulse with specific initial concentration gradient) due to surface discharging towards less negative values.
        5.  **Frequency-Dependent Plasticity (FDP):** The magnitude of long-term conductance change depends on the frequency of input voltage pulses.
        6.  **Chemical-Electrical Coincidence Detection (AND Logic):** Significant LTP requires the simultaneous presence of both a chemical input (reactant ion concentration increase) and an electrical input (voltage pulse).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors (STP, LTP, LTD, FDP, Coincidence Detection) are validated through numerical simulations using finite-element (FE) calculations solving the coupled PNPS and Langmuir kinetics equations (Eqs. 2-6).
        *   **Operational Definitions:** The behaviors are defined operationally via changes in channel conductance (g/g<sub>0</sub>) over time in response to specific input protocols (voltage pulses, concentration changes).
        *   **Control Experiments (Simulation):** Implicit controls exist (e.g., conductance before pulse, response to chemical signal alone vs. voltage alone vs. combined signal in Fig 4a).
        *   **Quantitative Analysis:** Results are presented quantitatively (Figs 2c, 3, 4a). An analytic approximation (AA) is also developed to provide mechanistic insight (Fig 2b), though quantitative agreement is limited.
        *   **Robustness/Reproducibility:** Not demonstrated (simulations for specific parameters).
        *   **Limitations:** Validation is purely computational; experimental verification is proposed but not presented. Sensitivity to parameters and noise is not explored.

---

#Key: [langenegger_-memory_2023]

# In-memory factorization of holographic perceptual representations

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is an "in-memory factorizer," a non-von Neumann compute engine designed to efficiently factorize high-dimensional holographic vectors (product vectors) into their constituent attribute vectors. It combines in-memory computing (IMC) using crossbar arrays of phase-change memory (PCM) memristive devices with an enhanced variant of a resonator network algorithm based on Vector Symbolic Architectures (VSA) / Hyperdimensional Computing (HDC). The core operation involves iterative matrix-vector multiplications (MVMs) performed in-memory to calculate similarities and projections against stored "codebooks" of potential attribute vectors. It utilizes sparse activations (winners-take-all) and exploits the intrinsic stochasticity of the PCM devices to enhance performance and escape limit cycles common in deterministic resonator networks. The purpose is to solve the "unbinding" problem in sensory perception and cognition, disentangling attributes combined in a sensory signal or cognitive representation, demonstrated with visual perception tasks using the RAVEN dataset after initial processing by a Convolutional Neural Network (CNN). Components include: PCM-based IMC cores (crossbar arrays, ADCs, DACs), a host computer/FPGA (for control, unbinding, activation, bipolarization), VSA/HDC framework, resonator network algorithm (enhanced with stochasticity and sparse activation), and codebooks stored in PCM.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name        | Value                             | Units    | Source (Fig/Table/Section)                 | Implicit/Explicit   | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit)                               |
        | :-------------------- | :-------------------------------- | :------- | :----------------------------------------- | :------------------ | :-----------------------------: | :------------------------------------------------------------ |
        | Activation Threshold (T) | 33 (Expt), Optimized (Sim)        | ADC count units (Expt) / arb units (Sim) | Fig 4c, Methods, Supp Note 2 | Explicit (Expt value) / Mixed (Sim: procedure described) | High (Expt) / Medium (Sim: Optimized) | Bayesian Optimization (Sim)                                   |
        | PCM Noise (Aggregated σ) | ~0.98 (Expt), [0.293, 1.277] (Optimal Sim Range) | µS | Ext. Data Fig 1, Supp Note 3 | Mixed             | Medium                          | Derived from device measurements and simulations (Supp Note 3) |

    *   **Note:** Only 5 parameters are requested, but 7 key ones relevant to implementation are listed for completeness. Units for thresholds are context-dependent (raw ADC counts vs normalized simulation values).

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Electrical energy is supplied to operate the CMOS circuitry (FPGA, host processor, IMC core peripherals like ADCs/DACs/drivers) and to perform the MVM operations within the PCM crossbar arrays (applying voltage pulses and sensing currents). The system requires external power.
    *   Value: Specific voltage/current levels for the overall system are not provided, but estimates for components exist (e.g., V_read=0.1V, VDD=0.8V for digital logic). Energy per query is estimated.
    *   Units: Volts (V), Amperes (A), Joules (J)

### **2.2 Energy Transduction**

    *   Content:
        1.  **Digital-to-Analog:** Digital input vectors (voltages) are converted to analog voltage pulses (constant pulse-width modulation mentioned) applied to the crossbar rows (wordlines). (Performed by DACs/drivers).
        2.  **Electrical-to-Physical (Computation):** Applied row voltages drive currents through PCM devices according to their conductance state (Ohm's Law). These currents sum along the columns (bitlines) according to Kirchhoff's Current Law, physically performing the MVM.
        3.  **Physical-to-Electrical (Sensing):** The summed currents on the bitlines are sensed and converted back to digital values by ADCs.
        4.  **Electrical (Digital Processing):** The host/FPGA performs digital operations (unbinding via element-wise multiplication in bipolar space, sparse activation, permutations, convergence check) consuming electrical energy in standard CMOS logic.
        5.  **Electrical-to-Phase Change (Programming - Initial Setup):** Electrical pulses are used initially to program the PCM devices to desired conductance states (SET/RESET operations involving amorphous/crystalline phase transitions), storing the codebooks. This is primarily for setup, not during factorization iterations.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper explicitly claims significant energy savings compared to a reference digital design. Supplementary Note 4 and Table S3 estimate the energy for a single query factorization (D=512, M=512) as 33.1 µJ for the in-memory factorizer vs. 402 µJ for the digital reference (12.2x saving). The energy efficiency for MVM operations is estimated at 182 TOPS/W (similarity) and 270 TOPS/W (projection) for the analog cores (Table S3, D=512, M=512). These figures suggest high efficiency for the core computational task compared to digital alternatives, justifying a high score. However, it's not perfectly efficient (score < 10) due to overheads in peripherals (ADC/DAC) and off-chip processing. Energy breakdown estimates (Fig S3, D=512, M=512) show peripherals (DAC/ADC) contribute significantly to the energy budget (e.g., DAC+ADC is ~87% for similarity, ~84% for projection).

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation occurs through several mechanisms:
        1.  **Joule Heating:** Resistive losses in the PCM devices and interconnects during MVM read operations (I²R). Quantified implicitly through the energy consumption of the crossbar operation itself (estimated in Supp Note 4, Fig S3).
        2.  **Peripheral Circuits:** Energy consumed by DACs (PWM generation), ADCs (conversion), voltage regulators, and other analog/digital support circuitry on the IMC core. Quantified contribution estimated in Supp Note 4, Fig S3.
        3.  **Digital Computation:** Energy dissipated by the host computer or FPGA performing unbinding, activation, and control logic. Estimated contribution in Supp Note 4, Table S3 ("Other peripherals" + assumed digital reference MVM cost).
        4.  **Capacitive Charging/Discharging:** Energy required to charge and discharge the capacitance of wordlines and bitlines during operation. Estimated contribution included in Supp Note 4 analysis.
        5.  **PCM Programming:** Energy used during the initial writing of the codebooks (significant but occurs offline). Not quantified during iterative factorization.
        Qualitatively, peripheral circuits appear to be a major source of dissipation during operation based on Fig S3.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding with M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term (characteristic of PCM)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 65,536 (per core, potentially scalable)
*   Units: cells / potential weights

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: High (Implied > 99%)
*   Units: % (Factorization Accuracy used as proxy)

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Addressed via drift analysis
    *   Units: µS/time or %/time (Implicit)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    <!-- | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |
    | Read (MVM)          | See Total Query Energy       | See Total Query Power           | µJ/Query, W | Medium           | Supp Note 4, Table S3 | Mixed             | Energy breakdown is for MVM operation, not per bit read. |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :----------------- | :------------------------------------------------ | :-------------------------------- | :------ | :------------------------ | :-------------------------------- | :------------------ | :------------------------------------------------- |
    | Noise Tolerance    | Range of aggregate noise σ for peak performance | [0.293, 1.277]                    | µS      | Attribute `noise_tolerance` | Ext. Data Fig 1, Supp Note 3    | Explicit          | Range explicitly identified in simulation results. |
    | Drift Coefficient ν | Rate of conductance decay over time               | 0.0428                            | unitless| Attribute `drift_coefficient_nu` | Supp Note 3                     | Explicit          | Explicitly derived from measurements.            |
    | Read Noise σ_r     | Standard deviation of noise during read           | 0.3951                            | µS      | Attribute `read_noise_std`  | Supp Note 3                     | Explicit          | Explicitly derived from measurements.            |
    | Program Noise σ_p  | Standard deviation after programming              | 1.1636                            | µS      | Attribute `program_noise_std`| Supp Note 3                     | Explicit          | Explicitly derived from measurements.            | -->

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**

### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", proceeding with M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog / Neuromorphic / Hybrid

### **5.3 Computational Primitive:**

    *   Content: Matrix-Vector Multiplication (MVM) / Dot Product. Specifically, the computation implemented in the crossbar is `output_vector = Matrix * input_vector`, where the matrix elements are encoded in the PCM conductances and the input/output vectors are voltages/currents. This is used for both similarity calculation (`αf(t) = x˜f(t)·Xf`) and projection (`xˆf(t+1) = sign(αf(t)·(Xf)T)`, where the MVM part is `αf(t)·(Xf)T`).

### **5.4 Embodied Computational Units**
| Unit ID    | Description        | Processing Power           | Energy/Operation        | Freq/Resp. Time | Bit-Depth                               | Data Source                | Implicit/Explicit   | Justification                                                                 |
| :--------- | :----------------- | :------------------------- | :---------------------- | :--------------: | :-------------------------------------- | :------------------------- |:-----------------:| :---------------------------------------------------------------------------- |
| IMC Core 1 | Similarity Calc. MVM | 52.4 TOPS (Peak, Est.)   | 2.87 nJ/iter (Est.)     | ~10 ns/iter      | 8 bits (ADC output, Est.)             | Supp Note 4, Table S3      | Mixed             | Peak TOPS, Energy, Time are explicitly estimated. Bit-depth is estimated based on ADC details. |
| IMC Core 2 | Projection MVM     | 13.1 TOPS (Peak, Est.)   | 1.94 nJ/iter (Est.)     | ~40 ns/iter      | 1 bit (ADC output, bipolarized, Est.) | Supp Note 4, Table S3      | Mixed             | Peak TOPS, Energy, Time are explicitly estimated. Bit-depth inferred from bipolarization. |

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description           | Value                           | Units    | Source                    | Implicit/Explicit   | Justification                                     |
        | :------------------------------ | :------------------------------ | :------- | :------------------------ | :------------------ | :------------------------------------------------ |
        | MVM Operation Time (per core)   | O(1) theoretical; ~10-40 actual | ns       | Abstract, Supp Note 4     | Mixed             | O(1) explicit theory; ns values estimated in SN4. |
        | Factorization Iteration Time    | ~350 (Est., D=512)              | ns       | Supp Note 4, Table S3     | Explicit (Estimate) | Calculated estimate in Supp Note 4.              |
        | Convergence Time (Avg, Expt)    | 3312 (iterations)              | iterations | Section II                | Explicit          | Explicitly reported experimental result.        |
        | Convergence Time (Avg, Expt)    | ~1.16 (3312 iter * ~350 ns/iter)| ms       | Section II, Supp Note 4 | Implicit          | Calculated from iteration count and time estimate. |
        | PCM Drift Timescale             | Seconds to Days+               | s        | Supp Note 3               | Mixed             | Measurement range T0=60s to 720,000s explicit.    |

### **6.2 Active Inference:**

    *   Content: Partial / Unclear

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is the **factorization** of a high-dimensional holographic product vector (`p`) into its constituent factor vectors (`xˆ1, xˆ2, ..., xˆF`) chosen from predefined codebooks (`X1, X2, ..., XF`). This is achieved through an iterative process involving unbinding, similarity calculation (via in-memory MVM), sparse activation, and projection (via in-memory transposed MVM). The system effectively performs a **combinatorial search** over the space of possible factor combinations (`MF` possibilities) in an efficient manner, leveraging vector superposition and stochasticity. A secondary observed behavior is the ability to handle **approximate** product vectors generated by a CNN from raw images and still achieve high factorization accuracy (visual disentanglement task).

### **8.2 Behavior Robustness:**

        1.  **Noise:** It leverages intrinsic device stochasticity (PCM programming noise, read noise, drift) effectively. Simulations show peak performance within a specific noise range [0.293µS, 1.277µS], and the experimental noise (0.98µS) falls within this optimal range (Ext Data Fig 1, Supp Note 3). It achieves 99.71% accuracy experimentally despite this noise.
        2.  **Approximate Inputs:** It achieves 99.42% accuracy even when factorizing approximate product vectors generated by a CNN from real images (RAVEN dataset), indicating robustness to input imprecision (Section II).
        3.  **Problem Size:** It solves problems up to five orders of magnitude larger than baseline resonator networks (Fig 3).
        4.  **Comparison to Digital:** Outperforms a deterministic digital equivalent in accuracy (99.7% vs 95.76%) even with fewer iterations (3312 vs 3802 avg), highlighting robustness against limit cycles (Section III, Discussion).
        Limitations might exist regarding sensitivity to hyperparameter choice (thresholds - though optimized) and potentially extreme noise levels outside the optimal range identified. The score reflects high demonstrated robustness but acknowledges potential uncharacterized limits.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary behavior (factorization) is validated through:
        1.  **Large-Scale Experiments:** Demonstration on two physical IMC cores using PCM devices (14nm node) for a D=256, M=256, F=3 problem, achieving 99.71% accuracy over 5,000 random queries (Section II). Key operations (MVM) are shown experimentally (Fig 4b).
        2.  **Comparative Experiments:** Direct comparison with a baseline resonator network (which failed completely) and a reference deterministic digital design (which achieved lower accuracy and required more iterations) under identical conditions (D=256, M=256, F=3) (Section II, III).
        3.  **Software Simulations:** Extensive simulations were performed to analyze the impact of stochasticity, sparse activations, dimensionality (D), codebook size (M), and number of factors (F) on operational capacity (Fig 3, Fig S1), compare activation functions (Fig S1, Supp Note 2), model noise effects (Ext Data Fig 1, Fig S4, S5, Supp Note 3), and optimize hyperparameters (Methods).
        4.  **Application Demonstration:** Successful application to disentangling attributes from real image representations (RAVEN dataset via CNN), achieving 99.42% accuracy on 1,000 test images (Section II, Fig 4d).
        Quantitative analysis includes accuracy percentages and average iteration counts. Robustness is assessed via noise sensitivity analysis (Supp Note 3, Ext Data Fig 1). Reproducibility is supported by code availability. The validation seems thorough, combining hardware experiments, simulations, comparative analysis, and application testing. Emergence is weakly claimed; the validation primarily supports the computational *function* and its efficiency/robustness rather than emergence in a strong sense.

---

#Key: [hu_monitoring_2018]

# Monitoring crack appearance and healing in coatings with damage self-reporting nanocapsules

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of three-component nanocapsules embedded within a polymer coating. The nanocapsules have a liquid core containing an inactive, colorless dye (Crystal Violet Lactone, CVL) dissolved in phenyl acetate, a protective poly(methyl methacrylate) (PMMA) shell, and color-developing silica nanoparticles coating the shell surface. The purpose of the system is to autonomously monitor both the appearance of mechanical micro-damage (cracks) in the coating and the subsequent healing process. When the coating is damaged, the embedded nanocapsules rupture, releasing the CVL dye. The dye interacts with the silica nanoparticles on the broken capsule surface, causing a chemical reaction (lactone ring opening) that develops an intense blue color (CVL+), visually highlighting the damaged area. This color is subsequently reversed (deleted) upon interaction with specific self-healing compounds (e.g., multivalent alkynes, diethylenetriamine, ethanol, often components or byproducts of self-healing systems), indicating that the damage is being healed or has been healed.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Key parameters relating to capsule dimensions, optical properties, and mechanical thresholds are provided.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input initiating the damage reporting is mechanical energy applied to the coating, leading to crack formation and subsequent rupture of the nanocapsules. A secondary input is the chemical potential energy difference driving the reaction between the released dye (CVL) and the developer (silica) or the reaction between the colored dye (CVL+) and the decoloring/healing agents.

### **2.2 Energy Transduction**

    *   Content: 1. Mechanical Energy -> Mechanical Energy: Applied stress concentrates at crack tips, exceeding the capsule's buckling pressure, leading to shell rupture. 2. Mechanical Energy -> Chemical Energy Release: Rupture releases the encapsulated CVL solution. 3. Chemical Potential Energy -> Chemical Reaction + Optical Energy Change: Released CVL reacts with silica (acidic surface groups catalyzing ring opening), converting chemical potential energy into the energy associated with the new chemical structure (CVL+) and resulting in absorption of visible light (blue color). 4. Chemical Potential Energy -> Chemical Reaction + Optical Energy Change: Healing agents react with CVL+ (e.g., neutralization, hydrogen bond disruption), converting chemical potential energy into the energy of the reformed CVL structure and eliminating the visible light absorption (color deletion).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any quantitative information to assess the energy efficiency of the mechanical rupture process or the subsequent chemical reactions in terms of energy conversion. The focus is on the functional outcome (color change) rather than energy balance. Qualitatively, the mechano-chemical transduction appears effective (rupture leads to visible color), but efficiency is undefined.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation occurs primarily through: 1. Mechanical processes during crack formation and propagation in the coating and capsule rupture (e.g., viscoelastic losses in the polymer, fracture energy, acoustic emission - not measured). 2. Heat released during the exothermic chemical reactions (CVL ring opening/closing, self-healing polymerisation - not measured or discussed). 3. Non-radiative decay pathways for the chromophore (CVL+), although the primary effect is absorption. Quantification is not provided. Qualitatively, mechanical dissipation during fracture is significant but localized. Heat from chemical reactions is likely low due to the small quantities involved per capsule.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding with M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable (Minutes to > 48 hours)
*    Units: Time (minutes, hours, days)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: ~1 bit (per location)
*   Units: Bit (or Binary State)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: High (Qualitative)

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**

### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip to Module 6.)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Capsule Rupture Event | Very short (<< seconds) | s | Inferred | Implicit | Mechanical fracture is typically rapid. Not measured. |
        | Color Development (CVL+ formation) | "immediately" | s (likely) | Text (p. 52), Movie 1 | Explicit | Qualitative description, suggests rapid reaction upon contact. |
        | Color Deletion (Ethanol) | minutes | min | Text (p. 52), Movie 4 | Explicit | Qualitative description from text and movie observation. |
        | Color Deletion (DETA) | minutes | min | Text (p. 52), Movie 3 | Explicit | Qualitative description from text and movie observation. |
        | Color Deletion (Multivalent Alkyne - pure) | "two hours" | h | Text (p. 52), Movie 2 | Explicit | Stated in text based on experiment with pure components. |
        | Color Deletion (Self-Healing System in Coating) | ~48 hours | h | Fig. 4c | Explicit | Observation from coating experiment corresponds to healing time. |
        | Memory Retention (in absence of healer) | > 48 hours (potentially much longer) | h | Fig 4b, Implicit | Mixed | Blue color persists at least until healing agent applied (Fig 4b shows color before healing). Implicit assumption of stability beyond this timeframe if no healing occurs. |
    *   **Note:** Timescales range from very fast (rupture) to seconds/minutes (some color changes) to hours/days (healing-coupled color deletion).

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip to Module 8.)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is autonomous, localized, visual reporting of material state through color change. Specifically: 1. Damage Indication: Rapid development of a visible color (blue) at the site of mechanical damage (crack formation) due to nanocapsule rupture and CVL reaction with silica. 2. Healing Monitoring: Reversible disappearance of the damage-indicating color in the presence of specific chemical species associated with self-healing processes. This allows tracking the progress and success of the healing reaction.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claimed behaviors (colorimetric damage reporting and healing monitoring) are validated through: 1. **Visual Observation:** Optical photographs and stereomicroscopy clearly show blue color appearing along induced scratches in capsule-containing coatings (Fig. 4b) and its subsequent disappearance upon application of healing agents (Fig. 4c). Control coatings show no color change (Fig. 4a). 2. **Spectroscopy:** UV/Vis spectroscopy confirms the absorbance peak of CVL+ (603 nm) upon reaction with silica and its decrease/disappearance upon reaction with decoloring/healing agents (Fig. 2b, 2c). FTIR confirms lactone ring opening (damage indication) and closure (color deletion) via characteristic band shifts (Fig. 2d). 3. **Microscopy:** SEM images confirm capsule presence and morphology (Fig. 3a, 3d) and visualize the crack path (Fig 4a-c, right column). TEM confirms core-shell structure (Fig 3b, 3c). AFM confirms mechanical rupture forces (Fig 3h, 3i). Reproducibility seems implied by consistent results across different characterization methods. Limitations include tests under specific lab conditions (room temp, specific healing agents, artificial scratches). Long-term performance and behavior under complex real-world stresses are not validated.

---

#Key: [xavier_autocatalytic_2020]

# Autocatalytic chemical networks at the origin of metabolism

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system analyzed is autocatalytic chemical networks, specifically Reflexively Autocatalytic Food-generated networks (RAFs), embedded within metabolic networks. RAFs are self-sustaining sets of chemical reactions where each reaction is catalyzed by a molecule produced within the set, and all molecules in the set can be generated from an initial 'food set' of molecules. The purpose is to investigate the role of such networks in the origin of metabolism and prebiotic evolution by identifying them in modern microbial metabolism (specifically anaerobic prokaryotes, acetogens like *Moorella thermoacetica*, and methanogens like *Methanococcus maripaludis*) using data from the KEGG database. The core components are metabolites (molecules) and reactions, with catalysis relationships defining the network structure. The system identifies the largest possible RAF (maxRAF) within a given metabolic network and food set using a specific algorithm.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                      | Value                                     | Units                | Source (Fig/Table/Section)      | Implicit/Explicit   | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit)   |
        | :---------------------------------- | :---------------------------------------- | :------------------- | :------------------------------ | :------------------ | :--------------------------------- | :-------------------------------- |
    *   **Note:** These parameters define the scale and boundaries of the reaction networks analyzed. Reliability is high as they are derived directly from curated databases (KEGG) and defined processing steps.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source considered for the ancient autotrophs (*M. thermoacetica* and *M. maripaludis*) is the geochemical H2-CO2 redox couple, providing the thermodynamic thrust for metabolism. ATP is also considered as part of the food set in some analyses but found not essential to *initiate* the RAFs when other organic cofactors are present. Alternative energy currencies like acyl phosphates, thioesters, and reduced ferredoxin are mentioned as relevant, especially in anaerobes.

### **2.2 Energy Transduction**

    *   Content: Energy is transduced through chemical reactions within the metabolic network. Key steps involve redox reactions (often mediated by NAD(P)H or Fe-S clusters) and phosphorylation/group transfer reactions (involving ATP or alternatives like acyl phosphates/thioesters). The H2-CO2 redox couple drives carbon fixation pathways (like the acetyl-CoA pathway). The energy captured is used to synthesize network components, including catalysts, facilitating the autocatalytic cycle.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide quantitative data or qualitative assessment of the energy efficiency of the identified RAFs. While it discusses energy sources (H2-CO2) and currencies (ATP, alternatives), the efficiency of converting input energy into network components or performing reactions within the RAF context is not analyzed.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation occurs implicitly through the thermodynamic irreversibility of chemical reactions (heat loss). The paper mentions the need for "thermodynamic thrust" provided by energy sources, implying that reactions must be exergonic overall or coupled to exergonic processes to proceed, inherently involving energy dissipation according to the second law of thermodynamics. However, specific mechanisms or quantification of dissipation (heat loss, entropy production) are not discussed. Assessment: Medium (inherent in chemical reactions).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**

    *   *Retention*: High, as long as the food set and conditions are maintained, the RAF persists.
    *   *Capacity*: Depends on the size (number of reactions/metabolites) of the maxRAF, which can be large (e.g., 1335 reactions). Represents the complexity of the stored "pattern" (the network itself).
    *   *Read-out accuracy*: High, in the sense that the network reliably produces its components given the food set.
    However, it's not reprogrammable memory for storing arbitrary external information; it's the memory *of* the system itself. It resembles structural memory rather than dynamic read/write memory. Score reflects good retention/capacity/readout for the *specific* purpose of self-replication, but lacks flexibility/re-writability of higher memory forms.

### **3.3 Memory Retention Time:**

*   Value: Long-term
*   Units: Qualitative Descriptor

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Up to 1335 (reactions) / Variable (depending on network and food set)
*   Units: Reactions (or Metabolites)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: High (Qualitative)

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Robustness_vs_CofactorRemoval | Change in maxRAF size upon removing specific cofactors from food set | Variable (e.g., ~50% reduction for NAD+) | % maxRAF size reduction | `MemoryNode` attribute: robustness | Fig S6 (Suppl. Mat.) | Explicit (Data in Suppl.) | Quantifies sensitivity of the 'memory' (RAF structure) to catalyst availability. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are the specific chemical reactions cataloged in the KEGG database (filtered as described). Each rule dictates how specific input metabolites (reactants) are transformed into output metabolites (products). Crucially, these rules also include catalysis requirements: a reaction can only proceed if at least one of its designated catalysts (derived from enzyme cofactors, metals, or 'peptide' for uncategorized enzymes, or spontaneous) is present and produced *within* the current network state (or available in the food set initially). The RAF algorithm iteratively enforces these rules: reactions are removed if their reactants OR *all* their potential catalysts cannot be produced by the remaining network from the food set.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :-------------------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The global order that emerges is the specific maximal Reflexively Autocatalytic Food-generated network (maxRAF). This is a subgraph of the initial reaction network, characterized by a specific set of interconnected reactions and metabolites that form a self-sustaining, collectively autocatalytic cycle capable of producing all its necessary components (including catalysts) from the provided food set. Examples include the 1335-reaction maxRAF in the global prokaryotic anaerobic network or the 172-reaction primordial network at the intersection of Ace and Met maxRAFs.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| MaxRAF Size | Number of reactions in the maximal RAF | `N_reactions` | 8 - 1335 | Reactions | Explicit | Quantifies the extent of emergent autocatalysis | RAF Algorithm | Section 2b, 2c, 2d |
| MaxRAF Size | Number of metabolites in the maximal RAF | `N_metabolites` | Variable | Metabolites | Explicit | Quantifies the extent of emergent autocatalysis | RAF Algorithm | Section 2d (Primordial network) |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description      | Value         | Units   | Source        | Implicit/Explicit | Justification                                         |
        | :------------------------- | :------------ | :------ | :------------ | :---------------- | :---------------------------------------------------- |
        | Early Earth Evolution      | ~4 Billion Ya | Years   | Section 1     | Explicit          | Context for the origin of life timeframe.             |
        | Eukaryote Origin           | < 2 Billion Ya| Years   | Section 2a    | Explicit          | Timeframe used for filtering reactions.               |
        | Cyanobacterial Photosynthesis | ~2.4 Billion Ya | Years   | Section 2a    | Explicit          | Timeframe used for filtering O2-dependent reactions. |
        | RAF Network Persistence    | Long-term     | Qual.   | Section 3 Imp. | Implicit          | RAFs are stable if food is supplied; relevant to evolution. |
    *   **Note:** The paper primarily discusses evolutionary timescales. The intrinsic timescales of the chemical network dynamics (reaction rates, formation time) are not analyzed by the static RAF model presented.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism of adaptation is the expansion or contraction of the maxRAF based on the availability of necessary components, particularly catalysts, in the food set. When a new cofactor is added to the food set, reactions that were previously non-operational because they lacked a producible catalyst (which now includes the added cofactor) might become operational. This can allow previously disconnected parts of the reaction network to become integrated into the RAF, leading to its growth. Conversely, removing a crucial cofactor from the food set can cause reactions dependent on it (if it cannot be synthesized within the remaining network) to be removed by the RAF algorithm, potentially causing the RAF to shrink. This is adaptation via changing boundary conditions (food set) affecting network feasibility according to the RAF rules. It is driven by environmental signals (presence/absence of cofactors).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior is collective autocatalysis: the formation of a self-sustaining chemical network (RAF) capable of producing all of its own components, including catalysts, from a simpler set of input molecules (food set). A key functional behavior demonstrated by these emergent RAFs (particularly the primordial network) is autotrophic metabolism: the synthesis of key biomolecules (acetyl-CoA, amino acids like Cys, Asn, Asp, Ala, Gly, Thr, and nucleosides UTP, CTP) from simple inorganic inputs (H2, CO2, NH3, etc.) and minimal organic catalysts/cofactors.

### **8.2 Behavior Robustness:**

        *   *Robustness*: The primordial network emerges robustly as the intersection of RAFs from diverse organisms (acetogen, methanogen). The overall anaerobic RAF is large, suggesting some redundancy.
        *   *Fragility*: The maxRAF size is highly sensitive to the presence of certain key cofactors in the food set, particularly NAD+. Removing NAD+ reduces maxRAF size by ~50% (Figure S6, Section 2f). This indicates fragility with respect to specific catalyst availability. ATP removal has no impact when other organics are present, showing robustness in that aspect.
        The score of 5 reflects this mixed picture: robust emergence of core features but significant sensitivity to specific essential components.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The emergence of autocatalysis (the RAF) is validated computationally by the execution of the RAF algorithm on curated biochemical network data (KEGG). The algorithm *operationally defines* the emergent structure (maxRAF). The functional behavior (synthesis of biomolecules) is validated by examining the metabolic pathways included within the identified maxRAF structure (e.g., Fig 3, Fig 4, Suppl. datasets). Robustness is tested via computational experiments involving cofactor removal from the food set (Section 2f, Fig S6). Control experiments involve comparing RAFs generated with different food sets (e.g., inorganic vs. cofactor-supplemented, Fig 2b; amino acid/base supplemented vs. cofactor supplemented, Section 2f/3). Limitations include reliance on curated database accuracy and the static nature of the RAF model (neglecting kinetics/concentrations).

---

#Key: [masila_emergence_2023]

# Emergence of intelligent collective motion in a group of agents with memory

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### 1.1 System Description

    *   Content: The system is an agent-based model simulating a bidisperse collective of agents moving in opposing desired directions (+x and -x) within a 2D domain with periodic boundaries in x and walls in y. The agents' motion is governed by a social force model. Each agent possesses a 'memory' mechanism, modeled as an integrated deviation of its velocity from its desired velocity over time. This memory influences the agent's dynamics through an additional force term, attempting to compensate for past non-optimal movement. The components are agents (two types with opposing goals), the 2D environment (periodic in x, bounded in y), and the interaction forces (restitution, memory, inter-agent repulsion, agent-wall repulsion). The purpose is to investigate how agent-level memory (representing a facet of intelligence) affects the emergent collective dynamics, specifically focusing on jamming and lane formation phenomena in crowded systems like pedestrian crossings or lane-less traffic.

### 1.2 Implementation Clarity


### 1.3 Key Parameters

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   Note: Units are scaled/dimensionless as described in Section II. Reliability is high for explicitly stated simulation parameters within the context of the model. `l_cr` value is not given, only its existence and role.

## M2: Energy Flow

### 2.1 Energy Input

    *   Content: The concept of energy input in a physical sense is not explicitly modeled. The driving force originates from the agents' desire to reach `v_0,i` (restitution term) and the memory term `βM_i`, which represent internal goals/drives rather than external energy sources. Computationally, energy is consumed to run the simulation.

### 2.2 Energy Transduction

    *   Content: Not explicitly modeled in terms of physical energy. Implicitly, the "potential energy" associated with deviation from desired velocity (`v_0,i - v_i`) and memory state (`M_i`) is transduced into kinetic energy changes via the restitution and memory forces. Kinetic energy is also modified by repulsive interactions (inter-agent and wall). These are computational transformations within the simulation dynamics.

### 2.3 Energy Efficiency

    *   Justification/Metrics: Physical energy efficiency is not defined or measured in this computational model. One could consider "goal achievement efficiency" (how quickly agents reach their destination or maintain desired velocity), which is indirectly measured by the jamming parameter `J`, but this is not energy efficiency.

### 2.4 Energy Dissipation

    *   Content: Not explicitly modeled as physical energy dissipation. Repulsive collisions implicitly represent inelastic interactions where kinetic energy is not conserved perfectly, analogous to dissipation. The model lacks explicit friction or viscous damping terms beyond the restitution term's effect (which drives towards `v_0,i`, not necessarily zero). Memory decay (`-M_i/α`) could be seen as a form of "information dissipation". Computationally, energy is dissipated as heat by the computing hardware. Assessment: Medium (implicit in collisions).

## M3: Memory

### 3.1 Memory Presence:

    *   Content: Yes

*(Conditional: M3.1 is "Yes", proceeding with M3.2-M3.8)*

### 3.2 Memory Type:


### 3.3 Memory Retention Time:

*   Value: `α` (Varied parameter, e.g., 0.3, 5, 10, 21)
*    Units: Time units (scaled by `τ_0`)

### 3.4 Memory Capacity (Optional - if applicable)

*   Units: Integrated velocity units (scaled)

### 3.5 Readout Accuracy (Optional - if applicable)

*   Value: 100% (within the model)
*   Units: %

### 3.6 Degradation Rate (Optional - if applicable)
    *   Value: `1/α`
    *   Units: Inverse time units (scaled by 1/`τ_0`)

### 3.7 Memory Operations Energy Cost (Optional - if applicable)
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### 3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### 4.1 Self-Organization Presence:

    *   Content: Yes

*(Conditional: M4.1 is "Yes", proceeding with M4.2-M4.7)*

### 4.2 Local Interaction Rules:

    *   Content: The local interaction rules are defined by the equations of motion for each agent `i`:
        1.  **Velocity Update (Eq 1):** `d v_i / dt = (v_0,i - v_i) + β M_i + Σ_{j≠i} F_ij + F_i,w`
            *   `v_0,i - v_i`: Restitution force (tries to restore desired velocity). `v_0,i` depends only on agent type.
            *   `β M_i`: Memory force (compensates for past deviations, depends on agent's own history `M_i`).
            *   `Σ_{j≠i} F_ij`: Sum of repulsive forces from neighboring agents `j` within distance `l_cr` (Eq 4). Depends on relative positions `d_ij`.
            *   `F_i,w`: Repulsive force from walls (Eq 5). Depends on agent's y-position relative to walls.
        2.  **Memory Update (Eq 2):** `d M_i / dt = - M_i / α + (v_0,i - v_i)`
            *   Depends only on the agent's own current deviation `v_0,i - v_i` and its own current memory state `M_i`.
        3.  **Repulsive Force (Agent-Agent, Eq 4):** `F_ij = -γ(d_ij - 2)^-3 * d̂_ij` if `d_ij < l_cr`, 0 otherwise. Depends on distance `d_ij` between agent `i` and `j`. `γ` is force strength. (Note: Paper has `(d_ij - 2)^-3` which might be a typo, often it's related to overlap distance, e.g., `(l_cr - d_ij)`). Assuming radii are 1, `d_ij-2` could represent overlap if `d_ij` is center-to-center distance. Let's assume the formula is as written.
        4.  **Repulsive Force (Agent-Wall, Eq 5):** `F_iw = -γ(||wall - y_i|| - 1)^-3 * d̂_ij` if `d_iw < l_cr`, 0 otherwise. Assumes wall interaction is similar, `d̂_ij` likely typo for `ŷ`. Depends on distance to wall.

### 4.2.1 Local Interaction Parameters:

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :-------------------- | :---: | :----------: | :----------------: | :------------: |
    | Eq 1, 2 | Memory effect | `α` | e.g., 0.3 to >20 | Scaled Time | Section II, Figs 1-5 | Explicit | Memory timescale parameter. |
    | Eq 1 | Memory effect | `β` | e.g., 4 | Dimensionless | Section II, Figs 1-5 | Explicit | Memory force strength parameter. |
    | Eq 1, 3 | Driving | `v_0` | +e_x or -e_x (Magnitude 1) | Scaled Velocity | Section II | Explicit | Desired velocity vector. |

### 4.3 Global Order:

    *   Content: The global order that emerges includes:
        1.  **Temporary Jammed Configurations:** Agents meet near the center and form a compact, slow-moving or stationary arrangement.
        2.  **Symmetric Interlocks:** A specific type of jammed state observed with short-term memory, characterized by an equal number of agents from each group in each 'lane' or layer within the jam (Fig 3A).
        3.  **Asymmetric Jammed Configurations:** Jammed states lacking the symmetry mentioned above, observed with very low memory (Fig 3A).
        4.  **Laned Configuration:** Agents segregate into distinct lanes based on their desired direction of motion, allowing for freer movement past the initial interaction zone. This is more prominent with long-term memory.

### 4.4 Predictability of Global Order:


### 4.5. Local Interaction Rules (for Self-Organization)
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Eq 1,2  | Memory Dynamics | `α`       | 0.3 - 21+   | Scaled time | Explicit | Parameter controlling memory timescale, influencing rearrangements and symmetry. | Sec II, Figs 1-5 |
| Eq 1    | Memory Force    | `β`       | e.g., 4     | Dimensionless | Explicit | Parameter controlling memory strength, influencing dynamics. | Sec II, Figs 1-5 |

### 4.6. Globally Emergent Order and Order Parameters
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Jamming | Probability of finding a slowly moving agent (<0.15 * desired speed) | Jamming Order Parameter (`J`) | ~0.05 - 0.4+ | Dimensionless | Explicit | Quantifies the degree of collective slowdown/jamming. | Calculated from agent speed histograms over time. | Sec III.B, Fig 2B, 2C |
| Symmetry | Fraction of layers in jammed config. with equal numbers from each group | Symmetry Order Parameter | ~0 - 1 | Dimensionless | Explicit | Quantifies the symmetry of the jammed structure. | Measured from snapshots of jammed configurations. | Sec III.B.1, Fig 3B, 3C |
| Heterogeneity | Standard deviation of speeds in local neighborhood | Speed SD | Varied (Boxplot range) | Scaled Velocity | Explicit | Quantifies local variation in agent movement, linked to efficient unjamming/laning. | Calculated from agent speeds in local neighborhood (radius 3*agent radius). | Sec III.B.2, Fig 5 |

### 4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    *   **Metrics:** Order parameters `J` and Symmetry; Standard Deviation of local speeds.

## M5: Computation

### 5.1 Embodied Computation Presence:

    *   Content: Yes

*(Conditional: M5.1 is "Yes", proceeding with M5.2-5.4)*

### 5.2 Computation Type:

    *   Content: Analog/Hybrid

### 5.3 Computational Primitive:

    *   Content: The most basic computational operation is the update step governed by Eq 1 and 2. This involves:
        1.  Sensing: Determining neighbor positions (`d_ij`) and wall distances (`d_iw`).
        2.  Integration: Accumulating velocity deviation into memory `M_i` (Eq 2, essentially time integration).
        3.  Weighted Summation/Force Calculation: Combining restitution force, memory force, and repulsive forces according to Eq 1.
        4.  State Update: Updating velocity based on the net force (numerical integration of Eq 1) and updating memory based on deviation and decay (numerical integration of Eq 2).
        This can be viewed as a form of reactive control computation with memory (similar to Proportional-Integral control, as mentioned in Section I and IV).
    *   **Sub-Type (if applicable):** Reactive Control Update / PI-like Control Step.

### 5.4 Embodied Computational Units
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### 6.1 Timescales:

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Memory Retention (`α`) | Varied (e.g., 0.3, 5, 10, 21) | Scaled Time (`τ_0`) | Sec II, Figs 1-5 | Explicit | Defines memory decay. Central parameter varied. |
        | Two-Agent Crossing (`t_c`) | Varied (Function of α, β) | Scaled Time (`τ_0`) | Fig 1B(iii), 1C | Explicit | Time for two agents to slide past each other. |
        | Restitution/Inertia (`τ_0`) | 1 (Implicit by scaling) | Scaled Time (`τ_0`) | Sec II | Implicit | Timescale used for non-dimensionalization. |
        | Jamming/Unjamming Time | Qualitative (Longer for symmetric jams) | Scaled Time (`τ_0`) | Sec III.B, III.B.1 | Mixed | Explicitly discussed qualitatively, but not quantified systematically. |

### 6.2 Active Inference:

    *   Content: Partial
        1.  *Prediction:* Implicit prediction of achieving `v_0,i`.
        2.  *Action Selection:* Forces in Eq 1 drive action to reduce deviation `(v_0,i - v_i)`.
        3.  *Internal Models:* Simple internal goal (`v_0,i`) and memory (`M_i`) representing integrated past deviations.
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:**
        *   Prediction Error Rate: Average magnitude of `||v_0,i - v_i||` over time, potentially weighted by `1/α`. Lower values might indicate more effective "inference".
        *   Anticipation Timescale: Not directly applicable as agents don't predict future environmental states.
        *   Model Complexity: Low, characterized by `v_0,i` and the memory integration timescale `α`.
        *   Quantify reduction in deviation achieved by memory term: Compare dynamics with `β=0` vs `β>0`. Metrics like average speed closer to `||v_0,i||` or reduced time spent moving away from `v_0,i` direction.

## M7: Adaptation

### 7.1 Adaptive Plasticity Presence:

    *   Content: Yes (at collective level), Partial (at individual level)

*(Conditional: M7.1 is "Yes", proceeding with M7.2)*

### 7.2 Adaptation Mechanism:

    *   Content: The mechanism for individual agent adaptation is the continuous update of the memory state `M_i` according to Eq 2: `d M_i / dt = - M_i / α + (v_0,i - v_i)`. The memory state `M_i` integrates the history of velocity deviations (`v_0,i - v_i`), weighted by the retention timescale `α`. This change is driven by the agent's own experience (its deviation from the goal) and internal dynamics (decay term). This stored historical information then modifies the agent's force calculation via `βM_i` in Eq 1, altering its subsequent motion. This resembles a form of internal state adaptation based on performance error feedback (deviation from `v_0,i`). It's analogous to integral control adaptation.

## M8: Emergent Behaviors

### 8.1 Behavior Description:

    *   Content:
        1.  **Collective Motion:** Agents move towards opposing desired directions.
        2.  **Collision/Interaction:** Agents exert repulsive forces on each other and walls upon proximity.
        3.  **Jamming:** Formation of dense, slow-moving or static configurations where agents hinder each other's movement towards their goal. Specific types include symmetric and asymmetric jams.
        4.  **Unjamming:** Spontaneous resolution of jammed states.
        5.  **Laning:** Spontaneous segregation of agents into parallel streams based on their desired direction, facilitating smoother flow.
        6.  **Sliding:** Individual agents maneuvering past each other, involving temporary vertical displacement.
        7.  **History Compensation:** Individual agents exerting additional force based on past velocity deviations (via memory).

### 8.2 Behavior Robustness:


### 8.3 CT-GIN Emergent Behavior Validation

     *  Content: Claims of emergent behaviors are validated through:
        1.  **Simulation:** Direct numerical simulation of the agent-based model (Eq 1-5).
        2.  **Qualitative Observation:** Descriptions of simulation dynamics (e.g., agents meet, form jams, unjam, form lanes - Sec II.B, III.B).
        3.  **Quantitative Analysis:**
            *   Order Parameters: Jamming order parameter `J` (Fig 2B, 2C) quantifies jamming. Symmetry order parameter (Fig 3B, 3C) quantifies jam structure.
            *   Metrics for Pair Dynamics: Time of crossing `t_c` and vertical displacement `d_v` for two agents (Fig 1C, 1D).
            *   Statistical Distributions: Speed histograms (Fig 2B), standard deviation of local speeds (Fig 5).
        4.  **Parameter Space Exploration:** Systematically varying `α` and `β` to map out different behavioral regimes (Fig 1C, 1D, 2C).
        5.  **Mechanism Analysis:** Explanations linking local dynamics (sliding time, hole formation, boundary effects, heterogeneity) to global outcomes (jamming, symmetry, laning - Sec III.B.1, III.B.2).
        *   Limitations: Validation relies solely on simulation within the defined model. No comparison with real-world experimental data is presented. Robustness tests are limited primarily to varying `α` and `β` and using different initial conditions.

---

#Key: [morningstar_deep_2018]

```markdown
# Deep Learning the Ising Model Near Criticality

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The systems analyzed are several types of unsupervised, generative neural network models: Restricted Boltzmann Machines (RBM - shallow), Deep Boltzmann Machines (DBM), Deep Belief Networks (DBN), and Deep Restricted Boltzmann Networks (DRBN - deep). These models are used to learn the probability distribution (Boltzmann distribution) of the two-dimensional (2D) square-lattice Ising model, a system of N binary spins (xi ∈ {0, 1}). The primary purpose is to investigate whether deep neural networks offer advantages over shallow networks in representing the physical data (thermal spin configurations) generated by the Ising model, particularly near its critical phase transition temperature (Tc). The components of the neural network models include visible layers (representing Ising spins), one or more hidden layers (composed of binary neurons), biases associated with each neuron, and weighted connections between neurons in adjacent layers (or across layers, depending on the model type). The networks are trained on synthetic data (spin configurations sampled via Markov Chain Monte Carlo from the Ising model's Boltzmann distribution) and then used to generate new configurations, from which physical observables (like average energy E and heat capacity C) are calculated and compared to the true values of the Ising system.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term / Permanent

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Proportional to N*Nh1 (RBM) or N*Nh1 + Nh1*Nh2 (2-layer deep models) + biases. e.g., RBM(64, 64) has ~64*64 + 64 + 64 = 4224 parameters. DBM(64, 24, 24) has ~64*24 + 24*24 + 64 + 24 + 24 = 2228 parameters.
*   Units: Number of parameters (weights + biases)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: Measured by % error in reproducing E and C near Tc. E.g., Fig 7 shows errors ranging from ~0% to ~-40% for C and ~0% to ~8% for E, depending on Nh1.
*   Units: Percent Error (%) or Absolute Difference (Dimensionless energy/capacity units)

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Delta_E   | Error in reproducing average energy per spin at Tc | Varies (See Fig 7) | Dimensionless energy units | `ReadoutEdge` attribute `accuracyValue` | Fig 7 | Explicit | Difference between model prediction and true value. |
    | Delta_C   | Error in reproducing heat capacity per spin at Tc | Varies (See Fig 7) | Dimensionless capacity units | `ReadoutEdge` attribute `accuracyValue` | Fig 7 | Explicit | Difference between model prediction and true value. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: Skipped M4.2-M4.7 as M4.1 is "No")**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: Skipped M5.2-5.4 as M5.1 is "No")**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Training Duration | 3x10^3 or 4x10^3 | Epochs | Table 1 | Explicit | Number of passes through dataset. |
        | CD-k Sampling Steps | 5 or 10 | Steps | Table 1 | Explicit | Number of Gibbs steps in Contrastive Divergence. |
        | MCMC Equilibration (Data Gen) | N^3 (e.g., 64^3 = 262144) | Monte Carlo Steps | Section 4 | Explicit | Steps discarded before collecting data. |
        | MCMC Decorrelation (Data Gen) | N (e.g., 64) | Monte Carlo Steps | Section 4 | Explicit | Steps between saving independent samples. |
        | Model Gibbs Sampling (Inference/Readout) | Not explicitly quantified (Requires "sufficient warm-up"/ "convergence") | Gibbs Steps | Sec 3.1, 3.2, 3.3, 3.4 | Implicit | Time needed for model to produce equilibrium samples post-training. |
    *   **Note:** MCMC timescales refer to the generation of the *training data*, not the operation of the neural network model itself, but are relevant context. Model sampling time is crucial but not quantified.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes (during training only)

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is the training algorithm used to optimize the model parameters (θ = {W, b, c, ...}). Specifically, the paper uses Contrastive Divergence (CD-k), which is an approximation to gradient descent on the negative log-likelihood of the training data (Sec 3.1, Appendix A). The parameter updates ∆θ are calculated based on the difference between data-dependent correlations (<vh^T>_data) and model-dependent correlations (<vh^T>_model), where the model correlations are estimated using k steps of Gibbs sampling (Eqs A.1-A.3). This process iteratively adjusts the weights and biases to make the model's probability distribution p(v) closer to the data distribution q(v). This is a form of supervised learning (in the sense of matching a target distribution, though the model itself is unsupervised/generative).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior of the trained neural network models (RBM, DBM, DBN, DRBN) is to act as generative models. Specifically, they statistically sample the learned probability distribution p(v) via block Gibbs sampling, producing synthetic spin configurations (v) intended to mimic those of the 2D Ising model at a given temperature T (Sec 3: "generate samples", "calculate physical estimators from the neural network"). A secondary behavior is the calculation of physical observables (average energy <E>, heat capacity <C>) from the set of generated samples (Sec 3, Sec 4). The overall goal is to accurately represent the target physical distribution q(v) ≈ p(v).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claim that the models generate samples representative of the Ising distribution (the primary behavior) is validated quantitatively. This is done by:
        1.  Generating a large set (10^4) of spin configurations using the trained models via Gibbs sampling (Sec 4).
        2.  Calculating physical observables (average energy <E>, heat capacity <C>) from these generated samples (Sec 4).
        3.  Comparing these calculated observables to the "exact" values obtained from direct Markov Chain Monte Carlo simulations of the target Ising model (Sec 4).
        Specific figures (Fig 5, Fig 6, Fig 7, Fig 8) explicitly show these comparisons across different temperatures and network architectures. Fig 7 directly plots the error (Delta_E, Delta_C) at the critical temperature Tc as a function of network size (Nh1/N). This provides quantitative validation of how well the generated behavior matches the target behavior. Reproducibility is implicitly assumed via standard methods. Limitations might include finite sampling size effects.

---

#Key: [hoffmann_trade-offs_2014]

# Trade-offs in exploiting body morphology for control: from simple bodies and model-based control to complex bodies with model-free distributed control schemes

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews and analyzes the relationship and trade-offs between a robot's physical morphology (shape, material properties, complexity, compliance) and the control strategies required to achieve desired behaviors. It explores the spectrum from simple, rigid bodies often amenable to model-based control, to complex, compliant, or "soft" bodies where model-free or distributed control strategies might be advantageous, potentially leveraging "morphological computation" (or more accurately, beneficial body dynamics). The system discussed is conceptual: the coupled robot body (plant) and controller system interacting with an environment to perform tasks. Components include morphology (dimensionality, linearity, compliance), controller (model-based/free, centralized/distributed), environment, and task. The purpose is to understand how morphology design can simplify control and the challenges associated with modeling and controlling complex bodies.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                | Value         | Units   | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :---------------------------- | :-----------: | :-----: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters represent the key conceptual dimensions discussed in the paper. Specific values are not applicable as it's a review discussing a range of systems.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The paper does not specify a single primary energy source, as it discusses a range of robotic systems. Energy input would typically be electrical (for actuators, controllers) or potential energy (e.g., for passive dynamic walkers starting on a slope). Energy expenditure is mentioned as a performance metric for controllers.

### **2.2 Energy Transduction**

    *   Content: Energy transduction mechanisms are inherent in the robotic systems discussed (e.g., electrical to mechanical in actuators, potential to kinetic in passive walkers), but the paper focuses on the *control* implications of the resulting dynamics, not the specifics of energy transformation physics. It discusses how body dynamics (resulting from energy flowing through the system) can be exploited for control.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper mentions energy expenditure as a performance criterion (page 3) and that some control schemes aim for minimal control actions (implicitly optimising efficiency). Passive dynamic walkers are implicitly energy efficient for their specific gait. However, no quantitative efficiency values or general assessment across the discussed spectrum is provided. Efficiency is acknowledged as a factor in the "design trading space" (Fig. 1 caption mentions finding a compromise between efficiency and flexibility).

### **2.4 Energy Dissipation**

    *   Content: Dissipation mechanisms like friction and damping are mentioned implicitly as parts of system dynamics (e.g., joint friction mentioned in Rückert and Neumann study, damping mentioned for passive walker example). However, these are not quantified or analyzed in detail across the different systems discussed.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8.)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: For the primary example of self-organization (passive dynamic walking [19]), the local interaction rules are the laws of physics: gravity, contact forces (ground reaction), conservation of momentum, and the constraints imposed by the robot's specific morphology (leg lengths, mass distribution, joint limits, foot radius). These rules govern how segments move relative to each other and interact with the environment (ground slope). For proposed distributed control systems [30], local rules would involve interactions between local control units based on sensor readings and mechanical states.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Physics | Morphology | Leg Length, Mass, Inertia, Foot Radius, Damping, Hip Mass Offset | Variable (System Dependent) | m, kg, kg·m², N·m·s/rad | [19] cited / [24] cited / [31] cited | Explicit (in cited works) | Parameters are explicitly varied/analyzed in cited studies like McGeer [19], Pekarek [24], Rückert [31]. | Citations [19], [24], [31] |
    | Physics | Environment | Ground Slope | Variable (System Dependent) | degrees / radians | [19] cited | Explicit (in cited works) | Ground slope is a key parameter in passive walking studies [19]. | Citation [19] |

### **4.3 Global Order:**

    *   Content: The primary example of emergent global order discussed is stable locomotion, specifically the periodic gait cycle in passive dynamic walkers [19]. This gait emerges from the local physical interactions. Other potential emergent orders mentioned relate to future distributed control systems, such as coordinated locomotion in a tensegrity structure [30].
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Gait-Characteristics | Step length, Period, Speed | Variable | m, s, m/s | Explicit (in cited works) | These quantify the resulting gait. | Simulation / Experiment | [19], [12] |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes (with caveats)

**(Conditional: M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog / Dynamical System (The paper argues against a strict computational view, favoring dynamics)

### **5.3 Computational Primitive:**

    *   Content: The paper suggests the "computation" performed by morphology is task-dependent and manifests as simplifying control. Primitives include:
        *   **Stability/Attractor Dynamics:** The body dynamics inherently possess stable states or trajectories (attractors) corresponding to desired behaviors (e.g., stable walking gait in passive walkers, self-stabilization). (Mathematical description: Related to eigenvalues/Floquet multipliers of the system's Jacobian/Poincaré map).
        *   **Physical Filtering/Compliance:** The body's mechanical properties can inherently filter noise or absorb impacts, simplifying sensing and control (e.g., soft bodies).
        *   **Constraint Satisfaction:** The morphology and its interaction with the environment automatically satisfy physical constraints (e.g., gripper conforming to an object shape).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Physical System Dynamics (e.g., oscillation period, settling time) | Variable | s/ms | Sections on dynamics, examples [19, 12] | Explicit (in examples) | Timescales are inherent to the dynamical systems discussed (e.g., walking period). | Examples [19, 12]|
        | Morphological Change Timescale (for adaptive morphology) | Variable (Slow/Fast) | s/min/hrs | Outlook [1] | Implicit (Concept) | Adaptive morphology change (e.g., stiffness) has inherent timescales. | [1] |

### **6.2 Active Inference:**

    *   Content: Unclear

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes (in controller or potentially morphology, not material)
        1.  **Controller Adaptation:** Learning optimal control policies for a given morphology, where the controller parameters change over time based on experience (e.g., Rückert and Neumann [31] learning policies using reinforcement learning).
        2.  **Morphological Adaptation:** The possibility of *online changes of morphology* (e.g., stiffness, shape) is mentioned as a future challenge/direction [1], allowing the body itself to adapt.
        This adaptation is not described as intrinsic material plasticity based on experienced stimuli, but rather as controller learning or actuated changes in morphology.

**(Conditional: M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content:
        *   **Controller Adaptation:** Explicitly mentions machine learning methods like stochastic optimal control / reinforcement learning used to learn control policies [31, 11]. The controller parameters are adjusted to optimize a cost function (e.g., maintain balance, minimize energy).
        *   **Morphological Adaptation:** The mechanism is not specified, but implied to be active changes (e.g., actuated stiffness adjustment) rather than passive material adaptation. Project LOCOMORPH [1] is cited in this context.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors discussed are related to achieving tasks through physical interaction, notably:
        *   **Locomotion:** Especially walking (passive dynamic walkers [19], RHex [12], bipeds [22]), but also potentially running or other gaits.
        *   **Grasping/Manipulation:** Demonstrated by the coffee balloon gripper [4] adapting to various object shapes.
        *   **Stabilization/Balancing:** Maintaining equilibrium, particularly for legged systems [31].
        *   **Trajectory Following:** The general control problem formulation [page 3].
        These behaviors emerge from the interplay of controller, body dynamics, and environment.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper relies heavily on citing prior work where emergent behaviors were demonstrated and validated:
        *   **Passive Dynamic Walking [19]:** Validated through physical prototypes and mathematical stability analysis (e.g., calculating stability based on parameters like scale, foot radius etc.).
        *   **Coffee Gripper [4]:** Validated experimentally, demonstrating grasping of diverse objects.
        *   **Simulated Balancing [31]:** Validated through physics-based simulation, measuring performance in maintaining balance against disturbances.
        *   **RHex Locomotion [12]:** Validated through physical robot experiments combined with dynamical systems analysis.
        *   **Evolutionary Robotics [18, 32]:** Validated through simulation and sometimes hardware implementations, evaluating task performance (e.g., locomotion).
     The paper itself doesn't present new validation but synthesizes findings. Limitations often involve the "reality gap" between simulation and hardware [page 6].

---

#Key: [aprahamian_non-equilibrium_2023]

# Non-equilibrium Steady States in Catalysis, Molecular Motors, and Supramolecular Materials: Why Networks and Language Matter

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper presents a theoretical perspective and review on chemical systems operating at a non-equilibrium steady state (NESS). It focuses on systems in catalysis (enantioselective synthesis), molecular motors (information ratchets), and supramolecular materials (actin polymerization, artificial gels). The core concept is that a chemical reaction network can be driven to a NESS by coupling it to a separate, spontaneous chemical process whose components are chemostated (held at fixed concentrations away from their equilibrium values). Key components discussed conceptually include the species within the reaction network, the components of the coupled spontaneous reaction, and catalysts or mediating species. The purpose is to analyze the common principles governing these diverse systems, emphasizing the requirements for achieving NESS (spontaneity of coupled process, kinetic asymmetry, coupling mechanism), highlighting common misconceptions, and advocating for clearer terminology.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name              | Value               | Units    | Source (Fig/Table/Section)                  | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit)                     |
        | :-------------------------- | :-----------------: | :------- | :------------------------------------------ | :-----------------: | :-----------------------------: | :-------------------------------------------------- |
        | Kinetic Asymmetry Factor    | Conceptual        | Unitless | Fig 4, Eq 2, SI                             | Explicit          | High (Conceptual)             | Ratio of specific rate constants (k+1/k-1 vs k+2/k-2) |
        | Ratcheting Constant (r₀)    | Conceptual        | Unitless | Fig 4, Eq 2, SI                             | Explicit          | High (Conceptual)             | Function of rate constants and concentrations       |

    *   **Note:** The paper discusses these parameters conceptually and through specific cited examples, but does not present new experimental/computational data providing specific numerical values for a *single* implemented system within the main text. Values are system-dependent.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source driving the system away from equilibrium is the free energy change associated with the coupled spontaneous chemical process, whose reactants are maintained (chemostated) away from their equilibrium concentrations. This is often described in terms of the free energy change associated with mass action (flow of substrate to product in the coupled reaction). The paper explicitly critiques the term "fuel" for this input.

### **2.2 Energy Transduction**

    *   Content: Chemical free energy from the spontaneous coupled reaction is transduced to perturb the equilibrium of the primary reaction network. This occurs because species within the primary network mediate steps in the coupled reaction (mechanistic linkage). This perturbation enables net flux through the primary network (e.g., driving catalysis, directional motion in motors, non-equilibrium assembly) which would otherwise be zero at equilibrium due to detailed balance. In information ratchet motors, this energy transduction allows the system to escape detailed balance and perform work by biasing transitions along a mechanical coordinate, driven by the kinetic asymmetry of the network coupled to the spontaneous reaction. The paper emphasizes that energy is not directly transferred like a "kick" but arises from biasing the probabilities of transitions within the network via the coupled process.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not explicitly discuss or quantify the thermodynamic efficiency of the energy transduction process for the discussed conceptual systems or specific examples. It mentions the maximum work extractable (related to free energy of mass action, SI sections 3.2, 3.3) but not the efficiency of converting the input free energy into useful work or maintaining the NESS. Efficiency is highly system-dependent. The focus is on the *conditions* for NESS, not optimizing efficiency.

### **2.4 Energy Dissipation**

    *   Content: Free energy is necessarily dissipated to maintain the system at NESS. This dissipation occurs through the ongoing, coupled spontaneous chemical reaction which is prevented from reaching equilibrium. The paper explicitly critiques the term "dissipative" as being potentially misleading or unhelpful if not used carefully, arguing that *any* spontaneous process dissipates free energy, and the key is whether this process is *coupled* to the network of interest. Specific mechanisms of dissipation (e.g., heat loss associated with the chemical reactions) are not quantified or detailed beyond the context of the overall spontaneous process.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Skipping M3.2-M3.8 as M3.1 is "No")**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial

**(Conditional: Including M4.2-M4.7 based on "Partial" answer for M4.1, focusing mainly on the supramolecular assembly aspect)**

### **4.2 Local Interaction Rules:**

    *   Content: For supramolecular assembly (actin example): Local rules involve monomer (ATP-actin) association/dissociation to filament ends, ATP hydrolysis within the filament (accelerated upon assembly), Pi dissociation, and ADP-actin dissociation. These processes have different rate constants (kinetic asymmetry mentioned, specific values not given) at the two distinct ends (barbed vs. pointed). For artificial gels (Fig 9), rules involve esterification/hydrolysis reactions and non-covalent interactions leading to aggregation (gelation). The paper emphasizes the *kinetics* (rate constants k_on, k_off, k_hyd) rather than detailed intermolecular forces.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID          | Description                                 | Parameter Name   | Parameter Value Range | Units     | Data Source               | Implicit/Explicit | Justification                  |
    | :--------------- | :------------------------------------------ | :--------------- | :-------------------- | :-------- | :------------------------ | :---------------- | :----------------------------- |
    | Actin Assembly   | Monomer Association/Dissociation Rate       | k_on, k_off      | Conceptual / Cited    | varies    | Fig 8, Refs 36, 38        | Explicit          | Explicitly discussed kinetics  |
    | Actin Hydrolysis | ATP Hydrolysis Rate within filament           | k_hyd            | Conceptual / Cited    | s⁻¹       | Fig 8, Refs 36, 38        | Explicit          | Explicitly discussed kinetics  |
    | Gelation (Fig 9) | Esterification/Hydrolysis/Assembly Kinetics | k±1, k±2, K_assem | Conceptual          | varies    | Fig 9                     | Explicit          | Explicitly shown in diagram    |

### **4.3 Global Order:**

    *   Content: For supramolecular systems: Emergent global order includes the formation of filaments (actin) or gel networks (artificial system in Fig 9). For actin, "treadmilling" is a global dynamic behavior emerging from different kinetics at the filament ends. For the artificial system, a macroscopic gel state emerges.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Explicit (Absence is explicit)

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID          | Description                      | Parameter    | Value Range | Units  | Implicit/Explicit | Justification                    | Source         |
| :--------------- | :------------------------------- | :----------- | :---------- | :----- | :---------------- | :------------------------------- | :------------- |
| Actin Kinetics   | Association/Dissociation/Hydrolysis | Rate Constants (k) | Conceptual  | varies | Explicit          | Discussed qualitatively (Fig 8)  | Refs 36, 38    |
| Gelator Kinetics | Esterification/Hydrolysis        | Rate Constants (k) | Conceptual  | varies | Explicit          | Shown schematically (Fig 9)     | Ref 40         |
| Gelator Assembly | Aggregation/Dissolution          | K_eq         | Conceptual  | varies | Implicit          | Implied by gel formation (Fig 9) | Ref 40         |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID      | Description               | Parameter               | Value Range | Units     | Implicit/Explicit | Justification                 | Protocol | Source         |
| :--------------- | :------------------------ | :---------------------- | :---------- | :-------- | :---------------- | :---------------------------- | :------- | :------------- |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :---------------- | :------------ | :----- |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Skipping M5.2-5.4 as M5.1 is "No")**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description                     | Value      | Units   | Source        | Implicit/Explicit | Justification                                                                     |
        | :---------------------------------------- | :--------- | :------ | :------------ | :---------------- | :-------------------------------------------------------------------------------- |
        | Time to reach NESS                       | Conceptual | Varies  | Section on MER | Explicit          | Mentioned as important for efficiency ("time taken to achieve the NESS is important") |
        | Reaction Rates (Individual Steps)         | Conceptual | s⁻¹, etc. | General, Eq 2 | Explicit          | Fundamental to the kinetic analysis presented.                                      |
        | Molecular Motor Cycle Time                 | Conceptual | s       | Motor Section | Implicit          | Implied by continuous operation at NESS.                                           |
        | Supramolecular Assembly/Disassembly Rates | Conceptual | Varies  | Assembly Section | Explicit          | Discussed for actin (k_on, k_off, hydrolysis rate).                                |
        | Pi Dissociation Time (Actin)              | ~6 min     | min     | Assembly Section | Explicit          | Explicitly stated value (t1/2 ≈ 6 min) cited from Ref 38.                       |

    *   **Note:** Values are mostly conceptual or cited, as the paper is a perspective, not a report of new experiments quantifying these for a specific system.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Skipping M7.2 as M7.1 is "No")**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors discussed are:
        1.  **Enantioselective Catalysis at NESS:** Achieving high enantiomeric excess (and potentially yield) in catalytic cycles by coupling to a spontaneous reaction (e.g., Moberg's MER).
        2.  **Autonomous Directed Molecular Motion:** Continuous, directional rotation or translation in synthetic molecular motors (information ratchets) by escaping detailed balance through coupling to a spontaneous reaction (e.g., Catenane 6).
        3.  **Non-Equilibrium Supramolecular Assembly:** Control over the steady-state structure and dynamics (e.g., concentration, treadmilling in actin) of self-assembling systems by coupling assembly/disassembly processes to a spontaneous reaction (e.g., ATP hydrolysis for actin, MeI hydrolysis for artificial gels).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper validates the concepts primarily through:
        1.  **Theoretical Derivations:** Using trajectory thermodynamics and kinetic network analysis (presented conceptually in the main text, detailed in SI) to demonstrate the *possibility* and *requirements* for NESS behaviors (Figs 2-4, Eqs 1-2).
        2.  **Citation of Experimental Examples:** Referencing specific published experimental work that demonstrates the behaviors (e.g., Moberg's MER reactions, Leigh's catenane motor 6, actin polymerization studies, van Esch's gelators). For motor 6, it mentions indirect validation via isotopic labeling and NMR analysis of separate cycle steps. For NESS assembly, it notes the difficulty of unambiguous experimental validation due to complex kinetics and confounding factors.
        3.  **Logical Argumentation:** Critiquing confusing terminology and promoting a physically consistent framework based on established thermodynamics (microscopic reversibility, second law).
        Limitations mentioned include the difficulty of direct observation (motors) and controlling/analyzing complex heterogeneous systems (assembly).

---

#Key: [goh_hydrodynamic_2023]

# Hydrodynamic pursuit by cognitive self-steering microswimmers

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system comprises two interacting microswimmers (a pursuer and a target) modelled as spherical squirmers immersed in a fluid simulated using Multiparticle Collision Dynamics (MPC). The target moves with a prescribed propulsion (straight or helical trajectory) characterised by speed v_t and stresslet parameter β (pusher/puller). The pursuer is an "intelligent Squirmer" (iSquirmer) with speed v_p and the same β. Its key feature is self-steering: it adapts its surface flow field (non-axisymmetric modes) to generate an angular velocity ω0 aimed at reorienting its propulsion direction e_p towards a desired direction e_aim. Two steering strategies are studied: (1) Reorientation towards the target's position (e_aim = e_c = r_c/|r_c|), and (2) Alignment with the target's propulsion direction (e_aim = e_t) combined with speed adaptation based on relative position (accelerating when behind, decelerating when ahead). The purpose is to investigate pursuit dynamics, capture strategies, and stable cooperative states mediated by hydrodynamic interactions and influenced by thermal noise, swimmer type (pusher/puller), and steering parameters (Péclet number Pe, maneuverability Ω). It also explores how the pursuer can influence the target's trajectory.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Energy input is implicitly chemical potential energy converted into mechanical work via the prescribed surface slip velocity (u_sq) on both squirmers. This active process drives self-propulsion and overcomes viscous drag. Additionally, thermal energy from the environment drives Brownian motion (accounted for by MPC fluid and D_R). For steering and speed adaptation, there's an implicit energy cost associated with modifying the surface flow field (changing C_11, C~11, B_1 parameters).

### **2.2 Energy Transduction**

    *   Content: Implicit Chemical Energy -> Kinetic Energy of squirmer motion (translation and rotation) via surface slip velocity (u_sq). Kinetic Energy -> Fluid Kinetic Energy via viscous interactions (hydrodynamic coupling). Thermal Energy -> Squirmer Kinetic Energy (Brownian motion). Implicit Control Energy -> Squirmer Rotational Kinetic Energy (steering via modification of non-axisymmetric flow modes C_11, C~11). Implicit Control Energy -> Squirmer Translational Kinetic Energy (speed adaptation via modification of B_1). Energy is also transduced between squirmers via hydrodynamic interactions and steric repulsion forces.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss or quantify the thermodynamic efficiency of propulsion, steering, or speed adaptation (e.g., work done against drag / energy input). Efficiency is not a focus of the study. Qualitatively, microswimmer propulsion at low Reynolds numbers is known to be generally inefficient.

### **2.4 Energy Dissipation**

    *   Content: Energy is primarily dissipated through viscous friction between the squirmers and the surrounding fluid (inherent in the low Reynolds number regime and MPC simulation). This dissipation occurs during translation, rotation, and due to the flow fields generated by the surface slip velocities. Dissipation also occurs implicitly during the steric repulsion interactions (energy lost/gained during collision compression/decompression is ultimately dissipated viscously). The MPC method itself conserves momentum locally but globally dissipates energy consistent with fluid viscosity. Thermal fluctuations represent an equilibrium energy exchange, not net dissipation, but contribute to random motion which is then damped viscously.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content:
        1.  **Hydrodynamic Interactions:** Squirmers interact via the flow fields they generate (pusher/puller characteristics determined by β) and experience from each other, mediated by the MPC fluid. These implicitly defined forces and torques depend on relative position and orientation. (Implicitly governed by Stokes flow for squirmers + MPC dynamics).
        2.  **Steric Repulsion:** Short-range repulsive force based on the separation-shifted Lennard-Jones potential U_LJ(d_s) (Eq. 23), preventing overlap. Acts along the center-to-center vector r_c.
        3.  **Pursuer Steering (Strategy 1 - Aiming):** The pursuer calculates a target angular velocity ω0 = C0 * e_p × e_aim, where e_aim = r_c / |r_c| (Eqs. 1, 13). This angular velocity is realized by adjusting surface slip modes C_11, C~_11 (Eqs. 10, 11). C0 determines maneuverability Ω (Eq. 2).
        4.  **Pursuer Steering (Strategy 2 - Alignment+Adaptation):**
            *   Alignment: As above, but e_aim = e_t (Eq. 14).
            *   Speed Adaptation: Pursuer acceleration v_p_dot is determined by its relative position (sign of e_p ⋅ e_c) and current speed relative to v_max, using friction coefficient κ (Eq. 19). Speed is updated via Euler scheme (Eq. 22).
        5.  **Target Dynamics:** Propelled with fixed speed v_t along e_t (potentially precessing for helical motion, Eqs. 15-18), subject to hydrodynamic forces/torques from the pursuer and fluid, steric repulsion, and thermal noise (implicit via D_R).
        6.  **Thermal Noise:** Both squirmers experience translational and rotational Brownian motion governed by temperature T and fluid viscosity η (implicit in MPC and represented by D_R).
    * **Implicit/Explicit**: Mixed


### **4.3 Global Order:**

    *   Content: The primary global order described is the formation of stable, cooperatively moving pursuer-target pairs with characteristic configurations and trajectories. Specific examples include:
        *   Stable head-to-tail touching configurations (Pullers, Aiming Strategy, Fig 2a-i).
        *   Stable leader-follower configurations with finite separation (Pushers, Aiming Strategy, Fig 2a-ii, 2b).
        *   Stable tracing configurations for helical target trajectories (Fig 3a).
        *   Stable cooperative circular motion (Pushers, Alignment+Adaptation Strategy, Fig 5a).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit


### **4.6. Globally Emergent Order and Order Parameters**

| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| PairSeparation | Average distance between pursuer and target | <r_c> | ~1σ to >2σ (pushers) / ~σ (pullers) | σ (squirmer diameter) | Explicit | Reported average values from simulations. | Time-averaging simulation trajectories. | Fig 2b, 3c, 4c |
| PairAlignment | Average alignment of propulsion directions | <e_p ⋅ e_t> | ~0.1 to ~1.0 | Dimensionless | Explicit | Reported average values from simulations. | Time-averaging simulation trajectories. | Fig 2d, 3b |
| RelativeOrientation | Orientation of pursuer relative to target center vector | P(e_t ⋅ e_c) | Distribution shown | Dimensionless | Explicit | Reported distributions from simulations. | Histogramming simulation trajectories. | Fig 4d, 4e |
| CooperativeSpeed | Center-of-mass speed of the pair | U_cm | ~0.5v_p to ~1.5v_p | v_p (pursuer speed) | Explicit | Reported average values from simulations. | Time-averaging simulation trajectories. | Fig 2c |
| InducedRotation | Angular frequency of target induced by pusher pursuer | Ω_ind | ~0 to ~0.38 Ω | D_R (rotational diffusion coeff.) | Explicit | Calculated from simulation data via Eq. 6 or autocorrelation fits. | Eq. 6 / Autocorrelation analysis. | Fig 5d |
| TrajectoryCurvature | Radius of induced circular motion | R_ind | ~5σ to ~30σ | σ (squirmer diameter) | Explicit | Calculated from v_t / Ω_ind. | Derived from v_t and Ω_ind. | Fig 5e |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | SteeringLogic -> PairConfiguration | How local steering affects the stable pair state | Medium-High | 6 | <r_c>, <e_p⋅e_t>, Stability Regions (Fig 2e) | Implicit | Predictable within stable parameter regimes, but sensitive near boundaries & stochastic. Score reflects good qualitative mapping but lack of quantitative prediction across all parameters. | Figs 2, 3, 4, 5 |
    | Hydrodynamics(β) -> PairConfiguration | How pusher/puller nature influences stable state | High | 8 | <r_c>, <e_p⋅e_t>, U_cm | Explicit | Clear, distinct differences observed and partly explained by theory (Eqs 3-5). | Figs 2, 3, 4, 5 |
    | Noise(Pe) -> PairConfiguration | How thermal noise affects stability/configuration | Medium | 5 | <r_c>, <e_p⋅e_t>, Stability Boundary Shift (Fig 2e) | Mixed | Noise generally destabilizes (explicitly stated), shifting stability boundaries. Effect on average values (<r_c>) shown, but quantitative impact on predictability not fully explored. | Fig 2, 4, 5 |
    | SpeedAdaptation -> PairConfiguration | How speed adaptation affects stable state (Pusher Alignment) | High | 8 | <r_c>, <e_t⋅e_c>, Ω_ind, R_ind | Explicit | Clear dependence of emergent configuration (distance, relative orientation, circular motion) on v_max. | Figs 4, 5 |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 7 (Rubric: 0=No link between local rules & global order; 5=Qualitative link established; 8=Quantitative link shown for key aspects; 10=Complete predictive model from local rules to all global features). The paper establishes clear qualitative and some quantitative links (e.g., Eqs 3-5 approximate some results) but lacks a full predictive theory covering all observed emergent states from the local rules, especially considering noise and complex near-field hydrodynamics.
    *   **Metrics:** Average distance <r_c>, alignment parameter <e_p⋅e_t>, stability phase boundaries (Fig 2e), center-of-mass velocity U_cm, induced angular velocity Ω_ind, radius of curvature R_ind, probability distributions P(e_t⋅e_c).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", including M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid

### **5.3 Computational Primitive:**

    *   Content: The most basic computational primitives are:
        *   **Vector Normalization:** Calculating unit vectors like e_c = r_c / |r_c| or e_t (assumed known).
        *   **Vector Cross Product:** Calculating e_p × e_aim to determine the axis of rotation (Eq. 1).
        *   **Scalar Multiplication:** Scaling the rotation axis by C0 (or κ for acceleration).
        *   **Vector Dot Product:** Calculating e_p ⋅ e_c for speed adaptation logic (Eq. 19) and e_p ⋅ e_x, e_p ⋅ e_y for determining slip modes (Eqs. 10, 11).
        *   **Thresholding/Comparison:** Comparing e_p ⋅ e_c to zero to switch between acceleration/deceleration modes (Eq. 19).
        *   **Linear Function/Saturation:** Calculating acceleration based on (v_max - v_p) or (-v_p) (Eq. 19).


## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Rotational Diffusion Time (Characteristic) | 1 / D_R | s (Implicit via D_R units) | Methods (D_R definition) | Implicit | Fundamental timescale for orientation randomization. D_R = kBT/(πησ^3). |
        | Pursuer Reorientation Time (Inverse Maneuverability) | 1 / (C0) ~ 1 / (Ω * D_R) | s (Implicit via Ω, D_R) | Eq. 1, 2 | Implicit | Timescale for the pursuer to significantly change direction via steering. |
        | MPC Collision Time | h = 0.02 | a√(m/kBT) | Methods | Explicit | Simulation timestep, shortest relevant timescale. |
        | Squirmer Transit Time (across diameter) | σ / v_p ~ σ^2 Pe D_R | s | Eq. 2 | Implicit | Time for squirmer to move its own diameter. Varies with Pe. |
        | Velocity Relaxation Time (Speed Adaptation) | R_sq / (κ * v_max) or R_sq / (κ * v_p(t)) | s | Eq. 19 (Implicitly) | Implicit | Timescale over which pursuer speed changes significantly during adaptation. Depends on mode (accel/decel) and current speed. |
        | Cooperative Oscillation Period (Induced Rotation) | 1 / Ω_ind ~ 1 / (0.1 - 0.4 Ω D_R) | s | Fig 5b, 5d | Explicit | Period observed in autocorrelation function for cooperative circular motion (Alignment strategy). |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism is a continuous-time velocity relaxation process based on relative position feedback. If the pursuer is behind the target (e_p ⋅ e_c < 0), it experiences a deceleration proportional to its current speed (-κ*v_p(t)^2 / R_sq term in Eq. 19, assuming the paper means v_dot ~ -kappa*v_p not -kappa*v_p^2 as written for deceleration in Methods for consistency with acceleration eq and units). If the pursuer is ahead (e_p ⋅ e_c > 0), it experiences an acceleration proportional to the difference between a maximum speed v_max and its current speed (κ*v_max*(v_max - v_p(t))/R_sq term in Eq. 19). The (dimensionless) parameter κ controls the rate of adaptation, and v_max sets the speed limit. This resembles a simple feedback control loop adjusting speed based on positional error relative to the target. It's a form of parameter tuning (adjusting v_p) based on performance feedback.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content:
        1.  **Pursuit:** The primary behavior is the pursuer actively following the target, attempting to reduce the distance or maintain a specific relative configuration.
        2.  **Capture/Stable Pairing:** Formation of stable, long-lasting cooperative states where the pursuer and target move together. This includes head-to-tail contact (pullers), stable finite-distance leader-follower arrangements (pushers), and stable relative configurations during helical motion.
        3.  **Target Manipulation (Indirect Steering):** In the alignment+adaptation strategy for pushers, the pursuing squirmer exerts hydrodynamic torques on the target, inducing a cooperative circular motion of the pair. The pursuer indirectly steers the target.
        4.  **Hydrodynamic Pushing/Pulling:** Depending on β and relative configuration, the swimmers exert hydrodynamic forces on each other, modifying their speeds and trajectories (e.g., pushers pushing the target, enhancing U_cm; pullers attracting, reducing U_cm).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors are validated through:
        *   **Numerical Simulations:** Using the MPC method to simulate the system dynamics under the defined rules.
        *   **Observation of Trajectories:** Visualizing and analyzing simulated trajectories (Figs 2a, 3a, 5a, Supp. Movies) provides qualitative evidence of pursuit, pairing, and cooperative motion.
        *   **Quantitative Analysis:** Calculating order parameters and average quantities like mean distance <r_c> (Fig 2b, 3c, 4c), mean alignment <e_p ⋅ e_t> (Fig 2d, 3b), orientation distributions P(e_t ⋅ e_c) (Fig 4d), center-of-mass speed U_cm (Fig 2c), induced angular velocity Ω_ind (Fig 5d), and radius of curvature R_ind (Fig 5e). These quantify the emergent states.
        *   **Phase Diagrams:** Mapping regions of stable vs. unstable pursuit as a function of key parameters (Pe, Ω, α) (Fig 2e) defines the conditions for emergence.
        *   **Comparison with Theory:** Comparing simulation results (e.g., for U_cm, stability threshold α) with analytical far-field approximations (Eqs. 3-5) provides further validation, showing semi-quantitative agreement in some regimes.
        *   **Control Simulations (Implicit):** Comparing different strategies (aiming vs alignment vs alignment+adaptation) and different parameters (pusher vs puller, different α, Ω, Pe) acts as a form of control to isolate the factors leading to specific emergent behaviors.
        *   **Reproducibility:** Averaging over multiple realizations (stated in Methods) demonstrates statistical reproducibility of the emergent average properties.

---

#Key: [zhao_phase_2021]

# Phase change mediated mechanically transformative dynamic gel for intelligent control of versatile devices

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a mechanically transformative dynamic gel composite. It consists of a crosslinked polyacrylic acid (PAA) network formed in situ within a phase change polyethylene glycol (PEG) melt. The purpose of the material is to exhibit a dramatic and reversible stiffness change (over 10^5 times) in response to thermal stimuli (heating/cooling) due to the melting/crystallization phase transition of PEG. This tunable stiffness allows for applications such as thermal interface gaskets (TIG), adaptive grippers, and high-temperature warning sensors. The PAA network provides structural support, preventing PEG leakage in the molten state, while the PEG phase transition dictates the mechanical state (soft gel vs. rigid solid).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Stiffness Change Ratio (P100L20) | >10^5 | dimensionless | Text p. 1230, 1234 | Explicit | Medium | Calculated from stiffness values |
        | PEG Loading (P100L20) | 85 | % (wt) | Text p. 1232, Fig. S3 | Explicit | Medium | Derived from TGA |

    *   **Note:** Stiffness values are highly dependent on PAA content (L#). Transition temperatures also show broadening based on composition. Transition times depend heavily on thickness and applied temperature gradient.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input is thermal energy, applied either through heating (e.g., external heater, heating gun) to induce melting or through cooling (e.g., ambient cooling, Peltier cooler) to induce crystallization. Specific temperatures used include 0, 15, 80, 90, 110 °C depending on the experiment.

### **2.2 Energy Transduction**

    *   Content: The primary energy transduction mechanism is the absorption or release of latent heat during the first-order phase transition (melting/crystallization) of the PEG component. Thermal energy input (heating) is transduced into increased molecular motion, breaking crystalline structures (melting), leading to a change from a rigid solid state to a soft gel state (mechanical energy state change). Conversely, removal of thermal energy (cooling) allows PEG chains to organize and crystallize, releasing latent heat and transducing the system from a soft gel state to a rigid solid state.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Efficiency is not explicitly quantified in terms of energy input vs. mechanical work potential change or heat storage capacity (latent heat). However, the phase change itself is a relatively efficient process for thermal energy storage/release (large latent heat of PEG is mentioned implicitly as the reason for temperature plateaus in Fig 2d/e). The *rate* of transition (Figs 2f-i), which affects practical efficiency, is limited by thermal conductivity (mentioned as a limitation on p. 1239) and geometry. The score reflects the inherent efficiency of phase change but acknowledges the lack of quantitative analysis and practical limitations like heat transfer rate.

### **2.4 Energy Dissipation**

    *   Content: The primary dissipation mechanism is heat loss to the surrounding environment during both heating and cooling cycles via convection and radiation. The paper acknowledges the low thermal conductivity of polymers as a limitation (p. 1239), implying inefficient heat transfer *within* the material and significant potential for heat loss to surroundings before the entire volume transitions phase. Hysteresis in the heating/cooling cycles (implied by difference between Tc and Tm in Fig S6) also represents a form of energy dissipation. Quantification is not provided. Qualitative assessment: Medium to High, especially for thicker samples or slower transitions.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term (Conditional)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 1
*   Units: distinct shape states

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: High (Qualitative)

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Low (over few cycles)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Shape Recovery Ratio | Ability to recover permanent shape after deformation/fixing cycle | >90 (approx, variable with compression) | % | `MemoryNode` attribute `fidelity_metric` | Fig. 4f | Explicit | Measures recovery of permanent shape, related to memory erasure fidelity. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local rules are the principles of polymer crystallization: nucleation (formation of stable crystal seeds) and growth (addition of polymer chains to the crystal lattice). These are governed by thermodynamics (minimization of Gibbs free energy below Tm), chain mobility (affected by temperature and confinement by the PAA network), and intermolecular forces (van der Waals, potential hydrogen bonding interactions between PEG and PAA mentioned on p. 1234). The PAA network introduces constraints, hindering chain mobility and affecting nucleation/growth kinetics and the final degree of crystallinity (Fig 2c, S5, S6, S7).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**

    *   Content: The emergent global order is the semi-crystalline structure of PEG dispersed within the PAA network in the rigid state. This results in macroscopic properties like high stiffness (up to 601 MPa) and opacity (Fig. 1b, 2a). The degree of crystallinity (amount of ordered phase) varies with PAA content (Fig. 2c, S7).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Crystallinity | Degree of PEG crystallization | Crystallization Fraction (Fc) | ~0 - 60 (approx, depends on L#) | % | Explicit | Calculated from DSC melting enthalpy relative to pure PEG. | DSC | Fig. 2c, Text p. 1234 |
| Stiffness | Macroscopic mechanical response in rigid state | Flexural Modulus (E_rigid) | 80 - 601 (depends on L#) | MPa | Explicit | Measured directly. | DMA | Fig. 2k, Text p. 1234 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Melting Time (200µm film, 90°C heat) | ~1-2 | s | Fig. 2f | Explicit | Time for phase transition (softening). Varies strongly with thickness, PAA content, temp gradient. |
        | Melting Time (4000µm film, 90°C heat) | ~50-80 | s | Fig. 2f | Explicit | Time for phase transition (softening). Varies strongly with thickness, PAA content, temp gradient. |
        | Crystallization Time (200µm film, 15°C cool) | ~1-3 | s | Fig. 2h | Explicit | Time for phase transition (rigidification). Varies strongly with thickness, PAA content, temp gradient. |
        | Crystallization Time (4000µm film, 15°C cool) | ~100-180 | s | Fig. 2h | Explicit | Time for phase transition (rigidification). Varies strongly with thickness, PAA content, temp gradient. |
        | DMA Temperature Sweep Rate | 2 | °C/min | Fig. S8 caption | Explicit | Rate at which temperature was changed during cyclic testing. |
    *   **Note:** Transition times are highly dependent on experimental conditions (thickness, temperature gradient, composition). The values provided are examples from specific conditions shown in the figures.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main behaviors emerging from the temperature-induced phase transition are:
        1.  **Drastic Stiffness Change:** Reversible transition between a soft gel state (kPa range) and a rigid solid state (MPa range), with a change ratio >10^5.
        2.  **Shape Memory:** Ability to fix a temporary shape in the rigid state and recover a permanent shape upon heating to the soft state.
        3.  **Tunable Adhesion/Conformability:** Soft state allows conforming to surfaces (macro and micro scale), rigid state allows fixing the shape and potentially stronger adhesion (exploited in TIG and gripper).
        4.  **Microstructure-Dependent Resistance Change:** Change in contact area/pressure of a microstructured surface upon softening leads to a sharp decrease in electrical resistance (used for sensing).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Behaviors are validated through quantitative measurements and functional demonstrations:
         *   Stiffness Change: Quantified using DMA (Fig. 2k), visual demonstration (Fig. 2j).
         *   Shape Memory: Demonstrated visually (Fig. 4a), quantified by shape recovery ratio (Fig. 4f).
         *   Adhesion/Conformability: Demonstrated visually (Fig. 3a), via SEM of replicated microstructures (Fig. 3c), quantified by adhesion/grip force measurements (Fig. 4g, 4h). TIG application validated via IR thermography (Fig. 3f, 3g, 3i, 3m).
         *   Resistance Change: Quantified by electrical resistance measurements vs. temperature (Fig. 5b), visualized via LSCM (Fig. 5f, 5g), demonstrated in a warning circuit (Fig. 5i).

---

#Key: [gepner_fluidic_2024]

# Fluidic FlowBots: Intelligence embodied in the characteristics of recirculating fluid flow

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of soft robots ("FlowBots") actuated by recirculating fluid flow (pneumatic or hydraulic). Control functionality is embedded directly into the robot's structure by leveraging fluid flow characteristics, specifically viscous energy losses and pressure asymmetries created by internal channel geometries (e.g., constrictions). This allows for complex behaviors (e.g., bidirectional actuation, gripping, swimming gait) with simplified external control inputs, often enabling monolithic additive manufacturing. Components include the soft robot body with internal fluid channels, actuators (bellows), inlet/outlet ports, a fluid reservoir, and a pump/pressure source. The purpose is to achieve complex soft robot control through embodied intelligence derived from fluid dynamics, reducing reliance on complex external valving and electronics.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is a pump (hydraulic case) or compressed air supply (pneumatic case) providing pressurized fluid flow.
    *   Value: 1.25 - 2.5 (Pressure range tested)
    *   Units: bar

### **2.2 Energy Transduction**

    *   Content: Potential energy (pressure) from the source is converted into kinetic energy of the fluid flow. This energy is partially dissipated as heat due to viscous friction within the channels, creating pressure drops. The remaining pressure potential energy is converted into mechanical work, causing the deformation (actuation) of the soft robot's structure (bellows). The fluid then returns to the reservoir (hydraulic) or vents (pneumatic).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Low. The paper explicitly states that constant recirculation "comes with the cost of energy expenditure (making the specific systems described in this paper less thermodynamically efficient than their counterparts shown in Figure 1)" (Section III-B). The primary goal is simplified control via embodied intelligence, not energy efficiency. Energy is continuously consumed to maintain flow, even when the actuator is static. No quantitative efficiency metrics are provided.

### **2.4 Energy Dissipation**

    *   Content: The primary dissipation mechanism is viscous friction between the flowing fluid and the internal channel walls, leading to pressure drops and heat generation. This dissipation is intentionally leveraged, particularly at constrictions, to create pressure asymmetries for control (Section II-A). Minor dissipation may occur due to turbulence, especially in bellow geometries and bends (Section III-B), and potential energy loss if the fluid returns to a reservoir at a lower elevation (hydraulic). Quantification is not provided, but viscous losses are central to the concept. Qualitative assessment: High (due to continuous recirculation and reliance on viscous effects).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip to Module 4.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operation appears to be the physical realization of fluid dynamic relationships, primarily relating flow rate, channel geometry (resistance/impedance), and pressure drop (analogous to Ohm's law, V=IR, or more complex fluid equations). The system physically computes the pressure distribution based on input flow and boundary conditions (vent constrictions). This pressure distribution then drives actuation. Specific examples translate this into:
        *   **Signal Weighting/Modulation:** Channel constrictions modulate the effect of flow on local pressure.
        *   **Thresholding (Implicit):** Actuation only occurs significantly above certain pressure differentials.
        *   **Signal Routing/Switching:** Changing flow paths/blocking ports switches actuators between series and parallel configurations (Fig 4, Fig 5), acting like a physical logic switch or multiplexer based on flow paths.
    *   **Sub-Type (if applicable):** Physical Law Implementation (Fluid Dynamics), Signal Modulation, Physical Switching

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Actuator Response Time (Water) | ~0.11 - 0.12 | s | Fig 3 (Supp) | Explicit | Measured time from start to max deformation. |
        | Actuator Response Time (Air) | ~0.075 - 0.08 | s | Fig 3 (Supp) | Explicit | Measured time from start to max deformation. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip to Module 8.)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors are controllable soft actuation:
        1.  **Bidirectional Bending:** A single actuator bends left or right based on flow direction and magnitude, controlled by outlet constrictions (Fig 3).
        2.  **Gripping:** Two bidirectional actuators act as fingers, allowing independent control for grasping objects, utilizing series/parallel flow configurations for simplified control (Fig 4, Fig 6).
        3.  **Swimming Gait:** A quadruped robot uses coordinated limb movements (pairs controlled analogously to the gripper) for locomotion in fluid (Fig 5).
        These behaviors arise from the designed interaction between fluid flow characteristics (viscous losses, pressure asymmetry) and the soft structure.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The behaviors are validated experimentally:
        *   **Operational Definition:** Behaviors (bending, gripping, swimming motions) are clearly defined and linked to control inputs (flow direction, vent constriction).
        *   **Demonstration:** Qualitative demonstrations via figures (Figs 3, 4, 5, 6) and referenced supplementary video.
        *   **Quantitative Analysis:** Actuator curvature vs. pressure difference is measured and plotted for bidirectional bending with air and water, showing consistent (though slightly asymmetric) analogue response (Fig 7, Section II-D, III-A). Response times are also quantified (Fig 3 Supp, Appx I-B.2).
        *   **Control Experiments:** Comparison between recirculating and static actuator variants (Appx I-B) helps isolate the effects of recirculation. Tests with air vs. water validate cross-fluid compatibility (II-D).
        *   **Reproducibility:** Experiments were repeated (3x) and across two specimens (II-D, Appx II-B) to assess consistency, revealing some variance attributed to manufacturing.
        *   **Limitations:** Gripping and swimming validation is primarily qualitative/demonstrative. Robustness testing against a wider range of perturbations (e.g., external forces, temperature changes during operation, long-term wear) is not presented. The term "emergent" is not used in the context of behavior validation; behaviors are presented as designed outcomes of the fluid dynamics.

---

#Key: [gumuskaya_motile_2024]

# Motile Living Biobots Self‐Construct from Adult Human Somatic Progenitor Seed Cells

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system describes "Anthrobots," which are spheroid-shaped multicellular biological robots (biobots) self-constructed in vitro from single adult human bronchial epithelial (NHBE) progenitor cells. These biobots range from 30-500 microns in diameter. After 2 weeks of culture in an extracellular matrix (ECM) followed by transfer to a low-adhesion environment, they develop external cilia and exhibit spontaneous, cilia-driven locomotion (5-50 microns/s) in diverse patterns (linear, circular, etc.). Their morphology and movement are correlated. They demonstrate the ability to traverse and induce rapid repair of scratches in cultured human neural cell sheets. The purpose is to explore the morphogenetic plasticity of adult human somatic cells, create novel motile biological structures without genetic editing, and investigate potential biomedical applications like tissue repair. The key components are the NHBE cells, the culture medium (BEDM), Matrigel (ECM), retinoic acid (inducer), and the low-adhesive culture environment.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters listed are central to the system's physical characteristics and construction protocol, as explicitly stated in the text.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is chemical energy derived from the nutrients within the specified culture medium (Bronchial Epithelial Differentiation Medium - BEDM), which the constituent NHBE cells metabolize.

### **2.2 Energy Transduction**

    *   Content: Chemical energy stored in nutrients from the culture medium is converted by the cells' metabolic processes into biochemical energy carriers (primarily ATP). This biochemical energy is then transduced into mechanical energy via the molecular machinery (dynein motors) within the cilia, causing them to beat and propel the Anthrobot through the environment.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any quantitative data or qualitative assessment regarding the energy efficiency of Anthrobot locomotion (e.g., energy input from media vs. mechanical work output). Biological systems, especially those involving cell movement, are generally considered to have low thermodynamic efficiency, but this is not discussed.

### **2.4 Energy Dissipation**

    *   Content: Energy is dissipated primarily through: 1) Heat loss associated with cellular metabolic processes (inherent inefficiency). 2) Viscous drag as the Anthrobot moves through the liquid culture medium, converting kinetic energy into heat in the fluid. Quantification is not provided. Qualitative assessment: Both likely significant, typical for biological microswimmers.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The local rules governing self-organization include:
        1.  **Cell Proliferation & Differentiation:** NHBE cells proliferate and differentiate in response to culture conditions (BEDM medium, Retinoic Acid).
        2.  **Cell-Matrix Interaction:** Initial culture in Matrigel promotes apical-in spheroid formation (Sec 2.1, Fig 1a.2). Cell interaction with high-viscosity matrix is hypothesized to orient basal cells outward (Sec 2.1).
        3.  **Cell-Environment Interaction (Polarity Switching):** Transfer to low-viscosity, low-adhesion media (post-Matrigel dissolution) is hypothesized to trigger apicobasal polarity switching, leading to cilia facing outward (Sec 2.1, Fig 1a.3). This is driven by cells sensing and responding to the change in microenvironment mechanics/adhesion.
        4.  **Cell-Cell Adhesion/Communication:** Cells adhere to form multicellular spheroids (implicit). Tight junctions (ZO-1 staining, Fig 1D) indicate organized epithelial structure formation.
        5.  **Cilia Coordination:** Coordinated beating of external cilia generates propulsion (Sec 2.1, Fig 1E, Fig S6). The mechanism of coordination is not detailed but is inherent to the function.
        6. **Cell Aggregation (Superbots):** Simple physical constraint (smaller dish) facilitates random self-aggregation of individual Anthrobots into larger "superbot" structures (Sec 2.7).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | 1 | Cell Proliferation/Differentiation | Retinoic Acid (RA) Conc. | 0.5 (final conc. in media addition step) | nM | Exp. Sec. | Explicit | Concentration used is stated. |
    | 2 | Cell-Matrix Interaction | Matrix Viscosity / Modulus (Matrigel) | 450 (elastic modulus) | Pa | Sec 2.1 (cited value [17]) | Explicit | Value cited from reference. |
    | 3 | Cell-Environment Interaction | Medium Viscosity / Modulus (Water-based) | ~2e9 (elastic modulus) | Pa | Sec 2.1 (cited value calculation) | Explicit | Value stated, calculation basis unclear in excerpt. |
    | 3 | Cell-Environment Interaction | Matrix Viscosity (Experimental Variation) | Baseline vs Higher Viscosity | Qualitative | Sec 2.1, Fig S2, S3 | Explicit | Experiment varied viscosity qualitatively. |

### **4.3 Global Order:**

    *   Content: The emergent global order manifests in two main ways:
        1.  **Morphological Order:** Formation of multicellular spheroids with distinct, classifiable morphologies (Morphotypes 1, 2, 3) characterized by variations in size, shape regularity (spherical vs. ellipsoidal/irregular), and cilia distribution patterns (uniform vs. polarized, density) (Sec 2.3, Fig 3).
        2.  **Behavioral Order:** Emergence of discrete, stable movement patterns (Movement Types 1: Circular, 2: Linear, 3: Curvilinear, 4: Eclectic) characterized by quantitative trajectory metrics (straightness, gyration) (Sec 2.2, Fig 2). Also, the collective behavior of superbots inducing neural tissue repair (Sec 2.7, Fig 6).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| RA Induc. | Induction of differentiation | Retinoic Acid Conc. | 0.5 (added) | nM | Explicit | Stated in protocol | Exp. Sec. |
| Cell Density | Modulates size/motility | Seeding Density | 15k, 30k, 60k | cells/mL | Explicit | Parameter varied experimentally | Sec 2.1, Fig S4, S5 |
| Viscosity | Modulates size/motility | Matrix Viscosity | Baseline vs High | Qualitative | Explicit | Parameter varied experimentally | Sec 2.1, Fig S2, S3 |
| Aggregation | Superbot formation | Physical Confinement | Smaller Dish | Qualitative | Explicit | Method used to create superbots | Sec 2.7 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Morphotype 1 | Small, spherical, uniform cilia | Max Radius | Lower range (mean visually smaller in Fig 3D) | µm | Mixed | Explicitly clustered; quantitatively smaller | PCA/Clustering | Fig 3 |
| Morphotype 1 | Small, spherical, uniform cilia | Shape Smoothness | Higher range (mean visually higher in Fig 3D) | Ratio (Unitless) | Mixed | Explicitly clustered; quantitatively smoother | PCA/Clustering | Fig 3 |
| Morphotype 2 | Large, irregular, dense cilia | Max Radius | Highest range (mean visually largest in Fig 3D) | µm | Mixed | Explicitly clustered; quantitatively larger | PCA/Clustering | Fig 3 |
| Morphotype 2 | Large, irregular, dense cilia | Shape Smoothness | Lowest range (mean visually lower in Fig 3D) | Ratio (Unitless) | Mixed | Explicitly clustered; quantitatively less smooth | PCA/Clustering | Fig 3 |
| Morphotype 3 | Intermediate size, irregular, polarized cilia | Polarity | Highest range (mean visually higher in Fig 3D) | Index (Unitless) | Mixed | Explicitly clustered; quantitatively more polarized | PCA/Clustering | Fig 3 |
| Movement Type 1 | Circular Motion | Gyration Index | Highest range (mean ~0.9) | Index (Unitless) | Mixed | Explicitly clustered; quantitatively highest gyration | Clustering/Tracking | Fig 2 |
| Movement Type 1 | Circular Motion | Straightness Index | Lowest range (mean ~0.1) | Index (Unitless) | Mixed | Explicitly clustered; quantitatively lowest straightness | Clustering/Tracking | Fig 2 |
| Movement Type 2 | Linear Motion | Straightness Index | Highest range (mean ~0.9) | Index (Unitless) | Mixed | Explicitly clustered; quantitatively highest straightness | Clustering/Tracking | Fig 2 |
| Movement Type 2 | Linear Motion | Gyration Index | Lowest range (mean ~0.1) | Index (Unitless) | Mixed | Explicitly clustered; quantitatively lowest gyration | Clustering/Tracking | Fig 2 |
| Behavior | Neural Repair | Gap Closure Pixel Density | ~Native Tissue Density | % Coverage | Mixed | Explicitly measured; compared to controls | Image Analysis | Fig 6 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Initial Matrix Culture | 14 | days | Exp. Sec., Fig 1A | Explicit | Protocol step duration. |
        | Polarity Reversal / Motility Onset | ~7-10 (peak change ~day 10 post-dissolution) | days | Sec 2.1, Fig 1C | Explicit | Observation from motility curve. |
        | Lifespan | 45 - 60 | days | Sec 1 | Explicit | Stated lifespan. |
        | Movement Analysis Window | 30 | seconds | Sec 2.2 | Explicit | Time period used for behavior classification. |
        | Timelapse Interval (Tracking) | 2.5 | seconds | Exp. Sec. | Explicit | Acquisition parameter. |
        | Neural Repair Observation | 72 | hours | Sec 2.7, Fig 6 | Explicit | Duration of repair experiment. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The primary mechanism described is developmental plasticity involving cellular differentiation and morphogenesis driven by changes in the microenvironment. Key steps include:
        1.  Differentiation of NHBE progenitor cells within Matrigel into apical-in spheroids (standard organoid formation process adapted).
        2.  A hypothesized apicobasal polarity switch triggered by the transition from high-viscosity/adhesive ECM (Matrigel) to a low-viscosity/low-adhesive liquid medium. This environmental cue is proposed to cause basal cells to migrate inward and apical (ciliated) cells to move to the exterior (Sec 2.1). Retinoic acid (RA) is also used, known to influence differentiation pathways.
        The adaptation changes the physical structure (morphology, cilia position) and consequently the function (motility). It's driven by environmental cues influencing inherent cellular programs for differentiation and organization, not by feedback based on performance or reward signals (as in reinforcement learning).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors observed are:
        1.  **Self-Construction:** Formation of multicellular spheroids from single progenitor cells (Sec 2.1, Fig 1A).
        2.  **Locomotion:** Spontaneous, cilia-driven movement through liquid media with varying patterns (linear, circular, curvilinear, eclectic) and speeds (5-50 µm/s) (Abstract, Sec 2.1, Sec 2.2, Fig 2). Includes stationary "wiggling" (Sec 2.4).
        3.  **Environmental Navigation:** Ability to traverse complex biological environments, specifically scratches in live human neuronal monolayers (Sec 2.6, Fig 5A).
        4.  **Tissue Repair Induction:** Aggregated "superbots" induce gap closure and repair of scratches in neuronal monolayers when bridging the gap (Sec 2.7, Fig 6).

### **8.2 Behavior Robustness:**

        *   **Self-Construction:** The protocol consistently produces Anthrobots, though with morphological variability (size 30-500µm, different morphotypes emerge - Fig 3), indicating reasonable robustness of the formation process itself. Parameter variations (density, viscosity) affect outcomes but don't completely abolish formation (Sec 2.1).
        *   **Locomotion:** Discrete movement types (linear, circular) are shown to be stable/persistent over time (Markov chain Fig 2G shows high self-transition probability, e.g., 92% for circular, 80% for linear), suggesting robustness of established movement patterns for individual bots. However, ~50% of spheroids are non-movers (Fig 1C caption), indicating variability in achieving motility.
        *   **Repair Induction:** The repair effect seems highly robust *under specific conditions* – 100% of "fully connected bridges" (5 out of 10 total experiments) induced repair (Fig 6D caption). This suggests robustness of the effect itself when the necessary configuration (bridging) is achieved and maintained, though achieving/maintaining that configuration occurred in 50% of trials. The negative control (agarose) showed no effect, supporting the specificity (Fig S11 ref).
        Overall, core behaviors are achieved consistently, stable patterns exist, and the repair effect is strong when conditions are met, but variability in outcomes (morphology, motility achievement, bridge stability) exists.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content:
         *   **Self-Construction/Morphology:** Validated through direct observation, imaging (phase contrast, confocal microscopy with immunostaining for structure - Fig 1D/E, Fig 3), quantitative morphology analysis (8 indices developed), and unsupervised clustering (PCA, Ward.D2) to identify distinct morphotypes (Sec 2.3, Fig 3).
         *   **Locomotion:** Validated using time-lapse microscopy, automated tracking (trackR), calculation of trajectory metrics (straightness, gyration), unsupervised clustering (Ward.D2, CEC) to identify discrete movement types, and Markov chain analysis for stability/transitions (Sec 2.2, Fig 2). Role of cilia confirmed by inhibitor (ciliobrevin) experiment (Sec 2.1, Fig S6).
         *   **Scratch Traversal:** Validated using time-lapse microscopy on neural scratches, tracking, and analysis of bot-scratch interactions (proportion on tissue, trajectory similarity) (Sec 2.6, Fig 5).
         *   **Tissue Repair:** Validated by placing superbots on neural scratches, time-lapse imaging over 72h, immunostaining (Tuj1) of neurons post-experiment, quantitative analysis of pixel density in gap vs. control areas (native tissue, adjacent/distant scratch), and comparison with a passive control (agarose block) (Sec 2.7, Fig 6, Fig S10, S11). Statistical tests (t-tests) used to confirm significance of density differences.
         *   **Reproducibility/Robustness:** Demonstrated through analysis of large numbers of bots (~200 for movement, ~350 for morphology) and experimental replicates (N=10 for repair). Variability is acknowledged (non-movers, morphotype distribution).
         *   **Limitations:** Mechanisms often hypothesized (polarity switch). Correlation shown, but causality between morphology and movement needs further work (Sec 3). Repair mechanism (biochemical vs physical cues) not identified (Sec 3).

---

#Key: [mugica_scale-free_2022]

# Scale-free behavioral cascades and effective leadership in schooling fish

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The study investigates behavioral contagion and cascades (avalanches) in groups exhibiting collective motion. It analyzes empirical data from schooling fish (black neon tetra, *Hyphessobrycon herbertaxelrodi*) and compares it to a computational model based on the Vicsek model with an added explicit leader subject to random heading changes. The system consists of 40 fish swimming in a rectangular tank, tracked via video. Avalanches are defined as sequences of consecutive frames where fish exhibit heading changes larger than a defined threshold angle (ϕth). The computational model involves N self-propelled particles (SPPs) in 2D space, aligning locally with neighbors (Vicsek mechanism) but also influenced globally by a designated leader particle whose direction changes randomly. The purpose is to characterize the scale-free nature of behavioral avalanches in fish and to explore the role of effective leadership and long-range interactions in generating such dynamics, using the model to support the interpretation.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters are key for defining the systems and the analysis performed. Reliability is high as they are directly stated experimental or simulation conditions/variables.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: For the fish school: Biological/metabolic energy sustaining fish movement. For the Vicsek model: Computational energy required to run the simulation (implicitly electrical energy).

### **2.2 Energy Transduction**

    *   Content: For fish: Chemical energy (ATP) -> Mechanical energy (muscle contraction) -> Kinetic energy (swimming). Interactions involve hydrodynamic and visual signal processing leading to behavioral changes (turns). For the model: Computational steps -> Update particle positions and orientations (kinetic energy analogous). Energy is transduced through the interaction rules (alignment, leader influence, noise).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Energy efficiency is not discussed or quantified for either the fish schooling or the computational model.

### **2.4 Energy Dissipation**

    *   Content: For fish: Viscous drag (hydrodynamic friction), metabolic heat loss, energy cost of sensory processing and decision-making (implicit). For the model: Analogous dissipation occurs through the noise term (η) which randomizes heading angles, and potentially through the averaging process in alignment rules, representing imperfect information transfer or execution. Not quantified. Qualitative assessment: Present in both systems, inherent to motion in fluid (fish) and stochasticity/interactions (model).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content:
        *   **Fish (Empirical):** Implicit rules based on visual/hydrodynamic cues leading to alignment and avoidance behaviors. The analysis focuses on the *outcome* (turning angles) rather than deriving the explicit interaction rules from first principles. The concept of "effective leadership" suggests some fish disproportionately initiate turns, implying heterogeneous influence, but the precise rule isn't specified.
        *   **Model (Vicsek+Leader):** Explicit rules:
            1.  **Alignment (SPPs i != 1):** Each SPP `i` aligns its velocity direction `θ_i(t+Δt)` with the average direction `∠[V_i^L(t)]` of particles (including the leader, particle 1) within a neighborhood `V_i^L = V_i ∪ {1}` (where `V_i` is the set of neighbours within radius R), perturbed by noise: `θ_i(t+Δt) = ∠[V_i^L(t)] + ηξ_i(t)`. `V_i^L(t) = (1/n_i^L(t)) * Σ_{j∈V_i^L} v_j(t)`.
            2.  **Leader Influence (SPPs i != 1):** The leader (particle 1) is always included in the neighborhood average calculation `V_i^L(t)` for all other SPPs, regardless of distance (long-range influence).
            3.  **Leader Dynamics (Particle 1):** The leader's velocity `v_1` is unaffected by neighbors. Its heading `θ_1` remains constant (`θ_L`) except during periodic, random reorientations by an angle `Δθ_L` uniformly distributed in `[-π, π]`.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Model: Alignment | Vicsek Interaction Radius | R | 1 | Length units (relative to particle size/density) | Methods: Vicsek model | Explicit | Stated parameter value. |
    | Model: Alignment | Vicsek Noise Strength | η | [0.1, 0.2, 0.3, 0.4] tested | dimensionless | Modeling section, Fig 4 | Explicit | Stated simulation parameter range. |
    | Model: Leader Dynamics | Leader Perturbation Interval | T_perturb | 250 | time steps | Modeling section | Explicit | Stated simulation parameter. |
    | Fish: Avalanche Definition | Turning Angle Threshold | ϕth | [0.20, 1.20] | radians | Results: Avalanche analysis, Fig 1 | Explicit | Range used for empirical analysis. |

### **4.3 Global Order:**

    *   Content:
        *   **Collective Motion:** Coherent, polarized movement of the school/flock (high value of order parameter φ).
        *   **Behavioral Cascades:** Avalanches of turning activity characterized by scale-free distributions of size (s) and duration (t), described by power laws `P(s) ~ s^-τs` and `P(t) ~ t^-τt`.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Model: Alignment | Noise influence on alignment | Noise Strength (η) | [0.1-0.4] tested | dimensionless | Explicit | Simulation parameter range. | Fig 4 |
| Model: Leadership | Leader perturbation timing | Perturbation interval | 250 | time steps | Explicit | Simulation parameter. | Modeling Section |
| Fish: Avalanche Trigger | Threshold for large turn | Turning Angle (ϕth) | [0.20, 1.20] | radians | Explicit | Parameter defining empirical avalanche start. | Fig 1 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Schooling | Polarization | Order Parameter φ | ~1 (low noise model); High (observed fish) | dimensionless | Mixed | Explicitly defined Eq(25), value implicit (high implies schooling). | Eq (25) | Methods, SF 2 |
| Avalanches | Size Distribution | Size Exponent τs | ~2.0 (fish); ~1.0-1.7 (model) | dimensionless | Explicit | Measured from data collapse/moments. | Table 1 | Results, Table 1 |
| Avalanches | Duration Distribution | Duration Exponent τt | ~2.4 (fish); ~0.3-4.0 (model) | dimensionless | Explicit | Measured from data collapse/moments. | Table 1 | Results, Table 1 |
| Avalanches | Size Scaling vs Threshold/Size | Scaling Exponent σs / D | ~3.1 (fish, vs ϕth); ~2.0 (model, vs L) | dimensionless | Explicit | Measured from data collapse/moments. | Table 1 | Results, Table 1 |
| Avalanches | Duration Scaling vs Threshold/Size | Scaling Exponent σt / z | ~1.7 (fish, vs ϕth); ~0.3-0.5 (model, vs L) | dimensionless | Explicit | Measured from data collapse/moments. | Table 1 | Results, Table 1 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | Local Turn -> Global Avalanche | How individual large turns propagate to collective events | Low (Specific event), High (Statistical Distribution) | 6 | Power-law exponents (τs, τt), Scaling exponents (σs, σt, D, z), Data collapse quality | Explicit | The paper explicitly maps local events (turns > ϕth) to global statistical patterns (P(s), P(t)) and quantifies this mapping using scaling analysis. | Results, Figs 2, 5, Table 1 |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 6.
        *   **Rubric:** 0=No clear link; 2=Qualitative link; 4=Quantitative link for average behavior; 6=Quantitative link for statistical distributions; 8=Predictive model based on local rules; 10=Complete formal mapping.
    *   **Metrics:** Power-law exponents (τs, τt), scaling exponents (σs, σt derived from collapse vs ϕth for fish; D, z derived from FSS vs L for model), average avalanche size vs duration exponent (m).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Experimental Frame Interval | 1/20 = 0.05 | s | Results: Empirical analysis | Explicit | Inverse of stated frame rate. |
        | Simulation Time Step | 1 (arbitrary units) | Δt | Methods: Vicsek model | Explicit | Assumed standard Δt=1 for Vicsek sims. |
        | Avalanche Duration (t) | Variable, Power-law distributed (P(t)~t^-τt) | frames (empirical), Δt (model) | Results, Fig 2b, 4d, Table 1 | Explicit | Key finding, lack of characteristic timescale due to power law. |
        | Avalanche Size (s) | Variable, Power-law distributed (P(s)~s^-τs) | total active fish-frames | Results, Fig 2a, 4c, Table 1 | Explicit | Key finding, lack of characteristic scale. |
        | Model Leader Perturbation Interval | 250 | Δt | Modeling section | Explicit | Parameter controlling external perturbation timing in model. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content:
        1.  **Collective Motion (Schooling/Flocking):** Individuals (fish/SPPs) coordinate their movement, resulting in a globally polarized group moving coherently.
        2.  **Behavioral Cascades (Avalanches):** Intermittent, collective rearrangements characterized by bursts of activity (large heading turns) propagating through the group. These avalanches exhibit scale-free statistics in their size (total activity) and duration.
        3.  **Effective Leadership:** Certain individuals (in fish) or a designated particle (in model) disproportionately initiate behavioral cascades.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation relies primarily on quantitative statistical analysis.
        *   **Operational Definitions:** Avalanches are operationally defined based on the turning angle threshold (ϕth) and consecutive activity. Leadership is operationally defined based on the probability of initiating an avalanche (χi).
        *   **Quantitative Analysis:** Power-law fitting and scaling collapse analysis (Eqs 4-8, Figs 2c-d, 5c-d) are used to validate the scale-free nature of avalanche size and duration distributions. Moment analysis is used for exponent estimation in the model.
        *   **Control/Comparison:** Comparison with a null model (randomized turning angles) demonstrates that correlations are necessary for the observed power-law behavior (Fig 2a-b, Fig 2e inset). Comparison between fish data and the Vicsek+Leader model shows qualitative agreement in avalanche statistics. Comparison with standard Vicsek (SF 4) shows the leader is necessary for scale-free avalanches in the model.
        *   **Reproducibility:** Analysis performed on three independent experimental recordings (A, B, C). Model results based on simulations with large numbers of avalanches.
        *   **Limitations:** Finite-size effects in experiments limit the extent of power-law observation. Model exponents depend on threshold and noise, unlike ideal SOC. Causal mechanism linking leadership directly to scale-free statistics is conjectured rather than Bproven.

---

#Key: [ebbens_catalytic_2018]

# Catalytic Janus Colloids: Controlling Trajectories of Chemical Microswimmers

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of catalytic Janus colloids, typically spherical microparticles (e.g., polystyrene) half-coated with a catalyst (e.g., Platinum). These colloids are placed in a fluid containing a chemical fuel (e.g., aqueous hydrogen peroxide). The catalyst asymmetrically decomposes the fuel, generating local gradients (e.g., concentration, electric field) that propel the colloid via phoretic mechanisms (self-phoresis). The primary purpose is to achieve autonomous motion ("swimming") at the microscale, enabling applications like targeted drug delivery, analyte gathering for diagnostics, microfluidic transport without external fields, and templating for materials science via self-assembly. The paper reviews methods to control the trajectories of these swimmers, both individually (e.g., using geometry, gravity, boundaries) and collectively (e.g., via chemical gradients, inter-particle interactions).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name        | Value           | Units        | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :-------------------- | :-------------- | :----------- | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Brownian Rotation Rate| ~1/r³ dependent | rad²/s or Hz | Text (implicitly D_r)     | Implicit          | Medium (Standard physics)       | Inferred from D_r = kT/(8πηr³)  |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Chemical energy stored in the fuel (typically hydrogen peroxide, H2O2). The energy is released through catalytic decomposition reaction mediated by the catalyst (e.g., Platinum) on the Janus colloid surface.
    *   Units: J/mol (Implicitly)

### **2.2 Energy Transduction**

    *   Content: Chemical energy released from the catalytic decomposition of H2O2 is converted into kinetic energy of the colloid. The mechanism is self-phoresis: the asymmetric reaction creates local gradients (e.g., concentration of reactants/products, potentially ions leading to electric fields) around the particle. These gradients interact with the particle surface, generating a net force and/or torque that drives translational and/or rotational motion. A significant portion of the chemical energy is also dissipated as heat.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Extremely low. The paper does not quantify efficiency, but chemo-mechanical transduction in synthetic swimmers is notoriously inefficient. Most chemical energy released is dissipated as heat rather than converted into useful kinetic energy for propulsion. Efficiency (kinetic power output / chemical power input) is typically << 1%. Qualitative Assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include:
        1.  **Heat:** The catalytic decomposition of H2O2 is exothermic; most released chemical energy likely dissipates as heat into the surrounding fluid (Qualitative Assessment: High).
        2.  **Hydrodynamic Drag:** Energy is continuously dissipated due to viscous drag as the colloid moves through the fluid (Qualitative Assessment: High, necessary consequence of motion in fluid).
        3.  **Reaction Entropy:** Irreversible entropy production associated with the chemical reaction itself (Qualitative Assessment: Medium-High, inherent in chemical reactions).
        Quantification is not provided in the paper.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The paper discusses several local interaction mechanisms contributing to self-organization and collective behavior:
        1.  **Phoretic Interactions:** Colloids interact via the chemical gradients (fuel/product concentration) they create and respond to. Particles can be attracted or repelled depending on their nature ("chemotaxis"). (Mentioned in "CHEMICAL GRADIENTS AND COLLECTIVE BEHAVIOR", refs 51, 53, 55).
        2.  **Hydrodynamic Interactions:** Moving colloids generate flow fields (Fig 4c) that affect nearby colloids. These are approximated as "pusher" or "puller" types in far-field models, but near-field complexities exist (Fig 4c discussion, refs 62, 69).
        3.  **Steric Interactions:** Simple excluded volume effects prevent overlap. (Implicit).
        4.  **Conventional Colloidal Interactions:** Background forces like van der Waals or electrostatic interactions may play a role, although not the focus here. (Mentioned implicitly as background physics).
        Specific equations governing these interactions are not provided in this review but are referenced (e.g., refs 51, 53, 62, 69). The rules lead to alignment/misalignment of caps in dimers (Fig 2a) and collective clustering/pattern formation (Fig 4a, 4b).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID                 | Description                       | Parameter Name        | Parameter Value Range | Units          | Data Source                  | Implicit/Explicit | Justification                                  |
    | :---------------------- | :-------------------------------- | :-------------------- | :-------------------- | :------------- | :--------------------------- | :----------------: | :--------------------------------------------- |

### **4.3 Global Order:**

    *   Content: The paper describes observed and predicted global order:
        1.  **Dimers/Agglomerates:** Small clusters form via self-assembly (Fig 2a).
        2.  **Dynamic Clusters:** Experimental observations show clustering phenomena (Fig 4b, refs 66, 67).
        3.  **Predicted Collective Phases:** Theoretical work predicts a wide range of configurations including dynamic oscillating structures, self-motile "molecules", and various phases depending on chemical/kinetic parameters (Fig 4a, refs 24, 51, 55, 58-65).
        4.  **Convective Flow:** At high volume fractions, symmetrical catalytic colloids induce collective circulating flows (ref 70).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID                 | Description                              | Parameter           | Value Range | Units   | Implicit/Explicit | Justification                                       | Source              |
| :---------------------- | :--------------------------------------- | :------------------ | :---------- | :------ | :----------------: | :-------------------------------------------------- | :------------------ |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID   | Description           | Parameter                 | Value Range | Units    | Implicit/Explicit | Justification                                       | Protocol             | Source                 |
| :------------ | :-------------------- | :------------------------ | :---------- | :------- | :----------------: | :-------------------------------------------------- | :------------------- | :--------------------- |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type                     | Description                                        | Predictability   | Yoneda Score | Metrics | Implicit/Explicit | Justification                                                                     | Source         |
    | :---------------------------- | :------------------------------------------------- | :--------------- | :-----------: | :------ | :----------------: | :-------------------------------------------------------------------------------- | :------------- |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 3 (Rubric: 0=No mapping; 2=Qualitative link; 4=Partial quantitative link; 6=Good quantitative link; 8=Predictive model; 10=Fully characterized functorial map). The paper establishes qualitative links (interactions -> aggregation; geometry -> motion) and some quantitative relationships (theory predicts phases), but a rigorous, predictive local-to-global map validated experimentally across diverse conditions is lacking, especially for complex collective behavior.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description     | Value                   | Units    | Source         | Implicit/Explicit | Justification                                                                 |
        | :------------------------ | :---------------------- | :------- | :------------- | :----------------: | :---------------------------------------------------------------------------- |
        | Brownian Rotation Time (τ_r)| Scales as ~r³           | s        | Text (implicit)| Implicit          | Characteristic timescale for loss of orientation correlation, D_r ~ 1/τ_r.    |
        | Ballistic Motion Regime   | Short                   | s        | Text (implicit)| Implicit          | Time before τ_r dominates, displacement ~ vt.                                   |
        | Diffusive Motion Regime   | Long (> τ_r)            | s        | Text (implicit)| Implicit          | Time after τ_r, motion becomes random walk like, displacement ~ sqrt(t).      |
        | Propulsion Velocity       | ~μm/s                   | μm/s     | Fig 1c         | Explicit          | Inverse of time to travel 1 μm ballistically.                                |
        | Angular Velocity (Driven) | Up to 2.5 Hz (observed) | Hz       | Text (Sec: Circles)| Explicit          | Characteristic time for one rotation (1/f).                                   |
        | Fuel Depletion Time       | Variable                | s to hrs | Text (Drawbacks) | Explicit          | Timescale over which fuel is consumed, limiting operation duration.          |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behavior is **autonomous directed motion (self-propulsion)** driven by catalytic fuel decomposition. Several emergent behaviors arise from this basic function and interactions:
        1.  **Enhanced Diffusion:** At long timescales, random reorientation leads to diffusive-like motion with an enhanced diffusion coefficient compared to passive colloids.
        2.  **Circular/Helical Trajectories:** Introduced rotational propulsion (due to cap asymmetry or self-assembly) combined with translation leads to circular paths. Gravity can further align these into helical trajectories (gravitaxis).
        3.  **Boundary Steering/Guidance:** Colloids follow edges, trenches, or posts due to hydrodynamic and steric interactions, enabling transport along defined paths.
        4.  **Chemotaxis/Diffusiophoresis:** Directed motion up or down chemical gradients.
        5.  **Self-Assembly:** Formation of dimers and clusters with distinct collective motile properties.
        6.  **Collective Motion/Clustering:** Aggregation and potentially coordinated movement in groups (observed clustering, theoretically predicted dynamic patterns).

### **8.2 Behavior Robustness:**

        *   *Self-propulsion* itself is robust as long as fuel is present.
        *   *Directionality* (persistence) of individual swimmers is low due to Brownian rotation, especially for smaller colloids (Fig 1d). (Low Robustness).
        *   *Circular/helical motion* due to designed asymmetry (glancing angle) can be well-defined (Fig 2b), but sensitivity to fabrication variations might exist. Helices are sensitive to rotational axis alignment relative to gravity (Fig 2c). (Medium Robustness).
        *   *Boundary steering* appears relatively robust for well-defined geometries (trenches, edges - Fig 3), providing a reliable guidance mechanism. (High Robustness).
        *   *Self-assembly* into specific structures (dimers) occurs, but the initial orientation is random (Fig 2a). (Medium Robustness for dimer formation, Low for specific orientation).
        *   *Collective behavior/clustering* is sensitive to parameters (concentration, particle properties, chemical gradients, hydrodynamics), and controlling specific collective states reliably is presented as a challenge. (Low-Medium Robustness experimentally).
        The score reflects an average, acknowledging robust guidance mechanisms alongside sensitive individual and collective behaviors.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors are validated through several methods:
        *   **Direct Observation:** Video microscopy is used to track individual trajectories (Fig 1b), observe self-assembly (Fig 2a), helical paths (Fig 2c), boundary steering (Fig 3), and clustering (Fig 4b). Time-lapse images are common (Fig 3c, 3d).
        *   **Quantitative Trajectory Analysis:** Statistical analysis of trajectories confirms ballistic-to-diffusive crossover and size dependence (Fig 1c, 1d, ref 27). Correlation between orientation and motion direction confirms co-rotation (Fig 1b details, ref 26). Measurement of translational and angular velocities (Fig 2b).
        *   **Flow Field Visualization:** Particle Image Velocimetry (PIV) or similar techniques using tracer particles map the hydrodynamic flow field around a swimmer (Fig 4c, ref 69).
        *   **Comparison with Theory/Simulation:** Experimental results (e.g., helical paths, clustering, flow fields) are often compared with Langevin simulations (Fig 2c) or analytical theory (Fig 4a, discussion on mechanisms).
        *   **Control Experiments:** E.g., comparison with non-catalytic particles or varying fuel concentration. Spun coat vs glancing angle deposition serves as control in Fig 2b.
        *   **Limitations:** Difficulty in fully incorporating all complex interactions (hydrodynamic, chemical, thermal) into theory/simulations. Experimental control over all parameters for collective behavior studies is challenging. Reproducibility across different labs/fabrication methods can sometimes be an issue (not explicitly stated, but common in the field).

---

#Key: [ciaunica_brain_2023]

# The brain is not mental! coupling neuronal and immune cellular processing in human organisms

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper presents a theoretical framework arguing that human cognition is not solely a product of the brain's neural activity. Instead, it proposes cognition as an emergent property of the entire organism, arising from the dynamic coupling and information processing between various cellular systems, primarily focusing on the interplay between the neuronal and immune systems. The system described is the human organism, viewed as a self-organizing biological entity. Its components include neuronal cells, immune cells (T-cells, B-cells, NK cells, microglia, etc.), other bodily cells, signaling molecules (neurotransmitters, cytokines, hormones), and the extracellular environment. The *purpose* or function described is biological self-organization, adaptation to the environment, maintenance of homeostasis/allostasis, and ultimately, survival and flourishing of the organism. The paper re-frames cognition itself as this distributed, multi-scale information processing geared towards self-maintenance.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Timescale of Immune Response | Variable (Rapid innate to slower adaptive) | seconds to days/years | Section 4 | Implicit | High (general immunology knowledge) | Inferred from descriptions of innate vs adaptive responses and memory. |

    *   **Note:** The paper primarily discusses concepts and mechanisms qualitatively, drawing on broad biological knowledge. Specific quantitative parameters defining the *entire system's dynamics* as proposed by the authors are not provided. The table lists parameters mentioned to characterize the scale and components involved.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The paper discusses biological systems maintaining states far from thermodynamic equilibrium (Section 1, referencing FEP, Schrödinger). The implicit energy source for the human organism described is metabolic/chemical energy derived from nutrient intake.

### **2.2 Energy Transduction**

    *   Content: Energy is transduced through complex biochemical pathways (metabolism), converted into bioelectrical signals (neuronal firing, membrane potentials in other cells), chemical signals (neurotransmitter/cytokine release), and mechanical work (cellular movement, tissue remodeling). The paper mentions bioelectricity (Section 2), chemical signaling (Section 4), and implicitly metabolic processes sustaining cellular activity.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper discusses the Free Energy Principle (FEP) which frames biological systems as minimizing free energy (a measure related to prediction error/surprise), implying an optimization principle related to (information-theoretic) efficiency (Section 1). However, it provides no quantifiable metrics for the thermodynamic efficiency of the proposed distributed cognitive processes. Biological systems are generally highly efficient compared to current computing, but this is not quantified here.

### **2.4 Energy Dissipation**

    *   Content: The paper explicitly mentions entropy and the tendency towards thermodynamic equilibrium (dissipation and decay) which biological systems resist through self-organization (Section 1). Mechanisms of dissipation in biological systems include heat loss from metabolic processes, electrical resistance, non-productive chemical reactions, etc. FEP frames minimizing free energy as equivalent to resisting dissipation. No specific mechanisms or quantification of dissipation are provided for the system discussed. Assessment: High (inherent in biological systems).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable (Seconds to Lifetime)
*    Units: Time (Qualitative Descriptors: "Short-term" to "Long-term/Lifelong")

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The local rules are the biochemical and bioelectrical interactions between cells (neurons, immune cells, etc.). These include:
        *   Release and reception of signaling molecules (cytokines, chemokines, neurotransmitters, hormones) activating specific intracellular pathways (Section 4).
        *   Direct cell-to-cell contact interactions (e.g., immunological synapses, gap junctions) (Section 4).
        *   Bioelectrical signaling (membrane potentials, action potentials in neurons, potentially others) influencing cell state and communication (Sections 2, 4).
        *   Metabolic exchanges and responses to local environmental cues (nutrients, oxygen, pH) (Implicit).
        *   Physical forces and mechanical interactions (Implicit).
        The rules are complex, context-dependent, and involve intricate signaling networks described qualitatively.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The emergent global order is the maintenance of the organism's structural and functional integrity, homeostasis, allostasis, adaptive behavior, and ultimately, cognition itself. It is the coherent functioning of the multicellular system as a unified whole, capable of resisting decay and interacting adaptively with its environment (Sections 1, 6). This includes the coordinated physiological states and behavioral patterns of the organism.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog/Biological/Chemical/Implicitly Neuromorphic/Other (Distributed Network Processing)

### **5.3 Computational Primitive:**

    *   Content: The paper suggests several primitives:
        *   **Pattern Recognition/Classification:** Immune cells distinguishing self/non-self/danger signals via receptor binding (Section 4).
        *   **Thresholding/Activation:** Cellular responses triggered when signal concentrations exceed certain levels (Implicit).
        *   **Signal Integration:** Cells integrating multiple inputs (cytokines, hormones, neural signals) to determine response (Section 4).
        *   **Decision-Making:** Immune system choosing between activation/suppression/tolerance (Section 4); Collective decision-making in immune networks (Section 5).
        *   **Memory Update/Learning:** Cellular changes reflecting past experience (immune memory/training, basal learning) (Sections 3, 4).
    *   **Sub-Type (if applicable):** Pattern Recognition: Molecular recognition via receptors. Decision-Making: Collective network dynamics / Thresholding.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Neuronal Signaling | Milliseconds to Seconds | time | Implicit | Implicit | Inferred from general neuroscience knowledge relevant to the discussion. |
        | Immune Response (Innate) | Minutes to Hours | time | Section 4 | Implicit | Inferred from description of innate immunity as "rapid". |
        | Immune Response (Adaptive) | Days to Weeks | time | Section 4 | Implicit | Inferred from description of adaptive immunity development. |
        | Immune Memory | Days to Lifetime | time | Section 4 | Explicit | Explicitly mentioned long-term nature of adaptive memory. |
        | Cellular Processes (Metabolism, Signaling Cascades) | Variable (ms to hours) | time | Implicit | Implicit | Inferred from general cell biology. |
        | Development | Months to Years | time | Sections 3, 6, 7 | Explicit | Explicitly discussed as a crucial timescale. |
        | Self-Organization Dynamics (e.g., FEP) | Continuous/Variable | time | Section 1 | Implicit | FEP operates continuously over multiple timescales. |
    *   **Note:** Timescales are mostly qualitative or inferred broad ranges based on the biological processes discussed.

### **6.2 Active Inference:**

    *   Content: Yes
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** Metrics related to minimizing prediction error (e.g., KL divergence between expected and observed states mapped onto immune/neural activity); Timescales of anticipatory responses in immune/neural signaling; Measures of the complexity/accuracy of the 'internal models' implicitly encoded in immune/neural network states; Correlation between actions (e.g., immune cell migration, cytokine release) and subsequent reduction in 'surprise' (deviation from homeostatic setpoints). Experimental setups could involve perturbing the system and measuring the dynamics of return to baseline, analyzing predictive coding patterns in neural/immune signals.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: Several mechanisms are described or implied:
        *   **Immune System:**
            *   Clonal Selection/Expansion: Proliferation of lymphocytes recognizing specific antigens (Implicit basis of adaptive immunity).
            *   Somatic Hypermutation/Affinity Maturation: Modification of antibody genes to improve binding (Implicit basis of adaptive immunity).
            *   Cellular Differentiation: Development into specialized memory or effector cells (Section 4).
            *   Epigenetic Changes/'Training': Altered responsiveness of innate immune cells (trained immunity, Section 4).
            *   Selection/Licensing: Processes tuning cell reactivity during development (e.g., T-cell selection, NK licensing, Section 4).
        *   **Basal Cognition:** Mechanisms for memory and learning in aneural cells/organisms (cited but not detailed, Section 3). Examples might include epigenetic changes, alterations in metabolic networks, or structural modifications.
        *   **Neural System:** Synaptic plasticity, neurogenesis (implicitly assumed context, Section 6).
        *   **General:** Feedback loops within and between systems (e.g., neuro-immune interactions, Section 4, 5) driving changes based on system state and environmental input, potentially guided by FEP (Section 1).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior described is **cognition** itself, reframed as a distributed, multi-scale process of information handling across cellular networks (neuronal, immune, etc.) enabling **biological self-organization**, **homeostasis/allostasis**, **adaptation** to the environment, and **survival** of the organism as a whole. Specific sub-behaviors discussed include: immune responses (pathogen clearance, inflammation, tolerance, sickness behavior, Section 4), neural processing (perception, action control, implicitly), developmental processes (Section 6), and learning/memory across different cell types (Sections 3, 4).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper validates its claims primarily by citing and synthesizing existing empirical evidence and theoretical work from diverse fields (neuroscience, immunology, theoretical biology, developmental biology, philosophy). It references specific studies demonstrating immune memory, neuro-immune interactions, basal cognition phenomena, FEP applications, etc. (numerous citations throughout). It does not present new experimental data or simulations to validate emergent behaviors directly but builds a case based on established findings interpreted through its proposed framework. Limitations include the reliance on interpreting existing data through a new lens and the lack of a single, integrated experimental model demonstrating all aspects of the proposed distributed cognition.

---

#Key: [riley_neuromorphic_2022]

# Neuromorphic Metamaterials for Mechanosensing and Perceptual Associative Learning

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a neuromorphic metamaterial designed for mechanosensing and associative learning. It comprises a multistable metamaterial sheet made of bistable dome units (3D printed TPU) embedded with piezoresistive sensors (conductive PLA). These units filter, amplify, and transduce mechanical inputs above a threshold force into electrical resistance changes. These signals program flexible, non-volatile polymeric memristors (ITO/PEDOT:PSS/Cu) which store sequences of spatially distributed mechanical inputs as analogue resistance states. The memristor array is configured to physically embody a Hopfield network, where memristance changes correspond to updating synaptic weights based on Hebbian learning rules triggered by dome inversions. The system's purpose is to demonstrate embodied mechanosensing, memory, and online learning of spatial patterns, mimicking tactile perception and associative memory without supervised training or significant external overhead. It learns applied patterns and allows their retrieval from the final memristor states.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |


## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The system uses two primary energy inputs:
        1.  Mechanical energy: Applied force/pressure to invert the bistable domes.
        2.  Electrical energy: DC voltage sources for powering the sensor readout circuit, the signal amplification/switching circuit, and the memristor programming/reading pulses. Specific voltages mentioned include 2V for sensor reading, 5V for the switch regulator output/memristor writing, 1.5V for memristor reading, and 4.3V for Hopfield training pulses.
    *   Value: Mechanical: Force ≥ F_th (e.g., 18 N, 22 N depending on geometry); Electrical: 1.5, 2, 4.3, 5 V
    *   Units: Mechanical: N; Electrical: V

### **2.2 Energy Transduction**

    *   Content: 1. Mechanical to Mechanical/Strain: Applied force above threshold causes dome snap-through (potential energy release) leading to strain in the surrounding membrane. 2. Mechanical/Strain to Electrical Resistance: Strain in the membrane changes the resistance of the embedded piezoresistive sensor. 3. Electrical (Resistance Change) to Electrical Voltage: The sensor resistance change alters the voltage drop across it in a voltage divider circuit, which is then amplified and used to trigger a switch regulator. 4. Electrical (Voltage Pulse) to Electrochemical/Resistance Change: Voltage pulses above V_th applied to the PEDOT:PSS memristor induce electrochemical changes (likely related to ion migration/redox reactions involving Cu/ITO electrodes and the polymer) that alter its resistance state non-volatily.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Efficiency is not explicitly calculated or discussed. The system relies on active components (amplifiers, regulators, power supplies) and involves resistive elements (sensors, memristors, series resistors), suggesting significant energy loss as heat. The mechanical actuation requires forces on the order of ~20N. Memristor programming uses relatively long pulses (5s) at several volts (4.3-5V), which, while potentially low power compared to CMOS for *parallel* computation, is not inherently energy-minimal per operation in absolute terms for this specific implementation. The comparison to event cameras suggests *potential* for power efficiency due to thresholding, but this is not quantified. Qualitative Assessment: Low-Medium. The thresholding nature avoids constant sensing power drain, but the active electronics and writing process likely consume non-trivial energy.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include: 1. Mechanical Damping/Viscoelasticity: Energy loss during dome snap-through and material deformation (TPU stress-softening mentioned, Fig S2). 2. Joule Heating: Resistive losses in piezoresistive sensors, memristors (especially in LRS), series resistors (R_s), and wiring. 3. Electronic Component Losses: Energy consumed by Op-Amps in the amplification circuit (Fig S7) and the switch regulator. 4. Electrochemical Side Reactions: Potential parasitic reactions during memristor programming. Quantification is not provided. Qualitative Assessment: Medium-High (due to active electronics, resistive elements, and potentially inefficient mechanical/electrochemical processes).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes
        1.  Structural Memory: The bistable domes retain their state (ground or inverted) after the force is removed, representing a 1-bit memory of whether the threshold force was exceeded. This state change affects the sensor resistance (Fig 2d, e).
        2.  Electrical Memory: Non-volatile memristors store analogue resistance values that represent the cumulative history of dome inversion events (specifically, pair-wise interactions in the Hopfield network implementation). The memristor state persists after power removal and influences the behavior of the Hopfield network (pattern retrieval) (Figs 3, 4).

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Non-volatile
*    Units: Qualitative Descriptor

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 3 (unique patterns); 6 (memristors storing interactions for 4 units)
*   Units: Patterns (for Hopfield network); Number of analogue states (per memristor, not specified but implied > 6 by Fig 3c)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: 91
*   Units: % (Pattern Retrieval Accuracy)

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Pattern Retrieval Accuracy | % of corrupted patterns correctly identified by Hopfield network | 91 | % | `HopfieldNetworkNode`.attribute: `accuracy` | Section: Spatially distributed input learning, SI A.14 | Explicit | Value explicitly stated |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Neuromorphic/Analogue (Hybrid aspects due to digital switch/logic)

### **5.3 Computational Primitive:**

    *   Content: The primary computational primitive embodied is **Associative Memory / Pattern Completion** via Hopfield network dynamics (energy minimization). This relies on underlying operations:
        1.  **Thresholding:** Mechanical input filtering by bistable domes.
        2.  **Analogue Weight Storage:** Storing synaptic weights (J_ij) as memristor resistance values.
        3.  **Weighted Summation (Implicit):** The physics of the interconnected network effectively performs weighted summation during recall (though not explicitly calculated as such).
        4.  **Non-linear Activation (Implicit):** Neuron state updates in Hopfield networks typically involve a sign function or threshold, implicitly realized during the pattern retrieval dynamics.
    *   **Sub-Type (if applicable):** Associative Memory (Hopfield Network)

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Dome Snap-Through | Fast (transient spike) | ms (estimated) | Fig 2c | Implicit | Duration of spike implies fast transition, but not quantified. |
        | Dome Inversion Cycle Test Interval | 30 | s | Fig 2c caption | Explicit | Explicitly stated interval between snaps. |
        | Memristor i-v Sweep Frequency | 100 | mHz | Fig 3b caption | Explicit | Explicitly stated frequency. |
        | Memristor Write Pulse Width (T_on) | 1, 2, 5 | s | Fig 3d | Explicit | Explicitly tested values. 5s used for Hopfield. |
        | Memristor Write Cycle Time | ~15 | s | Fig 3c caption | Explicit | Explicitly stated interval between pulses. |
        | Memristor Read Pulse Width | 10 | s | Fig 4b | Explicit | Explicitly stated duration. |
        | Hopfield INIT Pulse Width | 20 | s | Fig 4b | Explicit | Explicitly stated duration. |
        | Hopfield Calibration Pulse Width | 5 | s | Fig 4b | Explicit | Explicitly stated duration. |
        | Mechanical Test Rate | 20 | mm/min | Methods | Explicit | Explicitly stated test machine speed. |
    *   **Note:** The timescales range from potentially milliseconds (snap-through) to seconds (memristor operations, manual tests) and tens of seconds (cycle intervals).

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is based on Hebbian learning implemented within the physically embodied Hopfield network. Specifically, interactions between pairs of dome units (neurons) during the application of spatial patterns trigger changes in the corresponding memristors (synapses). An XOR gate logic (Fig S16) is used to determine when to update a memristor: a resistance change (write pulse) occurs when the two connected dome units are in *different* states (-1 and +1). This approximates the Hebbian/Hopfield learning rule where the change in synaptic weight J_ij is proportional to the product of the neuron states (ξ_i * ξ_j), strengthening connections between similarly active or inactive neurons and weakening connections between neurons with opposite activity (implemented here by only writing when states differ, effectively encoding the outer product, potentially with scaling/normalization described in SI A.13 via eq S.10). The cumulative, analogue change in memristor resistance stores the sum of these updates over the sequence of patterns. J_ij = Σ_μ ξ_i^μ ξ_j^μ (conceptual rule); physical implementation via XOR driving memristor ΔM ∝ -(State_i XOR State_j).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors are:
        1.  **Thresholded Mechanosensing:** Filtering mechanical inputs below a tunable force threshold (F_th).
        2.  **Signal Amplification/Transduction:** Converting supra-threshold mechanical events into significant, measurable electrical resistance changes.
        3.  **Spatio-temporal Memory Encoding:** Storing the occurrence and spatial pattern of sequential supra-threshold mechanical events as persistent analogue states in the memristor array.
        4.  **Associative Learning/Pattern Recognition:** Learning a sequence of applied spatial patterns via Hebbian updates to the embodied Hopfield network weights (memristor states).
        5.  **Pattern Retrieval/Completion:** Recalling stored patterns from the final memristor states when presented with noisy or incomplete versions (demonstrated via offline simulation using the physically learned weights).

### **8.2 Behavior Robustness:**

        *   **Mechanosensing:** Thresholding provides robustness against sub-threshold noise. Sensor baseline drift and potential damage after repeated cycles (Fig 2c, Fig S2, S6) indicate limitations in long-term robustness/repeatability.
        *   **Memory/Learning:** Memristors show device-to-device and cycle-to-cycle variability (Fig S10, S11, S13), potentially affecting learning fidelity. However, the Hopfield network successfully learned patterns despite this, achieving 91% retrieval accuracy from corrupted inputs, suggesting some robustness inherent in the network dynamics. Performance difference (5%) compared to ideal offline training indicates impact of physical imperfections. Long-term stability and robustness to environmental factors (temperature, humidity) are not assessed.
        *   Qualitative Assessment: Medium-Low. While the Hopfield network shows some pattern retrieval robustness, the underlying components (sensors, memristors) exhibit variability and degradation potential.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content:
        *   **Mechanosensing/Thresholding:** Validated experimentally using force-displacement tests (Instron) correlated with resistance measurements (Figs 2e, S4, S5). Control experiments involved varying dome geometry (thickness, height) to demonstrate tunable thresholds.
        *   **Memory Encoding:** Validated by demonstrating repeatable resistance changes corresponding to dome states (Fig 2c) and incremental, non-volatile resistance changes in memristors upon voltage pulsing (Fig 3c, d).
        *   **Associative Learning:** Validated by physically applying patterns to the 2x2 array, measuring initial/final memristor voltages (Fig 4c), calculating the interaction matrix (SI A.13), and demonstrating offline that this matrix yields an energy landscape with attractors corresponding to the input patterns (Fig 4d, dimensionality reduction visualization).
        *   **Pattern Retrieval:** Validated *offline* via simulation using the experimentally derived interaction matrix. Performance was quantified by presenting corrupted patterns and measuring retrieval accuracy (91% reported, SI A.14, Movie S2). Direct physical retrieval on the hardware was not demonstrated.
        *   **Limitations:** Retrieval tested offline; long-term stability/robustness not validated; scalability beyond 2x2 not shown.

---

#Key: [puy_perceived_2024]

# Perceived risk determines spatial position in fish shoals through altered rules of interaction

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of a shoal of 16 black neon tetra fish (Hyphessobrycon herbertaxelrodi), composed of two subgroups (8 habituated, 8 non-habituated) in a quasi-two-dimensional experimental tank. The purpose is to investigate how differences in perceived risk (manipulated via habituation) affect individual spatial positioning and interaction rules within the shoal. The system's behavior (individual trajectories) is recorded and analyzed using methods like force maps and machine learning to understand collective dynamics and interaction rules.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters define the core setup of the experiment. Data reliability is high as these are directly stated experimental conditions.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is metabolic energy derived from food consumed by the fish, powering their muscle contractions for swimming. External energy inputs include lighting for visibility (necessary for visual interaction and recording) and potentially heating to maintain water temperature.

### **2.2 Energy Transduction**

    *   Content: Metabolic energy (chemical) is transduced into kinetic energy (swimming motion) via muscle contractions. Sensory information (visual, potentially lateral line) about neighbors and the environment is transduced into neural signals, which inform motor commands, influencing the kinetic energy output (changes in speed and direction).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss or quantify the energy efficiency of swimming, sensory processing, or information transfer in the fish. Assigning a score would be purely speculative.

### **2.4 Energy Dissipation**

    *   Content: Energy is dissipated primarily as heat due to metabolic processes and hydrodynamic drag during swimming. Sound production might represent minor dissipation.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: > 5 days, < lifetime
*    Units: days (Qualitative Descriptor: "Long-term" relative to experiment duration)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Low (Essentially binary: Familiar/Unfamiliar)
*   Units: states

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The paper infers interaction rules using force maps and nearest-neighbor analysis:
        1.  **Local Spatial Interactions (Attraction-Repulsion):** Inferred from acceleration vs. relative position to nearest neighbor (Fig 2d,e,f). Shows repulsion at very short distances, attraction at longer distances, and an equilibrium distance. Non-habituated fish show weaker repulsion. Mechanism: Likely visual/lateral line sensing leading to speed/direction adjustments to maintain preferred distance.
        2.  **Local Alignment:** Inferred from acceleration vs. relative velocity to nearest neighbor (Fig S13). Fish tend to align with neighbors, particularly faster ones ("selective alignment"). Non-habituated fish show different occupancy in the alignment force map phase space, suggesting modified alignment responses. Mechanism: Likely visual/lateral line sensing of neighbor orientation/speed leading to turning adjustments.
        3.  **Risk-Dependent Modification:** Non-habituated fish exhibit altered rules compared to habituated fish (weaker repulsion, potentially different alignment strength/response characteristics, tendency to follow).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Attr/Rep | Equilibrium Distance | Preferred NN Distance (Peak of PDF) | ~3-4 | cm | Fig 2c | Explicit | Peak value from plotted PDF. |
    | Attr/Rep | Repulsion Strength | Max Repulsive Acceleration (Radial) | Hab: ~17.5, Non-hab: ~12.5-15 | cm/s² | Fig 2f | Explicit | Peak values from plotted force map analysis. |
    | Attr/Rep | Attraction Strength | Attractive Acceleration (Radial) @ 10cm | ~5 | cm/s² | Fig 2f | Explicit | Approximate value read from graph. |
    | Group | Attraction to Center | Attractive Acceleration (Radial) @ 20cm | ~10-15 | cm/s² | Fig 1g | Explicit | Approximate value read from graph. |

### **4.3 Global Order:**

    *   Content: The global order emerging includes:
        1.  **Cohesive Shoal Formation:** Fish maintain proximity, forming a distinct group (quantified by radius of gyration Rg, Figs S2c,d).
        2.  **Polarization:** Individuals tend to align their velocity vectors, leading to coordinated group movement (quantified by polarization ϕ, Fig 3b).
        3.  **Spatial Assortment:** Non-random distribution of individuals based on risk perception state. Non-habituated fish occupy more central positions (Figs 1a,b,c) and spend less time on the border (Fig 1d). Habituated fish tend towards the front (Fig 1c).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Attr/Rep | Attraction/Repulsion vs NN distance | Max Repulsion Accel. | ~12.5 - 17.5 | cm/s² | Explicit | Value read from Fig 2f | Fig 2f |
| Attr/Rep | Attraction/Repulsion vs NN distance | Equilibrium Distance | ~3-4 | cm | Explicit | Peak of PDF in Fig 2c | Fig 2c |
| GrpAttr | Attraction vs CM distance | Max Attraction Accel. | ~10-15 | cm/s² | Explicit | Value read from Fig 1g | Fig 1g |
| Follow | Leader-Follower Delay (Orientation) | Max Correlation Delay | -0.04 to +0.04 | s | Explicit | Peak positions in Fig 5a | Fig 5a |
| Follow | Leader-Follower Delay (Speed) | Max Correlation Delay | -0.04 to +0.04 | s | Explicit | Peak positions in Fig 5b | Fig 5b |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Cohesion | Group Size/Spread | Radius of Gyration (Rg) | ~10-15 (start), ~15-20 (end) | cm | Explicit | Approx. range from center in Fig 1g/S2f | Calculation from trajectories | Fig 1g, S2c/d/f |
| Polarization | Group Alignment | Polarization (ϕ) | Mean ~0.7-0.8 | dimensionless | Explicit | PDF peak/range in Fig 3b | Calculation from velocities | Fig 3b |
| Assortment | Spatial Segregation | Relative Density (Non-Hab center vs Hab) | >1 (center), <1 (periphery) | dimensionless | Explicit | Color scale Fig 1a, PDF ratios Fig 1b/c | Normalised counts | Fig 1a,b,c |
| Assortment | Border Occupancy | Avg. time on border (∆t_border) | Mean ~0.15 (short), Hab > Non-hab (long) | s | Explicit | PDF peak/tails Fig 1d | Convex hull analysis | Fig 1d |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    *   **Metrics:** Radius of Gyration (Rg), Polarization (ϕ), Nearest-Neighbor Distance PDF, Relative Density Maps, Border Time PDF, Interaction Force Maps (Attraction, Repulsion, Alignment), Leader-Follower Correlations.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Frame Rate (Sampling Interval) | 0.02 | s | Methods: Apparatus | Explicit | 1 / 50 fps |
        | Reaction Time (Leader-Follower Delay) | ~0.02 - 0.04 | s | Fig 5a,b; S14 | Explicit | Peak offset in correlation functions. |
        | Burst-and-Coast Period (∆t_BC) | Peak ~0.5-1 | s | Fig 6c | Explicit | Peak of PDF distribution. |
        | Short Border Duration | ~0.15 | s | Fig 1d | Explicit | Peak of PDF distribution. |
        | Long Border Duration | > 5 | s | Fig 1d | Explicit | Tail behavior of PDF. |
        | Habituation Period | 5 | days | Methods: Procedure | Explicit | Duration of pre-exposure. |
        | Experiment Duration | 120 | minutes | Methods: Test trial | Explicit | Duration of recording. |
        | Temporal Trend Analysis Window | 30 | minutes | Fig S2, S6, S8, S12, S15, S17 | Explicit | Comparison of first/last half-hour. |
    *   **Note:** Various timescales are relevant, from rapid reactions and movements (<1s) to experimental durations (hours) and learning (days).

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes
        1.  **Habituation (Learning):** The habituated group exhibits altered behavior (reduced risk perception, different positioning/interactions) due to prior experience in the tank. This is a persistent change resulting from environmental exposure (learning).
        2.  **Temporal Trends (Within-Trial Adaptation):** The paper notes changes over the 2-hour experiment (e.g., group expands, interactions weaken, Figs S2, S6, S8, S12, S15, S17). This suggests adaptation to the ongoing conditions within the trial, potentially due to decreasing novelty/stress or accumulating familiarity even for the non-habituated group over time, consistent with previous work [6].

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content:
        1.  **Habituation:** The mechanism is non-associative learning through repeated exposure to the experimental tank environment, leading to decreased perception of risk/novelty. The underlying neural/physiological mechanism is not investigated.
        2.  **Within-Trial Adaptation:** The mechanisms are not explicitly determined but likely involve factors such as decreasing stress/fear over time, increasing familiarity with the tank environment even for non-habituated fish, or potentially physiological changes (e.g., fatigue, changing motivation). The paper notes consistency with previous work [6] suggesting ongoing adaptation.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors described are:
        1.  **Collective Motion/Shoaling:** Fish form and maintain a cohesive, polarized group.
        2.  **Spatial Positioning/Assortment:** Individuals occupy specific locations within the shoal based on their habituation state (perceived risk), consistent with the Selfish Herd Hypothesis (non-habituated central, habituated peripheral/frontal).
        3.  **Altered Local Interactions:** Differences in nearest-neighbor interactions (attraction, repulsion, alignment) and leader-follower dynamics depending on the habituation state of the interacting pair.
        4.  **Burst-and-Coast Locomotion:** Individual fish exhibit characteristic oscillations in speed.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors are validated through:
         1.  **Quantitative Analysis:** Trajectory data is processed to calculate specific metrics (Rg, ϕ, relative positions, NN distances/velocities, border time, burst-coast parameters, correlations). Statistical distributions (PDFs) and averages are computed and compared between subgroups and over time (Figs 1-6, S1-S17).
         2.  **Force Map Method:** Used to infer average local interaction rules from observed accelerations and relative positions/velocities (Figs 1e-g, 2d-f, S13).
         3.  **Machine Learning:** An XGBoost classifier is trained on instantaneous or temporally aggregated features to predict subgroup identity, validating that behavioral differences are statistically significant and quantifiable (Table I, Fig S18).
         4.  **Replication:** The experiment was replicated three times (Series A, B, C) and consistency checked (Supplementary Figures).
         *   **Limitations:** Force maps show *average* interactions, potentially masking individual variation or state-dependent rules. ML classification provides statistical separability but doesn't fully elucidate causal mechanisms. Validation relies on statistical patterns in aggregate data.

---

#Key: [grodstein_closing_2023]

# Closing the loop on morphogenesis: a mathematical model of morphogenesis by closed-loop reaction-diffusion

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a theoretical/computational model describing morphogenesis in a 1D field of cells using a Reaction-Diffusion (RD) mechanism coupled with a Gene Regulatory Network (GRN) based feedback loop. The purpose is to achieve robust formation of a specific number (N) of RD pattern peaks (representing morphogenetic features like digits), independent of the field length (L). Components include: cells arranged in a 1D field, RD molecules (Activator A, Inhibitor I), GRN components within each cell (Pre* proteins, S* signaling molecules, 'done' state), and a top-level controller (conceptually). The system works by: 1) Allowing an RD pattern to form based on a characteristic length λ_RD. 2) Launching a "computation wave" (propagating S* signals) initiated at one end. 3) The GRN within each cell interacts with the local RD pattern ([A]) and the incoming wave signal to count the number of peaks ('A' concentration rising edges detected via Schmidt triggers) as the wave propagates. 4) The final count reaches the top-level controller at the other end. 5) The controller compares the count to the target N and adjusts λ_RD (by modifying diffusion or degradation rates implicitly) via negative feedback. 6) The process iterates (new RD pattern forms, new wave counts) until the target N peaks are achieved. The system also allows local modification of λ_RD based on the cell's position within the pattern (read from S* signals) to scale individual segments.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** λ_RD units are inferred from the definition λ_RD = sqrt(D_A / k_D,A); the paper gives D_A in m^2/s and k_D,A in s^-1, but reports λ_RD values without units, likely representing D_A directly as λ_RD is adjusted by changing D_A. Concentration units are relative. Field length is given in cells, which is proportional to physical length.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The energy source is implicitly the chemical potential driving the biochemical reactions (production and degradation of A, I, GRN components) and the diffusion processes. This is standard for RD and biological cell models but not explicitly discussed in terms of thermodynamics.

### **2.2 Energy Transduction**

    *   Content: Energy is transduced through:
        1.  Chemical reactions: Converting chemical potential into different molecular species (A, I, Pre*, S*) and heat.
        2.  Diffusion: Movement of molecules down concentration gradients, driven by thermal energy/chemical potential, resulting in redistribution of species and eventual homogenization if reactions cease.
        3.  Computation (GRN Logic): Energy expenditure associated with protein synthesis/degradation and conformational changes during GRN signal processing (implementing Schmidt triggers, logic gates).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss energy efficiency. Quantifying the thermodynamic efficiency of the simulated morphogenetic process or the GRN computation is outside the scope of the presented model.

### **2.4 Energy Dissipation**

    *   Content: Dissipation occurs primarily through the degradation of molecules (A, I, Pre*, S*), represented by the degradation constants (e.g., k_D,A, k_D,I) in the RD and GRN equations. The chemical reactions themselves are likely irreversible and dissipate heat (not quantified). Diffusion also leads to entropy increase. The rate of dissipation depends on the concentrations and the degradation constants. Qualitative assessment: Medium (continuous production/degradation cycles inherent to RD and GRN dynamics).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes
        1.  **GRN State Memory (Short-term):** Within each cell during wave propagation, the 'done' signal and the self-loop on the Pre* signals (e.g., `Pre0H = Pre0H or (...)`) latch the cell's decision once the wavefront passes, preventing it from changing state due to signals arriving later via diffusion loops (Section 2.2, Eq 1). This state persists until the `new_wave` signal resets it.
        2.  **Pattern State Memory (Longer-term):** The established RD pattern itself (concentrations of A and I across the field) represents a persistent state influenced by the history of λ_RD values. This pattern is the input for the next computation wave.
        3. **Controller State Memory (Implicit):** The top-level controller implicitly "remembers" the target N and the current value of λ_RD between iterations.

**(Conditional: M3.1 is "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable
*    Units: s (seconds) or Simulation time steps
    *   **GRN State Memory:** Retained for the duration of one computation wave propagation across the field, until reset by the `new_wave` signal (~order of L / wave_speed). Qualitative Descriptor: "Short-term" (relative to pattern formation).
    *   **RD Pattern Memory:** Retained for as long as the simulation runs under a constant λ_RD, or until λ_RD changes significantly, causing pattern reorganization (~timescale for RD pattern stabilization). Qualitative Descriptor: "Medium-term" (relative to one wave).
    *   **Controller State Memory:** Retained indefinitely between iterations as long as the control loop is active. Qualitative Descriptor: "Long-term" (relative to simulation steps).
    The paper doesn't provide specific numerical values for these timescales.

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Variable
*   Units: dimensionless states or bits
    *   **GRN State Memory:** Per cell, capacity is low, essentially storing which Pre* signal is active (encoding the count state passed through) and the 'done' state (1 bit). Aggregate capacity across L cells could be ~L bits * log2(MaxPeaks).
    *   **RD Pattern Memory:** Analog state potentially encodes N peaks (target), but also intermediate unstable states. Capacity could be related to the number of stable patterns possible for a given L and λ_RD range.
    *   **Controller State Memory:** Stores target N (integer) and current λ_RD (float). Low capacity.

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Peak Count Accuracy | % Success Rate in achieving target N | High (implied) | % | `RDPatternMemory.readout_accuracy` (functional) | Tables 1-5 | Implicit | Success in simulations implies high accuracy under tested conditions, but % not stated. |
    | GRN State Stability | Resistance to noise/diffusion loops | High (designed) | Qualitative | `GRNCellStateMemory.robustness` | Section 2.2 | Explicit | Mechanism designed for robustness against diffusion loops. Noise robustness discussed qualitatively. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content:
        1.  **RD Interactions:** Governed by the coupled partial differential equations (Eq 2). These describe:
            *   Local reaction kinetics: `G(A,I) = 1 / (1 + (I/A)^h)` (mutual activation/inhibition).
            *   Local degradation: `-k_D,A * A` and `-k_D,I * I`.
            *   Local diffusion: `D_A * ∂²A/∂x²` and `D_I * ∂²I/∂x²`.
        2.  **GRN Interactions (within a cell):** Governed by the Hill function logic derived from Boolean expressions (Section 2.2, Eq 1). These describe how intracellular concentrations of Pre* signals change based on:
            *   Local [A] concentration (via `very_high`, `very_low` thresholds based on 0.3 and 0.1).
            *   Incoming S* signals from neighboring cells (diffusion via gap junctions modeled in BITSEY).
            *   Internal 'done' state.
        3.  **Inter-Cell Communication:** Governed by diffusion of S* signals through gap junctions between adjacent cells (modeled in BITSEY, Section 2.5). The rate depends on gap junction density/permeability, implicitly linked to D_A adjusted by the controller.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Inter-Cell | Gap Junction Permeability (implicitly D_A) | D_A | 3.0e-6 to 1.5e-6 (example range inferred from Tables 1-5 for λ_RD adjustment) | m^2/s | Tables 1-5 | Implicit | D_A is adjusted; range inferred from simulation results. |

### **4.3 Global Order:**

    *   Content: The emergent global order is a stable, periodic spatial pattern of high and low concentrations of the activator molecule A (and inhibitor I) along the 1D field of cells. Specifically, the system aims for a pattern with exactly N distinct peaks of molecule A, where N is the target set by the controller. The pattern typically takes the form LHLH... or HLHL... across the field.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Cell_Comm | S* Diffusion Rate | Linked to D_A | ~1e-6 | m²/s | Implicit | Assumed S* diffusion linked to adjustable D_A | Sec 2.3, 2.5 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| PeakCount | Number of Activator (A) peaks in steady state | N_observed | Integer (e.g., 2-6 in simulations) | dimensionless | Explicit | Counted by computation wave / observed in simulation plots | Wave counting / Simulation Observation | Tables 1-5, Fig 1 |
| PeakSpacing | Characteristic length of one pattern repetition | λ_RD | ~sqrt(D_A/k_D,A) | m | Mixed | Related to parameters D_A, k_D,A; visually observable | Simulation Observation | Fig 1, Eq 2 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | RD Pattern Formation | Local RD rules lead to N-peak global pattern | Medium-High (See 4.4) | 7 | Peak Count Accuracy, Stability | Mixed | Local rules (Eq 2) given; global pattern emergence shown; path predictability moderate | Sec 2.5, 3.1, Fig 8 |
    | Computation Wave | Local GRN rules + RD pattern lead to global peak count readout | High (designed) | 8 | Peak Count Accuracy | Mixed | Local rules (Eq 1) given; global readout shown highly reliable in simulations if params tuned | Sec 2.2, 3.1 |
    | Feedback Control | Global peak count leads to adjustment of local parameter (D_A -> λ_RD) | High (deterministic) | 8 | Convergence to target N | Explicit | Controller logic explicitly ties global measure to local parameter change | Sec 2.3, Fig 7 |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 7. The system demonstrates a relatively faithful mapping from local rules (RD, GRN) to global structure (N-peak pattern) and function (counting, feedback). The local rules seem sufficient to explain the emergent global behavior simulated. The predictability issues (multiple stable states) slightly reduce the score, but the feedback mechanism generally ensures convergence. (Rubric: 0=No connection, 5=Qualitative connection, 7=Quantitative connection with moderate predictability, 10=Perfectly predictable mapping).
    *   **Metrics:** Peak Count Accuracy (functional output of wave), Convergence Rate/Success of feedback loop, Pattern Stability Analysis (implicitly via simulation success).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Hybrid (Analog + Digital-like)

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operations performed by the material (cells+molecules) include:
        1.  **Thresholding:** Detecting if local [A] crosses specific levels (0.1, 0.3) using Schmidt triggers implemented via Hill functions (Eq 1).
        2.  **Logic Operations:** Implementing AND/OR/NOT-like combinations using Hill functions to determine the next Pre* state based on current Pre*, incoming S*, local [A] state, and 'done' state (Eq 1).
        3.  **State Latching (Memory):** Maintaining the 'done' state and Pre* state once set during a wave cycle (Eq 1).
        4.  **Signal Propagation/Relaying:** Passing the computed S* signal to the next cell via diffusion.
        Collectively, these primitives implement the function of **Peak Counting via Wave Propagation**.
    *   **Sub-Type (if applicable):** Thresholding (Schmidt Trigger), Logic Gate Equivalents (via Hill functions), State Latch, Signal Relay.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | RD Reaction/Degradation | ~ 1/k_D,A, 1/k_D,I | s | Eq 2 | Implicit | Typical timescale related to rate constants, not calculated. |
        | RD Diffusion (across λ_RD) | ~ λ_RD²/D_A | s | Eq 2 | Implicit | Diffusion timescale, depends on chosen params, not calculated. |
    *   **Note:** The paper operates in simulation time steps, and while parameters have physical units, the actual simulated time for these processes isn't reported.

### **6.2 Active Inference:**

    *   Content: Partial
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:**
        *   Prediction Error Metric: Define error = |N_observed - N_target|. Measure how this error decreases over iterations (`BehaviorNode.errorReductionRate`).
        *   Model Complexity: Quantify the complexity of the implicit model relating λ_RD to N. (Could involve analyzing the stability landscape, Fig 8). (`ControllerNode.modelComplexity`).
        *   Action Efficiency: Measure the number of iterations or total change in λ_RD required to reach the target N. (`FeedbackLoopEdge.convergenceSpeed`).

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: If M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism is a **closed-loop negative feedback control system**.
        1.  **Measurement:** The computation wave counts the current number of RD peaks (N_observed).
        2.  **Comparison:** The controller compares N_observed to the target N.
        3.  **Adjustment:**
            *   If N_observed > N, increase λ_RD (e.g., by increasing D_A or decreasing k_D,A). The paper adjusts λ_RD by 10% per step in simulations (Table 1).
            *   If N_observed < N, decrease λ_RD (e.g., by decreasing D_A or increasing k_D,A). The paper decreases λ_RD by 10% per step (Table 2). Re-seeding the pattern may also occur when decreasing λ_RD.
        This iterative adjustment modifies the system's parameter (λ_RD) to steer the emergent pattern towards the desired state (N peaks). It functionally resembles a simple form of **reinforcement learning** or **gradient descent** on an implicit error function |N_observed - N|, where the system learns the appropriate λ_RD value.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content:
        1.  **Robust N-Peak Pattern Formation:** The primary behavior is the reliable formation of a stable RD pattern containing exactly N peaks, where N is a predefined target, largely independent of the total field length L.
        2.  **Pattern Segment Scaling:** The ability to locally adjust λ_RD based on a cell's computed ordinal position (derived from S* signals) allows for targeted stretching or shrinking of specific segments (inter-peak regions) of the established N-peak pattern (Figure 3).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors (robust N-peak pattern formation, segment scaling) are validated through computational simulations using the BITSEY simulator.
        *   **Operational Definitions:** The target behavior (N peaks) is clearly defined and measured by the computation wave mechanism. Segment scaling is defined by local λ_RD modification based on S* signals.
        *   **Control Experiments:** Implicit control is the comparison to basic RD systems which lack robustness to L. Different initial conditions (L, N) are tested (Tables 1-5).
        *   **Quantitative Analysis:** Simulation results are presented quantitatively (Tables 1-5 showing convergence to target N over iterations). Figure 3 visualizes the effect of segment scaling.
        *   **Robustness/Reliability:** Demonstrated across different L and target N values. Limits discussed (Section 2.4).
        *   **Reproducibility:** Code is stated to be available (Section 2.5), allowing for computational reproducibility.
        *   **Limitations:** Validation is purely computational; no experimental validation. Robustness to intrinsic biological noise or other perturbations not tested. Parameter space exploration seems limited.

---

#Key: [wills_reflexivity_2019]

# Reflexivity, coding and quantum biology

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper theoretically analyzes biological systems, focusing on the genetic coding apparatus (DNA, mRNA, tRNA, aminoacyl-tRNA synthetases (aaRS), ribosomes) as an information processing system. It investigates how biological information (especially genetic) acquires semantic content and meaning within the cellular molecular context. The core concept explored is "reflexivity," where the system's components (e.g., aaRS enzymes) are necessary for the interpretation and execution of the very information (genetic code) that specifies their own creation. The purpose is to understand the fundamental principles distinguishing biological computation and information processing (particularly coding) from purely physical/chemical processes, exploring its origins and relationship to self-organization, autocatalysis, and potentially quantum phenomena.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Codon Length | 3 | nucleotides | Section 2.7 | Explicit | High | Standard biological knowledge assumed and mentioned. |
        | Information bits per amino acid (approx) | 4-6 | bits | Section 2.7 | Explicit | Medium | Explicitly stated, value depends on definition, based on underlying analysis likely from cited work (Shore et al. 2019). |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Chemical energy, primarily from the hydrolysis of ATP (Adenosine triphosphate), used to drive processes like aminoacylation by aaRS enzymes and potentially proofreading.

### **2.2 Energy Transduction**

    *   Content: Chemical energy (ATP hydrolysis) is transduced into work required for specific molecular recognition and bond formation, such as the attachment of the correct amino acid to its cognate tRNA by aaRS enzymes (aminoacylation). Energy is also used for conformational changes in molecular machines (e.g., ribosomes, aaRS proofreading domains) and potentially for error correction mechanisms. The paper also discusses Maxwell's Demons using information to perform work (Sec 3.2).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide quantitative data or a qualitative assessment of the energy efficiency of the described processes (e.g., genetic coding, aaRS function). It discusses the *thermodynamic cost* of computation and information erasure (Sec 3.2) but doesn't quantify overall efficiency.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation occurs primarily as heat due to irreversible processes involved in molecular interactions, conformational changes, and chemical reactions. The paper explicitly mentions the thermodynamic cost (and thus implied dissipation) associated with information erasure required to satisfy the Second Law of Thermodynamics (Sec 3.2). Processes like proofreading (Footnote 5) involve extra energy expenditure (ATP hydrolysis) followed by dissipation.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Very Long-term (Generational/Evolutionary)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Very High (e.g., 10^9 - 10^10 bits for human genome)
*   Units: bits (or base pairs)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: High (but non-perfect)

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Very Low (but non-zero due to mutation/damage)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The local rules are the physical and chemical interactions between molecules:
        1.  **Catalysis:** Specific molecules (enzymes like aaRS, potentially ribozymes earlier) accelerating specific chemical reactions (e.g., aminoacylation, peptide bond formation, polymer replication). Specificity arises from molecular recognition (shape/charge complementarity). (Sec 2.2, 2.3, 2.7)
        2.  **Binding:** Non-covalent interactions (hydrogen bonds, van der Waals, hydrophobic effect) leading to specific binding events (e.g., aaRS binding amino acid and tRNA, codon-anticodon pairing). (Sec 2.6, 2.7)
        3.  **Polymer Synthesis/Degradation:** Chemical reactions forming or breaking covalent bonds in polymers (DNA, RNA, proteins) according to template information or catalytic action. (Sec 2.1, 2.2)
        4.  **Diffusion/Transport:** Movement of molecules within the cellular environment. (Implied, discussed wrt reaction-diffusion and compartments, Sec 2.4).
        5.  **Folding:** Polypeptide chains spontaneously folding into specific 3D structures based on amino acid sequence and physico-chemical interactions. (Sec 2.3, 2.7)
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The emergent global order includes:
        1.  **The Genetic Code:** A stable, near-optimal mapping between codons and amino acids, executed by the aaRS/tRNA system. (Abstract, Sec 2.3, 2.6, 2.8)
        2.  **Reflexively Autocatalytic Sets (RAF):** A network of molecules and reactions where components collectively catalyze their own production from basic food molecules. (Sec 2, 2.2)
        3.  **Functional Cellular Machinery:** Integrated systems like the translation apparatus (ribosomes, aaRS, tRNA) capable of complex, coordinated tasks. (Sec 1, 2.3, 3.1)
        4.  **Living Organisms:** Complex, self-maintaining, self-reproducing systems exhibiting purposeful behavior. (Implicit, overall context)
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| G4 | System Stability | Persistence Time | Long / Evolutionary | time | Explicit | Systems persist & evolve over long times (Sec 1, 4) | Observation / Theory | Sec 1, 4 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid (Primarily analog molecular recognition and dynamics, with discrete aspects like sequence units and potentially threshold effects). Some subsystems modeled as Digital (Boolean networks, Sec 1).

### **5.3 Computational Primitive:**

    *   Content: The most basic computational primitive central to the paper is **Information Interpretation/Mapping**: Specifically, the interpretation of a nucleotide codon (information token) to select and incorporate a specific amino acid during protein synthesis. This involves molecular recognition (aaRS selecting amino acid and tRNA, codon-anticodon pairing) which acts as a physical instantiation of the abstract code mapping. Other primitives mentioned include thresholding/switching (gene expression on/off, Sec 1) and potentially pattern matching (aaRS recognition).
    *   **Sub-Type (if applicable):** Mapping (Genetic Code) / Molecular Recognition

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Molecular Events (e.g., binding, catalysis) | Microseconds to milliseconds (Qualitative) | Time | Sec 4 | Explicit | Mentioned as range, specific values not given. |
        | Evolutionary Processes (Code Evolution) | Millions to Giga-years | Time | Sec 4 | Explicit | Explicitly mentioned Giga-years for tree of life timeframe. |
    *   **Note:** The paper explicitly contrasts fast molecular events with long evolutionary timescales (Sec 4), giving a qualitative range.

### **6.2 Active Inference:**

    *   Content: Unclear
        (1) *Internal Models:* The genetic code itself is presented as a "representation" or "map" of amino acid properties (Sec 2, 2.7, 2.8), acting like an internal model. Organisms need "internal representation of the world" (Sec 1, citing Davies).
        (2) *Prediction/Anticipation:* Not explicitly discussed in terms of predicting future states in the Active Inference sense.
        (3) *Action to Minimize Surprise:* The concept of optimization (e.g., code optimizing robustness, Sec 2.6) could be loosely related to minimizing detrimental outcomes (surprise), but the mechanism isn't framed as prediction error minimization. The discussion of purpose (Abstract) hints at goal-directedness.
        Overall, while suggestive elements exist (internal representation, optimization), the specific framework of Active Inference (prediction error minimization driving action) is not applied or validated.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The primary mechanism discussed is **Evolutionary Adaptation via Natural Selection coupled with Self-Organization**:
        1.  **Variation:** Random mutations occur in genes (e.g., aaRS genes). (Sec 2.6, 3.3)
        2.  **Selection:** Variants with improved properties (e.g., higher specificity, better integration leading to fitter organisms) are favored. (Sec 2.4, 2.6, 2.8)
        3.  **Feedback Loops:** Self-amplifying feedback plays a crucial role. Improved aaRS specificity leads to more accurate protein synthesis, including better aaRS synthesis, accelerating further optimization ("learn how to learn," Sec 2.8). Autocatalysis drives system growth and stability (Sec 2.2). Reflexivity ensures components needed for the code are produced by the code (Sec 2.3).
        4.  **Self-Organization:** Underlying physical/chemical propensities and interactions guide the evolutionary process towards certain types of stable, functional states (e.g., emergence of coding, RAF sets). (Sec 2.3, 2.8, 4)
        This process leads to ambiguity reduction and optimization of the code map (Sec 2.8, 2.9). Epigenetic mechanisms are also mentioned as transferring dynamic states across generations (Footnote 1).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior described is **Meaningful Information Processing for Self-Construction and Maintenance**. This encompasses:
        1.  **Genetic Code Execution:** Accurately translating nucleic acid sequences into functional protein sequences. (Sec 1, 2.3)
        2.  **Autocatalysis/Self-Replication:** The system collectively synthesizes its own components, including the machinery needed for information processing. (Sec 2.2, Abstract)
        3.  **Adaptive Evolution:** The system's ability to change and optimize its structure/function over time in response to selection (emergence and refinement of the code). (Sec 2.8, 2.9)
        4.  **Semantic Closure:** The information (genome) carries meaning defined by the system itself (cellular context, aaRS) and is necessary for the system's own operation (reflexivity). (Abstract, Sec 1, 2.3)

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper validates claims primarily through:
        1.  **Theoretical Arguments & Consistency:** Demonstrating logical consistency with known physics, chemistry, information theory, and established molecular biology (e.g., structure-function relationships, Sec 2.1; thermodynamics, Sec 3.2).
        2.  **Reference to Existing Models/Simulations:** Citing prior work on RAF sets (Hordijk et al.), coding emergence simulations (Füchslin & McCaskill, Markowitz et al.), and theoretical models (Bedian, Wills). (Sec 2.2, 2.3, 2.4, 2.9).
        3.  **Reference to Experimental Evidence:** Citing experimental work supporting specific claims, such as the properties of aaRS, code optimality studies, artificial code evolution, and quantum effects in biology (e.g., Carter & Wills, Freeland et al., McFadden & Al-Khalili, Engel et al.). (Sec 2.6, 2.7, 2.8, 3.3, 4).
        Limitations include the difficulty of directly observing evolutionary origins and the speculative nature of some arguments (e.g., precise path of code evolution, role of quantum computation).

---

#Key: [urban_key-and-lock_2018]

# Key-and-lock commodity self-healing copolymers

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of commodity copolymers, specifically poly(methyl methacrylate)/n-butyl acrylate [p(MMA/nBA)] and related derivatives, synthesized via various methods (ATRP, statistical free radical, colloidal). The key functionality demonstrated is autonomous self-healing upon mechanical damage within a narrow compositional range (45/55 to 50/50 MMA/nBA molar ratio). The components are MMA and nBA monomers copolymerized into chains with specific topologies (preferentially alternating with a random component). The purpose is to demonstrate and understand a self-healing mechanism based on the reversible disruption and reformation of interchain van der Waals (vdW) forces forming "key-and-lock" junctions, eliminating the need for external intervention, supramolecular/covalent rebonding, or encapsulated reactants.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters selected highlight the specific conditions for self-healing and key material properties related to the mechanism. Data reliability is high as these are directly reported experimental or simulation results for the specific system.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Mechanical energy input through physical damage (e.g., cutting, applying stress leading to strain/fracture).

### **2.2 Energy Transduction**

    *   Content: Input mechanical energy causes chain separation/displacement, disrupting the relatively weak interchain vdW forces ("key-and-lock" junctions). This stored potential energy (in deformed/separated chains) is then released as the chains relax and spontaneously reform the favorable vdW interactions upon removal of external force/closure of damage, leading to conformational changes (e.g., from globular to extended helix-like and back) and restoration of material integrity. Energy is transduced from macroscopic mechanical deformation to microscopic potential energy changes associated with vdW interactions and chain conformations.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide a thermodynamic efficiency calculation for the self-healing process (i.e., energy dissipated vs. energy restored mechanically). It quantifies the *outcome* in terms of mechanical property recovery (90-100% tensile strain recovery), which is very high, suggesting an effective process but not thermodynamic efficiency. A qualitative assessment would be "High" effectiveness in restoring mechanical properties, but thermodynamic efficiency is unknown.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation likely occurs as heat during the initial mechanical damage (viscoelastic processes, chain friction, fracture) and potentially during the rearrangement/relaxation process of self-healing. The formation of free radicals (though deemed unrelated to healing) also represents an energy pathway. However, the paper does not quantify these dissipation mechanisms. Qualitative assessment: Assumed to be present (as in any real mechanical process), but magnitude relative to stored/recovered energy is unknown.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceed to M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: ~14 (ATRP), ~86 (colloidal)
*    Units: hours

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: 90-100% (Tensile Strain Recovery)
*   Units: %

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Recovery_Strain | Tensile Strain Recovery Percentage | 90-100 (±5) | % | Attribute of `MemoryNode` | Text p.1, Fig 1B | Explicit | Measures functional readout fidelity |
    | Cycles_Robustness | Number of effective damage-heal cycles | Not specified (Implied >= few) | Cycles | Attribute of `MemoryNode` | fig.S12 | Implicit | Implied by repeated cuts having no effect |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rules are van der Waals (vdW) forces between adjacent polymer chain segments. Specifically, favorable vdW interactions occur between MMA and nBA units in adjacent chains, particularly for alternating/random sequences (e.g., BMBMB, where B=nBA, M=MMA). These interactions are sequence-dependent, with alternating pentads like BMBMB/BMBMB showing higher cohesive energies (CEp) than blocky sequences (Fig 3). These interactions drive chains towards conformations (extended helix-like) and packing arrangements (interdigitated "key-and-lock") that maximize favorable vdW contacts, leading to higher Cohesive Energy Densities (CED).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | vdW_Pentad | vdW interaction between specific pentad sequences | Cohesive Energy (CEp) | 258.2 - 313.6 | kJ/mol | Fig 3 | Explicit | MD calculated interaction energy for specific model sequences |
    | vdW_Bulk | Overall vdW contribution to bulk cohesion | vdW density (vdWeq) | 1.30 - 1.96 (x 10^5) | kJ/m³ | Table 1 | Explicit | MD calculated vdW contribution to total CED |

### **4.3 Global Order:**

    *   Content: The emergent global order within the self-healing compositional range (45/55 to 50/50 MMA/nBA) includes:
        1.  Extended helix-like chain conformations (average req ~34 Å).
        2.  Higher interchain packing and interwinding (Fig 2B).
        3.  Increased cohesive energy density (CEDeq up to ~2.03 x 10^5 kJ/m³).
        4.  Increased junction density (nj up to 123.6 mol/m³).
        5.  Macroscopically manifested as the ability to self-heal mechanical damage.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| vdW_Seq | vdW forces are stronger between alternating MMA/nBA sequences | CEp | 258.2 - 313.6 | kJ/mol | Explicit | Pentad interaction energies calculated. | Fig 3 |
| vdW_Conf | Specific compositions favor extended conformations to maximize vdW interactions | req | ~34 (healing range) vs ~25-29 (non-healing) | Å | Explicit | MD simulation results. | Table 1, Fig 2 |
| vdW_Packing | Favorable vdW forces lead to denser packing | Density / CEDeq | 1.99-2.03e5 (healing) vs lower (non-healing) | kJ/m³ | Explicit | MD simulation results. | Table 1, Fig 2 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Conformation | Average End-to-End Distance | req | ~34 (healing) | Å | Explicit | Measures chain extension | MD Simulation | Table 1, Fig 2C |
| Packing | Cohesive Energy Density | CEDeq | 1.99-2.03e5 (healing) | kJ/m³ | Explicit | Overall interaction strength | MD Simulation | Table 1, Fig 2A |
| Entanglement/Junctions | Junction Density | nj | 123.6 (healing) | mol/m³ | Explicit | Derived from DMA | DMA (Viscoelastic Length Transitions) | Text p.1, Table S6 |
| Function | Self-Healing Efficiency | Strain Recovery | 90-100 | % | Explicit | Macroscopic outcome | Stress-Strain Analysis | Fig 1B |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 0

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip to Module 6.)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Self-Healing Time (ATRP Copolymers) | ~14 | hours | Text p.1, Fig 1B | Explicit | Time for significant mechanical recovery. |
        | Self-Healing Time (Colloidal Copolymers) | ~86 | hours | Text p.1, Fig S1 | Explicit | Time for significant mechanical recovery. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism is the thermodynamically driven reformation of favorable interchain van der Waals (vdW) interactions ("key-and-lock junctions") between specific monomer sequences (alternating/random MMA/nBA). Mechanical damage provides the perturbation, increasing the system's energy. Upon removal of external constraints (or crack closure), the polymer chains have sufficient mobility (especially near Tg or due to potentially lower surface Tg) to rearrange. They spontaneously adopt conformations (extended helix-like) and packing arrangements that maximize the favorable vdW contacts, thus lowering the system's free energy and restoring the original structure and mechanical properties. It's driven by energy minimization towards the preferred equilibrium state.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is autonomous self-healing of mechanical damage under ambient conditions. Specifically, the material can recover a significant percentage (90-100%) of its original tensile strain after being cut or damaged, without external stimuli (like heat, light) or intervention (like added healing agents). This behavior is specific to a narrow compositional range of p(MMA/nBA) copolymers. Repetitive healing is also observed.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claim of self-healing (emergent behavior from molecular interactions) is validated through:
        1.  **Direct Observation:** Optical images showing crack closure over time (Fig 1A, movie S1).
        2.  **Macroscopic Functional Tests:** Stress-strain measurements before damage and after healing, quantifying recovery of tensile strain (90-100%) and stress (Fig 1B, fig S10, S11). Repetitive cut/heal tests (fig S12).
        3.  **Spectroscopic Evidence:** IR and NMR show reversible spectral changes associated with conformational changes linked to damage/healing only in healing compositions (figs S2-S6, text p.1). ESR shows free radicals are formed but not correlated with healing (fig S7, text p.1).
        4.  **Mechanical Analysis:** DMA measurements show increased junction density (nj) in the self-healing range (fig S8, table S6, text p.1).
        5.  **Computational Modeling:** MD simulations support the proposed mechanism, showing higher CED, specific conformations (helix-like req ~34Å), and favorable pentad interactions (Fig 2, Fig 3, Table 1) in the self-healing compositional range.
        *   **Limitations:** Validation primarily under ambient lab conditions (25°C, 50% RH). Long-term robustness and performance under diverse conditions are not fully explored in the excerpt.

---

#Key: [yao_nematic_2022]

# Nematic Colloidal Micro-Robots as Physically Intelligent Systems

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system comprises a four-armed ferromagnetic micro-robot fabricated from SU-8 photoresist, coated with Nickel (Ni), and surface-treated with DMOAP. This micro-robot is immersed in a nematic liquid crystal (NLC, 5CB) environment, typically confined within a planar cell with rubbed polyimide surfaces inducing uniform planar alignment. The micro-robot's shape, ferromagnetic nature, and hybrid surface anchoring conditions (homeotropic on Ni-coated top/sides, degenerate planar on SU-8 bottom) are designed to sculpt the NLC director field, creating topological defects and complex nemato-elastic energy landscapes. The purpose is to utilize this "physical intelligence" – the interactions embedded in the robot, NLC, and defects – for manipulating passive colloidal cargo (DMOAP-treated silica spheres with homeotropic anchoring). Actuation is achieved via external magnetic fields (uniform rotating fields or fields with gradients), enabling robot translation and rotation. Robot rotation induces far-from-equilibrium dynamics of the companion topological defect, which can be exploited for cargo capture, transport, release, juggling, and assembly of reconfigurable structures, including defect-propelled swimming of the robot itself.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Ericksen number (Er) | 0.06 - 18 | Dimensionless | Sec 2.3 | Explicit | Medium | Calculated (ωτ) |

    *   **Note:** Lists key parameters characterizing the system's implementation. Reliability is High for directly stated dimensions/values, Medium for derived dimensionless numbers like Er.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: External magnetic fields (rotating or gradient fields) provide the primary energy input to actuate the ferromagnetic micro-robot.
    *   Value: Few mT (field strength)
    *   Units: mT (magnetic field strength); Hz (frequency for rotation)

### **2.2 Energy Transduction**

    *   Content:
        1.  **Magnetic to Mechanical:** The external magnetic field exerts torque/force on the ferromagnetic micro-robot, converting magnetic energy into mechanical work (rotation and/or translation).
        2.  **Mechanical to Elastic (NLC):** The micro-robot's presence and motion distort the NLC director field, storing elastic energy in the NLC. Rotation specifically drives far-from-equilibrium defect dynamics, involving elastic energy storage and release.
        3.  **Elastic (NLC) to Mechanical (Colloid):** Gradients in the nemato-elastic energy landscape exert forces on passive colloids, converting stored elastic energy into mechanical work to move the colloids.
        4.  **Mechanical (Robot/Colloid) to Fluid Motion:** Movement of the robot and colloids displaces the NLC, transducing mechanical energy into kinetic energy of the fluid (hydrodynamic interactions/drag).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Low. The primary goal is controlled manipulation, not efficient energy conversion. Significant energy is dissipated through viscous drag in the NLC at the microscale (low Reynolds number regime). While interaction energies are large compared to thermal energy (~10^5 kT), the energy input required to move the robot against drag to achieve manipulation is substantial compared to the potential energy changes driving assembly. No quantitative efficiency metric is provided in the paper.

### **2.4 Energy Dissipation**

    *   Content: The primary dissipation mechanism is viscous drag as the micro-robot and colloids move through the NLC. This is explicitly mentioned when calculating interaction energies (U = ∫ F_drag ds). Energy is also dissipated during the relaxation of the NLC director field and topological defects when the driving magnetic field changes or ceases. Quantitative estimates of drag forces are implicit in the calculation of U (e.g., forces up to 100s pN are overcome/generated, Fig 3h-l discussion). Overall dissipation is High due to the low Reynolds number environment.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes
        1.  **Configurational Memory:** The position and orientation of the micro-robot, and the assembled positions of the colloids, represent a system state that persists after the specific manipulation step (stimulus) ends. This configuration dictates future possible interactions.
        2.  **Topological Memory:** The specific defect configuration associated with the micro-robot (e.g., dipolar state) persists and influences the surrounding energy landscape and interactions. This state can be history-dependent (e.g., irreversible transition from quadrupolar to dipolar, Sec 2.1). Defect dynamics during rotation (hopping, elongation) represent transient memory of the rotational stimulus.

**(Conditional: M3.1 is "Yes", proceed.)**

### **3.2 Memory Type:**

    *   Retention: Conditional, lasts as long as the configuration is maintained (robot position, colloid binding, defect state). Can be erased/rewritten by moving the robot, rotating it (inducing defect dynamics/release), or applying forces exceeding binding energy. Metastable defect states might offer longer retention but are susceptible to perturbations. Retention is generally short-to-medium term relative to experimental timescales unless actively maintained.
    *   Capacity: Limited. Defined by the number of docking sites on the robot, the number of colloids, and the possible stable/metastable defect configurations (dipolar, potentially others). Low number of distinct, controllable states.
    *   Read-out: Via microscopy (observing configuration/defects) or by observing subsequent interactions. Read-out is relatively accurate visually.

### **3.3 Memory Retention Time:**

*   Value: Variable / Conditional
*    Units: s (Qualitative: Short-to-Medium Term)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Low (e.g., ~4-5 docking sites, ~2 stable defect configs observed)
*   Units: Number of states/sites

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: High (Visual/Interaction based)

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", proceed.)**

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rule is the minimization of the nemato-elastic free energy of the system. Passive colloids move along the negative gradient of the potential energy landscape U created by the micro-robot's distortion of the NLC director field. This landscape U is influenced by the robot's shape, surface anchoring, defect structure, and the colloid's properties (anchoring, associated defect/distortion). Mathematically, the force on the colloid is F = -∇U. The paper approximates U using dipolar (f_dipole = 4πKp · n(∇ · n)) and quadrupolar (f_quadrupole = -8πK/3 * c * n·∇(∇·n)) contributions to the free energy density, where colloid motion follows the steepest descent on this landscape (Sec 2.2, Fig 3f,g). Hydrodynamic interactions also play a role, particularly near contact and during motion, but the primary driver for assembly site selection is the nemato-elastic interaction. Defect dynamics (hopping, elongation, merging) under rotation follow rules governed by minimizing elastic energy subject to viscous torques and boundary conditions (minimizing free energy in Q-tensor simulations, Eq 1-4, Exp. Sec).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | NematoElastic | Interaction Strength | Interaction Energy U | ≈ 10⁴ - 10⁵ | k<0xE2><0x82><0x99>T | Fig 3h-l | Explicit | Calculated from trajectory/drag. |
    | NematoElastic | Interaction Range | Max distance `d` | ≈ 2a - 6a (≈ 25 - 75 µm) | µm | Fig 3h-l (insets) | Explicit | Observed attraction distance. |
    | NematoElastic | Characteristic Force | Yield Force (for strong binding) | 10¹ - 10² | pN | Sec 2.2 (derived from U vs d slope) | Implicit | Derived from energy profiles. |
    | DefectDynamics | Control Parameter | Ericksen number Er | 0.06 - 18 | Dimensionless | Sec 2.3 | Explicit | Dimensionless ratio controlling dynamics. |
    | Simulation | Surface Anchoring | W (strength for planar) | 0.5L/Δx | L/Δx | Exp. Sec (Eq 2, parameters) | Explicit (relative value) | Anchoring strength parameter in simulation. |

### **4.3 Global Order:**

    *   Content: The emergent global order (relative to the micro-robot) includes specific, stable assembly configurations of colloids docked onto the micro-robot: Dipole-chaining, Antiparallel dipole, Dipole-on-hill, Dipole-in-well, Hybrid configurations (Fig 3a-e). More complex structures assembled using the robot near wavy walls (Fig 6b-d) also represent emergent order guided by both robot manipulation and wall interactions. The static defect structure (e.g., dipolar configuration, Fig 2) is also an example of emergent order.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| NematoElastic | Colloid follows neg. gradient of potential U | Force F = -∇U | Force: ~pN range | pN | Explicit | Colloid motion described as following energy gradient. Eq for U approx. given. | Sec 2.2, Fig 3f,g |
| DefectDynamics | Defect reconfiguration minimizes elastic free energy under viscous torque | Ericksen number Er | 0.06 - 18 | Dimensionless | Explicit | Er defines the ratio of viscous to elastic forces, governing dynamics. | Sec 2.3 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| AssemblyConfig | Specific docking configuration of colloid(s) on robot | Configuration Name | ["Dipole-chaining", "Antiparallel", "Dipole-on-hill", "Dipole-in-well", "Hybrid"] | Categorical | Explicit | The paper names and describes these configurations. | Visual Inspection (Microscopy) | Fig 3a-e |
| DefectStructure | Topology/Symmetry of companion defect | Defect Symmetry | ["Dipolar", "Quadrupolar"] | Categorical | Explicit | Paper identifies dipolar and metastable quadrupolar states. | Polarized Microscopy / Simulation (S=0.4 isosurface) | Fig 2, Sec 2.1 |
| AssembledStructure | Structure built using robot near wall | Structure Type | ["1D Lattice", "Chain", "Anisotropic"] | Categorical | Explicit | Paper shows examples of assembled structures. | Visual Inspection (Microscopy) | Fig 6b-d |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip rest of M5.)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Colloid assembly time | Variable (seconds to minutes) | s | Fig 3h-l, Fig 8ii | Mixed | Depends on initial distance, explicit examples given (e.g. Fig 8ii docking time), qualitative dependence stated. |
        | Robot rotation period (T) | 2 - 160 | s | Sec 2.6, Fig 7, Fig 8 | Explicit | Range of periods used in experiments. |
        | Defect relaxation time (τ) | ~ L²γ₁/K | s | Sec 2.3 | Mixed | Formula given, specific value depends on L, γ₁, K (typical values yield seconds-minutes). |
        | Defect hopping timescale (within rotation) | < T/4 | s | Fig 4a | Implicit | Hopping occurs within a quarter rotation. |
        | Robot swimming period | 160 | s | Fig 7a | Explicit | Period used for swimming demonstration. |
        | Cargo manipulation cycle (Fig 8) | ~250 | s | Fig 8 | Explicit | Total time for the demonstrated autonomous cycle. |

    *   **Note:** Covers timescales for colloid movement, robot actuation, defect dynamics, and overall processes.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip rest of M7.)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content:
        1.  **Directed Assembly:** Colloids are attracted to and dock at specific sites on the micro-robot (dipole-chaining, antiparallel, hill, well, hybrid) based on nemato-elastic interactions.
        2.  **Cargo Transport:** The micro-robot carries assembled colloids through the NLC via magnetic actuation.
        3.  **Cargo Release:** Assembled colloids are released from the micro-robot, potentially triggered by specific robot rotations inducing defect dynamics (Fig 5a) or by exceeding drag forces (Fig S5).
        4.  **Cargo Juggling/Rearrangement:** Multiple colloids on the robot can be rearranged or selectively released via complex defect dynamics during rotation (Fig 5b).
        5.  **Structure Formation:** Sequential delivery and release of colloids near patterned walls enables building of larger colloidal structures (1D lattice, chain, etc.) (Fig 6).
        6.  **Defect Dynamics:** Topological defects exhibit complex behaviors during robot rotation, including elongation, hopping between arms, sweeping across surfaces, merging, and separation (Fig 1c, Fig 4, Fig 5, Fig 7a).
        7.  **Defect-Propelled Swimming:** The robot translates unidirectionally due to asymmetric defect sweeping motion during rotation, enabling locomotion without field gradients (Fig 7).
        8.  **Trajectory Planning:** Robot swimming direction and speed can be controlled by tuning the rate and sense of rotation of the magnetic field (Fig 7b,c).

### **8.2 Behavior Robustness:**

        *   **Strengths:** Strong nemato-elastic interactions (~10⁴-10⁵ kT) make assembly robust against thermal fluctuations. Docking is specific to site and colloid polarity. Defect-propelled swimming appears reliable.
        *   **Weaknesses:** Cargo retention is limited by drag forces; colloids can be lost at higher transport speeds, especially for weaker binding modes (dipole-in-well) or different anchoring (planar colloids) (Sec 2.2, Fig S5, S7). Defect dynamics and subsequent manipulation outcomes (release/juggling) can be complex and potentially sensitive to small variations or noise, although consistent behaviors are demonstrated. Predictability of complex assembly (Fig 6) depends on precise robot control and wall interactions. Robustness to variations in fabrication or material properties is not explicitly tested.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Emergent behaviors are primarily validated through:
        1.  **Direct Observation:** Optical microscopy (bright field, cross-polarized) is used to visualize micro-robot motion, colloid trajectories, assembly configurations, and defect dynamics (Figs 2-8, Movies S1-S10).
        2.  **Trajectory Analysis:** Particle tracking is used to quantify colloid paths and speeds, allowing calculation of interaction energies (Fig 3f-l). Robot trajectories during swimming are tracked (Fig 7b,c, Fig 8).
        3.  **Comparison with Simulation:** Numerical simulations (Q-tensor method) are used to predict static defect structures (Fig 2c), colloid trajectories based on energy landscapes (Fig 3f(ii), g(ii)), and dynamic defect behavior during rotation (Fig 4b, Fig S8). Good qualitative agreement between experiments and simulations supports the proposed mechanisms.
        4.  **Control Experiments (Implicit):** Varying rotation rates (Er number) demonstrates different dynamic regimes (Sec 2.3). Comparing interactions for different colloid anchoring (homeotropic vs planar) highlights the role of defect type (Sec 2.2, Fig S6, S7). Comparing cargo retention at different speeds shows the limit of binding forces (Fig S5).
        *   **Limitations:** Reproducibility across multiple devices/batches isn't explicitly quantified. Sensitivity analysis to parameter variations (e.g., cell gap, temperature, NLC purity) is limited. Long-term stability/robustness isn't systematically studied.

---

#Key: [lee_what_2022]

# What Can Deep Neural Networks Teach Us About Embodied Bounded Rationality

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper presents a theoretical argument, not a specific material system or algorithm implementation. It explores the conceptual relationship between Deep Neural Networks (DNNs), the principle of Embodied Cognition, and Simon's concept of Bounded Rationality. The core argument is that DNNs, particularly their interactive nature during training and operation (e.g., AlphaGo, feedback systems) and their non-algorithmic (in the Turing-Church sense) decision-making processes, serve as empirical evidence for "Embodied Bounded Rationality". This view posits that cognitive processes (like human decision-making) are not just limited computations (Bounded Rationality) but are fundamentally interactive and embodied, potentially exceeding the capabilities of non-interactive Turing-Church computation while still being resource-limited. The system discussed is thus a conceptual framework linking AI (DNNs), cognitive science (Bounded Rationality), and philosophy of mind (Embodied Cognition, computation vs. interaction). It aims to reframe the understanding of rationality and intelligence by highlighting the limitations of purely computational models and the power of interaction. Components include concepts like: Turing-Church Computation, Algorithms, Interaction, Feedback, Embodied Cognition, Bounded Rationality, DNNs, Information Processing (contrasted with computation).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                  | Value               | Units   | Source (Fig/Table/Section)   | Implicit/Explicit   | Data Reliability (High/Medium/Low)   | Derivation Method (if Implicit)   |
        | :------------------------------ | :------------------ | :------ | :--------------------------- | :------------------ | :----------------------------------- | :-------------------------------- |

    *   **Note:** These are conceptual parameters defining the key elements of the author's argument, not physical parameters of a material system.

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip to Module 4.)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Other (Interactive Information Processing, potentially Non-Algorithmic)

### **5.3 Computational Primitive:**

    *   Content: The paper doesn't define a single "primitive" but contrasts the fundamental operations. For Turing-Church: Step-by-step execution of algorithmic rules on symbols (Section 2, 3). For the interactive systems discussed (DNNs, Embodied Cognition): Interaction/Feedback loop execution (sensing, acting, adjusting based on environmental response), potentially involving continuous dynamics and non-symbolic processing (Section 1, 5, 6). Examples mentioned include DNN classification (Section 1, 5), control actions in feedback systems (Section 6.1), causal reasoning via intervention (Section 6.4), adaptive self-modeling (Section 6.2).

### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description             | Value             | Units   | Source      | Implicit/Explicit   | Justification                      |
        | :-------------------------------- | :---------------- | :------ | :---------- | :------------------ | :--------------------------------- |
        | Weather Prediction Limit (Chaos)  | Few days          | days    | Section 3   | Explicit (via ref)  | Lorenz example cited           |
    *   **Note:** These are qualitative or example-based timescales discussed conceptually, not measured parameters of a specific implemented system.

### **6.2 Active Inference:**

    *   Content: Partial
        1.  *Prediction:* Implicit in feedback control systems (Section 6.1, 6.2) which compare current state to a desired state (an implicit prediction). Efference copies (Section 6.6) involve predicting sensory consequences of actions. Clark's work on predictive processing is cited (Section 6.2).
        2.  *Action Selection:* Central to the discussion. Decisions (rational or otherwise) are actions. Feedback control involves selecting actions (e.g., throttle adjustment) to reduce the discrepancy between current and desired states (Section 6.1). DNNs select classifications/moves (Section 1, 5). RCTs involve active intervention (action) to determine causation (Section 6.4). Act-to-sense involves action selection to improve perception (Section 6.5).
        3.  *Internal Models:* Implicit in controllers (desired state, system dynamics model - Section 6.1, 6.2), efference copies (model of sensorimotor loop - Section 6.6), causal reasoning (requires assumptions/models - Section 6.4), and potentially DNNs (the learned weights represent a model, albeit inexplicable - Section 1, 5). The paper mentions Bongard's robot learning a model of itself (Section 6.2) and Clark's emphasis on predictive models (Section 6.2).
        The paper emphasizes feedback, interaction, and adaptation based on environmental response, which are core components of active inference, but doesn't frame it within that specific theoretical framework or discuss the minimization of free energy/surprise explicitly.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: If M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The primary mechanism discussed in detail is **Feedback-driven adjustment**, specifically citing **Backpropagation** for DNNs (Section 1, 5). This involves iteratively adjusting internal parameters (weights) based on the error between the system's output and a target or based on the outcome of interactions (like winning/losing in AlphaGo). The paper links backpropagation historically to optimal control techniques (Kelley-Bryson method) which are also feedback systems designed to minimize error or optimize performance (Section 5). For embodied robots, the mechanism is described as **Self-modeling and calibration through interaction**, where the robot learns its own dynamics and adapts its movements (gait) based on sensory feedback (Section 6.2). For causal reasoning via RCTs, adaptation involves measuring user reactions to experimental changes and updating the system (e.g., user interface) accordingly (Section 6.4). For humans, mechanisms like learning through practice/interaction and potentially prospect theory influencing decisions are mentioned (Section 1, 5). The paper emphasizes that these adaptive processes, particularly in DNNs, are fundamentally interactive and may not be reducible to simple symbolic/algorithmic rules (Section 1, 5).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The paper discusses several high-level behaviors emerging from the described systems/concepts:
        *   **Complex Game Playing (Go):** DNNs (AlphaGo) achieving superhuman performance through learning via self-play (interaction), not explicit algorithmic reasoning about Go strategy (Section 1, 5).
        *   **Perception/Classification:** DNNs performing image classification, speech recognition, etc., described as more akin to intuition (System 1) than rational deduction (Section 1, 5).
        *   **Adaptive Control:** Cruise control maintaining speed despite environmental changes (hills); robots adapting gait after damage or modification (Section 6.1, 6.2).
        *   **Causal Reasoning:** Humans and potentially adaptive software determining cause-effect relationships through interaction/intervention (RCTs) rather than just observation (Section 6.4).
        *   **Enhanced Sensing (Act-to-Sense):** Cuttlefish discerning color despite colorblindness through active skin modulation; humans recognizing objects by active manipulation (Section 6.5, 6.6).
        *   **Solving Undecidable Problems (via Interaction):** A specific scheduling policy for Kahn-MacQueen networks solving a bounded memory problem shown to be undecidable for standard computation (Section 6.3).
        *   **Distinguishing Indistinguishable Systems (via Interaction):** Demonstrating that feedback/interaction can reveal differences between systems (Brock-Ackerman, Bisimulation examples) that passive observation cannot (Section 6.7, 6.8).
        *   **Human Decision-Making (Bounded/Embodied):** Making decisions under uncertainty using heuristics or intuition (System 1), potentially influenced by non-rational factors (fatigue, hunger), exhibiting post-hoc rationalization (Section 2, 5).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper validates claims primarily through:
         *   **Citing Empirical Results:** Referencing successful demonstrations like AlphaGo's performance (Silver et al., 2016), DNNs outperforming traditional methods (Section 1), robotics experiments (Bongard et al., 2006), psychology experiments (Danziger et al., 2011; Taleb, 2010 reporting experiments), and specific computer science results (Parks, 1995; Brock & Ackerman, 1981; Milner, 1980/1989).
         *   **Illustrative Examples:** Using analogies like cruise control (Section 6.1) and thought experiments/conceptual arguments (distinguishing computation types, interaction vs. observation).
         *   **Logical Argumentation:** Building a case by defining terms, contrasting concepts, and drawing inferences.
         There are no *new* experiments presented in this paper. The validation relies on the reader accepting the cited empirical findings and the logical coherence of the author's arguments linking these findings to the concepts of embodied bounded rationality. Limitations include the reliance on existing literature and the theoretical nature of the connections drawn.

---

#Key: [li_training_2024]

# Training all-mechanical neural networks for task learning through in situ backpropagation

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a Mechanical Neural Network (MNN), implemented physically using 3D-printed flexible material (Agilus30) forming a network of nodes connected by bonds (analogous to springs). It performs machine learning tasks (behavior learning, regression, classification) by mapping input forces applied to specific nodes to output displacements measured at other nodes. The MNN learns by adjusting the stiffness (spring constants, implemented as bond widths) of its constituent bonds using an *in situ* backpropagation algorithm derived from the adjoint variable method. The purpose is to demonstrate a physical hardware platform for machine learning that leverages mechanical principles and local learning rules for potentially faster and more energy-efficient computation compared to digital systems, and to create adaptable material systems.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Learning rates provided are examples for specific tasks mentioned. Other parameters like node count (n), bond count (m), spatial dimensionality (d) are mentioned theoretically but not always specified for each experiment. Reliability is High as these are directly stated design/method parameters.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is mechanical work done by external forces applied to the input nodes. In the experiments described, this is gravitational potential energy converted via hanging weights.
    *   Value: Variable (e.g., 0.005*9.8 N, 0.01*9.8 N)
    *   Units: N (force), J (work/energy)

### **2.2 Energy Transduction**

    *   Content: Gravitational potential energy of weights is converted into kinetic energy as weights are applied, then into mechanical work done on the MNN. This work is primarily stored as elastic potential energy in the deformed bonds (springs) of the network. Energy flows from the input nodes through the connected bonds to the rest of the network, resulting in node displacements. During training, the calculated gradients implicitly guide the modification of bond stiffness, which changes how energy is stored and distributed in subsequent operations.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify energy efficiency. It mentions energy efficiency as a motivation compared to digital and optical networks ("offering faster and more energy-efficient information processing", "hold promise for better energy efficiency") but doesn't provide measurements or calculations for the MNN itself during operation or training. Efficiency would involve comparing useful computational work (related to loss function reduction or task performance) to total energy input, considering dissipation. Qualitative Assessment: Likely low for a single computation compared to the potential energy input, but potentially high parallelism could offer system-level advantages. The training efficiency benefit mentioned relates to obtaining the gradient in two steps vs finite differences.

### **2.4 Energy Dissipation**

    *   Content: Dissipation mechanisms are not explicitly discussed or quantified. Potential mechanisms include:
        1.  Internal material damping within the flexible Agilus30 material during deformation.
        2.  Friction at the points where the MNN is suspended or where forces are applied (e.g., string friction).
        3.  Air resistance (likely negligible).
        4.  Energy loss during the physical process of changing bond widths (if implemented physically).
        The analysis assumes quasi-static equilibrium, implicitly neglecting dynamic dissipation effects like damping during the computation step. Qualitative Assessment: Likely Medium internal damping for a rubber-like material.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: m (number of bonds)
*   Units: tunable parameters (spring constants)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: High (Qualitative) / Low Error (Quantitative, e.g., <0.1 gradient error experimentally)

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Retrainability (Damage) | Ability to recover performance after bond pruning | ~80% accuracy recovered (Iris task) | % | `MemoryNode` attribute `robustness` | Fig 5e | Explicit | Demonstrates robustness/recoverability of the learned memory/function. |
    | Retrainability (Task Switch) | Ability to learn a new task starting from a previously trained state | High accuracy achieved (Class -> Reg -> Class) | % / MSE | `MemoryNode` attribute `rewritability` | Fig 5b, 5c | Explicit | Demonstrates the memory (parameters) can be overwritten/adapted for new tasks. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**
*(Proceeding as M4.1 is "Partial")*

### **4.2 Local Interaction Rules:**

    *   Content: The key local rule governs the update of each bond's spring constant k_i. The gradient of the loss function L with respect to k_i is calculated locally as the Hadamard (element-wise) product of the bond's elongation in the forward pass (e_i) and its elongation in the adjoint pass (e_adj,i): ∇L_i = e_adj,i * e_i (Eq. 6). The update rule is then a form of gradient descent: k_i(t+1) = k_i(t) - α * ∇L_i = k_i(t) - α * e_adj,i * e_i (Eq. 7), potentially using optimizers like Adam. The forward and adjoint elongations depend on the local displacement differences across bond i.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The "global order" that emerges is the specific spatial pattern of bond stiffnesses (k_i values) across the entire network after training. This pattern is not arbitrary but is functionally organized to perform the desired computation (e.g., implement the regression function or classification boundaries) by appropriately mapping input forces to output displacements. Different tasks result in different emergent patterns of stiffnesses (compare insets in Fig 3d, 4b, 5a, 5b, 5c, 5e).
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Stiffness Pattern | Spatial distribution of bond stiffnesses | Vector k | [1.5mm, 2.5mm] widths -> corresponding stiffness range | N/m | Mixed | Represents the learned configuration. Width range explicit, stiffness implicit. | Training (Gradient Descent) | Fig 3d, 4b, 5a,b,c,e (Insets) |
| Task Performance | Accuracy/Loss on specific task | Accuracy / MSE | e.g., >90% / ~10^-7 | % / (units of L) | Explicit | Quantifies the function implemented by the emergent stiffness pattern. | Testing on dataset | Fig 3b, 4b |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog / Neuromorphic

### **5.3 Computational Primitive:**

    *   Content: The fundamental computation is the linear transformation defined by the system's stiffness matrix D (or its inverse, the compliance matrix D^-1), mapping input forces F to output displacements u: u = D^-1 * F. In the linear regime, interactions are analogous to weighted sums mediated by the network structure. The training process modifies the elements of D (since D depends on k) to implement a desired complex mapping (regression function or classification boundary). For classification, the primitive includes comparing output displacements.
    *   **Sub-Type (if applicable):** Linear Transformation (Matrix Multiplication) / Weighted Summation / Comparison (for classification).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Computational Response Time (Prediction) | Quasi-static | s (?) | Implicit | Implicit | Assumed static equilibrium (Du=F) for computation/prediction. Actual time depends on achieving equilibrium (speed of sound/damping). |
        | Training Convergence Time | 100s - 1000s | Epochs | Fig 3b, 4b, 5 | Explicit | Number of iterations needed for the loss to converge during training simulations. |

### **6.2 Active Inference:**

    *   Content: Partial/Unclear
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** Rate of Loss Function Reduction (Prediction Error Reduction Rate), Convergence Time, Sensitivity of final state (k) to initial conditions/noise (related to model complexity/certainty). These could be attributes of the `AdaptationNode` or `AdaptationEdge`.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is supervised learning via gradient descent, implemented using the mechanical analogue of *in situ* backpropagation derived from the adjoint variable method. The gradient of a defined loss function L with respect to the tunable parameters (spring constants k_i) is calculated locally using information from two physical states (forward pass with input F, adjoint pass with adjoint force derived from ∂L/∂u). The parameters are then updated iteratively using this gradient (∇L_i = e_adj,i * e_i) and an optimization algorithm (like standard gradient descent or Adam): k_i(t+1) = OptimizerUpdate(k_i(t), ∇L_i, α, ...).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors are learned input-output mappings corresponding to machine learning tasks:
        1.  **Behavior Learning:** Achieving specific, desired displacement patterns at output nodes in response to a given input force (e.g., asymmetric displacement in Fig 2c, trajectory control in Fig S5).
        2.  **Linear Regression:** Mapping an input force magnitude to proportional displacements at output nodes according to learned slopes (Fig 3).
        3.  **Classification (Iris, Penguin):** Mapping input forces (representing features) to a pattern of output displacements where the node with the largest displacement indicates the predicted class (Fig 4).
        4.  **Adaptation/Retraining:** Modifying its behavior to learn new tasks or recover function after damage (Fig 5).

### **8.2 Behavior Robustness:**

        *   **Experimental Validation:** Good agreement between simulations and experiments (Figs 1, 2, 3, 4) suggests robustness to fabrication imperfections and measurement noise to some extent.
        *   **Retrainability after Damage:** The network can partially recover classification accuracy (~80% from ~50% baseline after pruning) through retraining (Fig 5e), showing resilience to structural damage.
        *   **Retrainability after Task Switching:** The network can be successfully retrained for a different task (Fig 5b, 5c), showing behavioral flexibility.
        However, robustness is not exhaustively tested against varying levels of noise, different types of damage, or environmental changes. The sensitivity analysis in Fig S9 shows performance varies significantly depending on *which* bond is pruned ("critical" vs "redundant" bonds), indicating fragility to certain perturbations. Mechanical instability (zero modes) can occur with damage. Linearity assumption may limit robustness to large deformations.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors (learned functions) are validated through:
        1.  **Quantitative Metrics:** Loss function values (MSE for regression, cross-entropy for classification) and accuracy are tracked during simulated training and reported for final trained models (Figs 3b, 4b, 5).
        2.  **Comparison with Targets:** Regression results are compared against target lines (Fig 3a, 3c). Classification results are compared against ground truth labels (Fig 4a vs 4c, confusion matrix Fig S6). Behavior learning compares displacements to target values (Fig 2, Fig S5).
        3.  **Experimental Confirmation:** Key behaviors (gradient calculation feasibility, behavior learning, regression, classification) are validated experimentally using 3D-printed MNNs, showing good agreement with simulations (Figs 1, 2, 3, 4).
        4.  **Robustness Tests:** Retrainability after damage and task switching provides evidence for the robustness and adaptability of the learned behaviors (Fig 5).
        Limitations: Experimental validation uses a limited number of tests/samples. Robustness testing is not exhaustive.

---

#Key: [boyd_correlation-powered_2017]

# Correlation-powered information engines and the thermodynamics of self-correction

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is a theoretical/computational model of an information engine, specifically a "thermal ratchet," designed to extract work from a thermal reservoir by utilizing temporal correlations in an input information source (an "information tape"). The system consists of a ratchet with internal memory states (A, B, C), an information tape providing a sequence of input symbols (e.g., 0s and 1s generated by a Hidden Markov Model - HMM), a thermal reservoir at temperature T, and a work reservoir. The ratchet interacts sequentially with symbols on the input tape, potentially changing the symbol (creating an output tape), and changing its own internal state according to probabilistic rules (Fig. 4). These transitions are coupled to energy exchange with the thermal and work reservoirs. The purpose is to demonstrate and analyze how information engines can leverage temporal correlations (beyond simple statistical bias) for work production and to study the thermodynamics of self-correction (synchronization) in the presence of noise (phase slips) in the input.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These are the key *control* parameters governing the ratchet's behavior and the input noise level. Boltzmann constant k_B is also crucial but is a fundamental constant. Interaction time τ is mentioned but not varied parametrically.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the thermal reservoir at temperature T. Additionally, the "ordered" or correlated nature of the input information tape acts as a thermodynamic resource (fuel), enabling the conversion of thermal energy into work. The engine randomizes the input sequence, increasing its entropy, and leverages this increase.

### **2.2 Energy Transduction**

    *   Content: The ratchet mediates energy exchange between the thermal and work reservoirs, driven by processing information from the input tape. During interaction transitions (ratchet state + tape cell change), heat (Q) is exchanged only with the thermal reservoir (Sec III.B, Appendix A). During switching transitions (ratchet moves to next tape cell), work (W) is exchanged only with the work reservoir (Sec III.B, Appendix A). The net effect in the "engine mode" (clockwise dynamic) is the absorption of heat from the thermal reservoir (Q > 0) and the performance of work on the work reservoir (W > 0), effectively converting thermal energy to work by leveraging the decrease in input tape order (increase in entropy rate from input to output). In the dissipative mode (counter-clockwise), work is converted to heat (W < 0, Q < 0).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper calculates average work production (Eq. 7, 12, B1, B2) and relates it to entropy production bounds (Eq. 5, 6, 10), specifically ∆h_µ (entropy rate change). However, it doesn't explicitly calculate a thermodynamic efficiency metric like η = <W> / <Q_absorbed> or compare <W> directly to the maximum possible work k_B T ln(2) ∆h_µ. It focuses on the *conditions* for positive work production (engine operation) rather than maximizing efficiency. The calculated work values (e.g., k_B T (1-δ)/e in the ideal case) are less than the theoretical maximum suggested by the entropy rate change (k_B T ln(2) H((1-δ)/e)), implying non-ideal efficiency. Qualitative assessment: Medium potential efficiency, dependent on parameters (δ, γ, c).

### **2.4 Energy Dissipation**

    *   Content: Dissipation occurs primarily as heat released into the thermal reservoir. This happens during non-optimal transitions. Specifically:
        1.  **Counter-clockwise mode:** Work is actively consumed from the work reservoir and dissipated as heat. In the δ=γ=0 case, -k_B T work is done per symbol (energy taken from work reservoir), which is then dissipated as heat (Sec IV.C, Fig 6).
        2.  **Synchronization/Self-Correction:** Transitions involving state C, especially when moving from the counter-clockwise mode or correcting for phase slips, involve energy exchange that may be dissipative, depending on parameters (δ, γ). The "synchronization heat" Q_sync = k_B T ln(δ/γ) represents energy absorbed (if positive) or dissipated (if negative) during the transition *out* of C (Sec IV.D). Energy is dissipated when the ratchet attempts to re-synchronize after a phase slip throws it into the counter-clockwise mode (Sec V).
        3.  **Imperfect Clockwise Mode:** Even in the engine (clockwise) mode, transitions might not be perfectly reversible or optimal, leading to some dissipation. The work extracted k_B T (1-δ)/e is less than the maximum possible implied by the entropy change, suggesting some energy loss/dissipation.
    *   Quantification: Specific dissipation rates are calculable from the work/heat expressions. E.g., In the δ=γ=0 case, average dissipation is k_B T (1 - 1/e)/2 per symbol (Sec IV.C). The net work <W> calculated in Eq. B1 implicitly includes dissipation; negative <W> indicates net dissipation.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding with M3.2-M3.8)**

### **3.2 Memory Type:**

    *   Retention: The state persists perfectly between the discrete time steps (interaction intervals τ) of the model. Qualitatively long-term within the model's operation.
    *   Capacity: Low (3 distinct states, log2(3) ≈ 1.58 bits).
    *   Read-out: Implicit in the state transition rules (Fig 4) - the current state determines the possible next states and their probabilities given an input. Fidelity is perfect within the model.
    *   Re-writability: The state is constantly updated at each step based on input and transition rules.
    The score reflects the presence of usable, stateful memory directly influencing function, but with low capacity and lacking mechanisms for long-term storage modification independent of the immediate processing cycle (like learning).

### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 3
*   Units: States (or log2(3) ≈ 1.58 bits)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: 100
*   Units: %

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: 0
    *   Units: % loss per interaction interval

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", proceeding with M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are the state transition probabilities of the joint ratchet-input bit system, depicted in Fig. 4 and mathematically defined by the conditional probability matrices M(y'|xy) in Appendix B. Given the ratchet is in state x ∈ {A, B, C} and reads input symbol y ∈ {0, 1}, it transitions to state x' ∈ {A, B, C} and writes symbol y' ∈ {0, 1} with a specific probability determined by parameters δ and γ. For example:
        *   If in state A⊗0: Transition to B⊗0 with prob 1-δ, or to C⊗0 with prob δ. Output y' is implicitly 0 (input 0 is transduced to output 0 in this branch).
        *   If in state B⊗1: Transition to A⊗0 with prob 1/e, or B⊗0 with prob (1-δ)/e, or C⊗0 with prob γ. Output y' is implicitly 0.
        *   (See Fig 4 and Appendix B for all rules).
        These rules depend only on the current ratchet state and the current input symbol.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :-------------------- | :---: | :----------: | :----------------: | :------------: |
    | T1 | A⊗0 -> C⊗0 | δ | [0, 1] | Dimensionless | Fig. 4 | Explicit | Transition probability defined by δ |
    | T2 | B⊗1 -> C⊗0 | γ | [0, 1] | Dimensionless | Fig. 4 | Explicit | Transition probability defined by γ |
    | T3 | C⊗0 -> A⊗1 | γ | [0, 1] | Dimensionless | Fig. 4 | Explicit | Transition probability defined by γ |
    | T4 | C⊗1 -> B⊗0 | γ | [0, 1] | Dimensionless | Fig. 4 | Explicit | Transition probability defined by γ |
    | T5 | A⊗1 -> C⊗1 | δ | [0, 1] | Dimensionless | Fig. 4 | Explicit | Transition probability defined by δ |
    | T6 | B⊗0 -> C⊗1 | δ | [0, 1] | Dimensionless | Fig. 4 | Explicit | Transition probability defined by δ |
    | ... | (Other transitions involving 1-δ, 1-γ, 1/e, (1-δ)/e) | δ, γ | [0, 1] | Dimensionless | Fig. 4, App B | Explicit | Transition probabilities involving complements or fixed factors |

### **4.3 Global Order:**

    *   Content: The emergent global order consists of distinct dynamical modes or attractors for the joint ratchet-environment system:
        1.  **Synchronized Clockwise Mode:** The ratchet's state transitions are phase-locked with the input sequence (e.g., A anticipates 1, B anticipates 0 for the 0101... sequence). This mode corresponds to net work production (engine function). It is the stable attractor when δ, γ > 0.
        2.  **Anti-synchronized Counter-Clockwise Mode:** The ratchet's state is out of phase with the input. This mode corresponds to net work dissipation. It is a stable attractor only when δ = γ = 0, otherwise it is transient.
        3.  **Synchronization State (C):** Acts as a transient bridge state, necessary for reaching the synchronized clockwise mode and enabling self-correction.
        The global order is therefore the specific, recurring pattern of state transitions adopted by the system over time, corresponding to a particular thermodynamic function (engine or dissipator).
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Explicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| SYNC_ENTRY | Transition *into* state C | δ | [0, 1] | Dim'less | Explicit | Prob. δ governs transitions A⊗0->C⊗0, A⊗1->C⊗1, B⊗0->C⊗1, B⊗1->C⊗1 (partially) | Fig. 4 |
| SYNC_EXIT | Transition *out of* state C | γ | [0, 1] | Dim'less | Explicit | Prob. γ governs transitions C⊗0->A⊗1, C⊗1->B⊗0 | Fig. 4 |
| CLOCKWISE | Transitions maintaining sync (engine mode) | 1-δ, 1/e, (1-δ)/e | [0, 1] | Dim'less | Explicit | Transitions between A⊗1 and B⊗0 characteristic of the clockwise mode | Fig. 4, Fig. 6 |
| C_CLOCKWISE | Transitions maintaining anti-sync (dissipative mode) | 1-δ | [0, 1] | Dim'less | Explicit | Transitions between A⊗0 and B⊗1 characteristic of counter-clockwise mode | Fig. 4, Fig. 6 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| P_MODE_ASYMP | Asymptotic probability distribution over joint ratchet⊗input states | π_{x⊗y} | [0, 1] | Dim'less | Explicit | Stationary distribution determines average behavior, reflects dominant mode | Solve T'π' = π', Eq. B1 derivation | App. B |
| WORK_AVG | Average Work Production Rate | <W> | [-k_B T, k_B T/e] | J/symbol or J/τ | Explicit | Positive value indicates engine mode dominance, Negative indicates dissipative. Zero at transition. | Eq. (1), (7), (12), (B1), (B2) | Eq. (1) / App. B |
| SYNC_RATE | Rate of synchronization (transition to clockwise mode) | R_sync | [0, 1] | τ⁻¹ | Explicit | Inverse avg time steps to reach clockwise mode from C. For c=0, R_sync=γ. | Sec IV.D | Calculation from definition |
| ENT_RATE_OUT | Entropy Rate of Output String | h'_µ | [0, 1] | bits/symbol | Explicit | Measures randomness added by ratchet; non-zero value required for work from periodic input. | Eq. (9), (B3) | App. B / Eq. B3 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 0

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6.)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Interaction Interval | τ | s | Sec III | Explicit | Fundamental time step of the ratchet's operation cycle. |
        | Synchronization Time | ~1/γ | τ (Interaction Intervals) | Sec IV.D | Explicit (for c=0) | Average time to transition from state C to the clockwise mode (for c=0). For c>0, more complex. |
        | Input Correlation Time | Depends on HMM | τ (Interaction Intervals) | Sec III.A | Implicit | Characteristic time over which input symbols are correlated (e.g., period=2 for Fig 3). |

    *   **Note:** Time scales are often relative to the fundamental interaction interval τ.

### **6.2 Active Inference:**

    *   Content: No
        1.  Builds an internal *predictive model* of the input sequence generator beyond recognizing its current phase via synchronization.
        2.  Selects actions (state transitions) explicitly to *minimize a prediction error* or surprise signal related to a predictive model.
        3.  Updates an internal model based on prediction errors.
        The synchronization mechanism helps align the ratchet's state with the environment's state (phase), which is a prerequisite for effective action, but it seems more like reactive adaptation or state-matching than active inference's generative model-based prediction and minimization of free energy/surprise.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Partial

**(Conditional: If M7.1 is "Partial (or Yes)", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism is dynamical state recovery via the synchronizing state C. When a phase slip occurs, the ratchet might transition into the state sequence corresponding to the dissipative counter-clockwise mode. From this mode, there's a probability δ for the ratchet to transition into state C during an interaction (Sec IV.C, Fig 7). Once in state C, the ratchet cannot directly return to the counter-clockwise mode. It eventually transitions out of C (with probability γ for c=0) into the synchronized, work-producing clockwise mode. This pathway through C allows the system to escape the dissipative dynamics induced by input errors and return to its functional operating mode. The effectiveness depends on the rates δ and γ relative to the phase slip frequency c. It is a pre-designed recovery pathway, not a modification of the transition rules themselves.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors are:
        1.  **Work Production (Engine):** In the synchronized (clockwise) mode, the system consumes information (correlations) from the input tape and absorbs heat from the thermal reservoir to produce positive average work output (<W> > 0). (Sec IV.C, Sec V, Fig 6b, Fig 12).
        2.  **Work Dissipation (Eraser/Dud):** In the anti-synchronized (counter-clockwise) mode or under high input error rates (c > c*), the system consumes work and dissipates it as heat (<W> < 0). Depending on entropy changes, this might correspond to information erasure or simply dissipation ('dud'). (Sec IV.C, Sec V, Fig 6a, Fig 12).
        3.  **Synchronization/Self-Correction:** The ability of the ratchet to dynamically align its internal state with the input phase and recover from input errors (phase slips) to maintain or re-establish the work-producing mode. (Abstract, Sec II, Sec IV.C, Sec V).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors (work production/dissipation modes, synchronization) are validated through rigorous theoretical analysis and calculation based on stochastic thermodynamics and computational mechanics (specifically, using HMMs and finite-state transducers).
        *   Operational Definitions: Work production is defined thermodynamically via energy exchange (Eq. 1, Appendix A). Synchronization is defined as the alignment of ratchet state with input phase, leading to the clockwise mode (Sec IV.C).
        *   Control Experiments (Theoretical Analogue): Comparison is made between the full model (δ,γ > 0) and the non-synchronizing case (δ=γ=0, Fig 5, Fig 6) to demonstrate the necessity of state C for robust work production and self-correction. Parameter studies (varying c, δ, γ) act as controls.
        *   Quantitative Analysis: Exact analytical expressions for average work production (<W>, Eq. 7, 12, B1, B2), synchronization rate (R_sync), and entropy rates (∆h_µ) are derived and plotted (Fig 8, 10, 11, 12). Stationary distributions are calculated (Appendix B).
        *   Robustness/Reliability/Reproducibility: Analysis demonstrates robustness up to c* (Sec V). As a theoretical model, results are perfectly reproducible given the parameters.
        *   Limitations: Validation is purely theoretical/computational. No experimental verification is presented.

---

#Key: [cao_quantum_2020]

# Quantum biology revisited

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews the process of photosynthetic light harvesting in pigment-protein complexes (PPCs), with a specific focus on the Fenna-Matthews-Olson (FMO) protein from green sulfur bacteria. It describes these systems as collections of pigments [(bacterio)chlorophylls, carotenoids] held by a protein scaffold within a solvent environment (the bath). The system absorbs light energy, creating collective electronic excitations (excitons) that are delocalized over multiple pigments. The purpose is highly efficient energy transport from antenna complexes towards the reaction center, where charge separation occurs. The review critically re-evaluates the hypothesis that quantum coherences (specifically long-lived interexciton coherences) play a functional role in directing this energy transfer, concluding that they do not and that observed long-lived oscillations originate from vibrations. It argues nature exploits engineered exciton-bath interactions (dissipation) for efficient energy flow.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Interexciton Coherence Dephasing Time (FMO, 300K, calculated) | ~50 | fs | Fig. 1E | Explicit | Medium | Theoretical Calculation (Mixed Quantum-Classical) |
        | Optical Coherence Dephasing Time (FMO, 300K, calculated) | ~75 | fs | Fig. 1F | Explicit | Medium | Theoretical Calculation (Mixed Quantum-Classical) |
        | Intercomplex Energy Transfer Timescale | Tens of Picoseconds | ps | Introduction | Explicit | High | General knowledge cited (refs 7-9) |
        | Exciton Relaxation Timescale (FMO, 300K, calculated) | Sub-ps to ~1 ps | ps | Fig. 1B, Text p. 7 | Explicit | Medium | Theoretical Calculation |
        | Temperature | 77, 277, 300 | K | Text p. 6, 7 & Fig 1 caption | Explicit | High | Stated Experimental/Simulation conditions |

    *   **Note:** These parameters represent key timescales and conditions discussed in the review, particularly concerning coherence lifetimes and energy transfer dynamics in the context of the FMO complex at relevant temperatures.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is light (sunlight under natural conditions, laser pulses in experiments like 2DES). Light absorption by pigments initiates the process.

### **2.2 Energy Transduction**

    *   Content: 1. Light absorption by pigments creates electronic excitation. 2. Strong coupling between pigments leads to the formation of collective excited states (excitons), delocalized over multiple pigments. 3. Excitation energy is transferred between excitonic states, typically flowing "downhill" in energy. This transfer is mediated by coupling between excitons and the surrounding environment (bath), involving dissipation of excess energy into vibrational modes. 4. Energy is ultimately transferred to the reaction center for charge separation (though this final step is outside the main focus on coherence). The key mechanism reviewed is exciton relaxation facilitated by exciton-bath (vibrational) coupling.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper states that the primary steps operate near theoretical quantum limits in efficiency. While it argues against the *mechanism* of coherence being responsible, it acknowledges the high efficiency itself. No single numerical value is given for the entire process in this excerpt, but "near theoretical quantum limits" implies very high efficiency. The score reflects this high efficiency, although the *reason* for it is re-interpreted (dissipation engineering, not coherence).

### **2.4 Energy Dissipation**

    *   Content: Dissipation is identified as a crucial mechanism, not a hindrance. It occurs through the coupling between electronic excitations (excitons) and vibrations of the pigments and the surrounding protein/solvent environment (bath). This exciton-vibrational coupling allows excess electronic energy to be converted into vibrational energy (heat), facilitating the downhill flow of energy between excitonic states towards the reaction center. The paper argues nature *exploits* this dissipation via engineered exciton-bath interactions. Quantification is primarily through coherence decay times (related to system-bath coupling strength, see M1.3) rather than direct thermal output. The reorganization energy (Eλ) is mentioned as a parameter quantifying the coupling strength relevant to pure dephasing. Decay times are on the order of tens of femtoseconds (Fig 1 E, F). Level: High (and functional).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip to Module 4.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip to Module 6.)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Interexciton Coherence Decay (calculated, 300K) | ~50 | fs | Fig 1E | Explicit | Value from cited calculation |
        | Optical Coherence Decay (calculated, 300K) | ~75 | fs | Fig 1F | Explicit | Value from cited calculation |
        | Intracomplex Exciton Relaxation | Sub-ps to ~1 ps | ps | Fig 1B, Intro | Explicit | Stated/calculated range |
        | Intercomplex Energy Transfer | Tens of ps | ps | Intro | Explicit | Stated order of magnitude |
        | Vibrational Coherence Decay (Observed) | >600 fs, potentially ps | fs/ps | Text p. 6 & 7 | Explicit | Lifetimes discussed in refutation of electronic coherence |

    *   **Note:** These timescales cover the key dynamic processes discussed: coherence lifetimes, energy relaxation within a complex, and transfer between complexes.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip to Module 8.)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is highly efficient, directed transfer of excitation energy from the site of light absorption (antenna pigments) to the photosynthetic reaction center. This involves relaxation through a ladder of excitonic states, guided by site energy differences and facilitated by dissipation into the vibrational bath. A secondary observed behavior (though argued non-functional) is the presence of oscillating signals in spectroscopic measurements (quantum beats), attributed mainly to vibrational coherences.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary behavior (energy transfer) is validated through extensive spectroscopic studies (e.g., 2DES, pump-probe) that map energy transfer pathways and timescales (Refs 13, 90, Fig 1B/C). Theoretical models (Frenkel exciton Hamiltonians, quantum dynamics simulations) reproduce and explain these experimental observations (Fig 1, Methods section). The *interpretation* of spectroscopic oscillations (quantum beats) is the core debate: the review cites specific studies (Refs 90, 91, 92, Fig 4) using techniques like temperature dependence, mutation analysis, and polarization-controlled 2DES to argue against the initial interpretation of long-lived electronic coherence and support a vibrational origin. Reproducibility is implied by multiple groups reaching similar conclusions. Limitations involve the complexity of deconvoluting signals in congested spectra and the reliance on theoretical models for interpretation.

---

#Key: [mead_neuromorphic_1990]

# Neuromorphic Electronic Systems

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper describes neuromorphic electronic systems built using analog Very Large-Scale Integration (VLSI) technology, primarily CMOS. These systems aim to emulate the computational principles of biological nervous systems to achieve high efficiency, particularly for sensory processing tasks where input data is ill-conditioned. Key components include transistors operating in their subthreshold (exponential) regime, capacitors for integration, resistive networks for spatial averaging, and floating-gate transistors for adaptive learning and memory. The purpose is to overcome the massive energy inefficiency of digital computation compared to biological brains (estimated 10^9 factor) by using the inherent physics of silicon devices as computational primitives and representing information with relative analog values. Examples include silicon retinas performing gain control, edge enhancement, and adaptation, and silicon cochleas for auditory processing. The ultimate goal is wafer-scale integration of these adaptive analog systems.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Brain Energy/Operation (Estimated) | ~10^-16 | J/operation | Section "TWO TECHNOLOGIES" | Explicit | Medium | Calculation based on synapse/neuron counts, firing rates, and total power. |
        | Analog Neuromorphic Energy/Operation (Retina Example) | ~10^-11 | J/operation | Section "NEURAL SILICON" | Explicit | Medium | Calculation based on device count, estimated operations/s, and power consumption. |
        | Brain Synapse Count | ~10^15 | synapses | Section "TWO TECHNOLOGIES" | Explicit | Low | Estimate based on neuroscience literature. |
        | Brain Operations/Second | ~10^16 | operations/s | Section "TWO TECHNOLOGIES" | Explicit | Low | Estimate based on synapse count and average firing rate. |

    *   **Note:** Parameters focus on energy efficiency comparisons and scale, central themes of the paper. Reliability varies based on whether it's a direct tech spec or a biological estimate.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Electrical power supplied to the VLSI chip. For the silicon retina examples, incident light energy is also an input, transduced into electrical signals by photoreceptors. UV light is used as an energy source for programming the floating gates in the adaptive retina.
    *   Value: Variable (e.g., ~10^-4 W for the retina example)
    *   Units: W (Watts)

### **2.2 Energy Transduction**

    *   Content: Electrical energy is transduced via the physical behavior of transistors and other components. Key mechanisms include:
        1.  Photodetection (Light energy to electrical current/voltage in photoreceptors).
        2.  Transistor I-V characteristics (Gate voltage controls channel current, often exponentially in subthreshold).
        3.  Capacitive charging/discharging (Electrical energy stored/released, implementing integration).
        4.  Resistive dissipation (Current flow through resistive elements generates heat).
        5.  Kirchhoff's Current Law (Current aggregation at nodes performs addition).
        6.  UV Photoinjection/Tunneling (UV light energy facilitates charge movement onto/off floating gates).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The central argument of the paper is the vastly superior energy efficiency of analog neuromorphic systems compared to digital ones. Metrics provided:
        *   Brain: ~10^-16 J/operation
        *   Analog Neuromorphic (Retina): ~10^-11 J/operation
        *   Digital (Chip Level, 1990): ~10^-7 J/operation
        *   Digital (Ultimate Limit Est.): ~10^-9 J/operation (Chip), ~10^-7 - 10^-6 J/operation (Box)
        The analog approach achieves ~4 orders of magnitude improvement over 1990 digital tech and is ~5 orders of magnitude less efficient than the brain. The score reflects this significant improvement over digital while acknowledging the gap to biological efficiency.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms identified are:
        1.  Charging wire capacitance: Explicitly stated as a dominant factor (~factor of 100 loss) in digital systems due to wires dominating node capacitance compared to gate capacitance. This also applies to analog systems, but potentially less severely due to local computation. (Qualitative: High in Digital, Medium in Analog).
        2.  Switching many transistors per operation: Explicitly stated as ~10,000 transistors switched per operation in typical digital implementations (~factor of 10,000 loss). Analog systems aim to use single or few transistors per primitive operation. (Qualitative: High in Digital, Low in Analog).
        3.  Resistive losses: Implicit in the use of resistive networks and current flow through transistors. (Qualitative: Medium/Low).
        4.  Leakage currents: Implicit in CMOS technology, though minimized by design. (Qualitative: Low).
        Quantification is primarily comparative (digital vs. analog vs. brain) rather than absolute per mechanism. Total power dissipation for the retina example is ~10^-4 W.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: "eons" (Qualitative)
*    Units: Time

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: In the adaptive retina (Fig. 3):
        1.  *Sensing:* Phototransistor current is proportional to local light intensity. Resistive network voltage represents a spatio-temporal average of neighboring light intensities.
        2.  *Comparison:* The output node voltage reflects the difference between the phototransistor current (center) and the current sourced by the pull-up transistor controlled by the network voltage via the floating gate (surround/prediction).
        3.  *Feedback (during UV illumination):* If the output node voltage is high (local intensity > prediction), UV light causes electrons to move from the output node to the floating gate, increasing its charge, thereby decreasing the pull-up current. If the output node voltage is low (local intensity < prediction), electrons move from the floating gate to the output node, decreasing its charge, thereby increasing the pull-up current. This negative feedback drives the output node voltage towards a common equilibrium potential across the array under uniform illumination.
        Rule: ΔQ_fg ∝ -V_out * I_UV (where Q_fg is floating gate charge, V_out is output node voltage relative to some baseline, I_UV is UV intensity).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The emergent global order in the adaptive retina is the compensation for fixed-pattern noise (transistor offset variations). Under uniform illumination, the output of an unadapted retina shows a random pattern reflecting these offsets. After adaptation (self-organization), the system drives all output nodes towards the same potential, resulting in a uniform output image under uniform illumination, reflecting the average input rather than device variations. This represents a globally coordinated state achieved through local feedback.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| SpatialAvg | Resistive Network Averaging | Lateral Resistance (R_lat) | Tunable | Ω | Explicit | Controls spatial scale constant. | Fig. 2, Section "RETINAL COMPUTATION" |
| SpatialAvg | Resistive Network Averaging | Coupling Conductance (Gm) | Tunable | S | Explicit | Controls spatial scale constant. | Fig. 2, Section "RETINAL COMPUTATION" |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| OffsetComp | Compensation for Fixed Pattern Noise | Output Voltage Standard Deviation (σ_Vout) across pixels under uniform illumination | Ideally -> 0 | V | Implicit | Adaptation aims to minimize output variation. | Measure pixel outputs under uniform light before/after UV adaptation. | Section "ADAPTIVE RETINA" |
| Uniformity | Uniform Output State | Mean Output Voltage (μ_Vout) under uniform illumination | Converges to target | V | Implicit | Adaptation drives outputs towards a common potential. | Measure pixel outputs under uniform light after UV adaptation. | Section "ADAPTIVE RETINA" |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog / Neuromorphic

### **5.3 Computational Primitive:**

    *   Content: The paper identifies several computational primitives directly implemented by device physics:
        1.  Exponential function: `I = I_0 * exp(V / V_T)` (Subthreshold transistor current vs. gate voltage, Fig 1d; also observed in neural elements, Fig 1a,b,c).
        2.  Addition: Implemented by Kirchhoff's current law (summing currents at a node).
        3.  Integration (Temporal): `V = (1/C) * ∫I dt` (Charging a capacitor C with current I).
        4.  Spatial Averaging: Implemented by resistive networks.
        5.  Difference/Comparison: Implicit in circuits like the retina output stage (difference between local photoreceptor and network potential).
        6.  Multiplication (Implicit): Can be implemented using combinations of exponential/logarithmic functions via transistors (though not explicitly detailed as a primitive here, it's achievable).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Retina Temporal Smoothing | Tunable | s | Section "RETINAL COMPUTATION" | Explicit | Explicitly stated that the time constant of the horizontal network is set by bias voltages and can be varied. No specific value given. |
        | Memory Retention (Floating Gate) | "eons" | time | Section "COMPUTATIONAL PRIMITIVES" | Explicit | Explicitly stated qualitative long retention time. |
        | Digital Operation Time (Implied) | ~10^-7 | s | Section "TWO TECHNOLOGIES" | Implicit | Inferred from ~10 million ops/s rate for microprocessors mentioned. |
        | Neural Spike Rate (Average) | ~10 | Hz | Section "TWO TECHNOLOGIES" | Explicit | Explicitly stated average rate of nerve pulse arrival at synapses. |

### **6.2 Active Inference:**

    *   Content: Partial
        1.  *Prediction:* The "model" box (e.g., the resistive network in the retina) computes a prediction based on input history (spatio-temporal average).
        2.  *Comparison/Prediction Error:* The "Compare" box calculates the difference between the actual input and the prediction. This difference *is* the prediction error.
        3.  *Model Update:* The "Correction" signal, derived from the difference, is used to update the model (e.g., modify floating gate charges in the adaptive retina). This aims to minimize future prediction errors (implicitly, driving outputs towards the average).
        However, the paper does not explicitly frame this using active inference terminology (like surprise minimization or free energy). The "action" component (influencing the environment or sampling) is less apparent in the described retina example, which focuses more on perceptual inference and adaptation.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The primary adaptation mechanism described is the modification of charge stored on floating-gate transistors using UV light (in the adaptive retina example). The difference between the local input (photoreceptor) and the network's prediction (spatio-temporal average, modulated by the current floating gate charge) determines the output node voltage. During UV illumination, this voltage difference drives charge transfer onto or off the floating gate (negative feedback), adjusting the transistor's operating point. This effectively changes the 'offset' or 'weight' associated with that pixel's contribution or response, allowing the system to learn the average input level and compensate for individual device variations. The paper refers to this as the "simplest form of learning." It also mentions tunneling and hot-electron mechanisms used in commercial EEPROMs as alternative ways to modify floating gate charge. The process resembles error correction or unsupervised learning based on prediction error.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors described are related to sensory processing, mimicking functions found in biological systems:
        1.  *Visual Processing (Silicon Retina):* Gain control (level normalization), contrast enhancement, edge detection (approximating Laplacian or Difference of Gaussians via center-surround computation), temporal differentiation (enhancing changes over time), adaptation (compensating for device offsets). Motion sensing is mentioned as achieved in other chips.
        2.  *Auditory Processing (Silicon Cochlea):* Decomposition of sound into features, sound source localization (binaural chips). Mentioned but not detailed.
        3.  *Learning/Adaptation:* Compensating for component variations, learning average input statistics.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation is primarily through demonstration of functionality that mimics biological counterparts (e.g., the silicon retina producing Mach bands, performing gain control) and through the successful operation of the adaptive mechanism correcting for offsets. The paper cites specific publications (e.g., [2], [4], [5]) describing these systems. It relies on functional analogy and the observed outcome of adaptation (uniform output) as validation, rather than rigorous quantitative analysis of emergence or comparison against null hypotheses within this text. Figures 2 and 3 describe the circuits, and the text describes their operation and the adaptation process. The claim that adaptation makes the system robust to failure is argued conceptually, not demonstrated experimentally here.

---

#Key: [hanczyc_chemical_2010]

# Chemical Basis for Minimal Cognition

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of an oil droplet (nitrobenzene containing oleic anhydride precursor) placed in an aqueous phase (water containing oleate surfactant). A chemical reaction (hydrolysis of oleic anhydride at the oil-water interface) produces more oleate surfactant and protons, locally changing pH and interfacial tension. This induces a Marangoni instability, leading to convective flow within the droplet and self-propulsion (movement) of the droplet through the aqueous phase. The system exhibits chemotaxis in response to pH gradients (either self-generated or externally imposed). Its purpose is to study the physicochemical origins of movement, minimal perception, cognition, sensory-motor coupling, homeostasis, and autopoiesis in a simple, non-living chemical system. Components: Nitrobenzene (oil), Oleic anhydride (precursor/fuel), Water (aqueous phase), Oleate (surfactant), NaOH (for pH adjustment).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name        | Value         | Units          | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :-------------------- | :-----------: | :------------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Values are taken directly or estimated from figures/text. Reliability is based on whether it's a set parameter, a direct measurement, or a visual estimate.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the chemical potential energy stored in the oleic anhydride precursor ("fuel"). This energy is released through the hydrolysis reaction at the oil-water interface.

### **2.2 Energy Transduction**

    *   Content: Chemical potential energy (oleic anhydride) is converted via hydrolysis into chemical products (oleate, protons). The accumulation of these products creates interfacial tension gradients along the droplet surface. This gradient provides the energy for the Marangoni effect (surface flow). The Marangoni flow drives internal convection and overcomes viscous drag, transducing the energy into kinetic energy of the fluid (convection) and the droplet as a whole (self-propulsion/movement).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any quantitative measure of energy efficiency (e.g., chemical energy input vs. kinetic energy output). Given the nature of the system (viscous fluid, complex flows, dissipation likely high), the efficiency of converting chemical energy into directed motion is expected to be very low. Score is a low qualitative estimate.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include: 1) Viscous dissipation within the droplet due to internal convection. 2) Viscous drag experienced by the droplet moving through the aqueous phase. 3) Heat generated by the exothermic hydrolysis reaction (though not explicitly mentioned as dissipation, it represents energy not converted to motion). 4) Diffusion of reactants/products (dissipating chemical gradients). Quantification is not provided. Qualitatively, dissipation due to viscosity is likely high.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: 1. **Chemical Reaction:** Hydrolysis of oleic anhydride at the oil-water interface, rate dependent on local precursor and water availability. Produces oleate and H+. `rate = k[anhydride][H2O]` (simplified). 2. **Interfacial Tension Modulation:** Local interfacial tension decreases with increasing oleate concentration (Fig 2b) and increases with decreasing pH (increasing H+) down to pH 9 (Fig 2a). `γ = f([oleate], pH)`. 3. **Marangoni Effect:** Surface flow is induced from regions of low interfacial tension to high interfacial tension. Force proportional to the gradient: `F_M ~ ∇γ`. 4. **Fluid Dynamics:** Flow within the droplet and in the surrounding fluid is governed by Navier-Stokes equations, subject to boundary conditions at the interface (stress balance including Marangoni stress). 5. **Diffusion:** Transport of reactants (anhydride) to the interface and products (oleate, H+) away from the interface.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :-------------------- | :---: | :----------: | :----------------: | :------------: |
    | 2       | Interfacial Tension (pH) | Tension (γ) | ~8-20 (Fig 2a range) | mN/m | Fig 2a | Explicit | Measured data shown. |
    | 2       | Interfacial Tension (oleate) | Tension (γ) | ~8-15 (Fig 2b range) | mN/m | Fig 2b | Explicit | Measured data shown. |

### **4.3 Global Order:**

    *   Content: The emergent global order includes: 1) The self-assembled spherical (or deformed for larger droplets) shape of the oil droplet itself. 2) The specific, organized pattern of convective flow within the droplet (e.g., "pair convective flow", "quadratic-vortex formation", Section 2). 3) The sustained, directional macroscopic movement (self-propulsion) of the entire droplet through the aqueous phase. 4) For larger droplets, specific non-spherical shapes (e.g., "horseshoe shape", Section 3.2, Fig 3) that correlate with motion.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| 1       | Hydrolysis Rate | [Anhydride] | 0.5 M initially | M | Explicit | Initial concentration set. | Appendix A.2 |
| 2       | Interface Tension (pH) | pH | ~7-11 | pH units | Mixed | Range inferred from Fig 2a and text mentioning pH drop to 7. Initial pH is explicit. | Fig 2a, Sec 3.1, Appendix A.1|
| 2       | Interface Tension (Oleate) | [Oleate] | >=10 mM | mM | Mixed | Initial concentration explicit (10mM), increases during reaction. Range in Fig 2b shown. | Fig 2b, Appendix A.1 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| GO1 | Droplet Shape | Diameter / Aspect Ratio | 0.1 - few cm | mm / dimensionless | Explicit | Visual observation, size range mentioned. | Microscopy | Sec 2, Sec 3.2, Fig 1, Fig 3 |
| GO3 | Droplet Motility | Velocity / Direction | Variable / Qualitative-Directional | mm/s / Degrees | Mixed | Movement described, speed/direction fluctuates (Fig 4), but directional in gradient (chemotaxis). Velocity not quantified. | Microscopy, Video | Sec 2, Sec 3.1, Fig 4 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :-------------: | :-----------: | :------ | :----------------: | :------------: | :-----: |
    | Local Rules -> Global Order | Mapping from chemical/physical rules to emergent shape, flow, motion | Medium (Score 6 from M4.4) | 2 | Qualitative descriptions, chemotaxis observations | Mixed | The paper describes the causal link (rules lead to order) but doesn't formalize it mathematically or provide quantitative predictability metrics beyond observing chemotaxis. The low Yoneda score reflects the lack of formal categorical mapping or rigorous verification of the local-to-global link's structure-preserving properties. | Sections 2, 3 |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 2 - The paper conceptually links local interactions (rules) to global behavior but does not formally treat this relationship through the lens of Category Theory or the Yoneda Lemma. There's no demonstration that the global behavior functor preserves the structure of the local interaction category. Score reflects only a basic conceptual link.
    *   **Metrics:** Qualitative observation of cause-and-effect (instability -> flow -> motion), observation of chemotaxis. No formal metrics used for mapping fidelity assessment.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes (interpreted minimally)

**(Conditional: M5.1 is "Yes", including M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog

### **5.3 Computational Primitive:**

    *   Content: Gradient Sensing/Response (Chemotaxis). The material system processes local chemical gradient information (input) and transforms it into directed motion (output). Mathematically, this could be approximated as `Velocity ~ f(∇C)`, where C is the concentration of the relevant chemical species (e.g., H+ for pH gradient). The function 'f' represents the complex interplay of interfacial tension, Marangoni flow, and fluid dynamics.
    *   **Sub-Type (if applicable):** Signal Transduction / Actuation based on Gradient

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Initial Movement Onset | Seconds | s | Sec 2 | Explicit | "within seconds after the introduction..." |
        | Gradient Response (no fuel) | Few seconds | s | Sec 3.1 | Explicit | "stop once it successfully equilibrates... (typically in a few seconds)" |
        | Shape/Motion Fluctuation (large droplets) | Seconds | s | Sec 3.2, Fig 4 caption | Explicit | "vary on the time scale of seconds", "each frame was taken at 8-s intervals" |
        | Sustained Movement Duration | Minutes to Hours (implicit) | min/hr | Sec 1, Appendix A.2 | Implicit | Movement lasts "as long as enough precursor oil remains". Depends on initial fuel amount (0.5M) and droplet size, suggesting longer timescales. |
        | Interfacial Tension Stabilization (Tensiometry) | Minutes (implicit) | min | Appendix A.5 | Implicit | "All values were taken after the tension reached a steady state", implies a non-instantaneous process. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: 1. **Self-Propulsion:** Autonomous, sustained movement of the oil droplet through the aqueous phase, driven by internal chemical reaction and Marangoni flow. 2. **Chemotaxis:** Directed movement of the droplet along a chemical gradient (specifically pH gradients, moving towards higher pH above pH 9, based on Fig 2a and movement mechanism). 3. **Internal Convection:** Organized flow patterns within the droplet, coupled to the self-propulsion. 4. **Shape Deformation (Large Droplets):** Significant changes from spherical shape (e.g., horseshoe) accompanying movement in larger droplets.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation relies primarily on: 1. **Direct Observation:** DIC microscopy and video recording are used to observe and document self-propulsion, internal convection, chemotaxis, and shape changes (Figures 1, 3, 4; Appendix A.3, A.4). 2. **Mechanism Plausibility:** Linking observed behavior to known physical principles (Marangoni effect, hydrolysis, fluid dynamics) and supporting measurements (interfacial tension vs. pH/concentration, Fig 2; Appendix A.5). 3. **Control Condition:** Mention of droplets stopping quickly without fuel (Section 3.1) serves as a basic control demonstrating the necessity of the reaction for *sustained* movement. Limitations include: lack of quantitative analysis of flow patterns/velocities/trajectories, limited exploration of parameter space for robustness, reliance on qualitative descriptions for some aspects (e.g., "predictably" moving in gradient). Reproducibility is implied but not explicitly quantified (e.g., % of droplets exhibiting behavior).

---

#Key: [huang_increasingly_2022]

# Increasingly Intelligent Micromachines

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: This review paper describes the field of "intelligent micromachines," defined as miniature systems (millimeters down to nanometers) capable of performing specific tasks autonomously at small scales. It covers the evolution from simple micro/nanomachines to complex systems (soft, compound, reconfigurable, encodable, multifunctional, integrated, swarm). Components discussed include mechanisms, sensors, controllers, actuators, power sources, and interfaces. The purpose is to provide a comprehensive overview of state-of-the-art technologies, define micromachine intelligence (µ-AI), classify existing systems, discuss core techniques (information media, transduction, processing, exchange, energy supply), and provide insights into future development and applications (medicine, bioengineering, cleaning, chemistry, device inspection).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters are representative examples discussed within the review, often referring to specific cited works. Absolute values for the entire field are not provided.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The review discusses various energy sources used for micromachines, including external fields (magnetic, acoustic, electric), light, chemical fuels, and wireless power transfer (radio frequency, infrared). Onboard sources like batteries and capacitors are also mentioned. Specific systems often rely on one primary type (e.g., rotating magnetic fields for helical swimmers, chemical reactions for nanorockets, light for LCE walkers).

### **2.2 Energy Transduction**

    *   Content: Energy is transduced into mechanical motion/force for actuation and locomotion (e.g., magnetic field gradient -> force/torque -> rotation/translation; chemical reaction -> bubble propulsion -> motion; light absorption -> thermal expansion/phase change -> deformation/motion; acoustic waves -> bubble oscillation/streaming -> motion; electrical energy -> piezoelectric actuation -> flapping). Energy is also transduced for sensing (stimulus -> electrical/optical signal) and potentially computation/data storage (e.g., electrical energy -> state change).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The review highlights significant challenges in energy supply and efficiency, particularly power autonomy for miniaturized systems (Sec 6.3). It mentions the high energy consumption (cost of transport) for terrestrial micromachines not scaling down favorably (Sec 6.3). While specific efficiency values aren't given for the field overall, the discussion emphasizes inefficiency and limitations as major hurdles. Qualitative Assessment: Low to Medium, depending on the specific system and energy source. Efficiency is not systematically quantified.

### **2.4 Energy Dissipation**

    *   Content: Dissipation mechanisms are mentioned implicitly through the discussion of microscale physics and challenges. Key mechanisms include viscous drag in fluids (dominant at low Reynolds numbers, Sec 2.1), friction, heat transfer/loss (especially related to power sources and computation, mentioned indirectly), and potentially material damping in soft/flexible structures. Quantification is absent. Qualitative Assessment: High, particularly viscous dissipation in fluid environments.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceed with M3.2-M3.8)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 256 (for nanomagnet example)
*   Units: States / Configurations

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", proceed with M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The review mentions local interactions conceptually but does not provide specific, operational rules or equations. Examples of interaction types include: short-range physical interactions (e.g., magnetic attraction/repulsion between particles regulated by external fields, Sec 2.5, Sec 7.8), hydrodynamic interactions, electrostatic interactions, physical contact, potentially chemical signaling (implied for chemotaxis, Sec 7.2), and DNA hybridization forces for self-assembly (Sec 2.11). The rules governing these interactions (e.g., force laws as a function of distance, field strength, chemical concentration) are not detailed in the review itself.

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
* **Note:** Specific interaction parameters are not provided in the review. The table lists conceptual parameters involved.

### **4.3 Global Order:**

    *   Content: Emergent global order includes collective formations of swarms (e.g., specific shapes, lines, aggregates, Sec 2.5, Fig 1l, Fig 1ad), coordinated or collective motion (e.g., swimming, locomotion, manipulation, Sec 2.10, Sec 7.8), self-assembled structures from DNA or molecules (Sec 2.11), potentially patterns arising from tactic behaviors (Sec 7.2).

### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
* **Note:** As stated in M4.2, specific operational rules and their quantitative parameters are not provided in the review.

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
* **Note:** The review describes the types of emergent order but does not define or quantify specific order parameters used to measure them.

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", proceed with M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Digital / Neuromorphic / Other (Molecular Logic)

### **5.3 Computational Primitive:**

    *   Content: Logic gate (implied for CMOS, spintronics, DNA), potentially thresholding or signal processing depending on the specific implementation (though not detailed). The review highlights basic logic operations as a target. Nanomagnetic logic units (Sec 4.3) and DNA aptamer-encoded logic gates (Sec 2.6, Sec 7.5) are specifically mentioned. Molecular logic gates are also cited for nanomachines (Sec 2.11).
    *   **Sub-Type (if applicable):** Logic Gate

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|
* **Note:** The review identifies types of computational units but does not provide performance metrics for them.

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
    *   **Note:** The review uses qualitative terms for time (fast, rapid, slow) but rarely provides quantitative values.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", proceed with M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanisms described are primarily:
        1.  **Material Response:** Intrinsic changes in material properties (shape, stiffness, etc.) triggered by external stimuli (light, heat, pH, magnetic fields), often pre-programmed via material composition or structure (e.g., smart hydrogels, LCEs, shape-programmable materials - Sec 2.3, Sec 2.5, Sec 3).
        2.  **Reconfiguration:** Active changes in the machine's structure or shape, often controlled externally (e.g., magnetic fields controlling swarm formation or origami folding - Sec 2.5).
        3.  **Control System Adjustment:** Modification of internal control parameters based on feedback from the environment (Level III µ-AI, Fig 4), although the implementation details (algorithms) within the micromachine are not specified.
        4.  **Tactic Behavior:** Active movement in response to environmental gradients (chemotaxis, phototaxis, etc. - Sec 7.2), implying sensing and response coupling.
        The review doesn't delve into specific learning algorithms (Hebbian, reinforcement) being implemented *within* the micromachines themselves, focusing more on the enabling hardware (materials, mechanisms) or conceptual control loops.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behaviors described include: Locomotion/Mobility (swimming, crawling, flying, climbing, walking, tumbling - Sec 2.1, Fig 1), Manipulation (transport, delivery, gripping, release - Sec 2.1, 2.2, Fig 1z, Fig 5), Sensing (various stimuli - Sec 4.3), Actuation (shape change, force generation - Sec 4.3), Reconfiguration/Shape Morphing (Sec 2.5, Fig 2), Data Storage (Sec 2.6), Collective Behaviors/Swarming (formation control, coordinated movement - Sec 2.10, 7.8), Self-Assembly (Sec 2.11), Energy Transfer (Sec 4.3, Fig 1ag).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: As a review paper, it does not present primary validation data. It describes behaviors observed in cited works (often referencing figures from those works, though not reproduced in detail). Validation methods used in the primary literature (e.g., experimental observation, performance quantification, control experiments) are generally not discussed in the review itself. Claims of emergence are based on the definition (global order from local rules without central control), particularly for swarms and self-assembly.

---

#Key: [theraulaz_spatial_2002]

# Spatial patterns in ant colonies

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of a colony of *Messor sancta* ants interacting with ant corpses within a confined circular arena. The study focuses on the emergent spatial patterns (cemeteries/clusters) that form as ants pick up and deposit corpses. The core components are the ants, the corpses (initially homogeneously distributed), and the physical arena (designed to approximate a 1D system with periodic boundaries due to ant thigmotactism). The purpose is to experimentally and theoretically analyze the dynamics of this pattern formation, specifically testing the hypothesis that it arises from a Local Activation-Long Range Inhibition (LALI) mechanism, where clusters grow autocatalytically (activation) and deplete the surrounding area of corpses (inhibition).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name             | Value                      | Units      | Source (Fig/Table/Section)   | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------------------- | :------------------------- | :--------- | :--------------------------- | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Ant Mean Velocity (v)      | 1.6 (± 0.7)                | cm/s       | Results, p. 9646             | Explicit          | High                            | Direct measurement (n=25)         |
        | Mean Free Path (l)         | 15.8                       | cm         | Results, p. 9646             | Explicit          | High                            | Derived from U-turn prob.       |
        | Diffusion Coefficient (D)  | 1.3 x 10^-3                | m^2/s      | Fig. 3 legend / Eq. 2, 3     | Explicit          | Medium                          | Calculated (D = v*l/2)          |
        | Perception Radius (δ)      | 0.5 - 1.0                  | cm         | Model Description, p. 9647   | Explicit          | Medium                          | Stated as from 'dedicated exp.' |
        | Initial Corpse Density     | 127 or 255 (small arena)   | corpses/m  | Methods, p. 9645             | Explicit          | High                            | Controlled experimental variable  |
        |                            | 127 or 255 (large arena)   | corpses/m  | Methods, p. 9645             | Explicit          | High                            | Controlled experimental variable  |
        |                            | 13 (large arena, low dens) | corpses/m  | Methods, p. 9645             | Explicit          | High                            | Controlled experimental variable  |

    *   **Note:** Dropping/picking probabilities (γ1-γ4, kd) are also key but are density-dependent functions (Fig 3), not single values. Listed densities correspond to 100/200 (small) and 200/400 (large) corpses.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source for the system's activity (ant movement, corpse manipulation) is chemical energy derived from the ants' metabolism, fueled by the food provided (seeds, crickets). The experimental setup itself (camera, lights) requires electrical energy, but this does not directly drive the pattern formation process.

### **2.2 Energy Transduction**

    *   Content: Chemical energy (ATP from metabolism) is transduced into kinetic energy for ant locomotion (walking, U-turns) and potential energy changes when ants lift and carry corpses. Sensory systems transduce local environmental information (corpse density) into signals influencing behavioral decisions (picking/dropping), though the energy involved in sensing/decision-making is likely negligible compared to locomotion.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The concept of energy efficiency is not relevant in the standard sense for this system. The study focuses on the emergent pattern formation dynamics and the underlying behavioral rules, not on the efficiency of energy conversion for performing work (like transport). Quantifying the metabolic cost specifically for corpse clustering versus other activities is not done.

### **2.4 Energy Dissipation**

    *   Content: Energy is primarily dissipated as heat due to metabolic processes within the ants, muscular activity during locomotion and corpse manipulation, and friction between the ants and the arena substrate. The magnitude is not quantified. Likely Low on a per-ant basis, but aggregated over the colony and time.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Hours to Days
*    Units: Time

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Variable, related to number and size of piles.

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Low (implied by pattern stability)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The local rules govern the probability of an ant picking up or dropping a corpse based on the perceived local density of corpses (c̄) within a small radius (δ).
        1.  **Picking:** Unladen ants pick up corpses with a probability rate that *decreases* as local density (c̄) increases (Eq. 3, term III; Fig 3b). Rate: `v * γ3 / (γ4 + c̄)^2 * ρ` (where ρ is density of non-carrying ants).
        2.  **Dropping (Stimulated):** Laden ants drop corpses with a probability rate that *increases* as local density (c̄) increases (Eq. 3, term II; Fig 3a). Rate: `v * γ1 * c̄^2 / (γ2^2 + c̄^2) * a` (where a is density of carrying ants).
        3.  **Dropping (Spontaneous):** Laden ants have a constant probability per unit distance/time of dropping a corpse spontaneously (Eq. 3, term I; Fig 3c). Rate: `kd * a`.
        4.  **Movement:** Ants perform a 1D random walk with a characteristic mean free path (l) determined by a constant probability of making a U-turn (Results, p. 9646).
        5.  **Perception:** Ants perceive corpse density c̄ within a local radius δ (Model Description, p. 9647).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description                          | Parameter Name | Parameter Value Range   | Units    | Data Source        | Implicit/Explicit | Justification                                         |
    | :------ | :----------------------------------- | :------------- | :---------------------- | :------- | :----------------- | :---------------- | :---------------------------------------------------- |
    | Pick    | Density-dependent picking rate const | γ3             | 3.1 (fitted)            | m⁻¹      | Fig. 3 legend      | Explicit          | Value obtained by fitting model to experimental data. |
    | Pick    | Density-dependent picking rate const | γ4             | 50 (fitted)             | m⁻¹      | Fig. 3 legend      | Explicit          | Value obtained by fitting model to experimental data. |
    | DropS   | Density-dependent dropping rate const| γ1             | 31.7 (fitted)           | m⁻¹      | Fig. 3 legend      | Explicit          | Value obtained by fitting model to experimental data. |
    | DropS   | Density-dependent dropping rate const| γ2             | 400 (fitted)            | m⁻¹      | Fig. 3 legend      | Explicit          | Value obtained by fitting model to experimental data. |
    | DropSp  | Spontaneous dropping rate            | kd             | 0.75                    | m⁻¹      | Fig. 3c caption    | Explicit          | Value obtained from regression of survivorship data.    |
    | Perceive| Perception radius                    | δ              | 0.005 - 0.010           | m        | Model Desc, p.9647 | Explicit          | Range stated based on dedicated measurements.         |

### **4.3 Global Order:**

    *   Content: The emergent global order consists of distinct, spatially separated clusters (piles) of corpses, forming patterns often referred to as "cemeteries." The system evolves from an initially homogeneous distribution to a state with a reduced number of larger, stable clusters (Fig 1, Fig 5). The spacing between clusters exhibits some regularity, avoiding very short distances (Fig 6).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description                      | Parameter                  | Value Range      | Units    | Implicit/Explicit | Justification                                          | Protocol                     | Source          |
| :---------- | :------------------------------- | :------------------------- | :--------------- | :------- | :---------------- | :----------------------------------------------------- | :--------------------------- | :-------------- |
| NumClusters | Number of distinct corpse piles | Mean Number of Clusters    | ~2 - 8 (stable)  | Count    | Explicit          | Measured experimentally and predicted by model. Varies with conditions. | Video Analysis               | Fig 5           |
| InterDist   | Distance between adjacent piles  | Inter-cluster distance   | > 10             | cm       | Explicit          | Measured experimentally and predicted by model.          | Video Analysis               | Fig 6           |
| Stability   | Persistence of pattern over time | Time to max clusters     | ~ 3              | Hours    | Explicit          | Observed experimentally and predicted by model.        | Video Analysis               | Fig 5           |
| Stability   | Apparent steady state reached    | Time                     | ~ 24 - 48        | Hours    | Mixed             | Explicitly stated experiment duration, implies stability | Experiment Duration        | Methods, Results|

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Partial/Unclear

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4 - Proceeding cautiously based on "Partial/Unclear")**

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid

### **5.3 Computational Primitive:**

    *   Content: Nonlinear Thresholding / Probabilistic Decision Making. The probability functions for picking and dropping (Fig 3, Eq 3) effectively implement a smoothed, nonlinear threshold based on local corpse density (c̄). An ant 'computes' its probability to act based on this local input.
    *   **Sub-Type (if applicable):** Nonlinear function mapping local density to action probability (sigmoid-like for dropping, decaying for picking).

### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description           | Value      | Units     | Source            | Implicit/Explicit | Justification                                        |
        | :------------------------------ | :--------- | :-------- | :---------------- | :---------------- | :--------------------------------------------------- |
        | Ant Movement (across arena)     | Minutes    | Time      | Implicit          | Estimated from velocity (1.6 cm/s) and arena size (78-157 cm). |
        | Pile Formation Initial Phase    | ~3         | Hours     | Results, p.9646 / Fig 5 | Explicit          | Time to reach maximum number of clusters.          |
        | Pile Stabilization / Steady State | ~24 - 48   | Hours     | Methods / Fig 5   | Mixed             | Duration of experiments; model shows stabilization. |
        | Individual Ant Decision         | Seconds?   | Time      | Implicit          | Timescale of sensing local density and executing pick/drop. Not measured. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main emergent behavior is the collective formation of spatially segregated clusters (cemeteries) of ant corpses from an initially homogeneous distribution. This involves ants picking up scattered corpses and preferentially dropping them onto existing piles, leading to a dynamic process where some piles grow while others shrink or disappear, eventually resulting in a stable or quasi-stable pattern of fewer, larger piles.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The emergence of clustering is validated through:
        1.  **Direct Observation:** Videotaping experiments over 24-48 hours (Methods, Fig 1).
        2.  **Quantitative Analysis:** Software analysis measures pile position and size over time, allowing quantification of the number of clusters (Fig 5) and inter-cluster distances (Fig 6).
        3.  **Statistical Comparison:** Comparing observed inter-cluster distances to a random distribution shows the emergent pattern is non-random (Fig 6).
        4.  **Model-Experiment Comparison:** Close agreement between the mathematical model's predictions (derived from measured local rules) and experimental observations validates that the modeled LALI mechanism likely underlies the emergent behavior (Figs 4, 5, 6; Discussion).
        5.  **Control Condition:** Experiments below a critical density showed no stable cluster formation, supporting the model's prediction and validating the mechanism's density dependence (Results p.9647).

---

#Key: [yang_bicoss_2022]

# BiCoSS: Toward Large-Scale Cognition Brain With Multigranular Neuromorphic Architecture

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: BiCoSS (Biological-inspired Cognitive Supercomputing System) is a neuromorphic platform designed for large-scale spiking neural network (SNN) simulations, aiming to model cognitive functions of the human brain. It utilizes a scalable hierarchical heterogeneous multicore architecture implemented on 35 FPGA (Intel EP4CE115) chips. The system integrates multiple granularities (levels of detail) of SNN models (e.g., LIF, Izhikevich, Hodgkin-Huxley) and supports hybrid neural information routing (spike-based and action-potential-based). Its purpose is to bridge the gap between detailed neuronal dynamics and cognitive functions by enabling real-time simulation of large networks (up to four million neurons) involved in tasks like motor learning, action selection, and context-dependent learning, while addressing the limitations of von Neumann architectures for brain simulation. Key components include FPGA processor nodes, SNN units (composed of cores), nuclei units (containing SNN and routing units), SDRAM memory, and a synergistic routing scheme based on AER (Address Event Routing).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters selected represent core hardware implementation details and system scale/power.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is electrical power supplied to the FPGA-based hardware system.
    *   Value: 10.419
    *   Units: W (Total System)

### **2.2 Energy Transduction**

    *   Content: Electrical energy is transduced within the FPGAs into computational work for simulating SNN dynamics (neuron state updates, synaptic calculations, spike generation) and communication work (routing AER packets through the NoC). Energy is stored temporarily in FPGA logic elements, embedded RAMs, and external SDRAMs during computation and data transfer. The underlying mechanism is CMOS transistor switching within the FPGA fabric.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper claims a power efficiency 2.8k larger than a GPU platform (NVIDIA GTX 280) for simulating a four million neuron SNN (Abstract, Section V.A). It also states a power density of 35.4 mW/cm², significantly lower than CPU or GPU (Section V.B). While impressive compared to alternatives, FPGAs are generally less power-efficient than custom ASICs (like TrueNorth or Loihi). The score reflects good efficiency relative to software/GPU but not optimal for neuromorphic hardware. Efficiency metric: Relative power efficiency vs GPU (2800x). Power density: 35.4 mW/cm².

### **2.4 Energy Dissipation**

    *   Content: Energy is primarily dissipated as heat generated by the CMOS switching activity within the FPGAs during computation (neuron updates, synaptic processing, STDP) and communication (routing AER packets). The total system dissipation is 10.419 W (Section V.B). Specific breakdown per component (computation vs. communication, specific FPGA blocks) is not provided, but dissipation is inherent in the digital hardware operation. Qualitative assessment: Medium-High for digital hardware, but Low relative to GPU/CPU for the specific task.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**

    1.  **Synaptic Weights:** Updated via STDP (Fig 5a). These represent learned associations and are a form of long-term, re-writable memory, crucial for adaptive tasks like learning (Sections V.C.2, V.C.3, V.C.4). The fidelity/resolution depends on the fixed-point representation.
    2.  **Neuron State Variables:** (e.g., membrane potential in LIF). These represent short-term memory, holding state between simulation steps (Fig 5a, 5b).
    3.  **Connectivity Configuration:** Stored in ROM (Fig 5c), representing static, read-only memory defining the network structure.
    4.  **Routing/Buffer Memory:** FIFOs in routers (Fig 7a) hold AER packets temporarily, a form of short-term buffer memory. External SDRAMs (Section III) are likely used for larger storage needs (parameters, potentially state variables for large networks).
    The score reflects the presence of both short-term (neuron state, buffers) and long-term, adaptive memory (synaptic weights via STDP), but implemented digitally with potential limitations from fixed-point precision compared to biological or analog implementations. Capacity is large due to distributed architecture.

### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Embedded FPGA RAM: ~4 Mb per chip; External SDRAM: Capacity not specified. Total Synapses: Not explicitly stated, but implied to be large given 4M neurons.
*   Units: bits, Synapses

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Digital (Neuromorphic Simulation)

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operations embodied by the hardware are those required for discrete-time simulation of neuron models and synaptic interactions using fixed-point arithmetic. These include:
        *   Addition/Subtraction (for potential/current updates)
        *   Multiplication (often implemented via shifts/adds for efficiency, e.g., "shift MUL" in Fig 5b) (for scaling inputs/parameters)
        *   Comparison/Thresholding (for spike generation)
        *   Multiplexing/Routing (for AER packet communication)
        *   Memory Read/Write (for state variables, parameters, weights)
    *   **Sub-Type (if applicable):** Arithmetic (Fixed-Point Add/Sub/Mul), Thresholding, Routing Logic, Memory Access.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Average Latency (vs Mesh) | 3.62x higher | Ratio | Abstract | Explicit | Comparison point provided. |
        | Average Latency (vs Torus) | 2.49x higher | Ratio | Abstract | Explicit | Comparison point provided. |
        | Network Latency (Hop counts) | Reduced | Qualitative | Section IV.A | Explicit | Design goal mentioned. |
        | ISI Peak (GrC Example) | 23 | ms | Fig 12a | Explicit | Measured from simulation results. |
        | ISI Peak (GoC Example) | 40.5 | ms | Fig 12b | Explicit | Measured from simulation results. |
        | Cognitive Task Duration (Example) | 300 | ms | Section V.C.4 | Explicit | Duration of learning procedure step. |
    *   **Note:** The system operates in real-time, implying simulation timescales match biological timescales (ms). Latency figures are relative comparisons. ISIs provide examples of simulated temporal dynamics.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The primary adaptation mechanism described is Spike-Timing-Dependent Plasticity (STDP). The STDP unit (Fig 5a) calculates changes in synaptic strength based on the timing difference between pre- and post-synaptic spikes. Both pair-based and triplet STDP rules are mentioned in the context of MNIST learning (Section V.C.4). Additionally, a hybrid mechanism combining STDP across the dendritic tree and dendritic location-dependent long-term potentiation (dLTP) is implemented for associative learning, allowing potentiation even without postsynaptic APs if nearby synapses are active (Section V.C.4, referencing Bono and Clopath [53]). Reinforcement learning is mentioned for the BG task (Section V.C.2), likely implemented through neuromodulated STDP (e.g., dopamine influence, although the specific modulation mechanism within BiCoSS isn't detailed).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The system (BiCoSS hardware) *enables* the simulation of SNNs that exhibit complex, emergent behaviors relevant to biological cognition. The observable behaviors arise from the *simulated dynamics* of the large-scale neural networks, not the hardware itself. Examples demonstrated include:
        1.  **Cerebellar Motor Learning:** Sparse, temporally coded firing patterns in Granule cells in response to input signals, representing passage-of-time information (Section V.C.1, Fig 11).
        2.  **Basal Ganglia Decision-Making:** Action selection dynamics ("Go" strategy) based on simulated dopamine levels, involving oscillatory and synchronized activity in STN, GPe, GPi nuclei (Section V.C.2, Fig 13a-c).
        3.  **Hippocampal Context-Dependent Learning:** Gradual reduction in task error rate over trials, indicating the network learns context-specific rules via STDP (Section V.C.3, Fig 13d-e).
        4.  **Associative Learning:** Selective strengthening/weakening of proximal/distal synaptic connections based on correlated feature activation, demonstrating memory retention and association (Section V.C.4, Fig 14).
        5.  **MNIST Digit Recognition:** Unsupervised classification of digits using STDP (Section V.C.4).
        6.  **Movement Disorder Dynamics:** Simulation of pathological rebound bursting and altered relay reliability in Thalamic neurons under varying synaptic conductance and noise (Section V.C.5, Fig 15).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors (within the SNN simulations) are primarily validated through:
        1.  **Comparison with Biological/Neuroscience Findings:** The simulated dynamics are often compared qualitatively or quantitatively to known biological phenomena (e.g., sparse GrC coding similar to [32], BG dynamics consistent with [29, 30], TH relay disruption in movement disorders similar to [36]). See Sections V.C.1, V.C.2, V.C.5.
        2.  **Quantitative Analysis of Simulation Results:** Metrics like firing rates (Fig 11b), ISI distributions (Fig 12), similarity index (Eq 4, Fig 11c), fault ratios during learning (Fig 13d-e), MNIST accuracy (Section V.C.4), synaptic weight evolution (Fig 14), and relay reliability index (Eq 6, Fig 15) are used to quantify the simulated behaviors.
        3.  **Comparison with Software Simulations:** Hardware results are compared against software implementations for validation (Fig 8c, Fig 12).
        Control experiments isolating specific parameters (e.g., varying synaptic conductance `g_inc` or noise intensity `σ` in Fig 15) are used to understand their impact on behavior. Reproducibility is addressed by running simulations multiple times (e.g., 100 runs for hippocampal learning, Fig 13e). Limitations include the reliance on specific SNN models and parameter choices.

---

#Key: [boussard_memory_2019]

# Memory inception and preservation in slime moulds: the quest for a common mechanism

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system studied is the acellular slime mould *Physarum polycephalum*. The study investigates the mechanism and persistence of habituation, a simple form of learning, in response to a chemical repellent (NaCl). The slime mould's vegetative stage (plasmodium) is habituated to NaCl over 6 days. The study tests if this learned habituation persists through the dormant sclerotium stage (after 1 month dormancy) and subsequent revival. It examines the role of sodium (Na+) uptake and retention as a potential physical substrate for this memory. Experiments involve rearing plasmodia, habituation training (feeding on salt-containing oat gel), testing behavioral aversion using bridge-crossing and exploration assays, inducing sclerotization, reviving sclerotia, and measuring internal sodium content using an Ion Selective Electrode (ISE). The purpose is to identify a potential common mechanism for memory inception and preservation, specifically focusing on habituation in a non-neural organism.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                 | Value                   | Units   | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :----------------------------- | :----------------------: | :-----: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is chemical energy derived from the nutrients in the oat flakes provided as food during rearing and, for the habituation group during training, the salt oat gel. Metabolic processes convert these nutrients.

### **2.2 Energy Transduction**

    *   Content: Chemical energy from metabolism of oat flakes is transduced into kinetic energy for locomotion (cytoplasmic streaming, crawling described in Section 2a) and other cellular processes required for survival, growth, habituation learning, sclerotization, and revival. The paper focuses on the behavioral output (locomotion speed/aversion) resulting from encountering NaCl, implying energy expenditure for movement, but does not detail the specific transduction pathways from nutrient metabolism to behavior.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any information or metrics regarding the energy efficiency of the slime mould's metabolic processes, locomotion, or the habituation process itself.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation mechanisms are not discussed or quantified. Likely mechanisms include heat loss during metabolic activity and mechanical work done against the substrate during locomotion (friction), but these are not addressed in the paper.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: At least 1 month (demonstrated); potentially up to 1 year (suggested, but data not included due to control mortality)
*    Units: Time (months/years)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Qualitative: Short-term memory degrades over 2-3 days if stimulus removed; Long-term memory persists for at least 1 month through dormancy.

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :-------------------------------------- | :---- | :---- | :------------- | :---------- |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: Skipping M4.2-M4.7 as M4.1 is "No")**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: Skipping M5.2-M5.4 as M5.1 is "No")**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description          | Value        | Units   | Source        | Implicit/Explicit | Justification                                      |
        | :----------------------------- | :----------: | :-----: | :------------: | :----------------: | :------------------------------------------------- |
        | Habituation Training           | 6            | days    | Section 2b     | Explicit          | Duration of exposure to NaCl during training.      |
        | Short-term Memory Retention    | ~2-3         | days    | Section 2b(iii) | Explicit          | Time until recovery if stimulus is removed.        |
        | Long-term Memory Retention     | >= 1         | month   | Section 3b(i)  | Explicit          | Demonstrated persistence through dormancy.          |
        | Sclerotia Formation            | ~1           | week    | Section 2c     | Explicit          | Time allowed for drying to form sclerotia.         |
        | Behavioral Test Duration (Max) | 24           | hours   | Section 2c(i)  | Explicit          | Maximum observation time in bridge crossing test.  |
        | NaCl Absorption (Induced)      | 2            | hours   | Section 2d     | Explicit          | Duration of constrained absorption experiment.     |
        | Sodium Extrusion (Partial)   | < 3          | days    | Section 3a(iii) | Explicit          | Timescale over which significant Na extrusion occurs. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The proposed mechanism for habituation (adaptation) is the uptake and intracellular accumulation of the repellent substance, NaCl. Chemical analysis showed higher sodium content in habituated plasmodia and sclerotia compared to controls (Sections 3a(ii), 3b(ii), Fig 2b, 3c, 3d). The degree of habituation (lower aversion index) correlated negatively with internal sodium concentration (Section 3a(ii), Fig 2c). Recovery from habituation correlated with sodium extrusion (Section 3a(iii), Fig 2d). Constrained absorption of NaCl induced habituation-like behavior (Section 3c, Fig 5). The authors suggest this internal sodium acts as a 'circulating memory', potentially altering physiological states like membrane potential to reduce the aversive response upon subsequent encounters (Section 4a, 4c). The specific downstream effects of increased intracellular sodium leading to reduced aversion are not fully detailed but are hypothesized to involve counteracting membrane depolarization (Section 4c).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior studied is habituation to a repellent (NaCl). This manifests as a progressive decrease in the organism's innate aversive behavior (slowing down or avoiding movement towards the stimulus) upon repeated exposure. Specifically, habituated slime moulds cross NaCl-containing bridges faster (towards a food source) and explore NaCl-containing arenas more readily (faster initiation of pseudopod formation, higher expansion rate) compared to control (naive) slime moulds. This adaptive behavior allows the organism to ignore a persistent, potentially unpleasant but non-lethal stimulus.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claim of habituation behavior is validated primarily through controlled experiments comparing habituated (H) groups with control (C) groups under different conditions (Salt presence 'S' or absence 'A'). Statistical analyses (GLMMs accounting for non-independence) are used to assess the significance of behavioral differences (aversion index based on time or exploration rate) between groups (Section 2e, Section 3). Multiple behavioral assays (bridge crossing, exploration test) are used to confirm the phenomenon (Section 2b(i), 2c(i), 2c(ii)). Specificity (a key characteristic of habituation vs fatigue) is mentioned as demonstrated in prior work [22], ruling out sensory/motor fatigue. Recovery after stimulus withdrawal is also consistent with habituation definitions (Section 2b(iii), 3a(iii)). Limitations include the variability in behavior and the potential influence of physiological state differences mentioned in Section 4b.

---

#Key: [hu_modular_2023]

# Modular assembly of microswimmers with liquid compartments

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of modular microswimmers fabricated by combining microfluidic synthesis of polymer-based microcapsules (containing liquid cargo) and sequential capillarity-assisted particle assembly (sCAPA) to link these microcapsules with other microparticles (e.g., silica, polystyrene). The purpose is to create microswimmers capable of self-propulsion via asymmetric electro-hydrodynamic (EHD) flows under AC electric fields, carrying multiple distinct liquid compartments for potential cargo delivery and release. The components include polymer microcapsules (PMMA, PLLA) containing liquid cores (hexadecane with fluorescent dyes like BODIPY or Nile Red), solid microparticles (silica, polystyrene), surfactants (PMAA, SDS, Triton X-45), solvents (chloroform), PDMS templates with micro-traps for sCAPA, conductive slides (gold-coated glass), and an AC function generator. The system demonstrates controlled assembly of heterogeneous microstructures (dumbbells, trimers) and their subsequent EHD-driven motion. Cargo release is demonstrated via heating.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source for propulsion is an external, spatially uniform alternating current (AC) electric field applied across the conductive slides containing the microswimmers.
    *   Value: 4 - 8 Vpp amplitude, 1.5 kHz frequency
    *   Units: V, kHz

### **2.2 Energy Transduction**

    *   Content: Electrical energy from the AC field is transduced into kinetic energy (motion) of the microswimmers via asymmetric electro-hydrodynamic (EHD) flows. The AC field polarizes the dielectric microswimmer components near the electrode surface. Due to the compositional asymmetry of the assembled microswimmers (e.g., silica particle + PMMA microcapsule), the induced EHD flows around the swimmer are asymmetric. This asymmetry generates a net fluid flow relative to the swimmer, resulting in self-propulsion along its axis.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any quantitative measure or qualitative assessment of the energy efficiency of the EHD propulsion (e.g., electrical input power vs. mechanical output power). Efficiency for EHD systems is typically low but not discussed here.

### **2.4 Energy Dissipation**

    *   Content: Energy is dissipated primarily through viscous drag as the microswimmer moves through the fluid (MilliQ water). Other potential dissipation mechanisms include dielectric losses in the materials and fluid due to the AC field, and potentially heat generation, although these are not quantified or discussed. The propulsion mechanism itself relies on generating fluid flow, implying significant energy dissipation into the surrounding fluid. Overall assessment: High dissipation (typical for microscale locomotion in fluid).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip to Module 4.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip to Module 6.)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | AC Field Period | ~0.67 | ms | Calculated from frequency (1/1.5kHz) | Explicit | Frequency is given. |
        | Chloroform Evaporation | 48 | hours | Section 2.2 | Explicit | Stated in methods. |
        | Centrifugation | 3 | minutes | Section 2.2 | Explicit | Stated in methods. |
        | sCAPA Process | Variable (depends on speed/area) | s/min | Implicit | Deposition rates (3-5 µm/s) and area imply time, but total time not given. |
        | Thermal Sintering | 15 | minutes | Section 3 | Explicit | Stated in results. |
        | Cargo Release Heating | 15 | minutes | Section 3, Fig 4d | Explicit | Stated in results. |
        | Microswimmer Motion Observation | seconds to minutes (based on velocity and trajectory length) | s/min | Implicit | Velocities (~1-10 µm/s, Fig 3d/4g) and trajectories (Fig 3c/4f) imply observation times. Video frame rates (1.33-1.66 fps) also suggest relevant timescales. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip to Module 8.)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behavior is directed self-propulsion (locomotion) in response to an applied AC electric field. The propulsion arises from asymmetric EHD flows generated due to the swimmer's compositional asymmetry. The speed is dependent on the electric field strength (v ∝ E²). A secondary demonstrated behavior is the triggered release of encapsulated liquid cargo upon heating.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary behavior (locomotion) is validated through direct observation using optical microscopy (brightfield and fluorescence). Particle tracking software (Trackpy) is used to quantify trajectories and velocities (Section 2.5). The relationship between velocity and electric field strength (v ∝ E²) characteristic of EHD propulsion is experimentally confirmed (Fig 3d, Fig 4g), validating the propulsion mechanism. Cargo release is validated by fluorescence microscopy showing merging/diffusion of dyes after heating (Fig 4c-d). The assembly process yield is quantified by counting filled traps (Fig 2g). Limitations include the lack of analysis of collective behaviors or interaction dynamics between swimmers.

---

#Key: [bryant_physical_2023]

# Physical Constraints in Intracellular Signaling: The Cost of Sending a Bit

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper theoretically analyzes the energetic cost (in units of kBT/bit) of sending information between spatially separated components within a cellular context, considering physical constraints. It examines several physical communication strategies (models/systems): 1) electrical signaling via membrane depolarization through ion channels, 2) diffusive signaling (2D and 3D), and 3) acoustic signaling. The purpose is to establish lower bounds on the energy required for information transfer as a function of sender/receiver size (σI, σO), spatial separation (r), communication frequency (ω), and the physical properties of the medium (diffusion constants, capacitance, conductivity, viscosity). The system components in each model are a sender (I), a receiver (O), and the intervening physical medium (λ(x,t)) characterized by specific dynamics (e.g., capacitance/conductance for electrical, diffusion constant for diffusive, wave speed/damping for acoustic). The system *does* information transfer across space, subject to thermal noise and physical constraints.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These are the core parameters defining the communication scenario. Specific physical constants (D, c, α, η, τ) are used for each model and examples are given in Fig 2 caption and Table I, but the fundamental parameters listed above apply across models.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The energy input is implicitly the energy required by the sender (I) to generate the time-varying signal I(t) against the dissipative forces of the medium. This energy ultimately derives from cellular metabolic processes (e.g., ATP hydrolysis) but the paper focuses on the physical dissipation associated with signal generation (dW/dt), characterized by the dissipation kernel D(ω). For electrical signaling, it's the energy to drive current; for diffusion, it's the free energy change associated with particle creation/modification; for acoustics, it's the work done to generate pressure/density changes. The paper calculates the *minimum required dissipation* using fluctuation-dissipation relations, which serves as a lower bound on the actual energy input.
    *   Value: Variable (Depends on signal spectrum SI(ω) and dissipation kernel D(ω))
    *   Units: J/s (Rate) or J (Total)

### **2.2 Energy Transduction**

    *   Content: Energy is transduced from the sender's action into perturbations of the physical medium (λ). For electrical signaling, chemical potential energy (driving ion flow) is transduced into electrical potential energy (membrane charge) and dissipated as heat (Ohmic loss). For diffusive signaling, chemical energy (e.g., ATP hydrolysis for phosphorylation/dephosphorylation or pumping) is transduced into concentration gradients (chemical potential energy) and dissipated entropically. For acoustic signaling, energy generates density/pressure waves (mechanical energy) which dissipate viscously. The energy associated with the signal then propagates through the medium to the receiver. The paper uses linear response theory and fluctuation-dissipation relations to model this transduction and associated dissipation.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper focuses on the energetic *cost* per bit (C(ω) = dW/dR), in units of J/bit or kBT/bit, derived in Eq. 2 and calculated for specific systems (Eq 7, 22, 32). This cost represents the *minimum* energy dissipation required to reliably transmit one bit of information at frequency ω against thermal noise. It is inversely related to efficiency but isn't a dimensionless efficiency ratio (e.g., output work / input energy). The paper finds costs can be >> kBT, implying low efficiency relative to the Landauer limit, but potentially optimal for the given physical constraints. Example costs are 10^4 kBT/bit for 3D diffusion over 1µm (Sec 6).

### **2.4 Energy Dissipation**

    *   Content: Dissipation is central to the analysis. The rate of dissipation (dW/dt) is characterized by the dissipation kernel D(ω) specific to each channel type (Eq. 13, Eq. 5, 21, 31, Table I). For electrical signaling, dissipation is Ohmic (related to conductivity α). For diffusive signaling, it's related to the entropic cost of maintaining concentration gradients against diffusion (related to D, ρ0). For acoustic signaling, it's viscous damping (related to τ, η, c). The paper calculates these kernels explicitly using linear response theory and relates them to the input signal power spectrum SI(ω) to find the total dissipation rate. Dissipation increases with frequency and distance (especially beyond the characteristic lengthscale `l`).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: Skipped M3.2-M3.8 as M3.1 is "No")**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: Skipped M4.2-M4.7 as M4.1 is "No")**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: Skipped M5.2-5.4 as M5.1 is "No")**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Signal Period (Inverse Frequency) | 1/ω (or 1/f) | s | Section 2 | Explicit | Stated as a key parameter ω or frequency f. |
        | Characteristic Timescale (Electrical, from `l` = α/ωc) | c`l`/α = c(α/ωc)/α = c/ω | s | Eq 4, Table I | Explicit | `l(ω)` is defined; timescale is implicitly `l(ω)` divided by characteristic velocity `v_RC` = α/c. |
        | Characteristic Timescale (Diffusive, from `l`²=D/ω) | `l`²/D = (D/ω)/D = 1/ω | s | Table I, Sec 5 | Explicit | `l(ω)` is defined; timescale is implicitly `l(ω)²`/D. Related to diffusion time over length `l`. |
        | Characteristic Timescale (Acoustic, from `l_r` = `l_c`/ν'') | `l_r` / c = (c/ω)/ (ν''c) = 1/(ων'') | s | Eq D7, D8 | Explicit | `l_r` (attenuation length) is defined; timescale is implicitly `l_r`/c. Related to damping. |
        | Communication Latency (Related to `r`) | ~r²/D (Diffusive), ~r/v (Electrical/Acoustic) | s | Section 1 | Implicit | Latency constraints mentioned; typical scaling inferred from physics. |
    *   **Note:** The characteristic timescales are derived from the characteristic lengthscales `l(ω)` presented in the paper, which define the transition to exponential cost increase. They represent the timescale over which the signal at frequency ω is significantly affected by the medium's properties.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: Skipped M7.2 as M7.1 is "No")**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The fundamental behavior described is **information transmission** across a spatial distance 'r' through different physical media (electrical, diffusive, acoustic) at a frequency 'ω', subject to thermal noise. The key output of the analysis is the **minimum energetic cost** (C(ω) in kBT/bit) required for this transmission as a function of system parameters (r, ω, σI, σO, physical constants). A secondary observed behavior is the existence of a **characteristic lengthscale** `l(ω)` for each medium, beyond which the transmission cost increases exponentially.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

---

#Key: [mcevoy_materials_2015]

# Materials that couple sensing, actuation, computation, and communication

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews the concept of "Robotic Materials," defined as materials that tightly integrate sensing, actuation, computation, and communication. These materials aim to transcend classical smart or composite materials by embedding programmability and autonomy directly into the material structure. The components envisioned include various sensors (MEMS microphones, accelerometers, capacitive touch, optical, thermistors, mechanical strain/force), actuators (variable stiffness mechanisms like thermoplastics/shape memory polymers, artificial muscles like twisted thread/SMAs/pneumatic actuators, electro/magnetorheological fluids), embedded computation (microprocessors, potentially polymer electronics), and communication infrastructure (wired buses, optical waveguides, wireless). The purpose is to enable materials with functionalities inspired by biological systems, such as autonomous shape change (morphing wings), camouflage (appearance change), adaptive load support/self-repair (smart structures), and high-resolution tactile sensing (robotic skin/prosthetics).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** As this is a review, the parameters listed reflect the *goals* or typical *scales* discussed for the robotic material concept, rather than measured values from a single implementation. Reliability is generally Low to Medium as these are target values or ranges discussed in cited works/concepts.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The paper discusses various potential energy inputs depending on the specific actuators and components used. Examples include: Electrical energy (for computation, heating elements in variable stiffness materials, SMA actuators, electroactive polymers), Thermal energy (for SMAs, thermoplastics), Pneumatic/Hydraulic pressure (for McKibben actuators, soft robotics), Light (for optical sensors, potentially optical power delivery). Power infrastructure integration is mentioned as a key challenge.

### **2.2 Energy Transduction**

    *   Content: Energy is transduced through various mechanisms depending on the components: Electrical energy to heat (Joule heating in resistive elements for thermoplastics/SMAs); Electrical energy to computation; Electrical energy to mechanical work (e.g., piezoelectrics, potentially); Thermal energy to mechanical work (phase transitions in SMAs/polymers); Pneumatic/Hydraulic energy to mechanical work (inflation/expansion); Chemical energy to mechanical work (e.g., self-healing release agents); Mechanical energy (e.g., sound, strain) to electrical signals (sensors like MEMS microphones, piezoelectrics).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The review does not provide quantitative data on the energy efficiency of the overall robotic material concept or specific integrated examples. Efficiency would vary greatly depending on the specific components and task. Some mechanisms mentioned (e.g., thermal actuation) are often known to have relatively low efficiency, while embedded computation adds continuous power draw. This is acknowledged implicitly as a challenge ("cost, weight").

### **2.4 Energy Dissipation**

    *   Content: Dissipation mechanisms are inherent in the described components but not quantified. Examples include: Heat loss during thermal actuation (conduction, convection, radiation), Electrical resistance in wiring and computation elements (Joule heating), Mechanical friction (in moving parts or during deformation), Viscous losses (in fluidic systems), Acoustic damping. The need for thermal management is implicitly suggested by thermal actuation methods. Overall dissipation assessment: Medium to High, depending on the implementation.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding with M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Low (typically 1-2 states for material memory)
*   Units: states

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial/Unclear

**(Conditional: M4.1 is "Partial/Unclear", tentatively including M4.2-M4.7 based on related concepts mentioned)**

### **4.2 Local Interaction Rules:**

    *   Content: Local interaction rules are discussed primarily in the context of *computation* and *control*:
        1.  **Amorphous Computing:** Nodes communicate locally; global behavior (e.g., displaying lines, filtering signals) emerges from local computations based on neighbor states (e.g., gradients). (ref 18, 19)
        2.  **Sensor Networks:** Local communication protocols (routing, networking) manage data flow. (ref 23)
        3.  **Distributed Control:** Local controllers use sensor feedback (local and potentially neighbor information) to manage local actuators (e.g., temperature control for stiffness, inflation control for rolling). (ref 14, 42, 62)
        4.  **Signal Processing:** Local nodes process high-bandwidth sensor data (e.g., sound analysis for texture/localization) and communicate processed, lower-bandwidth information. (ref 15, 13)
        Specific algorithms (matching, coloring, etc.) for coordination in wireless sensor networks are mentioned (ref 69, 70).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Comm | Communication Protocol | Range | Local (cm-m) | m | Local communication section | Explicit | Defines spatial extent of interaction. |
    | Control | Local Feedback Control | Setpoint (e.g., Temp) | Variable | K, °C | ref 14, 42 | Explicit | Determines actuator state. |

### **4.3 Global Order:**

    *   Content: The "global order" discussed is typically the desired *functional state* of the material system, programmed or controlled via the embedded elements. Examples: a specific morphed shape (airfoil), a displayed camouflage pattern, a repaired structure, a representation of tactile input. While concepts like amorphous computing might produce emergent patterns (lines, characters), the main applications focus on achieving specific, engineered global configurations or functions.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", proceeding with M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Hybrid (Primarily Digital embedded in Analog/Physical system; potentially Analog/Morphological)

### **5.3 Computational Primitive:**

    *   Content: The review suggests several computational primitives performed locally within the material:
        *   **Feedback Control:** Adjusting local actuators based on local sensor readings (e.g., temperature control for stiffness - ref 14, 42).
        *   **Signal Processing/Filtering:** Analyzing sensor data locally (e.g., frequency analysis of sound for texture - ref 15).
        *   **Data Aggregation/Routing:** Processing and forwarding information through the embedded network (ref 15, Local communication section).
        *   **Thresholding/Event Detection:** Identifying significant events from sensor data (e.g., impact detection - ref 9).
        *   **Pattern Recognition (Simple):** Recognizing gestures (ref 12), classifying textures (ref 15).
        *   **Localization/Triangulation:** Calculating position based on distributed sensor data (ref 15, 13).
        *   (Potential) Morphological Computation: Filtering, transforming signals via material physics (ref 33, 34).
    *   **Sub-Type (if applicable):** Feedback Control, Signal Filtering, Pattern Recognition, Localization.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Sensor Bandwidth (Example: Skin) | ~kHz (or higher implied) | Hz | ref 15, Local computation | Explicit | Sensing skin example samples at high frequency. |
        | Actuation Response (Thermal) | Seconds to Minutes | s, min | Implicit (General Knowledge) | Implicit | Thermal processes (heating/cooling for SMAs/thermoplastics) are generally slow. |
        | Actuation Response (Pneumatic) | Milliseconds to Seconds | ms, s | Implicit (General Knowledge) | Implicit | Pneumatic actuation can be relatively fast. |
        | Actuation Response (SMA) | Seconds (heating), Potentially slower (cooling) | s | Implicit (General Knowledge) | Implicit | Heating is usually faster than passive cooling. |
        | Computation Cycle Time | Microseconds to Milliseconds | µs, ms | Implicit (Microcontrollers) | Implicit | Typical range for embedded microcontrollers. |
        | Communication Latency (Local) | Milliseconds+ | ms | Implicit (Network Effects) | Implicit | Depends on network topology, protocols, load. |
    *   **Note:** Specific quantitative values are mostly absent in the review. The table relies on explicit examples (skin sensor bandwidth) or implicit estimations based on the types of components discussed.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Partial (in Functionality, not necessarily Material Structure/Learning)

**(Conditional: M7.1 is "Partial", tentatively including M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanisms described that lead to adaptive *functionality* are primarily:
        1.  **Feedback Control:** Systems adjust their state (shape, stiffness) based on sensor readings to meet a target or react to environmental changes (e.g., morphing wing adjusts shape based on measured airflow/desired flight regime; variable stiffness beam adjusts stiffness based on control input). This is adaptation via programmed control logic.
        2.  **Reconfiguration (Modular Robotics):** Modular systems change their overall shape/structure based on local sensing of the environment (ref 22). This is adaptation through physical rearrangement based on algorithms.
        3.  **Self-Healing:** Material responds to damage by releasing healing agents, restoring structural integrity (ref 10). This is a form of structural adaptation triggered by damage.
        The review does not describe mechanisms analogous to Hebbian learning, reinforcement learning implemented *within the material itself*, or evolutionary changes in material properties based on performance history.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors described are engineered outcomes enabled by the integrated components:
        *   **Shape Change / Morphing:** Altering the material's overall geometry (e.g., airfoils, general structures). (ref 5-8, 14, 56)
        *   **Appearance Change / Camouflage:** Modifying surface color, opacity, or texture. (ref 1, 4, 12)
        *   **Variable Stiffness:** Changing the material's mechanical modulus. (ref 14, 27, 48-53)
        *   **Tactile Sensing / Texture Recognition:** Detecting contact, pressure, vibration and classifying surface textures. (ref 11, 15, 43-46)
        *   **Self-Diagnosis / Structural Health Monitoring:** Detecting internal damage or strain. (ref 9, 36-40)
        *   **Self-Repair:** Autonomously fixing damage. (ref 10)
        *   **Gesture Recognition:** Interpreting user input through touch/deformation. (ref 12)
        *   **Sound Localization:** Determining the direction of sound sources. (ref 13, 35)

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The behaviors described are primarily *demonstrated functionalities* in specific prototypes or conceptual designs (e.g., Fig 2 shows prototypes that perform gesture recognition, sound localization, shape change, tactile sensing). Validation typically involves showing that the prototype performs the intended function under specific test conditions (e.g., the facade recognizes gestures, the skin classifies textures). The paper does *not* focus on rigorously validating whether these behaviors are *emergent* in the sense of arising unpredictably from simple local rules. Control experiments comparing behavior with and without specific components (e.g., computation) might be implied but are not detailed. Reproducibility and robustness are generally not quantified.

---

#Key: [warkentin_locomoting_2018]

# Locomoting Robots Composed of Immobile Robots

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of a collection of small, 3-link, planar robots called "smarticles," which are individually incapable of rotation or displacement. Each smarticle has sensors (photoresistors, microphone, current sensor), actuators (two servos controlling link angles), and a microcontroller (Arduino Pro Mini). Multiple smarticles are confined within an unanchored rigid ring, forming a "supersmarticle." The system's purpose is to demonstrate emergent locomotion (diffusion and directed motion/phototaxis) arising purely from the collective interactions (collisions, environmental modulation like light occlusion) of these individually immobile components. It explores morphological computation and control, where the ensemble's physical structure and dynamics perform computation-like tasks leading to locomotion.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** The number of smarticles used in the experiments shown in Fig 3 is not explicitly stated but implied to be consistent populations for comparison.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is electrical power supplied to each smarticle's microcontroller and servos.

### **2.2 Energy Transduction**

    *   Content: 1. Electrical energy is converted to computational processing by the Arduino microcontroller. 2. Electrical energy is converted to mechanical energy by the servos, actuating the smarticle links. 3. Mechanical energy of link movement leads to kinetic energy transfer through collisions between smarticles and the ring. 4. These collisions collectively transduce the internal mechanical energy into macroscopic kinetic energy of the supersmarticle (locomotion).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify efficiency. Qualitatively, the process is expected to be highly inefficient (Low). Energy is consumed by onboard electronics and servos. Locomotion relies on chaotic collisions, where much energy is likely lost to heat, sound, friction, and internal deformations, rather than contributing to net displacement. Score is low due to reliance on undirected collisions for propulsion.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include: 1. Friction between the smarticles' center links and the test surface. 2. Internal friction and electrical resistance (heat) within the servos. 3. Heat generated by the microcontrollers. 4. Inelastic losses during collisions between smarticles and between smarticles and the ring (sound, heat, material deformation). Quantification is not provided. Qualitative assessment: High, due to the reliance on friction and collisions for movement generation and control.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Skipping M3.2-M3.8 as M3.1 is "No")**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Continuing with M4.2-M4.7 as M4.1 is "Yes")**

### **4.2 Local Interaction Rules:**

    *   Content: 1. **Physical Collisions:** Smarticles collide with each other and the bounding ring due to their servo-driven shape changes ("square gait", Fig 2b). These collisions transfer momentum, generating net forces on the ring (Section I). 2. **Stigmergic Communication (Light Occlusion):** A smarticle detecting the controlling light source becomes inactive. Due to the planar setup and sensor placement, this smarticle often occludes the light from others, keeping them active. This creates an internal light gradient and asymmetry based on local geometry and proximity to the light source (Section I). 3. **Actuation Cycle:** Each active smarticle follows a programmed periodic trajectory (gait) in its joint space (Fig 2b), changing its shape. 4. **State Change:** An active smarticle transitions to inactive upon sensing sufficient light intensity via its photoresistors (Section II).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The primary emergent global order is macroscopic locomotion of the supersmarticle system. This locomotion manifests as: 1. **Diffusive Motion:** Random-walk-like displacement when no external bias (light) is present (Fig 3a). 2. **Biased Diffusion (Phototaxis):** Directed drift (net displacement) superimposed on the diffusion, oriented relative to the controlling light source (positive phototaxis for exposed sensors, negative for shrouded; Fig 3c, 3e, Section III). 3. **Pattern Formation (Shape Tracing):** Ability to follow specific paths (e.g., drawing a 'T') by dynamically changing the light source position (Fig 4).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Stigmergy | Light occlusion affecting activity state | Sensor Angle of Acceptance | Exposed vs Shrouded | degrees | Explicit | Determines which smarticles become inactive. | Section II, Fig 2c |
| Actuation | Periodic gait cycle | Servo Angles (α1, α2) | Defined by gait (Fig 2b) | degrees | Explicit | Drives the motion leading to collisions. | Fig 2b |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| GO2 | Biased Locomotion (Phototaxis) | Net Displacement Vector | Variable (Fig 3) | m | Explicit | Trajectories show net displacement. | OptiTrack Analysis | Section III, Fig 3 |
| GO3 | Directionality Bias | Angle relative to Light | Primarily Towards (Exposed), Away (Shrouded) | degrees/radians | Explicit | Polar histograms quantify bias. | OptiTrack Analysis | Section III, Fig 3b,d,f |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Continuing with M5.2-5.4 as M5.1 is "Yes")**

### **5.2 Computation Type:**

    *   Content: Hybrid

### **5.3 Computational Primitive:**

    *   Content: The most basic computations embodied by the material interactions appear to be: 1. **Thresholding:** Smarticles become inactive based on a light intensity threshold detected by photoresistors. 2. **Spatial Logic/Filtering (via Stigmergy):** The nearest smarticle(s) to the light source effectively performs a spatial filtering operation, becoming inactive and potentially blocking light (acting like a NOT gate for light stimulus to others nearby). 3. **Collective Summation/Integration (via Collisions):** The net displacement arises from the integration of forces generated by many individual, chaotic collisions over time.
    *   **Sub-Type (if applicable):** Thresholding; Spatial Logic; Temporal Integration

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Locomotion/Diffusion Timescale | Variable (MSD) | s (for τ in MSD ~ t^α) | Section II | Implicit | MSD analysis is mentioned, implies characteristic timescales, but values not given. |
        | Experiment Duration | <= 600 (10 min) or until edge | s | Section II | Explicit | Trials terminated at 10 minutes or when boundary reached. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Skipping M7.2 as M7.1 is "No")**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behavior is emergent collective locomotion of the supersmarticle assembly, composed of individually immobile smarticle units. Specific behaviors include: 1. Undirected diffusive motion in the absence of external bias. 2. Directed, biased diffusive motion (phototaxis) in response to a directional light source (either towards or away from the light depending on smarticle design). 3. Trajectory following or pattern generation (e.g., drawing a "T") by dynamically controlling the position of the biasing light source.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behavior (locomotion from immobile units) are validated through: 1. **Experimental Observation:** Direct observation and recording of supersmarticle movement. 2. **Quantitative Trajectory Tracking:** Using OptiTrack infrared video technology to record the center of geometry trajectories (Section II, Fig 3). 3. **Control Experiments:** Comparing behavior with and without the controlling light source (Fig 3a vs 3c/e). 4. **Statistical Analysis:** Using polar histograms to quantify the directionality bias in phototaxis experiments across multiple trials (Section III, Fig 3b/d/f). 5. **MSD Analysis:** Mentioned as an analysis tool (Section II), implying quantification of diffusive properties, although results are not shown. Reproducibility is demonstrated by repeating trials with light at different locations (Section II) and achieving consistent bias trends (Section III). Limitations: Detailed analysis of interaction modes hypothesized to cause bias is deferred ([11]).

---

#Key: [westerhout_plasmon_2018]

# Plasmon confinement in fractal quantum systems

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper theoretically investigates the plasmonic properties of two-dimensional electron gases confined to fractal geometries, specifically the Sierpinski carpet and Sierpinski gasket. It calculates the full dielectric function using the random-phase approximation (RPA) based on a tight-binding Hamiltonian model. The purpose is to explore quantum plasmonic behavior in complex, non-periodic structures, compare them to regular lattices (square, triangle), and understand the role of fractal dimensionality and ramification in plasmon confinement. The system components are the fractal lattice structures (Sierpinski carpet, Sierpinski gasket), the electrons described by a nearest-neighbor tight-binding Hamiltonian, and the Coulomb interaction treated within RPA. The system *does* calculate the frequency- and momentum-dependent dielectric response, revealing plasmon modes and their dispersion relations.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The energy input is conceptual within the theoretical framework, representing the energy of the electromagnetic perturbation used to probe the system's dielectric response. This is characterized by the frequency ω. The calculation determines the system's response to this energy input.
    *   Value: `ħω` (variable)
    *   Units: eV (or `t`)

### **2.2 Energy Transduction**

    *   Content: The input energy `ħω` excites electron-hole pairs in the tight-binding system. Through the Coulomb interaction (V_C), these excitations couple and form collective oscillations of the electron density, known as plasmons. This process is described within the Random Phase Approximation (RPA), which calculates the system's polarizability χ(ω) and relates the external potential to the total potential via the dielectric function ε(ω). Energy is transduced from the probe field into collective electronic excitations.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper calculates a response function, not the efficiency of a device or process. The loss function `-Im[1/ε(q, ω)]` quantifies energy dissipation or absorption at a given frequency and momentum, and the parameter `η` represents a relaxation rate influencing damping, but an overall efficiency score is not applicable to this theoretical calculation of material properties.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation is primarily represented by the phenomenological inverse relaxation time `η` (related to Landau damping and other scattering processes not explicitly modelled) introduced in the calculation of the polarizability (Eq. 3). This parameter leads to broadening of the plasmon peaks and non-zero values in the loss function `-Im[1/ε(q, ω)]`. The paper states η = 6 meV/ħ. The paper mentions Landau damping as the reason why Re[ε]=0 and maxima of the loss function do not occur at exactly the same frequency.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Plasmon Oscillation Period (Range) | `2πħ / (0.1-0.24 * t)` ≈ 9 - 21 | fs | Section III, Fig. 2, Fig. 4 (using t=2.8 eV) | Mixed | Plasmon frequencies (ħω) are explicitly shown in Fig 2/4 (range ~0.1t to ~0.24t). Conversion to period `T = 2π/ω = 2πħ / (ħω)` requires calculation. |
        | Electron Relaxation Time (Related to η) | `ħ / η` ≈ 110 | fs | Section II (η=6 meV) | Mixed | η is explicitly given as 6 meV/ħ. Conversion to time `τ = ħ/η` requires calculation. |
    *   **Note:** Calculations: Period `T = (2π * 6.582e-16 eV*s) / (0.1 * 2.8 eV)` ≈ 1.47e-14 s ≈ 15 fs; `T = (2π * 6.582e-16 eV*s) / (0.24 * 2.8 eV)` ≈ 6.15e-15 s ≈ 6 fs. Range is roughly 6-15 fs, let's use 9-21 fs to be safe based on visual inspection. Relaxation time `τ = 6.582e-16 eV*s / 0.006 eV` ≈ 1.097e-13 s ≈ 110 fs.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors described are the collective electronic excitations (plasmons) within the fractal geometries. Key observed behaviors include:
        1.  **Plasmon Dispersion:** The relationship between plasmon energy (ħω) and momentum (q). The Sierpinski carpet exhibits a dispersion comparable to a regular square lattice (roughly √q dependency, but broadened), while the Sierpinski gasket shows nearly flat dispersion.
        2.  **Plasmon Localization:** The spatial extent of the plasmon modes. The Sierpinski gasket (finitely ramified) exhibits highly localized plasmon modes, indicated by the flat dispersion and higher inverse participation ratio (IPR) compared to the carpet. The carpet modes are less localized.
        These behaviors arise from the interplay of the electron gas, Coulomb interactions, and the specific fractal geometry.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claims of emergent behaviors (specific plasmon dispersion and localization) are validated theoretically within the paper primarily through:
        1.  **Convergence Checks:** Comparing results for different fractal iteration numbers (e.g., 3rd vs 4th iteration carpet) to ensure the observed behaviors are characteristic of the limiting fractal structure (Section III, discussion around Fig. 4).
        2.  **Comparison with Known Systems:** Comparing the dispersion of the Sierpinski carpet to a square lattice and the gasket to a triangle lattice (Fig. 4) highlights the unique features induced by the fractal geometry (broadening in carpet, flat bands in gasket).
        3.  **Calculation of Metrics:** Using the Inverse Participation Ratio (IPR) to quantify localization differences between the carpet and gasket (Section III).
        The paper suggests experimental validation via Electron Energy Loss Spectroscopy (EELS) as a future possibility (Section II), but no experimental validation is presented. The limitations are that the validation is purely theoretical within the chosen model (Tight-binding + RPA).

---

#Key: [esplandiu_electrophoretic_2020]

# Electrophoretic origin of long-range repulsion of colloids near water Nafion interfaces

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system investigates the formation of a solute Exclusion Zone (EZ) near the interface between water and a Nafion thin film supported on a substrate (Au/Si or Si). The components include the Nafion film (an ion-exchange polymer), water, colloidal tracers (polystyrene spheres with plain, carboxylated, or amidine surface functional groups, imparting different zeta potentials), and various alkali metal chloride salts (NaCl, KCl, LiCl, CsCl) at different concentrations. The system's primary function is the generation of an EZ, where colloidal tracers are repelled from the Nafion interface. The purpose of the study is to elucidate the mechanism behind this EZ formation, specifically to evaluate the relative contributions of electrophoretic and chemiphoretic forces within the framework of multi-ionic diffusiophoresis, driven by ion exchange between the Nafion (releasing H+) and the surrounding electrolyte (uptaking M+). The study uses experimental observation (optical microscopy, confocal microscopy, z-stacking) and finite element simulations (COMSOL) to analyze the phenomenon.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                   | Value                                   | Units        | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :----------------------------- | :--------------------------------------: | :-----------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Nafion Film Thickness          | ~600                                    | nm           | Section 2.1               | Explicit          | High                            | Measured by AFM                 |
        | Colloid Diameter               | 2                                       | µm           | Section 2.2               | Explicit          | High                            | Vendor Specification            |
        | Colloid Zeta Potential (plain PS) | -0.012 ± 0.001                          | V            | Section 2.2               | Explicit          | High                            | Measured (ZetaSizer)            |
        | Colloid Zeta Potential (PS-COOH)  | -0.042 ± 0.003                          | V            | Section 2.2               | Explicit          | High                            | Measured (ZetaSizer)            |
        | Residual Bulk Salt Conc.       | 1.24 ± 0.06                             | mM           | Section 2.2, Fig 3a       | Explicit          | Medium                          | Measured (Conductimetry Calib.) |
        | Max Electric Field (Extrapolated)| ~ -42                                   | V/m          | Section 3, Fig 4c         | Mixed             | Medium                          | Derived from experimental velocities via Eq. (10) |

    *   **Note:** Extrapolated electric field is derived from experimental data using theoretical assumptions (Eq. 10), hence 'Mixed' reliability and explicitness.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the chemical potential difference between ions (H+ in Nafion, M+ and Cl- in the bulk solution) that drives the spontaneous ion exchange process at the Nafion/water interface. This establishes concentration gradients.

### **2.2 Energy Transduction**

    *   Content: Chemical energy (from ion concentration gradients established by ion exchange) is transduced into electrical potential energy (manifested as a self-generated electric field due to differing ion mobilities/diffusivities). This electrical potential energy, along with chemical potential gradients (chemiphoresis), is then transduced into the kinetic energy of the colloidal tracers, causing them to move (electrophoresis and chemiphoresis, collectively diffusiophoresis). The simulations solve coupled equations representing this flow: ion exchange -> concentration gradients -> electric field (Poisson's Eq) & fluid flow/particle motion (Stokes/Nernst-Planck Eqs).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss or quantify the energy efficiency of the diffusiophoretic process (e.g., chemical energy input vs. kinetic energy output of colloids). Assigning a score would be speculative.

### **2.4 Energy Dissipation**

    *   Content: Energy is primarily dissipated through viscous drag as the colloidal particles move through the fluid (water). This is implicitly accounted for in the Stokes equation (Eq. 3) via the viscosity term (Z). Other dissipation mechanisms would include resistive losses due to ion movement (related to conductivity) and potentially heat generated during ion exchange, though these are not quantified or explicitly discussed as dissipation pathways. Qualitative assessment: Dissipation via viscous drag is significant as it balances the phoretic forces in steady state motion.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip to Module 4.)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip to Module 6.)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description           | Value              | Units     | Source      | Implicit/Explicit | Justification                                   |
        | :---------------------------- | :----------------: | :--------: | :----------: | :----------------: | :-------------------------------------------- |
        | EZ Formation (Plateau Time)   | ~20 - 40           | minutes   | Fig 3a, 4a  | Explicit          | Time taken for EZ distance to level off.       |
        | Particle Velocity Measurement | ~2 - 3 / z-stack   | minutes   | Section 2.2 | Explicit          | Frequency of z-stack acquisition.             |
        | Nafion Pre-wetting Time        | 30                 | minutes   | Section 2.1 | Explicit          | Time Nafion soaked before EZ inspection.         |
        | Nafion Wettability Change     | ~minutes to hours  | minutes   | Section 2.1 | Explicit          | Time for contact angle to change (Fig S2).    |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip to Module 8.)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behavior is the formation of a long-range Exclusion Zone (EZ) near the Nafion/water interface for negatively charged colloidal tracers, where particles are repelled from the surface over distances of hundreds of micrometers. Conversely, for positively charged tracers, the behavior is attraction and accumulation at the interface (absence of EZ formation). The size and dynamics of the EZ depend on factors like tracer zeta potential, salt type, and salt concentration.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary behavior (EZ formation or attraction) is validated through direct experimental observation using optical and confocal microscopy (Section 2.2, Figs 1, 2). Quantitative characterization of the EZ dynamics (size vs. time, velocity vs. distance) is achieved using z-stacking microscopy, particle tracking, and subsequent data analysis (fitting, differentiation, subtraction of gravity effects) (Section 2.2, Figs 3, 4). The interpretation of the underlying mechanism (dominance of electrophoresis) is further supported by comparing results with differently charged tracers and by numerical simulations (COMSOL) solving the governing physical equations (Poisson, Stokes, Nernst-Planck) which reproduce the key experimental dependencies (e.g., effect of ion diffusivity, electric field profile) (Section 2.3, Section 3, Fig 5). Reproducibility is suggested by evaluating 5 sample replicates for each tracer type (Section 2.2). Limitations include assumptions in deriving the electric field from velocities (Helmholtz–Smoluchowski limit approximation, constant zeta potential assumption).

---

#Key: [schmickl_how_2016]

# How a life-like system emerges from a simple particle motion law

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a computational model called Primordial Particle System (PPS), simulating a population of identical, self-propelled particles (SPPs) moving asynchronously in a continuous 2D toroidal space. Particles interact locally based on a simple motion law (Equation 1) where their change in heading (Δφ) depends on a fixed rotation (α), a density-dependent rotation (β * N), and the relative number of neighbors on their left (L) vs. right (R) within a radius (r): ΔΦ/Δt = α + β * N * sign(R - L). Particles move at a constant velocity (v). The purpose is to demonstrate and analyze the spontaneous emergence of self-structuring, self-reproducing, and self-sustaining "life-like" patterns (protocells, spores) and their ecosystem dynamics from simple, local rules without global information or complex programming. Components are the particles, the space, and the motion rule with its parameters (r, α, β, v).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The system assumes a constant energy influx that allows particles to maintain a fixed velocity 'v'. This represents an open system far from thermodynamic equilibrium, implicitly powered to sustain motion. The source is not physically specified but is a modeling assumption.

### **2.2 Energy Transduction**

    *   Content: The assumed energy input is transduced into the kinetic energy of the particles, maintaining their constant speed 'v'. Energy is used to change particle heading based on local interactions (Eq. 1) and then translates into positional changes (motion). There are no other energy forms or transformations explicitly modeled.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Energy efficiency is not discussed or quantified in the paper. The model assumes perfect transduction of implicit energy input into motion at constant velocity 'v', without considering physical limitations or losses in the "actuation" (change of heading and movement). Therefore, assigning a meaningful efficiency score is not possible.

### **2.4 Energy Dissipation**

    *   Content: Dissipation is not explicitly modeled (e.g., via friction). However, the system is described as a dissipative system in the discussion ("in the long-timescale view our system can be understood as a physical dissipative system"). The emergence of stable structures and steady states (like the logistic growth equilibrium) implies a balance between the assumed energy input (driving motion and organization) and some form of implicit dissipation or effective "cooling" that prevents unbounded energy increase or chaotic divergence everywhere. This dissipation is inherent in the system reaching stable or statistically stationary states rather than exploding. Quantification is not provided. Assessment: Medium (implicit, required for stability).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable (e.g., ~2,000 to >25,000+ time steps for cells, potentially longer for spores)
*    Units: time steps (Qualitative Descriptor: Short-term to Long-term, depending on structure and environment)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Low (Qualitative)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Variable (Inverse of lifetime)
    *   Units: (time steps)^-1

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Robustness to Noise | Median lifetime vs. noise std dev (σ) | See Fig 6D | steps | Attribute `robustness_noise` of `MemoryNode` | Fig 6D | Explicit | Quantifies persistence (memory retention) under actuation noise. |
    | State Stability (Lifetime vs DPE) | Median lifetime vs. DPE | See Fig 6C | steps | Attribute `stability_DPE` of `MemoryNode` | Fig 6C | Explicit | Measures persistence (memory retention) dependence on environment. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rule is Equation 1: ∆Φ/∆t = α + β ⋅ N<sub>t,r</sub> ⋅ sign(R<sub>t,r</sub> − L<sub>t,r</sub>). Each particle senses other particles within a radius 'r'. It counts the number of neighbors to its left (L<sub>t,r</sub>) and right (R<sub>t,r</sub>) relative to its current heading. The total number of neighbors is N<sub>t,r</sub> = L<sub>t,r</sub> + R<sub>t,r</sub>. The change in heading (Δφ) per time step (Δt=1) is determined by a constant turn rate (α) plus a term proportional to the total neighbor count (β * N<sub>t,r</sub>), with the direction determined by whether there are more neighbors to the right (positive sign, turn right) or left (negative sign, turn left). After updating the heading, the particle moves forward a distance 'v' in the new direction: p<sub>t+1</sub> = p<sub>t</sub> + (cos(φ<sub>t+1</sub>), sin(φ<sub>t+1</sub>)) * v. Interactions are purely based on relative positions within radius 'r'.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :-------------------: | :---: | :----------: | :----------------: | :------------: |
    | Eq.1    | Interaction Rule | r              | 5 (default), Variable | space units | Methods      | Explicit         | Defines sensing range. |
    | Eq.1    | Interaction Rule | α              | 180 (default), [-180, 180] | degrees | Methods, Fig 7 | Explicit         | Constant rotation component. |
    | Eq.1    | Interaction Rule | β              | 17 (default), [-60, 60] | degrees | Methods, Fig 7 | Explicit         | Density-dependent rotation factor. |
    | Eq.1    | Interaction Rule | v              | 0.67 (default), Variable | space units/time step | Methods      | Explicit         | Particle speed. |

### **4.3 Global Order:**

    *   Content: The emergent global order includes:
        1.  **Spatial Structures:** Formation of distinct, persistent structures like 'premature spores', 'mature spores' (compact aggregates), and 'cells' (ring-like or triangle-like structures with internal density variations).
        2.  **Dynamic Patterns:** A characteristic life cycle (spore -> cell -> growth -> replication -> spore/death).
        3.  **Population Dynamics:** Logistic (sigmoidal) growth of the cell/spore population towards a carrying capacity.
        4.  **Ecosystem Formation:** Emergence of a "nutrient cycle" where free particles are consumed for growth and released upon decay, creating a self-sustaining system with resource competition.
        5.  Quasi-crystalline patterns at low densities (Fig 2C).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Eq.1    | Particle Heading Change | α         | [-180, 180] (180 default) | deg   | Explicit         | Controls intrinsic rotation. Varied in Fig 7. | Methods, Fig 7 |
| Eq.1    | Particle Heading Change | β         | [-60, 60] (17 default) | deg   | Explicit         | Controls density-dependent rotation. Varied in Fig 7. | Methods, Fig 7 |
| Eq.1    | Particle Sensing      | r         | 5 (default)         | space units | Explicit         | Defines local neighborhood size. | Methods |
| Motion  | Particle Movement     | v         | 0.67 (default)      | space units/step | Explicit    | Defines particle speed. | Methods |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| PopDyn      | Population Growth | Cell Count (X<sub>cells</sub>) | ~0 to >50 | count | Mixed | Explicitly measured (particle counts mapped to cells) and implicitly via logistic model fit (K<sub>cells</sub>). | Particle counting (color-based) + Size estimate (48 particles/cell) | Fig 3H, Methods |
| PopDyn      | Population Growth | Spore Count (X<sub>spores</sub>) | ~0 to >20 | count | Mixed | Explicitly measured (particle counts mapped to spores) and implicitly via logistic model fit (K<sub>spores</sub>). | Particle counting (color-based) + Size estimate (18 particles/spore) | Fig 3H, Methods |
| Structure   | Cell Size     | Particle Count per Cell | ~23 to ~60 (mean ~41.5 or ~48 used) | count | Mixed | Explicit analysis (Supp Fig S4), explicit values used for modeling (48), measured distributions (Fig 6A,B). | Particle counting in visually identified structures/color-based. | Supp Fig S4, Fig 6A/B, Methods |
| Structure   | Spore Size    | Particle Count per Spore | ~14 to ~22 (mean ~18) | count | Mixed | Explicit analysis (Supp Fig S4), explicit value used for modeling (18). | Particle counting in visually identified structures/color-based. | Supp Fig S4, Methods |
| Ecosystem   | Density Homogeneity | DHI(t)    | [0, 1] | ratio | Explicit | Custom index defined to measure spatial clustering. | Defined in Methods (Fig 7 caption). | Fig 7, Supp Fig S5 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    *   **Metrics:** DHI(t), Visual classification, Population counts, Logistic growth parameters (a, K), Structure lifetime/size distributions.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid

### **5.3 Computational Primitive:**

    *   Content: The basic computational operation performed by each particle is the evaluation of Equation 1: ΔΦ/Δt = α + β ⋅ N<sub>t,r</sub> ⋅ sign(R<sub>t,r</sub> − L<sub>t,r</sub>). This involves:
        1. Sensing/Counting: Determining L<sub>t,r</sub>, R<sub>t,r</sub>, N<sub>t,r</sub> within radius r.
        2. Comparison/Sign Function: Evaluating sign(R<sub>t,r</sub> − L<sub>t,r</sub>).
        3. Weighted Summation/Scaling: Calculating β ⋅ N<sub>t,r</sub> and adding α.
    *   **Sub-Type (if applicable):** Nonlinear function application based on local density sensing.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Simulation Time Step (Δt) | 1 (implied) | time steps | Methods | Implicit | Basic unit of time progression. |
        | Structure Formation | ~60-180 | time steps | Fig 2E-G | Explicit | Time from random state to initial spore/cell formation. |
        | Cell Replication Period | ~1000s to 10,000s (depends on DPE) | time steps | Fig 5A | Explicit | Time taken for a cell to divide. Strongly depends on environment. |
        | Cell Lifetime | ~1000s to >25,000+ | time steps | Fig 6C | Explicit | Persistence time of cells, depends on DPE. |
        | Population Growth Phase | ~0 to ~25,000 | time steps | Fig 3H, 3I | Explicit | Phase of sigmoidal growth before saturation. |
        | Population Saturation Phase | ≥ 25,000 | time steps | Fig 3H, 3J | Explicit | Phase where population is near carrying capacity. |
        | Long-term Ecosystem Stability | ~1,000,000 | time steps | Fig 3H (caption) | Explicit | Duration run was observed to maintain population. |
    *   **Note:** Consistent units (time steps) used.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Partial
        1.  **Population Adaptation:** The population size adapts to the carrying capacity (K) determined by particle density (DPE), following logistic growth (Fig 3H). This is an emergent adaptation.
        2.  **Structural Adaptation:** Cell size distribution adapts to DPE (Fig 6A,B), and cell lifetime adapts to DPE (Fig 6C). Replication time adapts to DPE (Fig 5A). Individual structures change size (grow/shrink) based on particle availability.
        However, the underlying interaction rules (Eq. 1) for individual particles do not change based on experience (no learning/rule plasticity). The adaptation arises from the fixed rules operating in different environmental contexts (DPE).

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism is emergent adaptation based on resource availability and density-dependent dynamics governed by fixed local rules (Eq. 1).
        *   **Population Level:** Higher density (DPE) provides more "nutrients" (free particles), increasing growth and replication rates (Fig 5A, 5D) and supporting a larger carrying capacity (Fig 3H relates K to density implicitly). Competition for limited particles creates density-dependent growth limitation (logistic model).
        *   **Structural Level:** Individual cells grow by incorporating free particles. Growth rate and stable size depend on the availability of these particles (DPE). Higher DPE leads to larger, longer-lived cells (Fig 6). The adaptation is a consequence of the balance between particle influx (attraction/capture) and efflux (intrinsic pressure/breakup) dictated by Eq. 1 under varying external densities. It's systemic adaptation, not rule modification.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors are:
        1.  **Self-Assembly/Structure Formation:** Spontaneous formation of persistent spores and cells from random initial conditions.
        2.  **Growth:** Structures (spores, cells) increase in size by consuming free particles.
        3.  **Self-Replication:** Cells divide into daughter cells (or produce spores).
        4.  **Life Cycle:** Structures progress through distinct stages (free particle -> spore -> cell -> growth -> replication -> death/decay -> free particle).
        5.  **Motility:** Structures (particularly spores) exhibit directed or collective movement (Fig 2B). Cells move slowly (Fig 2G-I).
        6.  **Interaction:** Structures repel each other after replication (Fig 2I); competition for free particles occurs.
        7.  **Ecosystem Dynamics:** Population regulation via density-dependent growth/decay, emergence of a nutrient cycle.
        8.  **Pattern Formation:** Hexagonal spacing at low density (Fig 2C); specific internal arrangements in cells (Figs 2G, 4F).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergence are validated through:
        1.  **Direct Simulation & Visualization:** Time-lapse sequences (Fig 2, Fig 3A-G, Supp Video 1) visually demonstrate the formation, growth, replication, and interaction of structures from random initial conditions.
        2.  **Quantitative Analysis:** Measuring structure properties (size distributions in Supp Fig S4, Fig 6A/B), population dynamics (counts over time, Fig 3H), structure lifetimes (Fig 6C), replication times (Fig 5A), internal particle configurations (phase plots in Fig 4), and spatial homogeneity (DHI index, Fig 7).
        3.  **Parameter Sweeps:** Exploring the effect of key parameters (α, β in Fig 7; DPE in Figs 5, 6) to map regions where specific behaviors occur and assess sensitivity ("region of life").
        4.  **Robustness Testing:** Introducing noise into particle actuation and measuring the impact on structure lifetime (Fig 6D).
        5.  **Control Comparisons (Implicit):** Different densities (Fig 2) and parameter settings (Fig 7) serve as implicit controls, showing that specific conditions are required for the "life-like" behaviors.
        Limitations: Validation relies entirely on simulation; no physical experiment. Some analyses use estimated parameters (e.g., typical cell size for population counts). Reproducibility across different simulation platforms isn't discussed.

---

#Key: [yang_self-amputating_2024]

# Self-Amputating and Interfusing Machines

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is a reversible cohesive interface based on a Bicontinuous Thermoplastic Foam (BTF) structure, composed of a styrene-isoprene block copolymer (SIS) scaffold infused with a silicone matrix. This interface allows for strong, temperature-modulated attachment and detachment of soft robot components or entire modules. The purpose is to enable soft robots to perform morphological editing (shape-shifting) through self-amputation (detaching a limb) or interfusion (joining with other modules) without direct human handling, mimicking biological adaptation mechanisms like autotomy and collective aggregation. Key components are the plasticized SIS, paraffin oil (plasticizer), silicone matrix (DragonSkin 10), and sacrificial sugar particles (for foam structure). Functionality is demonstrated in a soft quadruped robot that amputates a trapped limb and a cluster of soft crawlers that fuse to cross a gap.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Values are based on standard samples (1.5mm thickness, 850–1000μm pore size) unless otherwise specified. Standard deviations (SD) provided where available.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source for activating the joint's reversible mechanism (heating for connection/disconnection) is electrical energy supplied to embedded copper coil heaters. For the robotic demonstrations, this originates from an external power source.
    *   Value: 3.2 (per joint heater)
    *   Units: W (Watts)

### **2.2 Energy Transduction**

    *   Content: Electrical energy is converted into thermal energy via Joule heating within the embedded copper coil heaters. This thermal energy is then transferred via conduction to the BTF material. The absorbed thermal energy increases the temperature of the SIS component within the BTF, causing it to undergo a phase transition (solid to viscous liquid) above its suppressed melting temperature (≈115°C). This change in phase state alters the mechanical properties (specifically adhesive/cohesive strength) of the joint, enabling connection when hot and contacted, and allowing easy disconnection when reheated. Upon cooling, thermal energy dissipates to the environment, and the SIS solidifies, restoring the joint's strength.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper provides the energy required to reach the disconnection temperature (≈400 J for a 1.5 mm thick, 1 cm² BTF). However, it does not quantify the thermodynamic efficiency of the phase change process itself or the heat transfer efficiency from the heater to the active material versus losses to the surrounding robot body and environment. Joule heating itself can be efficient electrically, but significant heat loss to surroundings is expected in soft, uninsulated systems, especially during the ≈2 min heating time and ≈3 min cooling time. The process requires continuous power input for minutes to achieve state change, suggesting low overall efficiency in terms of energy used versus the mechanical work potential unlocked/locked. Efficiency value is not provided. Qualitative assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: Energy is primarily dissipated as heat to the environment during both the heating and cooling phases. Sources include: 1) Resistive losses in wiring leading to the heater (likely minor compared to heater). 2) Heat loss from the copper heater itself to the surrounding robot body (silicone) and the external environment via conduction, convection, and radiation during the heating phase (likely high dissipation). 3) Heat loss from the BTF joint and surrounding robot body to the environment during the cooling phase (necessary for solidification, high dissipation). 4) Mechanical energy dissipated during T-peel tests or cyclic loading (e.g., viscoelastic losses, fracture energy), though this is related to mechanical work, not the thermal activation energy input. Quantification is not provided, but qualitative assessment suggests high thermal dissipation to the environment due to the nature of soft materials and the required heating/cooling times.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Heating Time (for activation) | ≈2 | min | Exp. Section | Explicit | Time required to heat joint to 120°C with specified heater/power. |
        | Cooling Time (for solidification) | ≈3 | min | Exp. Section | Explicit | Time required for BTF to cool to room temp after heating. |
        | T-Peel Test Speed | 60 | mm min⁻¹ | Exp. Section | Explicit | Speed used during mechanical characterization. |
        | Cyclic Test Duration (Strength) | 0, 50, 100, 150 | cycles | Fig 2E,F | Explicit | Number of heat-cool cycles applied. |
        | Cyclic Test Duration (Loading) | 50, 200 | cycles | Fig 2G, S7 | Explicit | Number of mechanical load cycles applied at room temp. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Partial

**(Conditional: M7.1 is "Partial", including M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism described is material degradation, not adaptive learning. The paper suggests that after many cycles (>150), a portion of the melted SIS exits the silicone matrix, causing gradual connection degradation (Sec 2.3, inset Fig 2A). For repeated mechanical loading, the weakening could be due to Mullins effect, micro-damage accumulation, or gradual polymer chain rearrangement/scission within the SIS/silicone structure under stress. This is not a programmed adaptation or learning rule (like Hebbian or reinforcement learning). The change is a passive consequence of material fatigue and wear under cyclic thermal and mechanical stress.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors demonstrated by the systems *using* the BTF joint are: 1) **Self-Amputation:** A quadruped robot detaches one of its limbs when it becomes trapped, allowing the rest of the robot to escape and continue functioning as a three-legged robot. 2) **Interfusion/Collective Locomotion:** Individual crawling robots connect end-to-end using the joints to form a longer, single entity capable of bridging a gap that individual robots cannot cross. After crossing, they can detach again. The behavior of the *joint material itself* is reversible adhesion triggered by temperature.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper validates the *functional behaviors* (self-amputation, interfusion) enabled by the joint through physical demonstrations with soft robots (quadruped, crawlers) in controlled laboratory settings (Sec 2.4, 2.5, Fig 3, 4, Movies S1, S2). These demonstrations serve as proof-of-concept for the utility of the reversible joint in enabling adaptive morphologies. The validation focuses on achieving the intended function (escape, gap crossing) rather than rigorously quantifying the emergence properties or robustness across a wide range of conditions. Control experiments (e.g., robot without the joint attempting the task) are implicitly present (single crawler failing to cross the gap, Fig 4B,C). Limitations include the controlled setting and lack of extensive robustness testing against various perturbations or environmental factors.

---

#Key: [zhang_edge_2021]

# Edge of chaos as a guiding principle for modern neural network training

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a simple feedforward artificial neural network (ANN) with one hidden layer, trained on the Fashion-MNIST dataset using stochastic gradient descent (SGD) with momentum and weight decay regularization. The hidden layer has the same number of neurons as the input layer (784 neurons for flattened 28x28 images) and uses the tanh activation function. The output layer is a softmax layer for 10-class classification. The purpose is to study the training dynamics, specifically the transition between ordered and chaotic phases based on the hidden layer's weight distribution (mean J₀/√N, variance J²/N), and relate this to the network's generalization performance and hyperparameter tuning. The key finding is that optimal performance emerges near the "edge of chaos" (J ≈ 1), and specific scaling laws govern the approach to this edge during training in the ordered phase.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** J is related to the standard deviation of weights scaled by √N. J₀ is related to the mean. These parameters define the system's state in the order-chaos phase diagram.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary "energy" input driving the system dynamics (weight updates) is derived from the training data (Fashion-MNIST images and labels) processed via the SGD algorithm. This isn't physical energy but computational effort and information driving the optimization. A secondary aspect is the computational cost of performing the updates.

### **2.2 Energy Transduction**

    *   Content: Information from the training data (input images, target labels) is processed through the network (forward pass). The error (loss) between the network's output and the target label is calculated. This error signal is then back-propagated through the network to compute gradients. The SGD algorithm uses these gradients, along with momentum and weight decay terms, to update the network's weights (parameters). This process converts information about prediction error into changes in the system's state (weights), effectively minimizing a loss function (analogous to potential energy).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper doesn't quantify energy efficiency in physical terms (e.g., Joules per operation). It discusses training efficiency in terms of epochs required to reach optimal performance (edge of chaos). Fig 5 suggests that larger scale factors (η/((1-α)B)) lead to faster convergence in epochs, implying higher computational efficiency in *time*. However, a numerical efficiency score is not provided or derivable.

### **2.4 Energy Dissipation**

    *   Content: In a computational context, "dissipation" could correspond to wasted computational cycles or numerical inaccuracies. The stochastic nature of SGD introduces noise, which might be seen as a dissipative element in the optimization trajectory. The paper doesn't analyze physical energy dissipation (heat) or quantify computational waste explicitly. The chaotic phase (J > 1) might be considered less "efficient" or more dissipative in terms of optimization progress per epoch compared to the ordered phase (Fig 5a shows sub-linear scaling). Weight decay acts as a regularizer, potentially reducing overfitting ("wasted" learning on noise) but adding a computational step.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceed.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term (relative to training dynamics)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: ~ (784*784 + 784*10 + biases) parameters
*   Units: Network Parameters (Weights + Biases)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: ~90% (peak)
*   Units: Test Accuracy (%)

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", proceed.)**

### **4.2 Local Interaction Rules:**

    *   Content: The primary local rule is the weight update via SGD with momentum and weight decay (Eq. 6): ∆wₜ = vₜ = αvₜ₋₁ - η(gₜ + 2λwₜ). Here, wₜ is a weight, vₜ is its update amount, α is momentum, η is learning rate, λ is weight decay strength, and gₜ is the gradient of the loss function with respect to wₜ, estimated using a mini-batch of B samples. The gradient gₜ itself depends on the local weight value and the activations/errors propagated through the network, making it dependent on the state of connected neurons and the input data.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :-------------------- | :---: | :----------: | :----------------: | :------------: |
    | SGD_Update | SGD w/ Momentum + Weight Decay | Mini-batch Size (B) | e.g., 4 to 512 | count | Fig 3, 4, 5 captions | Explicit | Parameter studied |

### **4.3 Global Order:**

    *   Content: The emergent global order is the configuration of network weights that achieves optimal generalization performance (lowest test loss, highest test accuracy) on the Fashion-MNIST task. This optimal state is characterized by operating dynamically at the "edge of chaos," specifically where the order parameter derived from weight variance, J, is approximately 1. This state represents a balance learned through training.
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| SGD_Update | See M4.2 | B | 4 - 512 | count | Explicit | Parameter controlling number of samples for gradient estimation. | Sec III, Fig 3d |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Performance | Generalization | Test Accuracy | ~85% - 90% | % | Explicit | Measure of how well the organized state (weights) performs on unseen data. | Standard NN testing procedure. | Fig 2c, Fig 6b |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", proceed.)**

### **5.2 Computation Type:**

    *   Content: Neuromorphic/Analog (due to continuous weights and tanh activation) leading to Digital output (classification).

### **5.3 Computational Primitive:**

    *   Content: Weighted Summation + Non-linear Activation (Thresholding/Squashing). The core operation at each neuron (in the hidden layer) is calculating a weighted sum of its inputs followed by applying the tanh activation function: x_hidden = tanh(W · x_input) (Eq. 2). The output layer performs a similar weighted sum followed by softmax activation for classification.
    *   **Sub-Type (if applicable):** Activation: tanh; Output Activation: Softmax.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Training Duration | ~1 - 500+ | Epochs | Fig 2, 3, 5, 6 | Explicit | X-axis unit in plots showing evolution over training. |
        | Time to Reach Edge of Chaos (J=1) | Variable (depends on A) | Epochs | Fig 3a, Eq. 4 | Explicit | J² increases linearly with epochs in ordered phase, depends on slope A. |
        | Asymptotic Stabilization Time (for Chaos Test) | ~50 | Iterations | Appendix B | Explicit | Number of iterations used to determine asymptotic distance in Fig 1b analysis. |

    *   **Note:** Epochs are the primary unit of time used for tracking training dynamics.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", proceed.)**

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is supervised learning via Stochastic Gradient Descent (SGD) with momentum and L2 regularization (weight decay). The network adjusts its weights to minimize the difference (loss function, typically cross-entropy for classification) between its predictions and the true labels provided in the training data. The change is driven by the gradient of the loss function, computed via backpropagation, and modified by the momentum term (accumulating past gradients) and the weight decay term (penalizing large weights). Eq. 6 mathematically describes the weight update rule. This is a form of error-correction learning.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is image classification: mapping input images (Fashion-MNIST) to one of 10 output classes. A secondary, emergent behavior observed during the *training process* is the system's tendency to evolve towards the "edge of chaos" (J≈1) where classification performance is optimized. This includes the linear scaling dynamics within the ordered phase (J<1). Another behavior induced by regularization is the saturation of J near 1 when optimal weight decay is applied.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claim that optimal performance emerges at the edge of chaos (J≈1) is validated through computational experiments. The authors train the network under various conditions, track the evolution of J (calculated from weight statistics), and correlate it with test loss/accuracy (Fig 2). They show this occurs across different hyperparameter combinations (Fig 4, 5) and that explicitly regularizing to J=1 yields the best performance (Fig 6). The linear scaling in the ordered phase is validated by fitting empirical data (Fig 3). Validation relies on consistent observation across multiple runs and parameter settings on the Fashion-MNIST dataset. Limitations include using a single simple architecture and dataset.

---

#Key: [palagi_bioinspired_2018]

# Bioinspired microrobots

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews bioinspired microrobots (sub-millimetre mobile systems) designed to mimic microorganism functions like locomotion in complex media, environmental response, and self-organization. It focuses on strategies overcoming miniaturization challenges by embedding sensing, actuation, and control directly into materials. Key components discussed include magnetic materials, stimuli-responsive polymers (PNIPAM, LCEs), active matter systems (catalytic motors, Janus particles), and biohybrid systems. Propulsion strategies mimic flagella (helical propellers, flexible filaments) driven by external magnetic fields or internal mechanisms (light, chemical reactions). Smart materials enable controlled actuation, sensing, and delivery. Active matter facilitates autonomous motion, taxis, and collective behaviors. The purpose is to develop microrobots for applications like minimally invasive medicine and targeted drug delivery, potentially achieving sophisticated, autonomous functions.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name          | Value                                  | Units        | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :---------------------- | :------------------------------------- | :----------- | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Magnetic Field (Typical)| Not specified generally (varies)        | T or mT      | Sections: Helical propellers, Flexible filaments | Implicit          | Medium (common knowledge)        | External knowledge on magnetic actuation |

    *   **Note:** Parameters represent typical ranges or specific examples cited in the review. Reliability is 'High' when explicitly stated for a cited example or fundamental principle, 'Medium' if implied or generally known for the class of system.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Diverse energy sources are reviewed: external rotating/oscillating magnetic fields (for magnetic microrobots), light (for photoresponsive materials like LCEs, PNIPAM+nanorods, Janus particles), chemical energy (fuel like H₂O₂, urea, ATP for catalytic/enzymatic/biohybrid motors), thermal energy (for thermoresponsive PNIPAM), acoustic fields (mentioned briefly for potential manipulation, not primary focus), electric fields (for Quincke rollers, electrophoresis).

### **2.2 Energy Transduction**

    *   Content: Energy is transduced via several mechanisms:
        1.  **Magnetic:** External magnetic field energy -> magnetic torque on ferromagnetic components -> mechanical rotation/bending (propulsion).
        2.  **Photothermal:** Light energy (e.g., near-infrared) -> absorption by plasmonic nanoparticles or LCE photoswitches -> heat generation -> phase transition/shape change in responsive polymers (PNIPAM de-swelling, LCE nematic-isotropic transition) -> mechanical work (deformation, propulsion).
        3.  **Chemical:** Chemical free energy (e.g., H₂O₂ decomposition) -> generation of concentration/thermal/charge gradients across asymmetric particles OR bubble generation/ejection -> phoretic movement (self-diffusio/thermo/electrophoresis) or recoil -> kinetic energy (propulsion). Light can also drive photocatalysis.
        4.  **Thermal:** Environmental thermal energy change -> phase transition in thermoresponsive polymers (PNIPAM) -> mechanical work (swelling/de-swelling).
        5.  **Electrical:** Electric field energy -> electrokinetic effects / Quincke rotation -> kinetic energy (propulsion).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The review does not provide quantitative efficiency values. However, locomotion at low Reynolds numbers is known to be generally very inefficient due to viscous dissipation. Self-phoretic systems convert chemical energy, but efficiency is typically low. Overall efficiency across the reviewed systems is inferred to be low. Metrics like propulsion speed per unit field strength or fuel concentration are discussed, but not overall energy conversion efficiency.

### **2.4 Energy Dissipation**

    *   Content: The primary dissipation mechanism discussed is viscous drag in the fluid environment, which dominates at low Reynolds numbers. This is explicitly stated ("inertial forces are negligible compared with viscous forces"). Other mechanisms include heat loss to the surroundings (especially in photothermal and thermophoretic systems) and energy lost during chemical reactions (not all free energy is converted to motion). Quantification is not provided, but viscous dissipation is qualitatively High.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: Local interaction rules described or implied include:
        *   **Phoretic Interactions:** Self-generated gradients (concentration, thermal) around active particles lead to effective attractive or repulsive forces between them (e.g., self-diffusiophoresis, self-thermophoresis). Mentioned for Janus particles, catalytic motors.
        *   **Hydrodynamic Interactions:** Flow fields generated by swimming microrobots/particles influence the motion and orientation of neighbors. Mentioned generally for biological microswimmers and implied for artificial ones.
        *   **Electrostatic Interactions:** Charge imbalances and responses to external electric fields drive interactions, such as in Quincke rollers where particle rotation leads to translation near a boundary and collective motion at high densities.
        *   **Exclusion Volume/Steric Interactions:** Particles cannot occupy the same space, leading to packing effects and phase separation at high densities (e.g., dynamic clustering).
        *   **Chemical Reaction-Diffusion:** In BZ gels, local chemical reactions (oxidation/reduction of catalyst) coupled with diffusion of reaction species lead to propagating chemical waves.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID      | Description                         | Parameter Name                     | Parameter Value Range | Units          | Data Source           | Implicit/Explicit | Justification                                     |
    | :----------- | :---------------------------------- | :--------------------------------- | :-------------------- | :------------- | :-------------------- | :---------------- | :------------------------------------------------ |
    | Steric       | Particle density affecting packing | Packing Fraction / Number Density  | Varies                | Dimensionless/m⁻³| Clustering Refs. [109]| Implicit          | Density dependence is key, actual values vary.    |

### **4.3 Global Order:**

    *   Content: Emergent global orders described include:
        *   Dynamic Clusters (Fig 5e, Ref 110, 109)
        *   Phase Separation (Crystal-like and gas-like phases, Ref 109)
        *   2D Crystal Assemblies (directed by active particles, Fig 5e, Ref 111)
        *   Macroscopic Propagating Bands / Collective Directed Motion (Quincke rollers, Fig 5f, Ref 112)
        *   Chemical Waves / Peristaltic Shape Changes (BZ gels, Ref 62, 63)
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID      | Description                         | Parameter                 | Value Range | Units          | Implicit/Explicit | Justification                 | Source           |
| :----------- | :---------------------------------- | :------------------------ | :---------- | :------------- | :---------------- | :---------------------------- | :--------------- |
| Electrostatic| Quincke rotation interaction        | Electric Field Strength   | Threshold+  | V/m            | Explicit          | Drives collective motion      | Fig 5f, Ref 112  |
| Steric/Volume| Particle exclusion interaction      | Particle Number Density   | Critical+   | m⁻³/Dimensionless | Explicit          | Drives clustering/phase sep. | Sec: Collective beh. |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID      | Description                 | Parameter                 | Value Range         | Units          | Implicit/Explicit | Justification                               | Protocol        | Source             |
| :--------------- | :-------------------------- | :------------------------ | :------------------ | :------------- | :---------------- | :------------------------------------------ | :-------------- | :----------------- |
| DirectedMotion | Net motion of collective    | Mean Velocity / Polar Order | > 0 for ordered phase | µm/s           | Explicit          | Quantifies collective movement              | Microscopy/PIV  | Fig 5f, Refs 112, 113 |
| Crystallization| Formation of ordered arrays | Structure Factor Peak     | High for crystal    | Dimensionless  | Explicit          | Indicates long-range order                  | Microscopy/Scattering | Fig 5e, Ref 111 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type                 | Description                     | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification       | Source |
    | :------------------------ | :------------------------------ | :------------- | :-----------: | :------ | :---------------- | :------------------ | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description           | Value         | Units        | Source                  | Implicit/Explicit | Justification                          |
        | :------------------------------ | :------------ | :----------- | :---------------------- | :---------------- | :------------------------------------- |
        | Bacterial Motor Rotation Freq.  | ~100          | Hz           | Fig 1a                  | Explicit          | Stated value for biological example.  |
        | Magnetic Actuation Freq. (ω)    | Varies        | Hz / rad/s   | Sec: Flexible filaments | Explicit (Symbol) | Driving frequency is a key parameter.    |
        | BZ Gel Oscillation Period       | "long period" | seconds/mins | Sec: Spontaneous mov.   | Explicit          | Mentioned as a challenge (slow).       |
        | Particle Tumbling Time (Bacteria)| Varies        | s / ms       | Fig 5b / Sec: Artif. Taxis| Explicit          | Time between reorientations.           |
        | PNIPAM Response Time (Plasmonic)| "fast"        | ms / s (?)   | Sec: Controlled loco.   | Explicit (Qual.)  | Mentioned rapid heating/actuation.     |
        | LCE Response Time               | Varies        | ms / s       | Sec: Controlled loco.   | Implicit          | Implied by dynamic actuation.          |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors reviewed are:
        *   **Locomotion:** Swimming in fluids (Newtonian and non-Newtonian) via various mechanisms (helical propulsion, flexible filament beating/undulation, self-phoresis, bubble propulsion, amoeboid crawling analogs, surface walking).
        *   **Steering/Navigation:** Controlled movement direction via external fields (magnetic) or response to environmental cues (taxis).
        *   **Environmental Sensing (Implicit/Explicit):** Response to stimuli like temperature (PNIPAM), light (LCEs, phototaxis), chemical gradients (chemotaxis, pH response), flow (rheotaxis), magnetic fields (magnetotaxis).
        *   **Actuation/Manipulation:** Shape change for function (e.g., PNIPAM/LCE actuators, grippers for cell capture/excision, Fig 3e).
        *   **Cargo Delivery:** Transport and release of payloads (drugs, cells) triggered by stimuli (temperature, pH, light, magnetic heating).
        *   **Collective Behavior:** Self-organization into clusters, crystals, bands; swarming; synchronized motion (artificial cilia).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation methods implicitly rely on direct experimental observation via microscopy (optical, electron). Locomotion is validated by tracking particle trajectories and measuring speeds. Actuation (shape change, gripping) is visually confirmed. Taxis is demonstrated by observing biased movement in response to gradients. Collective behaviors (clustering, swarming) are validated by observing large populations and characterizing the resulting structures or dynamics. Control experiments (e.g., absence of fuel, field, or stimulus) are sometimes implicitly used to confirm the cause of motion/behavior. Quantitative analysis often involves measuring speed, size, frequency, or order parameters extracted from imaging data. Reproducibility is implied by publication in peer-reviewed literature but not explicitly discussed in the review itself. Limitations often relate to the idealized conditions used in many experiments compared to complex biological environments.

---

#Key: [lavergne_group_2019]

# Group formation and cohesion of active particles with visual perception–dependent motility

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of synthetic active Brownian particles (ABPs), specifically light-activated Janus particles (diameter s = 4.28 µm), whose motility is controlled via an external feedback loop based on visual perception simulation. Each particle 'perceives' others within a defined vision cone (half-angle α) using a metric perception function (Eq. 1) that decays with distance. If a particle's perception P exceeds a threshold P*, it is illuminated by a time-shared laser, triggering self-propulsion at a fixed velocity v0 = 0.2 µm/s. If P ≤ P*, the laser is off, and the particle is passive, undergoing only Brownian motion (translational and rotational diffusion). The purpose is to experimentally demonstrate and analyze group formation and cohesion arising solely from this perception-dependent motility change, without explicit pair attraction or alignment interactions.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Persistence Length (lp) | 21.4 | µm | Text (p. 71) | Explicit | Medium | Calculated (v0 * τR) |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source for particle *activity* (propulsion) is the external laser illumination directed onto the Janus particles when their perception P > P*. Thermal energy from the environment drives passive Brownian motion.

### **2.2 Energy Transduction**

    *   Content: Light energy from the laser is absorbed by the Janus particle's coating, creating a local chemical or thermal gradient (mechanism depends on specific Janus particle type, e.g., photocatalytic decomposition of fuel or self-thermophoresis, details likely in Ref 28 but not specified here) which induces self-propulsion (phoretic mechanism). This transduces light energy into kinetic energy of directed motion. Thermal energy from the environment is transduced into kinetic energy of random translational and rotational Brownian motion.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide information about the energy efficiency of the light-to-kinetic-energy transduction process for the Janus particles or the overall system (including the feedback loop). Phoretic mechanisms are typically very inefficient. Qualitative Assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: Energy is primarily dissipated through viscous drag as particles move through the fluid (both active propulsion and passive diffusion). Rotational motion also dissipates energy via rotational drag. Energy conversion in the Janus particle mechanism likely involves heat dissipation. The external control loop (camera, computer, laser) also consumes energy, but this is external to the particle system's intrinsic dynamics. Quantification is not provided. Qualitative Assessment: High (due to low Reynolds number fluid dynamics).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content:
        1.  **Perception Calculation:** Each particle `i` calculates its perception `P_i` based on the positions of other particles `j` within its vision cone `Vα_i` (half-angle `α` relative to orientation `p^_i`). The perception function is `P_i = Σ_{j∈Vα_i} 1 / (2π * r_ij)` where `r_ij` is the distance between `i` and `j` (Eq. 1). This interaction is non-reciprocal for `α < π`.
        2.  **Motility Rule:** If `P_i > P*` (perception threshold), particle `i` becomes "active" and propels with velocity `v0 * p^_i`. If `P_i ≤ P*`, particle `i` becomes "passive" with velocity 0 (undergoing only Brownian diffusion). (Fig 1D).
        3.  **Brownian Motion:** All particles undergo translational and rotational Brownian motion, characterized by rotational diffusion time `τR`. The orientation `p^_i` randomizes over time `τR`.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | 1 | Perception Calculation | Vision Cone Half-Angle (α) | [0, π] | radians | Text, Fig 3, Fig 4 | Explicit | Parameter varied in experiments/simulations. |
    | 1 | Perception Calculation | Particle Distance (r_ij) | > s/2 | µm | Implicit | Inferred from particle positions. |
    | 2 | Motility Rule | Perception Threshold (P*) | Variable (e.g., [0, 1.2]*Pc) | Dimensionless (relative) or units of P | Text, Fig 4 | Explicit | Parameter varied in experiments/simulations. |
    | 2 | Motility Rule | Active Velocity (v0) | 0.2 | µm/s | Text | Explicit | Fixed parameter. |
    | 3 | Brownian Motion | Rotational Diffusion Time (τR) | 107 | s | Text | Explicit | Fixed parameter. |

### **4.3 Global Order:**

    *   Content: The globally emergent order is a single, cohesive, dense, non-polarized group (an "active fluid") existing in otherwise empty surroundings. For low α, groups can be elongated; for α approaching π/2, they become more circular and dilute (Fig 3A). The group exhibits distinct internal structure with active particles concentrated at the core and passive particles predominantly at the edge, pointing outwards (Fig 2C-E).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| 1 | Perception Calculation | α | [0, π] | radians | Explicit | Parameter varied. | Text, Fig 3, 4 |
| 2 | Motility Rule | P* | Variable relative to Pc | - | Explicit | Parameter varied. | Text, Fig 4 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| GO1 | Group Cohesion | Mean Aggregate Size / N (<s>/N) | ~0 to ~0.8 | - | Explicit | Measures fraction of particles in largest aggregate. | Text, calculation described in (25) | Fig 3B |
| GO2 | Group Density | ρ(r) | ~0 to ~7*ρ0 | µm⁻² | Explicit | Radial density profile. | Image analysis | Fig 2D |
| GO3 | Group Polarization | ρr(r) | Negative (core) to Positive (edge) | µm⁻² | Explicit | Radial polarization density profile. | Image analysis | Fig 2E |
| GO4 | Response Horizon Fraction | fh | ~0 to ~0.6 | - | Explicit | Fraction of particles initially outside response horizon h. | Calculation based on h definition | Fig 3B |
| GO5 | Active Fraction | N_active / N | ~0 to ~1 | - | Explicit | Fraction of active particles over time. | State tracking | Fig 4 (inset) |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

    *   **Metrics:** Phase diagram boundaries, Mean aggregate size vs α, Density/Polarization profiles, Time evolution snapshots/movies.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Rotational Diffusion Time (τR) | 107 | s | Text (p. 71) | Explicit | Characteristic time for orientation randomization. |
        | Group Formation Time | ~30 * τR (~3210 s) | s | Text (p. 72) / Fig 2A | Explicit | Time to reach steady-state cohesive group from initial distribution. |
        | Feedback Loop Update Time (1 / update rate) | 0.5 | s | Text (p. 71) | Explicit | Time between perception recalculation and laser state update. Propulsion quasi-static. |
        | Persistence Time (τR) | 107 | s | Text (p. 71) | Explicit | Time over which active motion direction persists. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior is the formation and maintenance of a single, cohesive, non-polarized group ("active fluid") from an initially dispersed state under specific conditions (α, P*). This cohesion occurs without attraction or alignment forces. Key characteristics include: particle aggregation into a dense core, dynamic exchange of particles between active (core) and passive (edge) states, and outward pointing polarization of passive particles at the boundary preventing escape. Variations in parameters lead to different states: stable cohesive groups (elongated or circular), failure to form a group (particles remain largely passive and dispersed), or failure to maintain cohesion (group forms transiently but disperses).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behavior (cohesion) are validated through:
        1.  **Direct Observation:** Experimental snapshots (Fig 2A, 3A, 4) and time-lapse movies (Movie S1, cited) show group formation and stability.
        2.  **Quantitative Analysis:** Order parameters like mean aggregate size (<s>/N, Fig 3B), density profiles (ρ(r), Fig 2D), and polarization profiles (ρr(r), Fig 2E) are measured and analyzed to characterize the emergent state and transitions.
        3.  **Simulations:** Point-like particle simulations confirm the generic nature of the mechanism and show good agreement with experimental results (e.g., Fig 3B, Fig 4 boundaries).
        4.  **Phase Diagram:** Experimental variation of key parameters (α, P*) systematically maps out the different behavioral regimes (Fig 4), validating the dependence of emergence on local rules.
        5.  **Control Comparison:** Implicit comparison to permanently active particles (rapid dispersal) and passive particles (diffusion).
        6.  **Analytical Arguments:** Instability analysis (briefly mentioned, details in Ref 25) provides theoretical support for the cohesion mechanism based on non-reciprocity.
        Reproducibility seems implied by consistent results across experiments and simulations. Limitations might include the specific range of N tested and quantification of noise effects.

---

#Key: [lentz_low_2019]

# Low frequency cyclical potentials for fine tuning insulator-based dielectrophoretic separations

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is an insulator-based dielectrophoresis (iDEP) microfluidic device used for separating micron-sized particles (polystyrene beads). It utilizes low-frequency cyclical electrical potentials (custom step signals, sawtooth left signals) applied across a PDMS microchannel containing an array of cylindrical insulating posts. Separation is achieved by exploiting subtle differences in particle size or surface charge (zeta potential), which affect their electrokinetic (EK) and dielectrophoretic (DEP) mobilities under the time-varying non-uniform electric fields generated around the posts. The purpose is to achieve fine-tuned separation of particles with similar characteristics by carefully designing the applied electrical signal, aided by a custom Matlab/COMSOL signal designer program that simulates particle migration based on experimentally determined mobilities.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Signal Frequency (Range) | ~0.6 - 3.3 | Hz | Calculated from periods in Figs. 3(a), 4(a), 5(a) | Implicit | Medium | Freq = 1/Period |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is electrical energy supplied by a high voltage power supply, delivering time-varying (cyclical low-frequency) potentials.
    *   Value: 50 - 1800 (Peak/Range)
    *   Units: V (Volts)

### **2.2 Energy Transduction**

    *   Content: Electrical energy from the power supply is transduced into a non-uniform electric field within the microchannel by the electrodes and insulating posts. This electric field energy drives electrokinetic (EK) phenomena (electroosmotic flow (EOF) and particle electrophoresis (EP)) and dielectrophoretic (DEP) forces on the suspended particles. These forces result in the transduction of electrical energy into the kinetic energy of the particles and surrounding fluid. Energy is also transduced into thermal energy via Joule heating.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify the energy efficiency of the separation process (e.g., energy consumed per separated particle). Microfluidic electrokinetic systems are generally not optimized for energy efficiency in terms of converting input electrical energy into useful work (particle separation); most energy is likely dissipated as heat (Joule heating) and overcoming viscous drag. The efficiency score is qualitatively low based on the typical nature of such systems and the lack of any efficiency analysis. Currents up to 40 μA at 1800V imply significant power dissipation (P = IV = 1800V * 40e-6A = 72 mW) for a micro-device.

### **2.4 Energy Dissipation**

    *   Content: Energy is primarily dissipated as heat due to Joule heating within the suspending medium (conductivity 20-25 μS/cm, currents up to 40 μA mentioned). Energy is also dissipated through viscous drag forces acting on the moving particles and fluid (Stokes drag, implicit in the mobility equations which balance electric/DEP forces with drag). Quantification is not provided, but Joule heating is acknowledged as a potential issue, suggesting it's a significant dissipation mechanism, especially at higher voltages (1800V). Qualitative assessment: High (Joule Heating), Medium/High (Viscous Drag).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2 - M3.8)**

### **3.2 Memory Type:**
### **3.3 Memory Retention Time:**
### **3.4 Memory Capacity (Optional - if applicable)**
### **3.5 Readout Accuracy (Optional - if applicable)**
### **3.6 Degradation Rate (Optional - if applicable)**
### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2 - M4.7)**

### **4.2 Local Interaction Rules:**
### **4.2.1 Local Interaction Parameters:**
### **4.3 Global Order:**
### **4.4 Predictability of Global Order:**
### **4.5. Local Interaction Rules (for Self-Organization)**
### **4.6. Globally Emergent Order and Order Parameters**
### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2 - M5.4)**

### **5.2 Computation Type:**
### **5.3 Computational Primitive:**
### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Signal Period (Step, Charge Sep.) | ~0.33 | s | Fig. 3(a) | Implicit | Period = 1 / Frequency (estimated ~3Hz from text & Fig) |
        | Signal Period (Sawtooth, Charge Sep.) | 0.3 | s | Fig. 4(a) | Explicit | Text and Fig. 4(a) |
        | Signal Period (Step, Size Sep.) | ~0.65 (cycle) | s | Fig. 5(a) | Explicit | Fig. 5(a) |
        | Separation Time | 0.6 - 1.8 | s | Figs. 3(a), 4(a), 5(a) | Explicit | Duration shown in plots for achieving separation. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

### **7.2 Adaptation Mechanism:**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is the selective separation of microparticles based on either size or surface charge (zeta potential). This is achieved by inducing differential migration ("ratcheting") through an array of insulating posts using low-frequency cyclical electric fields. Faster-moving particles progress through the array while slower-moving particles oscillate within a constriction or move backward, leading to spatial separation over time. Specific behaviors include particle trapping (DEP force dominates EK force) and streaming (EK and DEP forces are comparable).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The observed behavior (particle separation) is validated through direct experimental observation using microscopy and image tracking (ImageJ). Particle trajectories are recorded and plotted vs. time (Figs 3a, 4a, 5a). The separation is visually confirmed by comparing the positions of different particle types over multiple signal cycles (Figs 1d, 3c, 4c, 5c). The experimental results are compared with predictions from the signal designer program (Fig 2d shows reasonable agreement for single particle type). Control isn't explicitly mentioned, but comparison between different particle types within the mixture serves as an internal control. Reproducibility is implied by presenting representative results, but not quantified (e.g., number of trials, statistical analysis of separation efficiency). The behavior is engineered based on known physics, not claimed as emergent in the CT-GIN sense.

---

#Key: [hauser_towards_2011]

# Towards a theoretical foundation for morphological computation with compliant bodies

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper proposes two theoretical models for morphological computation using compliant bodies, modeled as mass-spring systems, to approximate arbitrary time-invariant filters (operators) F with fading memory.
        1.  **Model 1 (Fig 1a,b):** A filter bank of linear mass-spring systems (B1,...,Bk) receives a common input stream u(t). These provide temporal integration (fading memory). Their outputs x(t) are fed into a static (memoryless) but potentially nonlinear readout function f (e.g., an ANN) to produce the final output y(t) = f(x(t)). The morphology (mass-spring bank) handles temporal integration; the readout handles nonlinearity.
        2.  **Model 2 (Fig 1c,d):** A single, potentially complex, recurrent network of *nonlinear* springs and masses realizes both the filter bank and a nonlinear kernel projection Q. This network receives input u(t) and its internal states (e.g., spring lengths l_i(t)) provide a high-dimensional representation Q(u)(t). Only a *linear*, static readout (weighted sum of internal states) is needed to approximate the target filter F: y(t) = w_out * Q(u)(t). The morphology handles both temporal integration and nonlinear projection.
        The purpose is to demonstrate mathematically and via simulation that the complex dynamics of compliant bodies can serve as a computational resource, simplifying control and learning by outsourcing computation (temporal integration and nonlinear mapping) to the physical body, reducing the learning task to adapting a static readout (potentially just linear weights).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name           | Value                                   | Units                 | Source (Fig/Table/Section)        | Implicit/Explicit   | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit)   |
        | :----------------------- | :-------------------------------------- | :-------------------- | :-------------------------------- | :------------------ | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameter units are often implicit in the physics equations rather than explicitly stated with SI units. Random ranges are explicitly given.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The input is described abstractly as a time-varying signal `u(t)` (mathematical function) or specifically as an external force applied to mass points (particularly input nodes `w_in * u` in Eq. 5).
    *   Units: N (Newtons) when interpreted as force in simulations (Eq 5). Dimensionless or V/A if representing other physical signals abstractly.

### **2.2 Energy Transduction**

    *   Content: The primary energy transduction mechanism described is the conversion of input force (potential energy source) into kinetic energy (mass movement, `m*p_ddot`) and potential energy stored in spring deformation (linear `k*x1` or nonlinear `p(x1)`). Energy is transferred between kinetic and potential forms within the mass-spring system dynamics. This mechanical energy storage and transfer embodies the system's memory and computational capability.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss or quantify the energy efficiency of the morphological computation process. It focuses on the computational capabilities (approximation accuracy, task performance) rather than thermodynamic efficiency. One could infer low efficiency due to ubiquitous damping, but this is not quantified.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation is explicitly modeled through damping terms: linear damping `d*x2` in Eq. 1 and nonlinear damping `q(x2) = d3*x2^3 + d1*x2` in Eq. 4. This converts mechanical energy into presumably heat (though not explicitly stated). Damping is essential for stability and the fading memory property, as it causes transients to die out (related to negative real parts of eigenvalues). Qualitative assessment: Present and essential for function (stability, fading memory). Quantification depends on specific parameters `d`, `d1`, `d3` and system state `x2`.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

* Units: s (seconds)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog / Reservoir Computing

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operation performed by the material body itself is the *dynamic transformation* of the input signal `u(t)` into the high-dimensional state `x(t)` (output of basis filters in Model 1) or `Q(u)(t)` (internal states like spring lengths in Model 2). This involves:
        1.  **Temporal Integration/Filtering:** Inherently performed by the mass-spring dynamics (Eq 1, 4), implementing fading memory filters B_i or the complex dynamics of the recurrent network. Mathematically, this corresponds to convolution with the system's impulse response.
        2.  **Nonlinear Projection (Model 2):** The nonlinear springs (`p(x1)`, `q(x2)` in Eq. 4) and recurrent connections introduce nonlinear interactions, effectively computing a complex, high-dimensional nonlinear function (kernel `Q`) of the input history.
    *   **Sub-Type (if applicable):** Filtering (Linear/Nonlinear Dynamic Systems), Nonlinear Kernel Projection.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description              | Value                | Units | Source           | Implicit/Explicit | Justification                                                                 |
        | :--------------------------------- | :------------------- | :---- | :--------------- | :---------------- | :---------------------------------------------------------------------------- |
        | Simulation Timestep                | 1                    | ms    | Sec 3, Sec 4.1.2 | Explicit          | Explicitly stated for simulations.                                            |
        | Fading Memory Timescale             | ~0.2 (for Fig 2 kernel) | s     | Fig 2, Sec 2/3   | Implicit          | Related to filter properties (Fading Memory definition, kernel width in Fig 2). |
        | System Dynamics (Oscillations etc.) | Dependent on m, k, d | s     | Eq 1, 4          | Implicit          | Determined by physical parameters, not explicitly quantified as a timescale. |
        | Input Signal Frequencies            | 2.11, 3.73, 4.33     | Hz    | Sec 3            | Explicit          | Explicitly given for specific simulation task.                                 |
        | Simulation Duration (Train/Test)    | 30 / 10 / 10         | s     | Sec 3            | Explicit          | Explicitly stated for simulation phases.                                      |
        | Washout Time                        | Not specified (Sec 3); 50 (Sec 4.3) | s | Sec 4.1.3, 4.3 | Explicit          | Explicitly mentioned, value given for Sec 4.3 task.                          |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes (in Readout Only) / No (in Morphology)

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism applies only to the static readout function:
        *   **Model 1 (Fig 1b):** Training a feedforward Artificial Neural Network (ANN). The simulation uses the BFGS quasi-Newton algorithm (supervised learning) to adjust ANN weights based on the error between the system output and the target filter output. (Sec 3)
        *   **Model 2 (Fig 1d):** Calculating optimal linear readout weights `w_out` using standard Linear Regression (LR), specifically via the Moore-Penrose pseudoinverse (`w_out* = L† * T`, where L contains spring lengths over time and T contains target outputs). This is a one-shot calculation based on collected data (supervised learning). (Sec 4.1.3)
        The paper mentions potential for other learning rules (reward-based, unsupervised like SFA) but demonstrates supervised methods. The adaptation adjusts the mapping from the body's state to the final output, not the body's dynamics itself.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behavior of the system is the approximation or emulation of a given complex, nonlinear, time-invariant filter `F` with fading memory. Specific examples demonstrated in simulations include:
        *   Approximating a specific quadratic Volterra series operator `V` (Eq 2, Fig 2, Sec 3, Sec 4.3).
        *   Emulating the dynamics of a damped pendulum (Eq 3, Sec 3).
        *   Representing the inverse dynamics of a two-link robot arm for a given trajectory (mapping end-effector trajectory (x,y) to joint torques (τ1, τ2)) (Sec 4.2).
        *   Emulating second-order (Eq 7) and tenth-order (Eq 8) nonlinear dynamical systems (Sec 4.3).
        In essence, the system behaves as a tunable filter that learns to replicate the input-output mapping of a target dynamical system or operator. The concept of "multitasking" is also demonstrated, where the same fixed morphology supports the approximation of multiple different filters simultaneously using different readouts (Sec 3, Sec 4.3).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary validation method is computational simulation.
        1.  **Operational Definition:** The behavior (filter approximation) is defined by comparing the system's output `y(t)` to a target output `Y_target(t)` using Mean Squared Error (MSE).
        2.  **Control Experiments:** Comparisons are made against baseline cases, such as:
            *   Using only the readout (ANN or LR) on the raw input, without the morphology (Fig 3c,d, Fig 9c,d,e). This demonstrates the morphology's contribution to temporal/nonlinear processing.
            *   Using a homogeneous morphology (all springs identical) versus a diverse one (Sec 3, Sec 4.2). This demonstrates the importance of diversity.
        3.  **Quantitative Analysis:** Performance is quantified using MSE on training, validation, and testing datasets (Sec 3, Fig 8f).
        4.  **Reproducibility:** While specific random seeds aren't given, the methodology (random parameter sampling, standard algorithms like BFGS/LR) suggests conceptual reproducibility. The analysis of 400 random networks (Fig 8f) assesses the statistical likelihood of achieving good performance.
        5.  **Limitations:** Validation is purely computational; no physical experiments are presented. Robustness testing is limited (mainly parameter variation). Generalization capabilities are mentioned (minimal VC-dimension for linear readouts) but not extensively tested across diverse inputs/tasks in the excerpt.

---

#Key: [lee_mechanical_2022]

# Mechanical neural networks: Architected materials that learn behaviors

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is a class of architected materials called Mechanical Neural Networks (MNNs). MNNs are lattices composed of interconnected tunable beams joined at nodes. The stiffness of these beams can be actively tuned via closed-loop control using voice coils (actuators) and strain gauges (sensors). The purpose of the MNN is to learn desired mechanical behaviors (e.g., shape morphing, specific responses to loads) by adjusting beam stiffness values, analogous to how Artificial Neural Networks (ANNs) tune weights. The paper demonstrates this concept experimentally with a 2D lattice of 21 tunable beams, showing its ability to learn two different shape-morphing behaviors using optimization algorithms (GA and PPS) to minimize the error between measured and target output node displacements under specific input loading conditions. The system also explores the effect of parameters like lattice size, configuration, algorithm type, number of behaviors, and stiffness linearity on learning performance through simulation.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is electrical power supplied to the control electronics, which drive the voice coil actuators within each tunable beam and the input loading actuators. Mechanical energy is also input via forces applied to the input nodes during the learning and behavior execution phases.

### **2.2 Energy Transduction**

    *   Content: 1. Electrical to Mechanical: Electrical energy is converted into mechanical force/displacement by the voice coil actuators in the tunable beams and the input loading system. 2. Mechanical to Electrical: Mechanical strain in the beams' flexures is converted into electrical signals by the strain gauges. 3. Electrical (Signal) Processing: Electrical signals from sensors are processed by the microcontroller and amplifiers to compute control signals. 4. Mechanical (Internal): Input mechanical forces are transmitted through the lattice structure (beams and nodes), causing deformations and storing/releasing elastic potential energy.

### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**

    *   Content: 1. Electrical Resistance (Joule Heating): In voice coils, wiring, and electronic components (resistors, transistors in amplifiers). Likely significant due to active control currents. (Qualitative: High). 2. Mechanical Friction/Damping: Internal material damping in the flexures (aluminum, strain gauges) and potentially minor friction in joints (though designed to be minimized). (Qualitative: Low-Medium, design targets minimal friction/hysteresis, Fig S10). 3. Acoustic Noise: Minor dissipation from vibrating components. (Qualitative: Low). 4. Control Overhead: Energy consumed by sensing, computation (microcontroller), and signal amplification, regardless of mechanical output. (Qualitative: High). The paper mentions hysteresis as a factor to minimize for successful learning (Fig. S10), implying awareness of mechanical dissipation, but doesn't quantify these losses.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Dependent on continuous power supply
*    Units: s (potentially indefinite if powered)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Learning Accuracy (MSE) | Mean Squared Error between target and actual output displacements after learning | GA: 0.006, PPS: 0.063 (exp, 2 behaviors) | mm² | Attribute of `BehaviorArchetypeNode` / `AdaptationNode` | Fig. 4 B,C | Explicit | Explicitly measured and reported results. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Partial

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog / Neuromorphic

### **5.3 Computational Primitive:**

    *   Content: Weighted Summation and Propagation (implicit physical equivalent). Each node sums the forces transmitted by the connected beams (whose contribution is weighted by their stiffness). This resulting net force/displacement state is propagated through the lattice. While there isn't an explicit activation function like in ANNs, the physics of force balance and material deformation at the nodes serves an analogous role in transforming and propagating signals through the layers. The fundamental operation is the linear elastic response of the interconnected beam network, mapping input forces/displacements to output displacements based on the stiffness matrix (influenced by tunable Kp values).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Learning Time (Experiment, GA) | 111.13 | hours | Discussion | Explicit | Explicitly stated learning duration. |
        | Learning Time (Experiment, PPS) | 2.68 | hours | Discussion | Explicit | Explicitly stated learning duration. |
    *   **Note:** Learning times are very long. Control loop and mechanical response times are not specified but are likely much faster (ms range typical for such actuators/structures).

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism involves updating the axial stiffness values (Kp parameter in the controller) of the tunable beams based on optimization algorithms (Genetic Algorithm or Partial Pattern Search). The process is: 1. Apply predefined input load(s). 2. Measure resulting output node displacements (using strain gauges indirectly, validated by cameras). 3. Calculate the Mean Squared Error (MSE) between measured and target output displacements. 4. The optimization algorithm (GA or PPS, running on an external controller) uses the MSE as a cost function and proposes new sets of stiffness values (within the allowed range [-2, 2.3 N/mm]) intended to reduce the MSE. 5. The controllers update the beam stiffnesses to these new values. This loop repeats until the MSE is minimized below a threshold or the algorithm terminates. GA uses population-based evolution and crossover, while PPS performs a directed search around the current stiffness values.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior demonstrated is *learned shape morphing*. Specifically, the MNN lattice learns to displace its output nodes to predefined target positions (forming specific shapes, e.g., sinusoidal contours in Fig. 1B, specific displacements in Fig. 4A) in response to specific, predefined input loading conditions (e.g., horizontal forces, vertical shear forces). The system learns to simultaneously achieve multiple such input-output mapping behaviors. Simulations also explore learning randomly generated shape-morphing behaviors.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary behavior (learned shape morphing) is validated experimentally by comparing measured output node displacements to target displacements under specific input loads after the learning process has converged. Quantitative validation is provided via Mean Squared Error (MSE) plots showing convergence over time/iterations (Fig. 4 B, C, D). Initial and final displacement plots visually compare achieved vs. target positions (Fig. 4 B, C). The indirect measurement of output displacements via strain gauges is validated against direct camera measurements (Fig. S6). Simulations further validate the concept for larger lattices and different conditions (Fig. 5). Limitations include validation on only two specific behaviors experimentally, potential influence of uncharacterized noise, and lack of validation for robustness claims (damage tolerance, reconfiguration).

---

#Key: [malevich_very-large-scale_2025]

# Very-large-scale reconfigurable intelligent surfaces for dynamic control of terahertz and millimeter waves

**Paper Type:** Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a very-large-scale spatial light modulator (SLM) for Terahertz (THz) and millimeter (mm) waves. It consists of an active-matrix array of 640x480 individually addressable subwavelength pixels (>300,000 total). Each pixel integrates a graphene-based THz modulator with thin-film transistor (TFT) backplane electronics. The core modulator structure involves laminated layers: a bilayer graphene top electrode, an ionic liquid electrolyte layer (~5µm or ~25µm thick porous membrane), and an Indium Tin Oxide (ITO) pixel electrode connected via the TFT. The TFT (amorphous-Si, double-back-gate) controls the charge on a pixel capacitor, which dictates the local charge density (holes/electrons) on the continuous graphene layer via electrolyte gating. Modulating the graphene's charge density tunes its Drude-like metallic response, thereby controlling the reflection and transmission of incident THz/mm-waves. The purpose is to dynamically manipulate THz/mm-waves in real-time for applications like imaging and next-generation communications, enabling programmable transmission/reflection patterns, beam steering, and pattern generation for techniques like single-pixel imaging.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name          | Value              | Units    | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :---------------------- | :----------------: | :------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters listed are key characteristics describing the physical implementation and basic performance. Values are directly stated in the text or figure captions. Operating frequency refers to the range tested/demonstrated. Switching time is the observed transition time.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input is electrical energy supplied to the active-matrix TFT backplane and the graphene layer. This includes: (1) Voltages (V_DH, V_DL) applied to drain lines, (2) Voltage pulses applied to gate lines to control the TFTs, (3) Common voltage (V_G) applied to the graphene layer, (4) Power for the chip-on-glass display driver and external microcontroller. The system also interacts with electromagnetic energy in the THz/mm-wave range, which is modulated but not the primary energy input for *operation*.
    *   Value: Voltage levels mentioned: V_DL = -2.4 V, V_DH = 10 V (Fig. 3b,c). Specific power consumption is not quantified.
    *   Units: Volts (V) for potentials. Power units (Watts) not provided.

### **2.2 Energy Transduction**

    *   Content: 1. Electrical energy (gate pulse) -> Controls TFT conductivity. 2. Electrical energy (drain voltage, V_DH/V_DL) + TFT state -> Charges/discharges pixel capacitor. 3. Electrical potential energy (stored in capacitor) + V_G -> Creates electric field across electrolyte. 4. Electric field -> Ion movement in electrolyte -> Changes charge carrier density in graphene layer (Electrostatic Doping). 5. Change in graphene charge density -> Modulates graphene's complex conductivity (Drude response) at THz/mm-wave frequencies. 6. Modulated graphene conductivity -> Alters reflection/transmission coefficient for incident THz/mm-wave electromagnetic energy.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Efficiency is not explicitly quantified in terms of electrical power input vs. modulated THz power output. However, the paper notes significant insertion loss, particularly for multi-layer graphene devices (Fig. 2c). THz transmission is stated as ~30% at 100 GHz even *before* modulation (implicit high baseline loss, Supporting Materials Fig. S5 mentioned). Modulation itself relies on changing graphene's absorption/reflection, which implies energy loss within the graphene layer when it is conductive. The reflection singularity operation (Fig. 3) involves tunable *absorption*. While effective for modulation, this implies potentially low energy efficiency in terms of reflected/transmitted power vs. incident power during certain states. The electrical efficiency of the TFT array is likely standard for a-Si display technology but not discussed. Overall assessment: Low efficiency due to inherent THz absorption and insertion losses.

### **2.4 Energy Dissipation**

    *   Content: Dissipation mechanisms include: 1. Resistive losses (Joule heating) in TFT channels, ITO electrodes, metal lines, and the graphene layer itself (especially when doped to be conductive). 2. Dielectric losses in the capacitor dielectric and polymer layers. 3. Energy loss due to ion transport/friction within the electrolyte during gating. 4. Absorption of THz/mm-wave energy in the graphene layer (part of the modulation mechanism but also a loss pathway), substrate, and other layers. The paper does not quantify these losses. Qualitative assessment: Expected to be significant, particularly THz absorption in graphene and resistive losses in the large TFT array and electrodes.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: ~milliseconds to seconds (Qualitative)
*    Units: Time

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: >300,000 (pixels) * log2(N_states)
*   Units: bits (approx.)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description         | Value           | Units | Source         | Implicit/Explicit | Justification |
        | :---------------------------- | :-------------: | :---: | :------------- | :----------------: | :------------: |
        | Pixel Switching Time          | ~1              | ms    | Fig. 2e inset  | Explicit          | Measured rise/fall time. |
        | TFT Gate Pulse Width          | 30 - 208        | µs    | Fig. 1f, Results | Explicit          | Control parameter range. |
        | Array Scan Rate               | 200             | Hz    | Results        | Explicit          | Operating parameter of driver. |
        | Refresh Cycle Time            | 5               | ms    | Calculated     | Implicit          | Derived from 200 Hz scan rate. |
        | THz Operating Frequency       | 0.3 - 3         | THz   | Abstract, Fig 3| Explicit          | Frequency range measured/used. |
        | mm-wave Operating Frequency   | 100             | GHz   | Fig 2a, 4a, 5a | Explicit          | Frequency used in demos. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behaviors are: 1. Spatially Resolved Modulation: Generating arbitrary, programmable 2D patterns of THz/mm-wave transmission and reflection intensity/phase. 2. Dynamic Beam Steering: Creating programmable diffraction gratings (binary, fork) to control the direction and shape (e.g., introducing orbital angular momentum) of THz/mm-wave beams. 3. Structured Illumination/Imaging: Generating specific patterns (e.g., Hadamard masks) for use in computational imaging techniques like single-pixel cameras. These behaviors arise directly from the collective, programmed states of the individual pixel modulators.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The demonstrated behaviors (spatial modulation, beam steering, imaging) are validated through direct experimental measurements and imaging results presented in Figures 2, 3, 4, and 5. For spatial modulation, THz camera images show the generated patterns (Fig. 2b). For reflection patterns and phase modulation, raster-scanned time-domain THz spectroscopy maps the reflectivity (Fig. 3d-f). For beam steering, a THz camera images the diffracted beams at different programmed grating periods (Fig. 5b,c). For single-pixel imaging, reconstructed images of objects are shown, validating the use of generated Hadamard patterns (Fig. 4b,c), with convergence analysis (Fig. 4d). Control experiments comparing different graphene layers are presented (Fig. 2c). Reproducibility is claimed (Abstract) but not explicitly shown through repeated trials or statistics in the main text. Limitations include spatial resolution limits of the THz cameras/spectrometer and potential environmental factors not controlled/reported. These are validations of *programmed behaviors*, not *emergent* behaviors in the sense of arising spontaneously from local rules.

---

#Key: [son_emergent_2024]

# Emergent functional dynamics of link-bots

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is the "link-bot," a V-shaped, single-stranded chain composed of N active, self-propelled "bots" (modified bristle-bots driven by vibration) connected by N-1 rigid links with specific geometric and rotational constraints (length L, notch angle θ, spread angle α). The dynamics are governed by these link constraints and simple steric interactions between bots. The purpose is to investigate how these constraints produce versatile emergent collective behaviors like locomotion, navigation in complex environments (walls, gaps, obstacles), object transportation (pushing, pulling), and interactions between link-bots (competitive, cooperative) without complex individual intelligence or external control. Key components are the individual bots (cylindrical body, tilted legs, top crest), the rigid links (center and side links with potentially differing angles), and the vibrating arena providing energy input.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the mechanical energy supplied by the vertical vibration of the circular arena/table, driven by an electromagnetic shaker.
    *   Value: Frequency = 80 Hz, Amplitude ≈ 70 µm
    *   Units: Hz, µm

### **2.2 Energy Transduction**

    *   Content: The vertical vibration energy of the arena is transduced into directional kinetic energy (self-propulsion) of individual bots due to the tilted legs. This mechanism is characteristic of bristle-bots. The self-propulsion force of each bot is then transmitted through the rigid links to neighboring bots, driving the collective motion and deformation (breathing, flapping) of the link-bot chain. Energy is also converted during interactions with boundaries/obstacles (collisions) and between bots (steric interactions).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper provides no information or metrics regarding the energy efficiency of the transduction from vibration energy to bot propulsion or collective motion. Efficiency is likely very low, typical for vibration-driven systems. Qualitative Assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: Energy is dissipated primarily through friction between the bot legs and the vibrating surface, inelastic collisions between bots (steric interactions), friction/collisions within the link joints (though modeled as rigid/constrained), collisions with boundaries (walls, obstacles), and potentially air resistance (likely negligible). The model includes diffusion mimicking random energy loss/thermal effects. Dissipation is not quantified. Qualitative Assessment: High (inherent in friction-based propulsion and collisions).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content:
        1.  **Self-propulsion:** Each bot moves with an average speed v0 due to vibration, subject to noise (modeled as Active Brownian Particle, Eq 2).
        2.  **Rigid Link Length Constraint:** Neighboring bots maintain a fixed distance L (Eq 3, `Flink`).
        3.  **Bot Overlap Constraint:** Bots repel sterically if their distance is less than d (Eq 3, `Foverlap`).
        4.  **Link Notch Angle Constraint (Translational):** Links connected to a bot exert repulsive forces (`Fnotch`) when the angle between them exceeds the limits defined by notch angles (θc, θs) and spread angles (αc, αs) (Eq 4, Fig 5B, 5C).
        5.  **Link Notch Angle Constraint (Rotational):** Each bot's orientation (φi) is constrained relative to its connecting links based on the notch angles (θc, θs) and spread angles (αc, αs). These constraints become tighter during breathing/flapping (Eq 5-6, Fig 5D, 5E).
        6.  **Boundary Interaction:** Bots interact with walls/obstacles via elastic collisions (model) or physical contact (experiment).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :-------------------- | :---: | :----------: | :----------------: | :------------: |
    | 1       | Self-propulsion | v0             | 8                     | cm/s  | Table I      | Explicit           | Parameter stated. |
    | 1       | Self-propulsion | D              | 1e-3                  | cm²/s | Table I      | Explicit           | Parameter stated. |
    | 2       | Link Length     | L              | 1.6                   | cm    | Table I      | Explicit           | Parameter stated. |
    | 3       | Bot Overlap     | d              | 1.5                   | cm    | Table I      | Explicit           | Parameter stated. |
    | 4, 5    | Notch Angle     | θc             | 10-180                | deg   | Table I      | Explicit           | Parameter range stated. |
    | 4, 5    | Notch Angle     | θs             | 10-180 (60 typical) | deg   | Table I, Sec III | Explicit           | Parameter range stated. |
    | 4, 5    | Spread Angle    | αc             | 10-90                 | deg   | Table I      | Explicit           | Parameter range stated. |
    | 4, 5    | Spread Angle    | αs             | 10-90                 | deg   | Table I      | Explicit           | Parameter range stated. |
    | 2, 3, 4 | Spring Const.   | k              | 2e5                   | N/m   | Table I, Sec VIIIB | Explicit       | Parameter stated. |

### **4.3 Global Order:**

    *   Content: The primary emergent global orders are the distinct collective behaviors:
        1.  **Neutral Configuration:** Stable V-shape maintained during free forward motion.
        2.  **Gaits at Wall:** Unidirectional translation, oscillation (periodic flipping), stationary pushing.
        3.  **Navigation Patterns:** Exploratory (passing gaps, traversing channels quickly, going around obstacles) vs. Exploitative (blocking gaps, slow channel movement, getting stuck).
        4.  **Transport Modes:** Pushing objects forward, pulling objects backward, enclosing/wrapping objects, passing by objects.
        5.  **Collective Interactions:** Competitive jamming at a gap, cooperative passage through a gap.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Explicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| 1 | Bot Self-Propulsion & Noise | v0 | 8 | cm/s | Explicit | Parameter stated | Table I |
| 1 | Bot Self-Propulsion & Noise | D | 1e-3 | cm²/s | Explicit | Parameter stated | Table I |
| 2 | Rigid Link Length | L | 1.6 | cm | Explicit | Parameter stated | Table I |
| 3 | Steric Repulsion (Bot Overlap) | d | 1.5 | cm | Explicit | Parameter stated | Table I |
| 4, 5 | Angle Constraints (Center Link) | θc | 10-180 | deg | Explicit | Parameter range stated | Table I |
| 4, 5 | Angle Constraints (Center Link) | αc | 10-90 | deg | Explicit | Parameter range stated | Table I |
| 4, 5 | Angle Constraints (Side Link) | θs | 10-180 | deg | Explicit | Parameter range stated | Table I |
| 4, 5 | Angle Constraints (Side Link) | αs | 10-90 | deg | Explicit | Parameter range stated | Table I |
| 2, 3, 4 | Interaction Force Scaling | k | 2e5 | N/m | Explicit | Parameter stated | Table I |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Locomotion | Free movement config. | Avg. Speed | ~8 (single bot) | cm/s | Explicit | Measured property | Tracking/Simulation | Sec I, Fig 1A, 2A |
| Oscillation | Periodic Gait | Oscillation Frequency | Varies (e.g., ~0.5 Hz in Fig 2Cii) | Hz | Explicit | Measured property | Simulation data | Fig 2Cii |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Single Bot Speed (v0) | 8     | cm/s  | Table I, Sec I | Explicit | Directly measured/modeled speed. |
        | Vibration Period      | 1/80 = 0.0125 | s     | Sec I, VIIIA | Explicit | Inverse of vibration frequency. |
        | Relaxation to Neutral (Experim.) | ~5    | s     | Fig 1B(iii) | Explicit | Time shown in figure sequence. |
        | Relaxation to Neutral (Sim.)   | ~1    | s     | Fig 2B      | Explicit | Time shown in figure sequence. |
        | Oscillation Gait Period (Example) | ~2    | s     | Fig 2C(ii) | Explicit | Period estimated from velocity plot. |
        | Typical Experiment Duration | 20 - 60+ | s | Fig 1A, 2A, Simulation Plots | Explicit | Time ranges shown in plots/trajectories. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors are collective responses emerging from local interactions and constraints:
        1.  **Locomotion:** Directed forward movement as a coherent V-shaped chain.
        2.  **Gait Generation:** Distinct, stable modes of interaction with boundaries (translation, oscillation, stationary).
        3.  **Navigation:** Environment-dependent path selection, including traversing or blocking gaps, following or leaving walls/curves, and maneuvering around obstacles.
        4.  **Object Transportation:** Collective pushing, pulling, enclosing, or avoiding mobile objects based on link parameters and object size.
        5.  **Interaction:** Competitive (jamming) or cooperative (assisting passage) behaviors between two link-bots.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors are validated through:
        1.  **Experiments:** Physical realization using 3D-printed bots and links on a vibrating table, with trajectories and behaviors recorded (Figs 1, 3, 4B, S-Figs, Videos S1-S5).
        2.  **Computational Modeling:** An agent-based model simulating bot dynamics and interactions (Active Brownian Particles with link constraints) reproduces the key behaviors observed in experiments (Figs 2, 3, 4A, S-Figs).
        3.  **Phase Diagrams:** Systematically mapping link parameters (θc, αs) to observed gaits, demonstrating predictability and control (Figs 1Ciii, 2Ciii, S7).
        4.  **Quantitative Analysis:** Measurement of bot speed, MSD, velocity profiles for different gaits (Figs 1A, 2A, 1Cii, 2Cii).
        5.  **Qualitative Observation:** Descriptions and visualizations (snapshots, trajectories, videos) of complex navigation and transport tasks.
        Reproducibility is demonstrated by the consistency between experiments and simulations and clear phase boundaries. Limitations might include the specific range of parameters tested and environmental complexity explored.

---

#Key: [tanaka_molecular_2018]

# A molecular neuromorphic network device consisting of single-walled carbon nanotubes complexed with polyoxometalate

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a molecular neuromorphic network device composed of a complex network of single-walled carbon nanotubes (SWNTs) functionalized (complexed) with polyoxometalate (POM, specifically Phosphododecamolybdic acid, PMo12). It is fabricated on a Si/SiO2 substrate with Au electrodes. The device exhibits spontaneous spiking behavior, noise generation, and Negative Differential Resistance (NDR). Its purpose is to serve as an alternative to silicon CMOS for neuromorphic hardware, mimicking spiking neural networks and demonstrating rudimentary learning capabilities via reservoir computing (shown through simulation based on a model of the device). The SWNTs form a dense, random network, and POM particles adsorbed onto them act as molecular junctions with multi-redox properties.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Focuses on key physical and operational parameters mentioned in the excerpt.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is an external electrical potential (bias voltage, VB) applied across the terminal electrodes.
    *   Value: Up to 150 V (or higher, leading to instability)
    *   Units: V (Volts)

### **2.2 Energy Transduction**

    *   Content: Electrical energy input drives electron transport through the SWNT network. Energy is transduced at the POM/SWNT junctions via electrochemical redox reactions within the POM molecules (charge accumulation and discharge). This process leads to conductance switching (NDR), generating electrical current fluctuations, oscillations, and spikes (impulses). Potential energy stored in accumulated charges within POM is converted back into kinetic energy of electrons during discharge events, potentially cascading through the network.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any quantitative measure of energy efficiency for computation or information processing. The device operates at relatively high voltages (>>1V) and generates significant noise and current impulses, suggesting substantial energy dissipation rather than efficient processing. The primary focus is on demonstrating neuromorphic dynamics, not optimizing efficiency. Efficiency is likely very low (hence the low score).

### **2.4 Energy Dissipation**

    *   Content: Energy is dissipated primarily as heat due to resistive losses during electron transport through the SWNT network and junctions (Joule heating). Energy is likely also lost during the electrochemical redox cycles within the POM molecules (e.g., structural changes, interactions with counter-ions). The generated noise and chaotic current fluctuations represent dissipated energy not used for coherent computation. Quantification is not provided, but dissipation is likely high given the operating conditions and observed phenomena (noise, instability).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**

*   Units: States

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial

### **4.2 Local Interaction Rules:**

    *   Content: The paper proposes a CA model with specific local interaction rules governing charge transfer between adjacent POM cells (nodes) connected by SWNTs (edges) on a 2D grid. 1. Calculate potential gradients (Δa_k) between a cell (i,j) and its neighbors k. 2. Find the neighbor with the largest gradient (Δa_max). 3. If cell charge a_i,j < threshold a_TH: stochastically transfer a small number of charges N[m](Δa_max) to the neighbor with Δa_max, following probability P_c(a_i,j) = p / (e^(2(a_i,j - a_TH)) + 1)^q. 4. If cell charge a_i,j >= threshold a_TH: discharge *all* charges, transferring 90% to the neighbor with Δa_max and 10% to the neighbor with the second-largest gradient. Source electrode cells have constant charge VB, drain cells have zero charge.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | 3 | Charge Transfer Probability | p | 0.6 (PAc), 1 (PBc) | Dimensionless | Fig 4c, 4g, 4h | Explicit | Parameters p and q for the probability function Pc are explicitly given for simulations PA and PB. |
    | 3 | Charge Transfer Probability | q | 0.3 (PAc), 0.95 (PBc) | Dimensionless | Fig 4c, 4g, 4h | Explicit | Parameters p and q for the probability function Pc are explicitly given for simulations PA and PB. |
    | 3, 4 | Discharge Threshold | a_TH | 5, 40 | Dimensionless (Charge Units) | Fig 4f, 4g, 4i | Explicit | Values for a_TH used in simulations are explicitly stated. |

### **4.3 Global Order:**

    *   Content: The emergent global order described (primarily through the CA model) is the generation of collective, network-wide current impulses (spikes) observed at the drain electrode. These impulses can be random or exhibit periodic/aperiodic oscillations, representing synchronized or cascading discharge events propagating through the network. Spatio-temporal patterns of charge distribution also emerge across the 2D grid (Fig 4f insets).
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| 3 | Charge Transfer Probability (Pc) | p | 0.6, 1 | Dimensionless | Explicit | Values used in CA simulations PA and PB. | Fig 4g, 4h |
| 3 | Charge Transfer Probability (Pc) | q | 0.3, 0.95 | Dimensionless | Explicit | Values used in CA simulations PA and PB. | Fig 4c, 4g, 4h |
| 3, 4 | Discharge Threshold | a_TH | 5, 40 | Dimensionless (Charge Units) | Explicit | Values used in CA simulations. | Fig 4f, 4g, 4i |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Global_Current | Total current measured at drain electrode | Mean Current | ~0 - 1.5e-6 (Exp. A), ~0 - 8e-7 (Exp. B), 0-3 (Sim.) | A (Exp.), N_c/cycle (Sim.) | Explicit | Measured/Simulated average current characterizing overall network conduction. | Electrical Measurement/CA Simulation | Fig 3a, 3c, 4g, 4h |
| Spiking_Behavior | Generation of current impulses | Impulse Rate / ISI distribution | Variable (depends on VB, RP2S) | Hz / s | Explicit | Primary emergent dynamic behavior analyzed experimentally and in simulation. | Electrical Measurement/CA Simulation/ISI Analysis | Fig 3b, 3d, 3e, 3f, 4f |
| Oscillation | Periodic current modulation | Frequency | ~25 | Hz | Explicit | Observed in experimental data (Sample B). | Electrical Measurement | Fig 3d |
| Noise | Random current fluctuations | Variance/Amplitude | Increases with VB | A^2 / A | Explicit | Observed experimentally and in simulation. | Electrical Measurement/CA Simulation | Fig 2c, 4i |
| NDR | Non-monotonic I-V characteristic | Peak Voltage / Peak Current | Variable (depends on sample/sweep) | V / A | Explicit | Key electrical property linked to spiking. | Electrical Measurement/CA Simulation | Fig 1b, 3a, 3c, 4g, 4h |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | Local Rules to Global Spiking | Mapping from CA cell rules to network current impulses | Low (timing), Medium (qualitative type) | 4 | Qualitative comparison (NDR, Noise, Spike patterns), ISI statistics | Implicit | The paper validates the CA model by showing it qualitatively reproduces experimental phenomena (NDR, noise, random spikes), suggesting a mapping exists, but predictability of exact spike trains is low. Yoneda embedding is not explicitly invoked. Score reflects qualitative match but lack of quantitative prediction/formalism. | Fig 3 vs Fig 4 |
    *   **Metrics:** Qualitative comparison of I-V curves, noise properties, spike presence/randomness (via ISI distributions, Poincaré plots).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Reservoir Computing (Neuromorphic)

### **5.3 Computational Primitive:**

    *   Content: The most basic computational primitive arising from the material itself is the generation of complex, non-linear temporal dynamics, specifically spontaneous spiking (impulse generation) and noise. These dynamics, driven by the NDR and network interactions (as modeled by the CA), serve as the computational substrate (the "reservoir state") used by the external RC framework. Within the CA model, the thresholding operation (a_i,j >= a_TH leading to discharge) is a key primitive.
    *   **Sub-Type (if applicable):** Non-linear transformation/Spike generation/Thresholding.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Current Oscillation Period | ~40 (1/25Hz) | ms | Fig 3d | Explicit | Period calculated from explicitly stated frequency. |
        | Spike Duration (Impulse Width) | ~<1 (based on visual inspection of Fig 3b inset) | ms | Fig 3b | Implicit | Estimated visually from the plot; not explicitly quantified. |
        | Inter-Spike Interval (ISI) | ~10^-3 to 10^2 | s | Fig 3e, 3f | Explicit | Range taken from axes of Poincaré plots. |
        | I-V Sweep Time (implicit) | ~1.67 (100 PLC / 60 Hz) | s | Methods | Implicit | Calculated based on averaging over 100 power-line cycles at 60 Hz mentioned in Methods for Fig 3a/c measurements. Represents timescale of quasi-static measurements. |
        | CA Model Cycle | Arbitrary (represents discrete time step) | cycles | Fig 4 | Explicit | Simulation time is measured in cycles. Physical time correspondence is not defined. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Partial

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation/learning discussed is extrinsic, occurring within the reservoir computing framework applied to the *model*. This involves training linear readout weights (typically using methods like linear regression or ridge regression) to map the high-dimensional, non-linear state of the reservoir (the network's dynamic activity) to a desired output. The physical reservoir (the SWNT/POM network model) itself is generally considered fixed during this training process. No mechanism for *intrinsic* adaptation of the physical device (e.g., synaptic plasticity analogues) is described or demonstrated in the excerpt.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors observed are: 1. Negative Differential Resistance (NDR) in I-V characteristics. 2. Generation of spontaneous electrical noise (Gaussian-like at lower voltages, non-Gaussian/impulse-like at higher voltages). 3. Generation of spontaneous current impulses (spikes), which can be random or exhibit periodic/aperiodic oscillations. 4. Complex temporal dynamics suitable for use as a reservoir in Reservoir Computing (demonstrated in simulation).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The emergence of NDR, noise, and spiking is validated experimentally through electrical measurements (I-V curves, current vs. time plots, ISI analysis via Poincaré plots - Figs 1b, 2c, 3). The claim that these behaviors emerge from network effects and POM properties is supported by comparing experimental results with a 2D Cellular Automata (CA) model (Fig 4). The CA model, based on local rules embodying the hypothesized physics (charge accumulation, threshold discharge, NDR), qualitatively reproduces the key experimental behaviors (NDR shape, noise increase with voltage, random spike generation). This agreement between experiment and a model based on local interactions supports the claim of emergence. Reproducibility is demonstrated by presenting results from two types of samples (A and B). Limitations include the simplified nature of the 2D CA model compared to the real 3D network and the lack of quantitative prediction of exact spike timings.

---

#Key: [li_particle_2019]

# Particle robotics based on statistical mechanics of loosely coupled components

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of multiple loosely coupled "particle" robots. Each particle is disk-shaped and only capable of uniform volumetric (radial) expansion and contraction. Particles lack individual identity or addressable position. They couple passively via dangling magnets. The system's purpose is to demonstrate robust collective behaviors (locomotion, object transport, phototaxis) emerging from the simple, locally controlled oscillations of many stochastic components, controllable via a global signal modulating the phase of oscillation. The global behavior emerges when particle oscillation phases are coordinated, for example, based on an environmental gradient (light intensity). The system explores scalability and robustness to individual component failure.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name        | Value         | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :-------------------- | :-----------: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | System Speed (10 P.) | 1.24 ± 0.44 | mm/s  | Methods (derived from % dia/cycle) | Mixed          | Medium                          | Calculated from Fig 4a / Methods text |

    *   **Note:** System speed is derived from % diameter per cycle reported in Methods/Fig 4a for 10 particles, 0% dead, assuming a specific frequency, e.g., 0.6 Hz, and min diameter (1.24 mm/s = 9.6% * 155 mm / cycle * 0.6 cycle/s is an estimate).

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Electrical energy stored in batteries within each particle, powering the motor for radial oscillation and control/communication components.

### **2.2 Energy Transduction**

    *   Content: Electrical energy (battery) -> Mechanical energy (motor rotation) -> Mechanical energy (radial expansion/contraction via transmission mechanism). This mechanical energy performs work against internal friction, particle inertia, friction with the ground, inter-particle friction, and magnetic coupling forces, resulting in collective motion. Sensor (light sensor implied) transduces photonic energy (light stimulus) to an electrical signal used by the control circuitry to modulate oscillation phase.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Low. The paper does not provide any quantitative efficiency metrics. However, the mechanism involves many particles oscillating, generating friction (internal, ground, inter-particle) and overcoming magnetic forces. Much energy is likely lost as heat due to friction and motor inefficiency. Collective motion is described as incremental shifts, suggesting low conversion efficiency from individual oscillations to net displacement. The paper mentions optimization in terms of power efficiency is needed (Discussion).

### **2.4 Energy Dissipation**

    *   Content: High. Major dissipation mechanisms include:
        1.  Internal friction within each particle's actuation mechanism.
        2.  Friction between particles and the ground surface.
        3.  Inter-particle friction as particles rub against each other during oscillation and movement.
        4.  Heat loss in motors and electronics.
        5.  Energy loss during breaking/re-establishing magnetic couplings (hysteresis, inelastic collisions implied).
        Quantification is not provided.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: Skipped M3.2-M3.8 as M3.1 is "No")**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.2-M4.7 included as M4.1 is "Yes")**

### **4.2 Local Interaction Rules:**

    *   Content:
        1.  **Physical Coupling:** Particles adhere via flexible magnetic protrusions if close enough. Force is attractive but weak enough to allow passive breaking/re-establishing of connections (Fig 1e, Methods). Repulsive forces when compressed (modeled as spring/damper in simulation - Methods). Friction exists between particles and ground, and between particles.
        2.  **Control Rule:** Each particle sets its oscillation phase offset ($\phi_i$) based on a locally sensed signal ($s_i$, e.g., light intensity). $\phi_i \propto s_i$. Simplified approach uses a direct formula/lookup. Adaptive approach involves broadcasting sensed values, receiving others', and interpolating. (Methods: Control algorithms...). All particles require a synchronized clock.
        3.  **Actuation:** Each particle oscillates radially: $r_i(t) = R_{mean} + A \sin(2\pi f t + \phi_i)$, where $f$ is the global frequency.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID      | Description              | Parameter Name                                | Parameter Value Range | Units   | Data Source                     | Implicit/Explicit | Justification                       |
    | :----------- | :----------------------- | :-------------------------------------------- | :-------------------- | :------ | :------------------------------ | :---------------- | :---------------------------------- |
    | Physical     | Magnetic Coupling        | Attraction Force                              | > Static Friction       | N       | Abstract, Methods               | Mixed             | Relative strength stated explicitly |
    | Physical     | Repulsion (Sim)          | Spring Constant, Damping Coefficient          | Fitted empirically     | N/m, Ns/m | Methods (Simulation)          | Explicit          | Mentioned in simulation section   |
    | Physical     | Friction (Sim)           | Friction Coefficients                         | Fitted empirically     | unitless| Methods (Simulation)          | Explicit          | Mentioned in simulation section   |
    | Control      | Phase Range (Implied)    | e.g., 0 to 2$\pi$ or equivalent range mapped | radians | Fig 2c-g examples               | Implicit          | Implied by phase offset examples  |
    | Control      | Update Interval (Exp)    | 2-5                                           | min     | Methods (Physical experiment) | Explicit          | Stated in experimental setup      |
    | Actuation    | Oscillation Amplitude (A)| 4                                             | cm      | Fig 1d (derived: (23.5-15.5)/2) | Mixed             | Derived from explicit diameters   |

### **4.3 Global Order:**

    *   Content: Coordinated collective motion (directed locomotion, object transport), phototaxis (movement towards a light source), clustering from scattered state, and obstacle avoidance (mentioned in Fig 1f caption, demonstrated in Fig 3c). The emergent order manifests as a net displacement of the system's center of mass or rotation of the aggregate.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID   | Description                                                          | Parameter             | Value Range                  | Units      | Implicit/Explicit | Justification                         | Source                              |
| :-------- | :------------------------------------------------------------------- | :-------------------- | :--------------------------- | :--------- | :---------------- | :------------------------------------ | :---------------------------------- |
| coupling  | Passive magnetic adhesion                                            | Attraction Strength   | > Static Friction            | N          | Mixed             | Qualitative strength explicit         | Abstract, Methods                   |
| actuation | Radial oscillation                                                   | Frequency (f)         | 0.3, 0.6 (Exp), Tunable (Sim) | Hz         | Explicit          | Values used in experiments given      | Fig 2 caption, Methods              |
| actuation | Radial oscillation                                                   | Amplitude (A)         | 4                            | cm         | Mixed             | Derived from explicit diameters       | Fig 1d                              |
| physics   | Friction (Particle-Ground, Particle-Particle)                        | Coefficient ($\mu$)   | Empirically Fitted (Sim)     | unitless   | Explicit          | Existence explicit, value implicit  | Methods (Simulation env.)           |
| physics   | Repulsion (Particle-Particle contact, Sim)                           | Spring Constant (k)   | Empirically Fitted (Sim)     | N/m        | Explicit          | Stated for simulation model         | Methods (Simulation env.)           |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description           | Parameter                  | Value Range       | Units              | Implicit/Explicit | Justification                           | Protocol                                         | Source                               |
| :---------- | :-------------------- | :------------------------- | :---------------- | :----------------- | :---------------- | :-------------------------------------- | :----------------------------------------------- | :----------------------------------- |
| locomotion  | Directed movement     | Avg. Centroid Speed      | 0 - ~1.5 (Sim)    | mm/s               | Mixed             | Plotted in Fig 4a, values derived       | Track centroid position over time                | Fig 4a, Methods                      |
| phototaxis  | Movement towards light| Angle relative to Source | ~0 ± ~30 (Exp)    | degrees            | Explicit          | Plotted with distributions in Fig 3d    | Track centroid, measure angle to light source  | Fig 3d                               |
| robustness  | Function under failure| Speed degradation        | ~40-52% speed loss @ 20% dead | %                  | Explicit          | Explicitly stated range from Fig 4a | Simulate with % dead particles, measure speed  | Fig 4a, Discussion                 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type          | Description                                                  | Predictability | Yoneda Score | Metrics                                | Implicit/Explicit | Justification                                                                                              | Source       |
    | :----------------- | :----------------------------------------------------------- | :------------- | :----------- | :------------------------------------- | :---------------- | :--------------------------------------------------------------------------------------------------------- | :----------- |
    | Local_Rule_to_Global_Behavior | Mapping from particle phase rule to collective locomotion | Medium-High  | 6            | Mean direction (Fig 3d), Speed Variance (Fig 4a) | Explicit          | Paper shows clear link between phase strategy (local rule) and resulting motion (global), with quantifiable predictability. Yoneda score reflects this functional mapping. | Fig 3, Fig 4 |
    | Particle_Count_to_Speed_Variance | Relation between number of units and predictability of speed | High           | 7            | Std Dev of Speed (Fig 4a)              | Explicit          | Clear inverse relationship shown in Fig 4a. Higher score reflects clearer quantitative link.         | Fig 4a       |
    | Particle_Failure_to_Speed | Effect of local failure on global speed                | High           | 7            | Avg. Speed vs % Dead (Fig 4a)        | Explicit          | Clear relationship quantified in Fig 4a. Higher score reflects clear quantitative link.              | Fig 4a       |

    *   **Yoneda Embedding Fulfillment Score [0-10]:** 6 (Rubric: 0=No mapping; 3=Qualitative mapping; 6=Quantitative mapping for some aspects, predictable trends; 8=Formal mathematical mapping derived; 10=Complete isomorphism proven). The paper demonstrates clear, quantifiable relationships between local rules/states (phase, particle count, failure rate) and global emergent behaviors (direction, speed variance, average speed), supporting a functional mapping akin to Yoneda principles, though not formally derived using CT.
    *   **Metrics:** Mean Angle of Motion, Standard Deviation/Variance of Speed, Average Speed.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.2-5.4 included as M5.1 is "Yes")**

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid

### **5.3 Computational Primitive:**

    *   Content: Sensor mapping / Proportional control / Distributed consensus (interpolation variant). The most basic operation is mapping the local analog sensor value ($s_i$) to an analog phase offset ($\phi_i$), likely via a linear transformation ($\phi_i = k \cdot s_i + \phi_0$). The interpolation method involves averaging or weighting broadcast values, a form of distributed consensus finding.
    *   **Sub-Type (if applicable):** Proportional Mapping or Distributed Averaging.

### **5.4 Embodied Computational Units**
| Unit ID | Description                    | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification                                          |
| :------ | :----------------------------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :----------------------------------------------------- |

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description        | Value     | Units | Source                    | Implicit/Explicit | Justification                                  |
        | :--------------------------- | :-------: | :---- | :------------------------ | :---------------- | :--------------------------------------------- |
        | Particle Oscillation Period  | ~1.7 - 3.3 | s     | Fig 2 caption (f=0.3, 0.6 Hz) | Explicit          | Calculated as 1/f                              |
        | Phase Update Interval (Exp)  | 2-5       | min   | Methods                   | Explicit          | Stated in experimental setup description     |
        | Collective Locomotion (Exp)  | ~10-45    | min   | Fig 3                     | Explicit          | Duration shown in experimental result figures |
        | Simulation Duration          | 14.4 M loops / 2 hr | units | Fig 4 caption             | Explicit          | Stated in simulation results description       |
        | Dynamics Timescale (Sim)     | ~1200 cycles| cycles| Fig 4 caption             | Explicit          | Represents # oscillations in simulation duration |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes
        1.  **Behavioral Adaptation:** The collective behavior (direction, speed) adapts based on the sensed environmental gradient (light). The *phase offsets* of individual particles change over time as the system moves relative to the stimulus, leading to adaptive steering (phototaxis, obstacle avoidance implied). This change happens at the update intervals (2-5 min).
        2.  **Structural Adaptation:** The loose coupling allows the aggregate's shape and connectivity to change dynamically ("mutable unit", Methods) as it moves and interacts with obstacles or transports objects. Particles can detach and reattach ("passive connectivity can be broken and re-established spontaneously", Abstract). This allows the system to physically adapt its configuration.

**(Conditional: M7.2 included as M7.1 is "Yes")**

### **7.2 Adaptation Mechanism:**

    *   Content:
        1.  **Behavioral:** Sensory Feedback Control. Each particle adjusts its oscillation phase ($\phi_i$) based on the currently sensed stimulus intensity ($s_i$) at discrete update intervals. The rule $\phi_i \propto s_i$ (or interpolation) serves as the feedback mechanism. This is adaptation via parameter tuning based on environmental feedback, not learning in the sense of modifying the underlying rule itself.
        2.  **Structural:** Passive Physical Rearrangement. Governed by the physics of loose magnetic coupling, particle oscillations, and interactions with the environment (ground friction, obstacles). Changes in configuration are not actively controlled or learned but emerge passively from the dynamics.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content:
        1.  **Collective Locomotion:** Net translation of the particle aggregate's center of mass. Can be directed (forward motion, turning) based on phase patterns (Fig 2c-g).
        2.  **Phototaxis:** Directed locomotion towards a light source (stimulus gradient following) (Fig 3a, 3d).
        3.  **Object Transport:** Collective movement carrying an object embedded within the aggregate (Fig 2b, 3b).
        4.  **Clustering:** Aggregation of initially scattered particles (Fig 2a).
        5.  **Obstacle Avoidance:** Maneuvering around obstacles (mentioned Fig 1f caption, demonstrated Fig 3c).
        6.  **Robustness:** Maintaining locomotion capability despite a percentage of malfunctioning ("dead") particles (Fig 4).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Emergent behaviors (locomotion, phototaxis, transport, obstacle avoidance, robustness) are validated through both physical hardware experiments (up to 25 particles, Fig 1, Fig 3) and computational simulations (up to 100,000 particles, Fig 2, Fig 4). Quantitative analysis includes tracking centroid position/speed (Methods, Fig 3d, Fig 4a), measuring angle relative to light source (Fig 3d), and statistical analysis of speed degradation with particle failure (Fig 4a). Control experiments include varying light source location to eliminate bias (Fig 3d) and testing different phase patterns (Fig 2). Reproducibility is suggested by repeating simulations with random initializations (Fig 4a, 10 runs per condition). Limitations include 2D demonstration, specific particle design used, and potential simulation artifacts (e.g., particle detachment at large N).

---

#Key: [puy_signatures_2024]

# Signatures of criticality in turning avalanches of schooling fish

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of schools of black neon tetra fish (Hyphessobrycon herbertaxelrodi) with varying numbers of individuals (N=8, 16, 32, 50) freely swimming in an approximately two-dimensional experimental tank (L=100 cm side, 5 cm water depth). The purpose is to investigate spontaneous behavioral cascades, specifically "turning avalanches," where large directional shifts (measured by turning rate ω derived from velocity and acceleration) propagate through the group. The study analyzes avalanche metrics (duration T, size S, interevent time ti) using tools from statistical physics (criticality, power laws, scaling, data collapse) and seismology (aftershocks, Omori law) to identify signatures of criticality and understand the dynamics and biological function (e.g., collective decision-making) of these events. Fish trajectories were recorded and digitized.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** The turning rate ω and its threshold ω_th are derived quantities (Eq. 1, Appendix B) but explicitly defined and used as key parameters for defining avalanches. Pixel units are considered "natural units" in the paper.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the metabolic energy derived from the fish consuming food (external to the experiment's focus). Within the experiment, energy is manifested as kinetic energy for swimming and turning. The paper mentions the "burst-and-coast" mechanism powered by fish muscles (Sec V).

### **2.2 Energy Transduction**

    *   Content: Chemical (metabolic) energy stored in the fish is transduced into kinetic energy for swimming (translational movement) and rotational kinetic energy during turns via muscle contraction. Information transfer (behavioral cascade) could be viewed as a form of energy/signal propagation through the group, but the paper doesn't frame it in terms of physical energy transduction.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss or quantify the energy efficiency of fish swimming, turning, or the propagation of behavioral cascades.

### **2.4 Energy Dissipation**

    *   Content: Energy is dissipated primarily through hydrodynamic drag as the fish move through the water and internal physiological processes (heat). The "coast" phase of the burst-and-coast mechanism (Sec V) implies energy dissipation due to drag. Quantification is not provided. Qualitative assessment: Medium/High (typical for biological locomotion in fluid).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: ~250 (tc); related to Omori decay `p=2.2`
*    Units: frames

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Related to Omori exponent `p = 2.2 ± 0.1`

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The paper does not explicitly define the local interaction rules between fish (e.g., alignment, attraction, repulsion forces as functions of distance/orientation). It analyzes the *emergent consequences* of these interactions. The rules are implicitly assumed based on standard models of collective motion referenced in Sec I (e.g., Refs [11-14, 27-30]). The definition of an 'active' fish (ω > ω_th) acts as a local rule for avalanche participation, but not the underlying interaction causing the turn. Fish also interact with tank walls (Sec V).
    * **Implicit/Explicit**: Implicit

### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**

    *   Content: Global order emerges in the form of:
        1.  **Polarization (φ):** The degree to which fish align their velocity vectors (Sec VI, Fig 6b). Values range from ~0.5 to ~0.9.
        2.  **Collective Motion:** Coordinated movement of the school as a whole (measured by center-of-mass velocity `v_cm`, Sec V, VI).
        3.  **Statistical Order:** Scale-free (power-law) distributions of avalanche duration (T), size (S), and interevent times (ti), indicating long-range spatiotemporal correlations characteristic of systems near criticality (Sec III, IV, Fig 2, 3). Avalanche shapes also show scaling (Sec IV, Fig 4).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| GO-1 | Polarization | φ | ~0.5 - 0.9 | dimensionless | Explicit | Measured group alignment (Sec VI) | Eq in Sec VI text | Fig 6b |
| GO-2 | Avalanche Duration Scaling | α | 2.4 ± 0.2 (ω_th=0.1) | dimensionless | Explicit | Power-law fit to P(T) (Sec III) | Linear regression on log-log plot | Fig 2a |
| GO-3 | Avalanche Size Scaling | τ | 1.97 ± 0.14 (ω_th=0.1) | dimensionless | Explicit | Power-law fit to P(S) (Sec III) | Linear regression on log-log plot | Fig 2b |
| GO-4 | Duration-Size Scaling | m | 1.41 ± 0.06 (ω_th=0.1) | dimensionless | Explicit | Power-law fit to <S>_T vs T (Sec III) | Linear regression on log-log plot | Fig 2c |
| GO-5 | Interevent Time Scaling | γ | 1.62 ± 0.08 (ω_th=0.1) | dimensionless | Explicit | Power-law fit to P(ti) (Sec III) | Linear regression on log-log plot | Fig 2d |
| GO-6 | Avalanche Shape Scaling | m (exponent) | 1.41 (used) | dimensionless | Explicit | Scaling collapse fit n_t * T^(1-m) vs t/T (Sec IV) | Visual collapse using derived m | Fig 4 |
| GO-7 | Aftershock Decay | p | 2.2 ± 0.1 | dimensionless | Explicit | Omori law fit to aftershock rate (Sec VII) | Least-squares fit | Fig 7c |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Avalanche Duration (T) | Variable (Power-law tail α≈2.4) | frames | Fig 2a | Explicit | Measured distribution |
        | Avalanche Size (S) | Variable (Power-law tail τ≈2.0) | fish*frames | Fig 2b | Explicit | Measured distribution |
        | Interevent Time (ti) | Variable (Power-law tail γ≈1.6) | frames | Fig 2d | Explicit | Measured distribution |
        | Correlation Time (tc) | ~250 | frames | Fig S6, Sec VII | Explicit | Calculated from MSD |
        | Aftershock Decay Time Constant (c) | 4.3 ± 0.4 | frames | Fig 7c, Sec VII | Explicit | Fitted Omori parameter |
        | Data Sampling Interval | 1/50 = 0.02 | s | Appendix A | Explicit | Inverse of frame rate |

    *   **Note:** Durations T, S, ti are stochastic variables characterized by distributions, not single values. tc is an average timescale derived from dynamics.

### **6.2 Active Inference:**

    *   Content: Unclear/Partial
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** Potential metrics could involve quantifying the change in predictive error (e.g., likelihood of future wall collision) before and after an avalanche, measuring the information-theoretic cost/benefit of turning vs. not turning, or modeling the fish's internal state representation of the environment (tank boundaries, group configuration) and how avalanches update this model. CT-GIN could model the belief propagation during an avalanche.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main emergent behaviors described are:
        1.  **Turning Avalanches:** Spatiotemporal cascades where large changes in heading direction (ω > ω_th) propagate through the fish school. These avalanches exhibit variable duration (T) and size (S). (Sec II, III)
        2.  **Scale-Free Statistics:** The distributions of avalanche duration P(T), size P(S), and interevent times P(ti) exhibit power-law scaling over intermediate ranges, suggesting criticality. (Sec III, Fig 2)
        3.  **Scaling Relationships and Data Collapse:** Robust scaling relations exist between T and S (<S>_T ~ T^m), and data collapses are observed for P(T) and P(ti) at fixed activity rates, and for P(ti) normalized by <ti>, and for the avalanche shape n_t. (Sec III, IV, Fig 2c, 3, 4)
        4.  **Temporal Clustering (Aftershocks):** Avalanches tend to occur in clusters, with the probability of observing an "aftershock" avalanche decaying with time according to an Omori-like law following a "main" event. (Sec VII, Fig 7)
        5.  **Collective Turning/Reorientation:** Large avalanches often result in a significant change in the school's overall direction of motion, potentially linked to collective decision-making, especially in response to walls. (Abstract, Sec V, VI, Fig 5b, 6c)

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors (avalanches, scale-free stats, clustering, collective turns) are validated through:
        *   **Operational Definitions:** Clear definitions for avalanches based on turning rate threshold ω_th, duration T, size S, interevent time ti (Sec II).
        *   **Quantitative Analysis:** Calculation of probability density functions (PDFs) for T, S, ti (Fig 2, 3). Measurement of average size vs duration <S>_T (Fig 2c). Calculation of avalanche shapes n_t (Fig 4). Analysis of CM speed, polarization, wall distance during avalanches (Fig 6). Calculation of aftershock statistics, proximity η, rescaled time/space T_j/R_j, Omori law fit (Fig 7).
        *   **Statistical Tests:** Power-law fitting with confidence intervals for exponents α, τ, γ, m (Sec III). Dragon king detection test (p-values given, Sec V, Appendix D). Comparison with null models (randomized data for T_j/R_j in Fig 7b, uncorrelated model in Appendix C).
        *   **Data Collapse:** Demonstrating universal scaling functions by collapsing PDFs for different N or ω_th (Sec IV, Fig 3, 4).
        *   **Reproducibility:** Mention of multiple recordings for each N suggests consistency (Appendix A), although variability between recordings isn't explicitly quantified.
        *   **Limitations:** Finite experiment duration limits the observation range of power laws. Potential influence of tank size (finite-size effects mentioned for tails, Sec III). Limited number of fish (up to 50).

---

#Key: [nelson_delivering_2023]

# Delivering drugs with microrobots

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system comprises microrobots, devices ranging from several nanometers to submillimeter sizes, designed for targeted therapeutic delivery within the human body. Their primary purpose is to overcome limitations of systemic drug delivery, such as toxicity, by transporting cargo (e.g., drugs, liposomes, stem cells) precisely to disease locations like tumors or blood clots. Components typically include a chassis (often polymer or polymer-metal microarchitectures), potentially magnetic materials for actuation/tracking, and the therapeutic payload. Various propulsion mechanisms are explored, including chemical reactions, light, ultrasound, and externally applied magnetic fields, with the latter considered most viable for *in vivo* navigation. Designs include individual units, swarms, superstructures, and composites. Key functionalities involve deployment (often via catheter/endoscope), navigation through body fluids/tissues, cargo release at the target, and subsequent degradation/clearance.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Values are extracted directly or represent examples given in the text. Reliability is High for explicitly stated specs like imaging resolution, Medium for typical/example values like speed or clearable size which might vary.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Primary energy sources are external fields (magnetic, ultrasound, light) or chemical potential energy (reactions on the device surface using local fuels like H2O2 or enzymatic reactions). Magnetic fields are highlighted as the most viable near-term option for *in vivo* actuation.

### **2.2 Energy Transduction**

    *   Content: Energy is transduced into kinetic energy for propulsion (swimming, gliding). Magnetic fields generate forces/torques on magnetic components for locomotion. Ultrasound can trap and move structures. Light can impart momentum or induce photocatalytic reactions. Chemical reactions on the surface generate gradients or bubbles for propulsion. Magnetic fields can also be transduced into thermal energy (hyperthermia) or electrical energy (via magnetoelectric materials). Shape transformation is another transduction pathway (e.g., magnetic field energy to mechanical deformation).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The text doesn't provide quantitative efficiency values but highlights significant challenges related to propulsion at small scales ("vastly different physics," "profound engineering challenges"). Locomotion speeds are low (~100 µm/s), implying potentially low efficiency in converting input energy into useful motion against viscous forces at the microscale. The challenges mentioned for chemical (trajectory control, fuel concentration) and light (penetration depth) propulsion also suggest limitations impacting overall effectiveness/efficiency in a practical sense. Magnetic actuation is presented as more viable but still faces challenges. Score is low due to emphasis on challenges and low speeds mentioned. Qualitative Assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: The dominant dissipation mechanism at these scales is viscous drag within body fluids. The text implicitly addresses this by discussing the challenges of propulsion ("vastly different physics," low Reynolds number regime where viscous forces dominate). Other potential dissipation mechanisms, though not explicitly quantified, include heat loss during magnetic hyperthermia, potential energy loss through material deformation/friction in complex mechanisms (e.g., microcars), and inefficiencies in chemical reactions. Qualitative Assessment: High (dominated by viscous drag).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Microrobot Locomotion Speed | ~100 | µm/s | Section 2, para 6 | Explicit | Example value given. |
        | Example Travel Time (Arm->Brain) | up to 3 | hours | Section 2, para 6 | Explicit | Calculated example based on speed and distance. |
    *   **Note:** Specific timescales beyond the locomotion example are mostly discussed qualitatively or implied.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is controlled locomotion through biological fluids and tissues to a target site (e.g., tumor, blood clot) for the purpose of delivering a payload (drugs, cells). Secondary behaviors include payload release (triggered), potential therapeutic actions like hyperthermia (magnetic field induced), shape transformation (externally triggered), and eventual degradation/clearance. Collective behaviors involve deployment and navigation as swarms, superstructures or composites, followed by disassembly at the target.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper primarily validates behaviors through referencing preclinical *in vivo* studies in animal models (mice, rats) for tasks like navigation, tumor targeting (drug-loaded liposomes via magnetotactic bacteria (Ref 6)), liver cancer treatment (MRI-trackable robots (Ref 7)), stem cell delivery (Ref 8), and enzymatic propulsion in the bladder (Ref 9). Tracking methods mentioned include MRI and X-ray CT, although limitations for single robots are noted. Diagnostic cerebral angiography with magnetic robotic catheters in large animal models is mentioned to demonstrate human-scale navigation feasibility. Validation relies on demonstrating the intended function (e.g., reaching target, delivering cargo, achieving therapeutic effect) in these models. Limitations include the gap between animal models and human clinical settings, and challenges in precise tracking and control.

---

#Key: [zhang_computing_2021]

# Computing with non-orientable defects: nematics, smectics and natural patterns

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper presents a theoretical and computational framework for modeling the energetics, dynamics, and interactions of topological defects (dislocations, disclinations) in ordered media, specifically nematics, smectics, and natural patterns (like convection rolls). The core idea is to enrich macroscopic coarse-grained theories (based on order parameters like director fields `k`) with additional fields derived from continuum mechanics concepts of defects in solids. Primarily, it introduces a field `B` representing the 'singular' part of the gradient of the director field (`Dk`), allowing the 'regular' part `A = Dk - B` to remain well-behaved. This enrichment, along with associated defect densities (disclination density `π = curlA = -curlB`, dislocation density `γ = -curlk`), allows for thermodynamically consistent models with integrable energy densities, resolving singularities present in naive macroscopic theories. The system components include the phase field `θ`, the director field `k`, the defect field `B`, derived defect densities `π` and `γ`, and a free energy functional `ψ`. The purpose is to provide a unified, mathematically well-posed, and computationally tractable approach to study defect behavior across different material systems where classical models fail due to non-integrable energy densities associated with defect cores. It uses gradient flow dynamics based on the free energy and conservation of topological charge to model defect evolution and interaction.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value                  | Units            | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---------------------: | :---------------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | `P1`           | 100 (Nematic default)  | Dimensionless    | Sec 3.2                   | Explicit          | High (Specified for simulation) | Non-dimensionalized             |
        | `ε`            | 1 (Nematic default)    | Dimensionless    | Sec 3.2                   | Explicit          | High (Specified for simulation) | Non-dimensionalized             |

    *   **Note:** Parameters listed are key components of the free energy (Eq. 7) and computational examples. `K` and `ξ` set the energy and length scales. `P1`, `P2`, `α`, `K*`, `ε` are non-dimensionalized parameters used in the simulations, controlling different energy contributions. Values provided are defaults for nematic simulations from Sec 3.2. `P2` is 0 for nematics, 1 for smectics. `K*` is 5 for nematics. Units for K are inferred based on its role in Frank energy; others are dimensionless as presented in the non-dimensionalization section (3.1) or simulation section (3.2).

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The system, as modeled by the gradient flow dynamics (Eq. 13) used for computations, does not have an external energy input. It models the relaxation of an initial (potentially high-energy) configuration towards a local minimum of the free energy `ψ` (Eq. 7). The energy driving the dynamics is the internal free energy stored in the field gradients and defect structures.

### **2.2 Energy Transduction**

    *   Content: The primary energy transformation is the dissipation of the system's internal free energy `ψ` (Eq. 7) during the relaxation process described by the gradient flow dynamics (e.g., Eq. 10, 11, 13). The free energy, stored in various forms (elastic energy of director gradients `K|Dk-B|^2`, core energy `ε|π|^2`, director magnitude penalty `P1(|k|-1)^2`, etc.), drives changes in the fields `k` and `B`. This "potential" energy is converted into "dissipated" energy through the flow rules, analogous to conversion into heat in a physical system undergoing relaxation. The mobility parameters (`M_k`, `M_B` in Eq. 10-11, implicitly 1 in Eq. 13) mediate the rate of this dissipation.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The concept of energy efficiency (useful work output / energy input) is not applicable to the described system, which models spontaneous relaxation towards a minimum free energy state via gradient flow. The process is inherently dissipative, converting internal potential energy into a dissipated form (analogous to heat).

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation is inherent in the gradient flow dynamics used to model the system's evolution towards equilibrium (Eq. 10, 11, 13). The rate of change of total free energy is non-positive (`d/dt ∫ ψ dv ≤ 0`), as shown in the derivation (Sec 2.3, Eq. 9 leading to 10-11). Dissipation occurs through the "frictional" resistance to changes in the fields `k` and `B`, modulated by mobility coefficients (`M_k`, `M_B`, assumed constant and positive, or implicitly 1 in the non-dimensionalized Eq. 13). The paper does not quantify the absolute rate of dissipation or provide numerical values for mobilities in physical units, but the mechanism is explicitly part of the model's thermodynamic foundation. Qualitative Assessment: High (during relaxation from a non-equilibrium state).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are encoded in the free energy density functional `ψ` (Eq. 7) and the gradient flow evolution equations (Eq. 10, 11 for general thermodynamics; Eq. 13 for computation).
        *   **Energy Density `ψ` (Eq. 7):** `ψ = P1(|k|-1)^2 + P2|curlk|^2 + αK*f(|B|) + K|Dk-B|^2 + ε|π|^2`. This defines the local energy cost based on the magnitude of `k`, its curl, the magnitude of the defect field `B` (via the non-convex function `f`), the mismatch between `Dk` and `B`, and the magnitude of the disclination density `π = -curlB`.
        *   **Evolution Equations (e.g., Eq. 13):**
            *   `∂k/∂s = ...` describes how the director field `k` changes locally based on terms involving `k`, `B`, `Dk`, `curl k`, and parameters (`P1`, `P2`).
            *   `∂B/∂s = ...` describes how the defect field `B` changes locally based on `Dk - B`, the derivative of `f`, and `curl B` (or `π`), along with parameters (`α`, `K*`, `ε`).
        These equations define how each field evolves based on its local neighbourhood values and gradients, minimizing the total energy `∫ ψ dv`.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description                     | Parameter Name | Parameter Value Range    | Units         | Data Source | Implicit/Explicit | Justification                          |
    | :------ | :------------------------------ | :------------- | :----------------------- | :------------: | :----------: | :----------------: | :-------------------------------------: |
    | Eq. 7   | Director magnitude penalty      | `P1`           | 100 (Nem); 1 (Smec)    | Dimensionless | Sec 3.2, 3.2.5 | Explicit          | Specified for simulations              |
    | Eq. 7   | Director curl penalty           | `P2`           | 0 (Nem); 1 (Smec)      | Dimensionless | Sec 3.2, 3.2.5 | Explicit          | Specified for simulations              |
    | Eq. 7   | Defect field `B` potential      | `α`            | 10, 50 (Nem); 50 (Smec) | Dimensionless | Sec 3.2, 3.2.5 | Explicit          | Specified for simulations              |
    | Eq. 7   | Defect field `B` potential scale | `K*`           | 5 (Nem, Smec)          | Dimensionless | Sec 3.2, 3.2.5 | Explicit          | Specified for simulations              |
    | Eq. 7   | Disclination core energy scale| `ε`            | 1 (Nem); 0.1 (Smec)    | Dimensionless | Sec 3.2, 3.2.5 | Explicit          | Specified for simulations              |
    | Eq. 7   | Non-convex potential shape    | `f(|B|)`       | Wells at 0, 2/(aξ)     | Dimensionless | Sec 2.3, 3.2   | Explicit          | Described qualitatively and used   |

### **4.3 Global Order:**

    *   Content: The framework describes the emergence of various global (or extended) ordered states and defect structures. Examples computationally realized include:
        *   Isolated point defects (disclinations) of strength ±1/2 and +1 in 2D nematics (Figs 4, 9, 11, 7).
        *   Composite defects like strength -3/2 disclinations (Fig 12).
        *   Disclination loops in 3D nematics (Fig 15).
        *   Smectic grain boundaries, potentially containing defect arrays (disclination dipoles/dislocations) (Figs 19, 20).
        *   Stripe patterns (discussed in context of Swift-Hohenberg and Cross-Newell, Fig 22).
        *   Domain structures with different director orientations separated by defect layers/walls (implicit in the `B` field construct).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description                   | Parameter Name | Value Range           | Units          | Implicit/Explicit | Justification                        | Source         |
| :------ | :---------------------------- | :------------- | :-------------------- | :-------------: | :----------------: | :--------------------------------- | :-------------: |
| ψ-P1    | Director magnitude penalty    | `P1`           | 1 to 100              | Dimensionless  | Explicit          | Controls cost of deviation from |k|=1 | Sec 3.2, 3.2.5 |
| ψ-P2    | Director curl penalty         | `P2`           | 0 to 1                | Dimensionless  | Explicit          | Enforces zero curl (smectics)      | Sec 3.2, 3.2.5 |
| ψ-αK*   | Defect field `B` potential    | `α`, `K*`, `f` | α: 10-50, K*: 5       | Dimensionless  | Explicit          | Governs energy of B, wells at 0, 2 | Sec 2.3, 3.2   |
| ψ-ε     | Disclination core energy    | `ε`            | 0.1 to 1              | Dimensionless  | Explicit          | Energy cost of disclination density| Sec 3.2, 3.2.5 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID        | Description                  | Parameter          | Value Range            | Units          | Implicit/Explicit | Justification                           | Protocol                                 | Source      |
| :----------------- | :--------------------------- | :----------------- | :--------------------- | :-------------: | :----------------: | :-------------------------------------- | :--------------------------------------- | :---------- |
| DirectorField      | Local average orientation    | `k(x)`             | Vector, \|k\|≈1        | Dimensionless  | Explicit          | Primary order parameter field           | Solved from Eq. 13                       | Sec 2.1, 3  |
| DisclinationDensity| Density of orientation mismatch| `π(x) = -curlB`    | Tensor field           | 1/Length^2     | Explicit          | Measures disclination charge            | Calculated from B field                | Sec 2.3, Eq.6|
| DislocationDensity | Density of layer mismatch    | `γ(x) = -curlk`    | Vector field           | 1/Length       | Explicit          | Measures dislocation charge (smectics)| Calculated from k field                | Sec 2.3, Eq.6|
| DefectStrength     | Topological charge (point)   | ∫ π•dA or angle(k) | ±1/2, ±1, etc.         | Dimensionless  | Explicit          | Characterizes point defect topology     | Calculated from field configuration    | Sec 3.2     |
| GrainBoundary      | Interface between domains    | `B` localisation   | \|B\|≈2 on layer       | 1/Length       | Explicit          | Region where director flips rapidly     | Identified from B field simulation     | Sec 3.2.5   |
| PhaseField         | Layer position (smectics)    | `θ(x)`             | Scalar field           | Radians        | Explicit          | Reconstructed from k (if curlk=0)     | Solved via Helmholtz decomposition (Eq. 20)| Sec 3.2.5   |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description                 | Predictability | Yoneda Score | Metrics        | Implicit/Explicit | Justification                                      | Source |
    | :-------- | :-------------------------- | :------------- | :----------- | :------------- | :----------------: | :------------------------------------------------- | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units            | Source       | Implicit/Explicit | Justification                                       |
        | :-------------------- | :----: | :---------------: | :-----------: | :----------------: | :-------------------------------------------------- |
        | Simulation Time Step  | `∆s`  | Dimensionless    | Sec 3.2 (FEM) | Explicit          | Used in numerical gradient flow evolution.        |
        | Non-dimensional Time  | `s̃`   | Dimensionless    | Sec 3.1       | Explicit          | Scaled time used in gradient flow equations (Eq 13). |
        *   **Note:** The paper primarily uses non-dimensionalized time `s̃` related to physical time `t` via `s˜ = (KM/ξ^2) t`. The characteristic physical relaxation timescale depends on material parameters (`K`, `M`, `ξ`) which are not assigned physical values in the excerpt, only non-dimensional combinations for simulations. `∆s` is the numerical time step size, value not specified.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors described and simulated are the formation, interaction, and equilibrium configurations of topological defects in ordered media. This includes:
        *   Formation of stable/metastable point disclinations (±1/2, +1, -3/2 strength) with characteristic director fields and localized core energies (Figs 4, 5, 7, 9, 11, 12, 13).
        *   Formation of disclination loops in 3D (Fig 14, 15).
        *   Energetics of defect interactions, such as the splitting of an unstable +1 defect into two +1/2 defects and their subsequent repulsion (Sec 3.2.2, Fig 10).
        *   Formation of smectic grain boundaries, potentially incorporating arrays of defect dipoles (dislocations) to lower energy (Sec 3.2.5, Figs 18, 19, 20, 21).
        *   Relaxation towards minimum energy states via gradient flow dynamics.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation of the emergent behaviors (defect configurations and interactions) is primarily achieved through:
        1.  **Computational Simulation:** Solving the derived PDEs (Eq. 13) using the Finite Element Method (Appendix B) for specific scenarios (initial conditions, boundary conditions, parameters) to obtain equilibrium states (Sec 3.2).
        2.  **Physical Plausibility:** Comparing the computed field configurations (`k`, `B`, `ψ`) and structures (defects, layers) with known theoretical expectations and qualitative features of defects in nematics and smectics (e.g., director patterns around ±1/2 defects, Fig 4; Frank energy decay `1/r^2` outside core, Fig 6; structure of smectic boundaries, Fig 20).
        3.  **Energetic Analysis:** Comparing the total free energies of different configurations to assess relative stability (e.g., split +1 defect pair vs. single +1 defect, Sec 3.2.2; defected vs. defect-free smectic boundaries, Sec 3.2.5).
        Limitations include reliance on specific parameter values chosen for simulations and lack of direct comparison with quantitative experimental data within the excerpt.

---

#Key: [chen_synthetic_2024]

# A synthetic protein-level neural network in mammalian cells

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a synthetic protein circuit implemented in mammalian cells (HEK293T) designed to perform winner-take-all (WTA) neural network computation entirely at the protein level. Its purpose is to classify molecular signals based on their relative concentrations. The core components are engineered fusion proteins combining de novo designed heterodimers (DHDs) for input sensing and weighting, split viral proteases (TEVp and TVMVp) for signal processing and mutual inhibition, and degrons (DHFR-based) for controlling protein stability and implementing self-activation. Input proteins (DHD binders) regulate the reconstitution of split proteases. The reconstituted proteases can cleave degrons from themselves (self-activation) and cleave dimerization domains fused to the *other* protease type (mutual inhibition). The protease with the higher activity (driven by higher corresponding input concentration and weight) dominates, cleaving the opposing protease components and stabilizing

---

#Key: [beck_dynamic_2025]

# Dynamic Markov Blanket Detection for Macroscopic Physics Discovery

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is a theoretical framework and an associated class of unsupervised algorithms for dynamic Markov blanket detection (DMBD). Its purpose is to partition complex, high-dimensional dynamical systems (observed microscopic dynamics) into interacting macroscopic subsystems ("objects" or "things"), identify the boundaries (Markov blankets) of these objects, classify object types based on their boundary statistics/dynamics, and discover the macroscopic physical laws governing their interactions. The core components are: (1) The Free Energy Principle (FEP) and associated concepts (Markov blankets, ontological potentials, maximum caliber principle); (2) A generative modeling approach using variational Bayesian expectation maximization (VBEM); (3) Latent variables representing macroscopic states (internal `z`, boundary `b`, environment `s`) and latent assignment variables (`ω`) labeling microscopic elements according to their role (internal, boundary, environment). The algorithm dynamically identifies and classifies objects (like components of Newton's cradle, a burning fuse, Lorenz attractor, simulated cell in examples) and their interaction rules from partial observations of microscopic dynamics, leveraging Bayesian attention and Occam's razor for model selection. It aims to provide a data-driven method for systems identification and macroscopic physics discovery, particularly for systems with dynamic or porous boundaries and material turnover.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters listed are key hyperparameters of the DMBD algorithm implementation described or used in examples. Values are often context-dependent within the paper's experiments.

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping to Module 4.)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The "local" rules governing the computational self-organization are embedded within the VBEM algorithm applied to the specific generative model (Eqs. 22-24 for the linear example). Key rules include:
        1.  **Conditional Independence (Markov Blanket Structure):** The generative model structure (Eqs. 4-6 conceptually, Eq. 22 for linear model with block-zero matrix A, Eq. 20 for observation model) enforces conditional independencies based on the latent labels `ω`. `s` does not directly influence `z`, and vice versa. Observations `y_i` depend only on the latent state corresponding to their label `ω_i`.
        2.  **Label Transition Constraints:** Label transitions (`ω_i`) are governed by a transition matrix T (Eq. 23), constrained such that direct S <-> Zn transitions are prohibited (T_SZn = T_ZnS = 0). Transition probabilities may depend on boundary variables `{b_n}` (Eq. 21), though simplified to be independent in the linear example (Eq. 23).
        3.  **VBEM Update Rules:** The iterative updates for the approximate posterior distributions `q(s,b,z)` and `q(ω)` (Eqs. 26-27) constitute the core local rules. Each variable's posterior is updated based on the expected values of the others under their current posteriors and the model parameters `Θ`. Parameter updates `q(Θ)` follow standard rules for conjugate priors (mentioned as matrix normal gamma, matrix normal inverse Wishart, Dirichlet).
        4. **Prior Assumptions:** Priors on dynamics (e.g., favoring linear dynamics in Eq. 22, Gaussian noise) and parameters implicitly act as rules guiding the inference.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The global order that emerges from the algorithm's operation is the specific partitioning of the observed system into macroscopic objects (defined by the MAP estimates of the assignment variables `ω_i(t)`) and the characterization of these objects via the learned macroscopic dynamics (parameters `A, B, Σ_sbz`) and object-environment interaction rules (parameters `C, D, Σ_ω, T`). This represents a low-dimensional, structured description of the original high-dimensional system. Examples: identifying moving vs. stationary balls in Newton's cradle (Fig 5), separating burned/unburned regions in the fuse (Fig 9), distinguishing attractor basins in Lorenz (Fig 10), segmenting cell components in Lenia (Fig 11).
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Partition | Assignment labels | ω_i(t) | {S, B_n, Z_n} | Category | Explicit | Direct output of the algorithm | VBEM MAP estimate | Sec 3, Figs 5, 9, 10, 11 |
| Interaction Law | Observation mapping | C_ω, D_ω | Real matrices/vectors | Depends on y units | Explicit | Learned parameters based on label ω | VBEM Mean estimate | Eq. 24 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6.)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Observation Timestep | Implicitly discrete (t, t+1) | Arbitrary (depends on simulation) | Throughout (e.g., Eq 22-24, Fig 4) | Implicit | Dynamics are modeled in discrete or continuous time, examples use discrete steps. |
        | Algorithm Convergence Time | 50 epochs mentioned | Training Epochs | Section 4 | Explicit | Time for the VBEM algorithm to train. |
        | System Dynamics Timescale (e.g., Newton's Cradle period) | Varies by example | Arbitrary (depends on simulation) | Figs 5, 8, 10 | Explicit (from plots) | Characteristic time of the simulated physical systems being analyzed. |
        | Label Assignment Update Timescale | Per observation timestep | Arbitrary (depends on simulation) | Section 3, Eq 23 | Explicit | Labels ω_i can potentially change at each timestep t. |
    *   **Note:** The relevant timescales are either related to the algorithm's operation (convergence) or the dynamics of the systems *being analyzed* by the algorithm. The framework itself deals with paths/trajectories over time (τ).

### **6.2 Active Inference:**

    *   Content: Partial
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** Testable metrics would apply to the *discovered systems*, not the DMBD algorithm itself. If the algorithm were extended to model action:
        *   `PredictionErrorReductionRate`: Measure how quickly the system's internal model (`q(s,b,z)`) converges or reduces prediction error (e.g., KL divergence between predicted and actual next state) after an action. (Edge attribute on `FeedbackLoop`).
        *   `BehavioralPolicyComplexity`: Quantify the complexity (e.g., information cost, dimensionality) of the learned policy `p(a|o)` derived from blanket statistics `p(b_τ)`. (Attribute of `PolicyNode`).
        *   `AnticipationTimescale`: How far into the future does the system's internal model accurately predict environmental states or consequences of actions? (Attribute of `PredictionEdge`).
        *   `GoalAchievementRate`: If goals are defined, measure the success rate or efficiency of actions chosen based on minimizing expected free energy. (Attribute of `BehaviorArchetypeNode`).

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is Variational Bayesian Expectation-Maximization (VBEM) applied to the parameters (`Θ`) of the generative model.
        1.  **Expectation (E-step):** Compute the posterior distribution over latent variables (`s, b, z, ω`) given the current parameter estimates (using forward-backward for dynamics). See Eqs 26, 27.
        2.  **Maximization (M-step):** Update the posterior distribution over the parameters (`q(Θ)`) to maximize the Expected Lower Bound (ELBO) given the current latent variable posteriors. This involves updating the sufficient statistics of the assumed conjugate prior distributions (e.g., matrix normal gamma, matrix normal inverse Wishart, Dirichlet mentioned in Sec 3.1). The paper mentions using stochastic natural gradient descent with a learning rate (Sec 3.1, Sec 4) for iterative updates, essentially adapting parameters based on gradients derived from the ELBO and data mini-batches. This is a form of Bayesian learning/parameter tuning.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary "behavior" of the system (the DMBD algorithm) is **Dynamic System Partitioning and Macroscopic Law Discovery**. Given time-series data from microscopic elements, it outputs:
        1.  A dynamic assignment of each microscopic element to a role (internal, boundary, or environment) within discovered macroscopic objects.
        2.  The identification of distinct macroscopic objects or subsystems.
        3.  The learned dynamical laws (e.g., state transition matrices, observation models) governing the behavior of these macroscopic objects and their interactions.
        Observable behaviors in the *examples* include identifying: colliding/stationary balls (Newton's cradle), the combustion front/boundary (fuse), attractor switching (Lorenz), cell membrane/nucleus formation (Lenia).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The validation of the emergent behavior (system partitioning) is primarily qualitative and based on visual inspection and comparison with human intuition or known physics for the simple simulated examples.
        *   **Operational Definitions:** The algorithm's output (MAP assignments `ω_i(t)`, learned parameters `Θ`) serves as the operational definition of the discovered partition and laws.
        *   **Control Experiments:** No explicit control experiments are described (e.g., comparing against null models or alternative partitioning algorithms).
        *   **Quantitative Analysis:** The primary quantitative measure used is the ELBO, maximized during training, which indicates model fit but not necessarily the "correctness" or usefulness of the partition. No quantitative metrics comparing the discovered partition to a ground truth are used (ground truth might be ambiguous anyway). Principal components of latent variables are plotted (Fig 6), but mainly for visualization.
        *   **Robustness/Reliability/Reproducibility:** Mentioned running simulations multiple times (>=10) and choosing the best ELBO run (Sec 4), suggesting some check for reproducibility/convergence. Acknowledged finding less sensible solutions sometimes (Sec 4.2).
        *   **Limitations:** Validation relies heavily on subjective assessment ("sensibly labels", "consistent with human intuition") for simple systems. Lack of quantitative comparison metrics. Untested on complex, real-world data.
        *   **Citations:** Section 4, Figures 5, 6, 8, 9, 10, 11.

---

#Key: [scheidegger_modelling_2020]

# Modelling Artificial Immune – Tumor Ecosystem Interaction During Radiation Therapy Using a Perceptron – Based Antigen Pattern Recognition

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a computational model simulating the interaction between a tumor ecosystem (including tumor sub-clones and host tissue) and an artificial immune system during and after simulated radiation therapy (RT). The ecosystem dynamics involve repopulation, mutation, competition, and radiation/immune-induced cell death. The immune system's adaptive response is modeled using a perceptron for antigen pattern recognition. The perceptron learns via reinforcement learning based on antigen presence and a 'danger signal' derived from cell death. The perceptron's output governs antibody generation, which in turn influences cell death rates. The purpose is to investigate the dynamic interactions, particularly the evolution of perceptron weights, to generate hypotheses about immune system responses to RT, rather than direct clinical application. Components include: tumor cell populations (T_ik), host tissue population (H), necrotic cell populations (N_ik, N_H), antibody population (I_n), antigen pattern vectors (X_i, P_kl), perceptron weights (w_i), danger signal (D), perceptron response (Y), and radiation dose effects (Γ). The model is described by a system of ordinary differential equations (ODEs).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameter reliability is assessed based on the paper's statement that most values describe an *artificial* model, except radiation sensitivities (α, β) which are cited as known for real tissues. k_mut (mutation rate), a (perceptron learning rate), and  (sigmoid exponent) are model-specific assumptions.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The model is computational; there is no explicit physical energy input discussed. The simulated radiation therapy (RT) acts as an external perturbation influencing cell death, analogous to an energy input causing damage, but it's modeled via dose (Gy) and biological effect parameters (α, β, Γ), not explicit energy units (Joules).

### **2.2 Energy Transduction**

    *   Content: No physical energy transduction is modeled. Information flows through the system: RT dose impacts cell populations → cell death generates danger signal (D) and antigen patterns (X) → perceptron processes D and X → perceptron output (Y) influences antibody (I_n) production → antibodies influence cell death rates. This is an information processing cascade, not energy transformation.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The concept of energy efficiency is not applicable to this computational model, as physical energy consumption and conversion are not modeled or discussed.

### **2.4 Energy Dissipation**

    *   Content: No physical energy dissipation mechanisms are modeled. Within the simulation, numerical dissipation might occur due to the integration method (Runge-Kutta), but this is not discussed. Biological processes modeled (cell death, competition) inherently involve energy dissipation in real systems, but it's not quantified or explicitly modeled here.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding to M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Potentially Long-term (persistent unless updated)
*    Units: Qualitative Descriptor

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 9
*   Units: weights (continuous values)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: 0 (within the model)
    *   Units: d⁻¹

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial

**(Conditional: M4.1 is "Partial", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The "local" rules are the terms within the ODEs governing the rate of change of each state variable (cell populations T_ik, H; necrotic cells N_ik, N_H; antibodies I_n; perceptron weights w_i; radiation effect Γ). Key interaction rules include:
        *   Cell Growth: Linear growth (k_T, k_aH).
        *   Competition: Quadratic inhibition terms (-k_TT*T, -k_bH*H, -k_TH*T, -k_HT*H).
        *   Mutation: Transfer between tumor subclones (-k_mut*T_ik, +k_mut*q_il*T_il).
        *   Spontaneous Death: Linear decay (-k_eT*T_ik, -k_eH*H).
        *   Radiation Death: Dependent on dose rate R, sensitivity (α, β), and repair dynamics (Γ) via Γ-LQ model; impacts T_ik and H.
        *   Immune Death: Dependent on antibody presence I_n and antigen-antibody match r_ik = I•P_ik; impacts T_ik and H (-r_ik*k_IT*T_ik, -r_k*k_IH*H). Included in Eq.6 as factor r_ik. The equation appears to simplify this to `-r_ik*k_IT*T_ik` and `-r_k*k_IH*H` where `r_ik` is calculated via Eq.9 and the rate constants `k_IT` and `k_IH` scale the effect, but Eq.6 just shows `r_ik` multiplied by the population size `T_ik` or `H`. Let's assume `r_ik` in Eq.6 represents the full immune killing term rate `r'_ik = r_ik * k_IT` based on the text description preceeding Eq.6.
        *   Necrosis Production: Proportional to cell death rates.
        *   Necrosis Removal: Linear decay (-k_n*N).
        *   Danger Signal (D) Generation: Sigmoid function of total necrosis (Eq. 8).
        *   Antigen Pattern (X_i) Generation: Sigmoid function of relevant cell populations (Eq. 1).
        *   Perceptron Weight Update (w_i): Reinforcement learning rule, proportional to input X_i and error (D-Y) (Eq. 3).
        *   Perceptron Response (Y): Sigmoid function of weighted antigen sum Σ (Eq. 4).
        *   Antibody (I_n) Production: Proportional to perceptron output Y and antigen presence X_n (Eq. 10).
        *   Antibody Removal: Spontaneous decay, radiation effects, and consumption via immune interaction (Eq. 10).
        *   Radiation Repair (Γ): First-order decay dynamics (Eq. 5).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Growth | Tumor Growth Rate | k_T | 3.46e-2 | d⁻¹ | Table 1 | Explicit | Value from table |
    | Competition | Tumor-Tumor Interaction | k_TT | 1e-4 | d⁻¹ | Table 1 | Explicit | Value from table |
    | Mutation | Tumor Mutation Rate | k_mut | 1e-3 | d⁻¹ | Table 1 | Explicit | Value from table |
    | Immune Death | Tumor Immunogenic Elimination Scaling | k_IT | 10 (or 1) | d⁻¹ | Table 1 | Explicit | Value from table |
    | Learning | Perceptron Weight Update Rate | a | 1 | d⁻¹ | Table 1 | Explicit | Value from table |
    | Rad Death | Tumor Radio-Sensitivity (alpha) | α_T | 0.3 | Gy⁻¹ | Table 1 | Explicit | Value from table |
    | Danger Signal | Necrosis Activation Level | L_act | 1 | (norm. cells)² | Table 1 | Explicit | Value from table |
    | Perceptron Resp | Activation Steepness |  | 9 | dimensionless | Table 1 | Explicit | Value from table |

### **4.3 Global Order:**

    *   Content: The main emergent global order described is the separation of perceptron weights (w_i) after simulated RT. Weights corresponding to tumor-associated antigens (odd indices in the paper's setup) remain positive, while weights corresponding to host tissue antigens (even indices) can evolve to negative values (Fig. 4b, 5). This differentiation reflects the system learning to distinguish between tumor ('non-self'/'danger-associated') and host ('self'/'less-danger-associated' post-RT) patterns under the influence of RT-perturbed ecosystem dynamics. Other global patterns include tumor regrowth dynamics, host tissue repopulation/suppression, and oscillatory or stable states of the populations.
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| WeightSep | Separation of weights | w_i (even) | >0 to <0 | dimensionless | Explicit | Values shown in Fig 4b, 5, 6, 7, 8, 9 | Simulation | Fig 4b, 5-9 |
| WeightSep | Separation of weights | w_i (odd) | >0 | dimensionless | Explicit | Values shown in Fig 4b | Simulation | Fig 4b |
| PopDyn | Tumor Population Dynamics | T_ik | 0 to ~250 | norm. cells | Explicit | Values shown in Fig 4a | Simulation | Fig 4a |
| PopDyn | Host Population Dynamics | H | 0 to ~1 | norm. cells | Explicit | Values shown in Fig 4a | Simulation | Fig 4a |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", including M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Neuromorphic/Analog

### **5.3 Computational Primitive:**

    *   Content: Weighted Summation followed by Non-linear Thresholding (Activation). The core computation involves calculating Σ = Σ(w_i * X_i) (Eq. 2) and then applying a sigmoidal activation function Y = Σ^ξ / (Y_act^ξ + Σ^ξ) (Eq. 4) to produce the perceptron's response. This combination is the fundamental operation of a single-layer perceptron. The weight update rule (Eq. 3) represents a learning operation (Delta rule variant).
    *   **Sub-Type (if applicable):** Thresholding: Sigmoid; Learning: Reinforcement (Delta Rule)

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Simulation Duration   | 1200-1500 | d     | Methods | Explicit | Stated in text. |
        | Numerical Time Step   | 10⁻³      | d     | Methods | Explicit | Stated value for Runge-Kutta. |
        | Radiation Fraction Interval | 1 or 2 | d     | Methods, Fig 7 | Explicit | Different schemes simulated. |
        | Radiobiological Repair Halftime (implicit) | ~ln(2)/γ ≈ 0.17 | d | Table 1 (γ=4) | Implicit | Calculated from repair constant γ. |
        | Tumor Growth (characteristic time ~1/k_T) | ~29 | d | Table 1 (k_T=3.46e-2) | Implicit | Calculated from growth rate constant. |
        | Weight Update Rate (characteristic time ~1/a) | ~1 | d | Table 1 (a=1) | Implicit | Calculated from learning rate constant. |
        | Weight Separation Onset Post-RT | ~54 | d | Results (p545) | Explicit | Stated observation from Fig 4. |
        | Ecosystem Dynamics (Oscillations/ Equilibration) | 100s - 1000s | d | Fig 4a | Explicit | Visual inspection of simulation results. |
    *   **Note:** Multiple timescales govern the system, from numerical integration steps (ms) to radiation scheduling (days), biological processes (days to weeks), and long-term ecosystem evolution (months to years).

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism is reinforcement learning, specifically a form similar to the Delta Rule, applied to a single-layer perceptron. The rate of change of each weight w_i is proportional to the input signal X_i associated with that weight and an "error" or reinforcement signal (D - Y). D represents the "desired" state (high danger signal indicates a need for strong response), and Y represents the current perceptron response. The learning rate is 'a'. Equation: dw_i/dt = a * (D - Y) * X_i. This rule adjusts weights to make the perceptron's output Y better match the context provided by the danger signal D for given antigen patterns X.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors are:
        1.  **Differential Immune Recognition:** The system learns to differentiate between tumor and host tissue antigen patterns, reflected in the separation of perceptron weights (positive for tumor, potentially negative for host post-RT). This leads to a potentially targeted or suppressed immune response.
        2.  **Ecosystem Dynamics Modulation:** The adaptive immune response (governed by the perceptron) interacts with RT effects and intrinsic population dynamics (growth, competition, mutation) to shape the overall evolution of tumor and host populations (e.g., tumor regrowth, host suppression/repopulation dynamics shown in Fig. 4a).
        3.  **Immune Escape Mimicry:** Tumor mutation leads to the loss of antigen expression (Fig. 2 structure), allowing some tumor sub-clones (like T32) to potentially evade the learned immune response and grow towards an equilibrium (Eq. 11).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claims of emergent behaviors (dynamics, weight separation) are validated through numerical simulation of the ODE model. The specific patterns are observed outcomes of the simulations under different conditions (initial values, RT schedules, parameter values). Control conditions (e.g., no RT, different parameter sets) are implicitly compared when analyzing the effect of RT or specific parameters (Figs. 4-9). The paper presents simulation results graphically (Fig 4-9) and discusses the observed dynamics. Robustness is partially assessed via parameter sweeps (Fig 5-9). However, validation is purely computational; there's no comparison to experimental data. Reproducibility relies on the provided model equations and parameters. Limitations include the artificiality of the model and parameters, lack of stochasticity, and absence of experimental validation.

---

#Key: [thedford_promise_2023]

# The Promise of Soft-Matter-Enabled Quantum Materials

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews the emerging field of using soft matter self-assembly techniques to synthesize and structure quantum materials (QMs). It highlights the limitations of traditional QM synthesis (MBE, PLD, crystal growth) in controlling structure beyond the atomic scale and their limited form factors. Soft matter methods (using synthons like surfactants, block copolymers (BCPs), colloids, DNA, MOFs) offer tunable mesoscale (nm to μm) structural control, diverse form factors, and potentially lower-cost, scalable solution-based processing. The *purpose* is to bridge the gap between soft matter and hard condensed matter physics to explore novel emergent QM properties arising from mesoscale architecture. *Specific examples discussed* include structuring superconductors (Pb, Sn, NbN, NbCN, TiN), topological materials (photonic crystals, MOFs, COFs), and magnetic materials (multiferroics like CFO/PZT, NFO/BTO, spin ices like Co, Ni) using techniques like structure direction, templating (soft/hard), and co-assembly. The *components* are broadly soft matter synthons and precursor materials for inorganic QMs. The system *does* enable mesostructural control over resulting QMs.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters reflect the general scope discussed in the review. Reliability is 'High' as these are explicitly stated/cited values or concepts central to the review's topic.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Primarily thermal energy (for annealing, processing, self-assembly driving forces) and chemical potential (driving precursor conversion, phase separation). Specific examples mention light (laser annealing, lithography), mechanical energy (shear alignment), electrical fields (electrodeposition), magnetic fields (particle alignment). Energy inputs relate to the *synthesis/processing* methods, not necessarily the operational energy of the final quantum material (which isn't the focus).

### **2.2 Energy Transduction**

    *   Content: During *synthesis/processing*: Chemical energy is converted to structural order via precursor reactions and self-assembly driven by minimizing free energy (enthalpic/entropic contributions). Thermal energy drives diffusion, annealing, phase transitions, and template removal (calcination). Light energy drives photochemistry (lithography) or rapid localized heating/melting/crystallization (laser annealing). Electrical energy drives electrochemical deposition. Mechanical energy drives alignment. In the *final materials* (briefly touched upon): discussion of transductions relevant to QM properties like electron-phonon coupling (superconductivity), spin-orbit coupling (topology), light-matter interactions (photonics), magnetoelectric coupling (multiferroics).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The review focuses on synthesis methods and resulting properties, not the energy efficiency of these processes or the final materials in operation. While soft matter processing is often described as "facile and cost-effective" (Section 1), suggesting potential for higher efficiency compared to UHV methods, this is qualitative and not quantified. No efficiency metrics are provided.

### **2.4 Energy Dissipation**

    *   Content: Dissipation mechanisms are inherent in the synthesis processes (heat loss during annealing/calcination, resistive losses in electrodeposition, scattering/absorption in laser annealing) but are not quantified or discussed in detail. The review mentions dissipation (scattering/absorption losses) as a challenge in soft matter photonic/acoustic topological materials compared to electronic counterparts (Section 2.2). Non-Hermitian effects due to dissipation are mentioned as potentially introducing new topological features (Section 2.2).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The review implicitly refers to the thermodynamic principles governing self-assembly. For BCPs: interplay of enthalpic repulsion between dissimilar blocks (quantified by Flory-Huggins parameter χ) and entropic cost of chain stretching, aiming to minimize Gibbs free energy (Section 3.1). For colloids: van der Waals forces, electrostatic interactions, steric repulsion, depletion forces; sometimes specific interactions like DNA hybridization (Section 3.2, 3.4). For surfactants/amphiphiles: hydrophobic effect, electrostatic interactions, packing parameter (Section 3.1). For MOFs: coordination chemistry between metal ions and organic linkers (Section 3.3). These rules are local, determining how individual synthons interact with their neighbors.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | BCP Phase Separation | Interaction Strength | Flory-Huggins Parameter (χ) | Variable (depends on A, B blocks) | Dimensionless | Section 3.1 | Explicit | Defines enthalpic repulsion. |
    | BCP Phase Separation | Chain Stretching Cost | Degree of Polymerization (N) | Variable | Dimensionless | Section 3.1 | Explicit | Defines entropic penalty. |
    | Colloidal Assembly | Particle Size | Colloid Diameter (d) | nm - μm | Length | Section 3.2, Fig 2 | Explicit | Determines packing and lattice constant. |
    | MOF Assembly | Coordination Geometry | Metal Ion Coordination Number | Variable | Dimensionless | Section 3.3 | Explicit | Determines network topology. |

### **4.3 Global Order:**

    *   Content: Various periodic mesostructures emerge globally from local interactions. Examples explicitly mentioned include: Spherical micelles, Lamellar sheets, Hexagonal phases, Bicontinuous structures (e.g., Double Gyroid, Single Gyroid), Close-packed colloidal crystals (FCC, HCP), Inverse Opals, Superlattices (e.g., cubic from DNA origami), Honeycomb lattices (MOFs/COFs).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| BCP_Interaction | Block Repulsion | χ (Flory-Huggins) | >0 | Dimensionless | Explicit | Determines immiscibility strength. | Section 3.1 |
| BCP_Entropy | Chain Stretching | N (Degree of Poly) | 10s-1000s | Dimensionless | Explicit | Entropic penalty opposing segregation. | Section 3.1 |
| DNA_Hybridization | Base Pairing | Binding Energy | kJ/mol | Energy | Implicit | Governs DNA-mediated assembly. | Section 3.4 |
| MOF_Coordination | Metal-Ligand Bonding | Coordination Bond Strength | kJ/mol | Energy | Implicit | Determines MOF framework stability. | Section 3.3 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Periodicity | Lattice Spacing | Unit cell dimension (d) | nm - μm | Length | Explicit | Characteristic length scale of the pattern. | Scattering (SAXS, SANS) | Fig 2, Section 1 |
| Long-Range Order | Crystal Quality | Correlation Length / Domain Size | nm - mm | Length | Explicit | Describes the extent of defect-free order. | Scattering, Microscopy | Section 3.5, 5.1 |
| Crystallinity (Atomic) | Atomic Lattice Quality | Crystallite Size / % Crystallinity | nm / % | Length / Fraction | Explicit | Describes order within the inorganic component (if applicable). | XRD | Section 3.1, 5.1 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | QM Phenomenon (e.g., coherence time) | Variable (fs to ms or more) | time | Implicit | Relevant for QM properties but not focus of synthesis review. | Section 1 (general QM) |
        | Ferroic Switching (in Multiferroics) | Variable (ps to ms) | time | Implicit | Relevant property timescale for materials mentioned (MOFs, heterostructures). | Section 2.3 |
    *   **Note:** Timescales are highly variable depending on the specific system (soft matter assembly or quantum phenomenon) and often not specified in the review, which focuses on structure-property relations. Laser annealing is an exception.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behaviors discussed are the quantum material properties themselves, which arise from the combination of atomic-level quantum effects and the imposed mesostructure. These include: Superconductivity (zero resistance, Meissner effect, Josephson coupling, altered Tc/Hc, vortex pinning), Topological Properties (edge/surface states, topological invariants, protected transport - electronic, photonic, phononic), Magnetic Properties (ferromagnetism, ferroelectricity, multiferroicity, magnetoelectric coupling, spin frustration/spin ice behavior, giant magnetoresistance). Mesoscale structure itself emerges from self-assembly (see M4.3).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The review cites experimental validation methods standard in condensed matter physics for the claimed emergent behaviors. Examples include: Transport measurements (resistance vs temperature/field for superconductivity), Magnetization measurements (SQUID for superconductivity, magnetism), Angle-resolved photoemission spectroscopy (ARPES for topological band structure, Fig 4e,f), Microwave transmission (photonic Weyl points, Section 1), Small-angle neutron scattering (SANS for flux lattices, Section 2.1), Scanning tunneling microscopy (STM for MOF structure, Fig 4e), Electron microscopy (SEM/TEM for morphology, Fig 3, 5), X-ray diffraction/scattering (XRD/SAXS for crystal/mesostructure, Section 2.1, Fig 4d). Robustness is implicitly addressed by the ongoing challenge to create large, defect-free samples for reliable measurements (Section 3.5).

---

#Key: [soto_programmable_2023]

# Programmable Shape Morphing Metasponge

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a "metasponge," described as a metamaterial hydrogel composite. It consists of an absorbent hydrogel material (sodium polyacrylate powder) integrated within a highly stretchable and robust elastomer matrix (Ecoflex). The purpose is to create a programmable material that morphs into customized sizes and shapes upon exposure to aqueous solutions, dynamically tuning its physical properties (mechanical, optical, sonic) and enabling functionalities like robotic actuation, light guidance, cloaking, sampling, and drug delivery. The morphing occurs as the absorbent material swells within the elastomer matrix upon water absorption. Active (swelling) and passive (non-swelling elastomer only) regions can be spatially distributed to create complex, asymmetric shape changes.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Lists key parameters characterizing the system's *implementation*. Always include units if applicable.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source driving the system's main function (shape morphing) is the chemical potential difference leading to the absorption of the surrounding aqueous solution (typically water) by the sodium polyacrylate hydrogel component. Thermal energy influences the rate of swelling (Fig 2g). Other inputs are used for specific applications (e.g., magnetic field for robocar actuation, light for optical guidance, acoustic energy for testing).

### **2.2 Energy Transduction**

    *   Content: The primary transduction is Chemical Energy (from water absorption) -> Mechanical Energy (volumetric expansion/swelling of the hydrogel within the elastomer matrix). This mechanical energy leads to shape morphing and changes in physical properties. Secondary transductions include: Mechanical Energy -> Kinetic Energy (robotic actuation, jellyfish propulsion), Chemical Energy (catalysis) -> Kinetic Energy (jellyfish buoyancy via gas production), Optical Energy -> Optical Signal (light guiding), Acoustic Energy -> Acoustic Signal (transmission measurement). Environmental conditions (pH, ions, temperature) modulate the primary Chemical -> Mechanical transduction efficiency/rate.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any quantitative measure of energy efficiency for the primary swelling process (Chemical -> Mechanical) or subsequent actuations. Efficiency is likely low for actuation tasks, typical for swelling-based actuators, but this is not quantified. Efficiency depends heavily on the specific application (e.g., light guiding efficiency could be measured, but wasn't reported in thermodynamic terms).

### **2.4 Energy Dissipation**

    *   Content: Dissipation mechanisms during swelling primarily involve viscoelastic losses within the elastomer matrix as it stretches and frictional forces related to water diffusion through the polymer network. During actuation, energy dissipates through viscous drag (if in fluid), friction with surfaces, and internal material damping. Heat is likely generated during swelling (enthalpy of mixing) and potentially during mechanical deformation, though not quantified. In acoustic measurements, energy is dissipated via absorption and scattering within the material. In light guiding, dissipation occurs via scattering and absorption.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Environment-dependent (hours to days typically)
*    Units: hours/days (Qualitative Descriptor: "Volatile - Requires Hydration")

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Cycle Stability | Swelling ratio retention over cycles | ~Consistent over 5 cycles | Fold (L/L₀) | MemoryNode attribute: cycleStability | Figure 2b, S4 | Explicit | Graph shows consistent peak swelling ratios. |
    | Environmental Sensitivity | Swelling dependence on ions, pH, temp | Variable (See Fig 2d,f,g) | Fold (L/L₀) | MemoryNode attribute: envSensitivity | Figure 2d,f,g | Explicit | Graphs quantify swelling under different conditions. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial/No

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

**(Skipping M4.2-M4.7 as M4.1 is Partial/No)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

**(Skipping M5.2-M5.4 as M5.1 is No)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Initial Swelling (Significant Growth) | ~1 | hour | Fig 2a | Explicit | Graph shows near-doubling in length within 1 hour. |
        | Swelling to Plateau | ~12 | hours | Fig 2a | Explicit | Graph shows swelling plateaus around 12 hours. |
        | Deswelling (Passive Drying) | Overnight (~8-12) | hours | Section 2, Fig 2b | Explicit | Text states drying overnight reverses swelling. |
        | Microfluidic Valve Actuation (Flow Stop) | ~2 | minutes | Fig 6a, 6c | Explicit | Text and graph indicate flow stops around 2 mins. |
        | Temperature Effect (30 min swelling) | 30 | minutes | Fig 2g | Explicit | Experiment duration explicitly stated for Fig 2g. |
        | Drug Release (Sustained) | >1 | hour | Fig 6k | Explicit | Experiment showed release over 1 hour. |
    *   **Note:** Relevant timescales identified from experimental results.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism of adaptation is the physical process of hydrogel swelling/deswelling within the confining elastomer matrix. Water absorption by the sodium polyacrylate, driven by osmotic pressure/chemical potential gradients, causes the polymer chains to uncoil and expand, stretching the elastomer matrix. This process is modulated by environmental factors: ionic strength (ions shield charges, reducing swelling), pH (protonation/deprotonation of carboxylic groups affects water interaction), temperature (affects diffusion rates and polymer chain mobility), and solvent polarity. The spatial distribution of active/passive material dictates the macroscopic shape change resulting from this local swelling. The adaptation is reversible via dehydration. It's a direct physical response, not based on complex learning rules like Hebbian or reinforcement learning.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behaviors demonstrated are:
        1.  **Shape Morphing/Actuation:** Controlled changes in shape (e.g., 2D to 3D transformation, bending, curling, expansion) driven by differential swelling, enabling robotic functions (walking star, grappling, bridge formation, jellyfish propulsion).
        2.  **Tunable Physical Properties:** Changes between dry and swollen states alter mechanical stretchability, optical opacity/translucency (encryption/camouflage, light guiding), and acoustic impedance (cloaking/transmission).
        3.  **Fluid Absorption/Sampling:** Uptake of aqueous solutions enables sampling (pH, bacteria, hemoglobin) and triggered release (drug delivery).
        4.  **Flow Control:** Swelling used to block microfluidic channels (autonomous valve).
        5.  **Enhanced Catalysis:** Swelling increases surface area for embedded enzymes (lactase).
        6.  **Locomotion Modification:** Swelling of robocar wheels changes diameter/width, altering displacement and terrain navigation ability.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper validates the described behaviors through direct experimental demonstration and quantification.
        *   **Shape Morphing:** Photographic evidence (Fig 1b, 1c, 4a-e, 5a, 5d-f, 6a, 6j-k), time-lapse imaging (Videos S4-S9, S12), and quantitative measurements of size change (Fig 2a,c-i, 5b).
        *   **Tunable Properties:** Mechanical tests (Fig 3a-c), optical transmission observations/measurements (Fig 3d-g, Video S3), acoustic transmission measurements (Fig 3h-k).
        *   **Sampling/Release:** Demonstrations using pH indicators, bacteria imaging (Fig 6h), hemoglobin assays (Fig 6i, S10, S11), dye release imaging (Fig 6k).
        *   **Flow Control:** Measurement of output volume over time (Fig 6b,c, Video S10).
        *   **Catalysis:** Measurement of glucose byproduct (Fig 6e,f).
        *   **Locomotion:** Measurement of displacement (Fig 5c,d, Video S7).
        Reproducibility is implied by providing sample sizes (n=...) for quantitative data. Control experiments are used (e.g., elastomer only, dry vs. swollen). Limitations: Long-term stability and robustness under complex, uncontrolled conditions are not fully explored. Claims are generally well-supported by the presented data within the tested scope.

---

#Key: [chen_enormous-stiffness-changing_2022]

# Enormous-stiffness-changing polymer networks by glass transition mediated microphase separation

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a polymer network composed of poly(N-isopropylacrylamide) (PNIPAm) crosslinked within an ionic liquid (IL) solvent (e.g., [C1MIM][NTf2]). It functions as a material with widely switchable stiffness, transitioning reversibly between a soft ionogel state (~0.6 kPa) and a rigid plastic state (~85 MPa) upon temperature change. This transition is achieved by exploiting liquid-liquid phase separation (LLPS) intercepted by polymer vitrification (glass transition) near the Berghmans' point in the binary gel's phase diagram. Heating above the Upper Critical Solution Temperature (UCST) results in a homogeneous, soft gel. Cooling below the UCST and the glass transition temperature (Tg) of the polymer-dense phase induces microphase separation into a bicontinuous structure of a vitrified (glassy) polymer-dense phase and a gelated (soft) polymer-sparse phase, leading to a rigid state. The purpose is to create materials with enormous, reversible, and tunable stiffness changes for applications like soft robotics and flexible electronics. Key components are the PNIPAm polymer network, ionic liquid solvent, and a crosslinker (Ethyleneglycoldimethacrylate).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name              | Value                   | Units     | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :-------------------------- | :----------------------: | :-------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input is thermal energy (heat) used to control the temperature of the system, driving it across the UCST and T_g transitions.

### **2.2 Energy Transduction**

    *   Content: Thermal energy is transduced into changes in the material's phase structure and mechanical properties. Input thermal energy changes the miscibility of the polymer (PNIPAm) and solvent (IL), driving liquid-liquid phase separation (LLPS) below the UCST. Below the Berghmans' point temperature, this LLPS is coupled with the vitrification (glass transition) of the polymer-dense phase. The change in phase structure (homogeneous gel vs. bicontinuous vitrified/gelated structure) directly corresponds to a change in the macroscopic mechanical stiffness. Energy is stored/released during the phase transitions (enthalpy changes, implicitly mentioned via DSC measurements).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify the energy required for the transition versus the mechanical work the material can perform or the energy stored/released during the phase change in a way that allows for efficiency calculation. The focus is on the state change and property contrast, not energy conversion efficiency. Qualitatively, phase transitions usually involve latent heat but are driven primarily by reaching specific temperatures, not continuous energy conversion for work output.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation likely occurs primarily as heat exchanged with the environment during heating and cooling cycles to reach the transition temperatures. Internal friction during phase separation and rearrangement might contribute minor dissipation. Hysteresis in heating/cooling curves (implied by cooling rate dependence in Supp. Fig 5) suggests dissipative processes. Mechanical testing (Fig 2d) shows plastic deformation in the hard state and likely viscoelastic losses in the soft state, indicating mechanical energy dissipation under load, but this is separate from the switching energy. Quantification is not provided. Qualitative assessment: dissipation via heat exchange is significant for temperature cycling; mechanical dissipation depends on usage.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term (stable indefinitely under constant conditions)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 2 (Binary)
*   Units: distinct states

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: High (Qualitative)

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interactions governing self-organization are:
        1.  **Polymer-Solvent Interaction:** Temperature-dependent miscibility between PNIPAm and the IL. Below the UCST, unfavorable interactions drive phase separation (demixing) into polymer-rich and polymer-poor regions. Governed by Flory-Huggins theory principles, influenced by specific Lewis acid-base interactions between polymer and IL anions/cations (explicitly mentioned).
        2.  **Polymer Chain Dynamics/Vitrification:** Below the glass transition temperature (Tg) of the polymer-dense phase (which itself depends on local composition), polymer chain mobility drastically reduces. This kinetic arrest traps the transient bicontinuous structure formed during LLPS. Governed by glass transition physics.
        3.  **Crosslinking Constraints:** The permanent crosslinks in the polymer network prevent macroscopic phase separation and influence the domain size and connectivity of the emergent structure.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID                      | Description                      | Parameter Name        | Parameter Value Range | Units          | Data Source                             | Implicit/Explicit | Justification                                     |
    | :--------------------------- | :------------------------------- | :-------------------- | :-------------------- | :------------- | :-------------------------------------- | :---------------- | :------------------------------------------------ |
    | Polymer-Solvent Interaction  | UCST Behavior                    | UCST Temperature      | ~40 - 100+            | °C             | Fig 2b, Fig 3b, 3d, 3e, 3f              | Explicit          | Phase diagrams show UCST vs composition/IL type. |
    | Polymer Chain Dynamics       | Glass Transition                 | Tg                    | ~20 - 60+             | °C             | Fig 2b, Fig 3d, 3e, 3f                  | Explicit          | Tg curves shown in phase diagrams.                |
    | Crosslinking Constraints     | Crosslink Density                | Crosslinker conc.     | 0.5 - 5               | mol% (of mono) | Methods section                        | Explicit          | Stated in preparation method.                    |

### **4.3 Global Order:**

    *   Content: The emergent global order is a stable, bicontinuous microstructure consisting of an interconnected, vitrified polymer-dense phase and an interconnected, gelated polymer-sparse (IL-rich) phase. The characteristic length scale is mentioned as micrometers.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID                      | Description                           | Parameter            | Value Range             | Units          | Implicit/Explicit | Justification                                                                 | Source                     |
| :--------------------------- | :------------------------------------ | :------------------- | :---------------------- | :------------- | :---------------- | :---------------------------------------------------------------------------- | :------------------------- |
| Polymer-Solvent Interaction  | Temperature-dependent miscibility     | Temperature          | Relative to UCST        | °C / K         | Explicit          | Driving force for LLPS explicitly linked to T vs UCST.                         | Fig 1a, 2b, Discussion     |
| Polymer Chain Dynamics       | Vitrification kinetics                | Temperature          | Relative to Tg          | °C / K         | Explicit          | Arrest of structure explicitly linked to T vs Tg (Berghmans' point).         | Fig 1a, 2b, Discussion     |
| Network Constraint           | Polymer Network Connectivity        | Crosslink Density    | 0.5 - 5 mol%            | Unitless       | Explicit          | System is a crosslinked network, constraining macroscopic separation.           | Methods                    |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID        | Description              | Parameter                 | Value Range              | Units    | Implicit/Explicit | Justification                                                  | Protocol                    | Source                     |
| :----------------- | :----------------------- | :------------------------ | :----------------------- | :------- | :---------------- | :------------------------------------------------------------- | :-------------------------- | :------------------------- |
| Characteristic Size| Domain Size              | Length Scale              | Micrometer               | µm       | Explicit          | Fig 2c caption mentions micrometer scale.                      | NanoCT                      | Fig 2c caption             |
| Macroscopic State  | Mechanical Stiffness     | Young's Modulus (E)       | ~10^-3 - 10^2            | MPa      | Explicit          | Measured stiffness reflects the global structural state.       | Tensile Test, DMA           | Fig 2d, 2e, Abstract       |
| Phase Composition  | Dense Phase Composition  | Polymer Weight Fraction Φ<sub>B</sub> | Determined by Berghmans' Pt | Unitless | Explicit          | Composition fixed by vitrification at Berghmans' point.        | Phase Diagram Analysis      | Fig 1a, 2b, Discussion     |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog

### **5.3 Computational Primitive:**

    *   Content: Thresholding (Temperature-dependent state switch). The material system acts like a comparator or switch. If Temperature > UCST, Output = Low Stiffness. If Temperature < Berghmans' Point (UCST & Tg intersection region), Output = High Stiffness. The computation is the physical execution of the phase diagram rules.
    *   **Sub-Type (if applicable):** Temperature Thresholding / State Switching

### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description          | Value          | Units   | Source        | Implicit/Explicit | Justification                                                       |
        | :----------------------------- | :------------: | :-----: | :------------ | :---------------- | :------------------------------------------------------------------ |
        | Heating/Cooling Rate (Typical) | 1 - 10         | °C/min  | Methods       | Explicit          | Rates used for DSC, transmittance, DMA tests are specified.         |
        | Phase Transition Time         | Minutes (Qual) | min     | Fig 4b        | Mixed             | Photos show transition over ~2.5 min, but depends on heating/cooling rate/thermal mass. Not precisely quantified. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behavior is an enormous, reversible change in mechanical stiffness (elastic modulus switching between ~kPa and ~tens of MPa) triggered by temperature changes around the material's UCST and Berghmans' point. Associated behaviors include:
        *   Isochoric switching (minimal volume change).
        *   Shape reconfigurability/adaptability (can conform to shapes when soft, then hold shape when rigid).
        *   Enhanced interfacial adhesion/reduced impedance when transitioned in contact with a surface (due to soft-state conformation).
        *   Shape memory effect (Supplementary Info).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary emergent behavior (stiffness switching) is validated through quantitative mechanical testing (tensile tests, Fig 2d, 2e; rheology, Fig S2). Stress-strain curves and calculated Young's moduli clearly demonstrate the >10^5 stiffness change between states/temperatures. The structural basis (bicontinuous microstructure) is validated using Nano Computed Tomography (NanoCT, Fig 2c). Phase behavior (UCST, Tg, Berghmans' point) is validated using optical transmittance (cloud point) and Differential Scanning Calorimetry (DSC) / Dynamic Mechanical Analysis (DMA) (Fig 2b, Fig 3, Supp. Figs). Shape adaptability and interfacial effects are demonstrated visually and via pull-off tests / impedance spectroscopy (Fig 4). Reproducibility is implied (n>=3 experiments for error bars). Limitations: Long-term cycling fatigue/degradation not shown. Robustness to other stimuli not tested.

---

#Key: [hughes_embodied_2022]

# Embodied Artificial Intelligence: Enabling the Next Intelligence Revolution

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper describes the concept of Embodied Intelligence (EI) as a research field and paradigm shift in AI and robotics. It does not describe a single specific material system or algorithm. EI posits that intelligence arises from the interaction between an agent's physical body (materials, morphology, sensors, actuators) and its environment, rather than solely from computation within a disembodied 'brain'. Key components discussed include the physical body/system, materials (e.g., smart/soft materials), design, interaction with the environment, and potentially algorithmic intelligence (conventional AI). The purpose is to understand natural intelligence (human, animal, plant), develop more robust and adaptive artificial systems capable of handling unstructured tasks, and potentially enhance conventional AI by integrating physical interactions and constraints. It reviews trends like bio-inspiration, soft robotics, artificial life, bio-hybrid systems, neuroscience links, and applications like manipulation and haptics.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
    *   **Note:** The paper discusses concepts and trends, not a specific implemented system with quantifiable parameters. No key parameters characterizing a specific *implementation* are provided.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Not specified. The paper discusses EI conceptually. Energy input would depend on the specific realization (e.g., electrical power for a robot, chemical energy for bio-hybrid systems, metabolic energy for biological examples). The paper mentions exploiting 'passivity' in soft materials, implying potential use of environmental energy, but does not detail primary sources for a general EI system.

### **2.2 Energy Transduction**

    *   Content: Not specified in detail. The paper implies energy transduction occurs through physical interactions, material properties (e.g., smart materials responding to stimuli), actuation (e.g., in soft robotics, bio-hybrid systems using muscle cells), and potentially computation (analog/digital). However, specific mechanisms and energy flow pathways are not detailed for the general EI concept. Examples mentioned (e.g., walking motion programmed physically, Xenobots) involve various transduction mechanisms not elaborated upon here.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Not discussed. The paper mentions exploiting 'passivity' which might imply efficiency gains in some cases, but overall efficiency is not assessed or quantified for EI systems in general. The free-energy principle is mentioned as a potential unifying approach, which relates to optimization, but not directly to quantifiable efficiency metrics in this text.

### **2.4 Energy Dissipation**

    *   Content: Not discussed. Dissipation mechanisms would depend entirely on the specific physical implementation of an EI system (e.g., friction in locomotion, heat in computation, material damping). No general discussion or quantification is provided.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: Not specified. While self-organization is implied, the paper does not detail the specific local interaction rules (physical, chemical, biological) that would govern component behavior in a general EI system or even in the specific examples mentioned (like Xenobots). It speaks broadly of "physical interactions," "embodied interactions," and exploiting "passivity" or "soft body interactions."
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: Not specified generally. Examples imply various forms of global order: coordinated locomotion (walking motion programmed physically, Sec 1), specific functions (manipulation, Sec 2.5), potentially self-replication patterns (Xenobots, Sec 2.2), adaptive behavior (recovering from failures, Sec 1). However, the paper does not define a specific emergent global order for the EI concept itself.
    * **Implicit/Explicit**: Implicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Explicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid (potentially others depending on realization).

### **5.3 Computational Primitive:**

    *   Content: Not specified. The paper discusses computation conceptually (e.g., "physical controllers," "computation through raw materials," "plants enable computation"). It mentions specific outcomes like 'walking motion' being programmed physically, implying underlying computations related to control or pattern generation, but does not define the most basic computational operation performed by the material/physical system itself (e.g., thresholding, filtering, logic).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
    *   **Note:** Timescales are not discussed or quantified. The paper mentions processes like development (Sec 2.3), learning (Sec 2.3, 2.5), and adaptation, which occur over time, but relevant timescales are not specified.

### **6.2 Active Inference:**

    *   Content: Partial/Unclear

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: Not specified in detail. The paper highlights the *need* for adaptation and learning (contrast with conventional AI, biological examples, developmental perspective) but doesn't detail specific mechanisms (e.g., parameter tuning, structural change, reinforcement learning rules) by which EI systems would adapt. It mentions the free-energy principle [6] which suggests optimization/learning rules based on minimizing prediction error, and cites work on learning sensorimotor repertoires through contextual inference [9], but doesn't elaborate on these as general EI mechanisms.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The paper describes several functional behaviors associated with EI systems:
        *   **Robust Locomotion:** e.g., walking motion programmed physically (Sec 1).
        *   **Manipulation:** e.g., soft grippers exploiting environmental interactions (Sec 2.5, ref [2]), manipulating food items (Sec 2.1).
        *   **Adaptation/Failure Recovery:** e.g., finding alternative solutions by exploiting physicality (Sec 1).
        *   **Learning/Skill Acquisition:** e.g., sensorimotor coordination, learning via haptic devices (Sec 2.5).
        *   **Computation:** e.g., computation through raw materials (Sec 1), plants' computation (Sec 2.1).
        *   **Self-X Capabilities:** e.g., self-replication, self-healing, self-sensing in Artificial Life contexts (Sec 2.2).
        *   **Human-Machine Interaction:** e.g., haptic devices for embodied interaction (Sec 2.5).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Not applicable. The paper is a review and does not present specific experimental results or validation methods for emergent behaviors in a particular system. It cites other works [e.g., 2, 3, 4, 9, 12] where such validation would reside.

---

#Key: [england_dissipative_2015]

# Dissipative adaptation in driven self-assembly

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### 1.1 System Description

    *   Content: The paper presents a theoretical perspective ("dissipative adaptation") arguing that driven, fluctuating many-body systems far from thermal equilibrium tend to self-organize into structures that are particularly effective at absorbing and dissipating work from the driving forces. It reviews historical context (Onsager, Prigogine, Crooks-Jarzynski fluctuation theorems) and builds upon them to propose a general mechanism. The system components are generally described as assembling particles under the influence of an external drive (e.g., mechanical shaking, voltage, light field) and a thermal reservoir. Specific examples discussed include self-replicating amyloid fibers, conducting beads under voltage, and assembling silver nanoparticles in a laser field. The purpose is to provide a theoretical thermodynamic understanding and potential predictive framework for non-equilibrium self-organization, particularly relevant to biological systems and nanoscale design.

### 1.2 Implementation Clarity


### 1.3 Key Parameters

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These are key parameters within the theoretical framework discussed. Specific values depend on the particular system being considered, which are only briefly exemplified.

## M2: Energy Flow

### 2.1 Energy Input

    *   Content: Energy is input via external driving forces that perform work (W) on the system over time. Examples mentioned include mechanical agitation (shaking), electrical fields (voltage drop), and optical fields (laser light). Thermal energy is also available from the heat bath (T). The primary *non-equilibrium* input is the work done by the drive.
    *   Value: Variable (depends on specific drive)
    *   Units: J (Work), K (Temperature)

### 2.2 Energy Transduction

    *   Content: The external work (W) done on the system by the drive can increase the system's internal energy (ΔE) or be dissipated as heat (ΔQ) into the thermal reservoir (W = ΔE + ΔQ, conservation of energy implied). The core idea is that specific system configurations are more "competent" to absorb work from the drive (e.g., via resonance) and subsequently dissipate it, allowing the system to overcome activation barriers and transition to new states (Fig 3). This absorbed and dissipated work drives the system away from equilibrium and fuels irreversible state changes.

### 2.3 Energy Efficiency

    *   Justification/Metrics: The framework focuses on the *dissipation* of absorbed work as the driving force for self-organization into adapted states, rather than the efficiency of energy storage or conversion into useful output in the traditional sense. Systems that are "better" adapted are those that are more effective at absorbing *and dissipating* work, implying inefficiency in terms of energy storage. Quantifying a general efficiency score is not applicable to the core concept presented.

### 2.4 Energy Dissipation

    *   Content: Heat dissipation (ΔQ) into the thermal reservoir is a central element. It is explicitly linked to irreversibility (Eq. 2, Eq. 3, Fig 2, Fig 3 discussion). The framework posits that systems adapt to configurations that enhance work absorption *and subsequent dissipation*. The rate of entropy production (related to dissipation) is discussed in the context of Prigogine's work, although dissipative adaptation goes beyond minimizing entropy production in the far-from-equilibrium regime. Specific dissipation mechanisms depend on the system (e.g., viscous damping, electrical resistance, inelastic scattering). Quantification depends on the specific system and drive. Qualitatively, higher dissipation is linked to higher irreversibility and faster adaptation towards dissipation-competent states.

## M3: Memory

### 3.1 Memory Presence:

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding to M3.2 and M3.3.)**

### 3.2 Memory Type:**


### 3.3 Memory Retention Time:**

*   Value: Variable / Persistence-dependent
*    Units: s (Qualitative Descriptor: "Process-dependent", potentially "Long-term" relative to fluctuation timescales if adaptation is strong)

### 3.4 Memory Capacity (Optional - if applicable)**

*   Units: States / Configurations

### 3.5 Readout Accuracy (Optional - if applicable)**


### 3.6 Degradation Rate (Optional - if applicable)**

### 3.7 Memory Operations Energy Cost (Optional - if applicable)**

### 3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### 4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", proceeding to M4.2-M4.7)**

### 4.2 Local Interaction Rules:**

    *   Content: The paper does not specify universal local interaction rules but assumes they exist based on underlying physics (e.g., inter-particle forces, collisions, chemical reactions). The crucial addition is the interaction with the time-varying external drive, which depends on the system's configuration. The effective "rule" emerging from the theory is that configurations better able to absorb and dissipate work from the specific drive are probabilistically favored over time (Eq. 4 discussion). The kinetic accessibility between states (π*(j* → i*) term in Eq. 4) also plays a role, determined by the energy landscape (barriers). For specific examples: amyloid fibers interact via polymerization/breakage; beads interact electrostatically/hydrodynamically; nanoparticles interact via optical forces/van der Waals.
    * **Implicit/Explicit**: Mixed

### 4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### 4.3 Global Order:**

    *   Content: The emergent global order is the specific macroscopic state or structure that the system adopts, characterized by its enhanced ability to absorb and dissipate work from the drive. Examples include: specific length distributions of self-replicating fibers, dynamic branched structures of conducting beads, specific aggregated shapes/orientations of nanoparticles with shifted plasmon resonance, co-oriented flows of actin filaments. The paper emphasizes that while the *details* might vary between realizations, the *property* of being adapted to the drive (high dissipation history) is common to the likely outcomes.
    * **Implicit/Explicit**: Explicit

### 4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### 4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### 4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| GlobalOrder_1 | Enhanced Dissipation | Average Work Absorption/Dissipation Rate | Higher than initial/random | W (Watts) or J/s | Explicit | Core concept of dissipative adaptation. | Measure energy input & heat output. | Text |
| GlobalOrder_2 | Specific Structure/Pattern | Structure Metric (e.g., Branching factor, Aggregate size, Resonance shift) | Depends on system | Varies (dimensionless, nm, Hz) | Explicit (via examples) | Examples provided illustrate specific emergent orders. | Microscopy, Spectroscopy, etc. | Text, Refs 21, 22, 26-29 |
| GlobalOrder_3 | Kinetic Accessibility | Transition Rate between states | Depends on barriers | s⁻¹ | Implicit | Factor in Eq. 4, related to how easily states are reached. | Measure transition dynamics. | Eq. 4 |

### 4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### 5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

### 5.2 Computation Type:**


### 5.3 Computational Primitive:**


### 5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### 6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Observation/Process Time (τ) | Variable | s | Eq. 3, 4, Fig 1 | Explicit | Time duration considered in transitions/evolution. |
        | Relaxation Time (to adapted state) | Variable | s | Text (e.g., bead example Ref 26) | Implicit | Timescale for the system to self-organize into the adapted state. |
        | Fluctuation Timescale | Variable | s | Text (general discussion) | Implicit | Characteristic time of thermal fluctuations driving micro-state changes. |
        | Drive Timescale (e.g., frequency) | Variable | s or Hz | Text (e.g., optical drive Ref 21) | Implicit | Characteristic time/frequency of the external driving force. |
        | Adapted State Lifetime/Durability | Variable | s | Text | Implicit | Timescale over which the adapted structure persists before spontaneously decaying. |

### 6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### 7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", proceeding to M7.2)**

### 7.2 Adaptation Mechanism:**

    *   Content: The mechanism is thermodynamic selection based on work history. Systems explore configuration space via thermal fluctuations and driving forces. Transitions involving higher work absorption and subsequent dissipation are statistically favored and more irreversible (Eq. 2, 3, 4). Over time, the system preferentially occupies configurations that have histories of high work absorption/dissipation relevant to the specific drive. It's a selection process analogous to natural selection but driven by physical dissipation rather than biological fitness. It's not based on explicit learning rules like Hebbian or reinforcement learning but emerges from the underlying statistical mechanics of driven systems (Eq. 4). The system "learns" which configurations are good dissipators through biased exploration of the state space.

## M8: Emergent Behaviors

### 8.1 Behavior Description:**

    *   Content: The primary emergent behavior described is **structural or dynamical self-organization** into states that are well-adapted to absorb and dissipate energy from the specific external drive. This manifests differently depending on the system: e.g., formation of specific length amyloid fibers, formation of branched conducting structures, aggregation of nanoparticles into specific shapes with shifted optical resonance, collective motion patterns in actin filaments. The overarching behavior is the system finding and maintaining configurations optimized for energy dissipation under driving.

### 8.2 Behavior Robustness:**


### 8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation primarily comes from citing experimental studies (Refs 21, 22, 26-29) that observed phenomena consistent with the dissipative adaptation hypothesis. For example: bead structures evolving to higher dissipation states (Ref 26), nanoparticle assemblies shifting resonance towards the driving laser frequency (Ref 21), specific fiber lengths selected under shaking (Ref 22). The validation is currently correlational – observed behaviors align with the theoretical expectation. Direct experimental measurement confirming that work absorption/dissipation history *causes* the selection according to Eq. 4 is lacking in the cited examples or the perspective itself. Control experiments isolating the effect of dissipation history are needed for stronger validation. The paper relies on plausibility arguments and consistency with existing experiments.

---

#Key: [paixao_leveraging_2022]

```markdown
# Leveraging physical intelligence for the self-design of high performance engineering structures

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system comprises a 3D printer (specifically, ZMorph model VX using fusion mass modeling with ABS material), an online vibration testing system (loudspeaker for excitation, vibrometer PDV-100 for measurement, NI-9234 for acquisition), and a decision algorithm implemented in MATLAB. It aims to autonomously manufacture high-performance engineering structures, specifically a simply-supported plate with an integrated beam-like vibration absorber, by closing the loop between manufacturing and testing. The system iteratively modifies the structure (beam absorber geometry) based on real-time frequency response function (FRF) measurements (accelerance) to meet a target performance criterion (equal-peak design for vibration attenuation). The purpose is to circumvent the performance-robustness trade-off inherent in traditional model-based design by leveraging "physical intelligence" (in situ measurements) to account implicitly for manufacturing variability and modeling uncertainties, resulting in tailored, high-performance designs. Components include the 3D printer, plate structure with absorber, excitation source (loudspeaker), sensor (vibrometer), data acquisition hardware, and control/decision software (MATLAB script).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Nominal Optimal Beam Length (Initial Design) | 46.07 | mm | Section: Initial design | Explicit | Medium | Calculated via FEA+Optimization |
        | Decision Threshold (Th) | Not specified (Visually implied in Fig 8f) | Dimensionless (Ratio or Difference) | Fig. 8f | Implicit | Low | Inferred from visual representation of stopping condition |

    *   **Note:** The decision threshold 'Th' is crucial but not numerically specified; its effective value is implied by the stopping points shown in Fig 8f. The optimal beam length is derived from a model, hence Medium reliability for the *initial* design (the whole point is to adapt *from* this).

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy sources are: 1) Electrical energy to power the 3D printer (heating element, motors). 2) Electrical energy for the online testing system (computer, DAQ, amplifier, vibrometer). 3) Acoustic energy from the loudspeaker used to excite the structure for vibration testing. The energy for material deposition (heating ABS) and mechanical motion (printer axes) is distinct from the energy used for sensing (acoustic excitation, laser vibrometer). The *information-gathering* process relies on the acoustic energy input and subsequent measurements.

### **2.2 Energy Transduction**

    *   Content: 1) **3D Printer:** Electrical energy -> Thermal energy (heating element) + Kinetic energy (motor movement). Thermal energy -> Phase change (melting ABS). Kinetic energy -> Material deposition/Positioning. 2) **Testing System (Excitation):** Electrical energy (amplifier) -> Acoustic energy (loudspeaker). 3) **Testing System (Sensing):** Acoustic energy -> Mechanical energy (plate vibration). Mechanical energy (plate velocity) -> Optical signal modulation (vibrometer laser Doppler shift) -> Electrical signal (vibrometer output). 4) **Control System:** Electrical signal (sensor) -> Digital data (DAQ) -> Information processing (MATLAB algorithm) -> Control signals (to 3D printer). The crucial transduction for the "physical intelligence" feedback loop is Acoustic -> Mechanical (Vibration) -> Optical -> Electrical -> Information -> Control Signal -> Material Deposition.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss or quantify the energy efficiency of either the manufacturing process or the testing/control loop. Efficiency is not a focus of the study.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation occurs through: 1) Heat loss from the 3D printer extruder and heated bed. 2) Mechanical friction in the 3D printer's moving parts. 3) Electrical resistance in all electronic components (printer, computer, DAQ, amplifier). 4) Acoustic energy dissipation into the environment from the loudspeaker. 5) Damping within the vibrating plate structure itself (explicitly mentioned as modal damping ~2%). 6) Heat generated during plastic deformation/cooling of the deposited material. Quantification is not provided. Damping in the structure is mentioned qualitatively and modelled with assumed values (1.93%, 2.10%).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*    Units: Qualitative Descriptor: "Long-term" / "Permanent" (for the manufactured object's lifetime)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip to Module 6.)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Vibration Test Duration (Signal Sweep) | ~ Few seconds (estimated) | s | Section: Integration... (10-1500Hz sweep) | Implicit | Inferred from typical sine sweep durations for the specified frequency range and acquisition needs. |
        | FRF Computation Time | ~ Real-time (Implied) | s | Section: Integration... | Implicit | Stated signal processing is performed in real-time. |
        | Decision Algorithm Execution Time | ~ Milliseconds to Seconds (estimated) | s | Section: Decision algorithm... | Implicit | MATLAB script execution is typically fast for simple logic. |
        | Design Modification (Single Step Print Time) | Minutes (estimated) | min | Section: Decision algorithm... (0.2mm height addition) | Implicit | Inferred from typical 3D printing speeds for small layer additions. |
        | Total Self-Design Loop Time (One Iteration) | Minutes (estimated) | min | Fig 8 (Number of steps varies) | Implicit | Sum of testing, computation, and printing for one modification step. |
        | Total Manufacturing Time (Per Sample) | Hours (estimated from ~10-40 steps in Fig 8) | h | Fig 8 | Implicit | Inferred by multiplying loop time by number of steps. |
    *   **Note:** Most timescales are estimated based on typical process durations, as precise timings are not provided.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is a form of closed-loop optimization or feedback control applied to the manufacturing process. It operates as follows: 1) Measure the current system state (FRF, identify peaks P and Q). 2) Compare the state to the target criterion (P=Q). 3) Apply a predefined decision rule (Eq. 5): If P > Q+Th, apply modification 1 (add mass at tip to decrease frequency); If Q > P+Th, apply modification 2 (add mass at base to increase frequency); If |P-Q| <= Th, stop. 4) Implement the chosen modification by adding material using the 3D printer. This process iteratively adjusts the beam's physical parameters (mass distribution, stiffness implied) to tune its resonant frequency relative to the plate, driving the system towards the equal-peak condition. It's a heuristic optimization strategy implemented via physical modifications guided by real-time measurements. It resembles a simple form of gradient descent or iterative refinement in the physical domain, guided by the PQ space representation.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior of the system *as a whole* is self-optimizing manufacturing. It autonomously adapts the physical design of a component (vibration absorber beam) during its fabrication process to achieve a specific performance target (equal-peak vibration attenuation) tailored to the individual characteristics of that specific sample, thereby compensating for manufacturing variability and model uncertainty. The behavior of the *final product* is enhanced vibration attenuation compared to a potentially mistuned standardized design.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary behavior (self-optimizing manufacturing leading to improved vibration attenuation) is validated experimentally on five samples (PB1-PB5). The process is operationally defined (Fig 5 flowchart, Methods). Performance improvement is quantitatively analyzed by tracking the H-infinity norm of the accelerance FRF and the evolution in PQ space (Fig 8a-f). The results show convergence towards the equal-peak diagonal and reduction in peak FRF amplitude for all samples, demonstrating reproducibility of the adaptive process working on variable starting points. Limitations include the small sample size (n=5) and lack of analysis on convergence speed, potential local minima, or sensitivity to the specific modification rules chosen.

---

#Key: [xiao_artificial_2020]

# Artificial visual memory device based on a photo-memorizing composite and one-step manufacturing

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a composite material consisting of thermoplastic polyurethane (TPU) as a viscoelastic polymer matrix and carbon black (CB) nanoparticles (30 wt%) as conductive nanofillers with photothermal conversion effects. This composite intrinsically combines light sensing and memory functions. Light irradiation causes CB nanoparticles to convert light into heat. This heat induces irreversible viscous flow in the TPU matrix, changing the spatial distribution and distance between CB nanoparticles. This alteration permanently modifies the composite's electrical resistance, thus storing a memory of the light exposure. The material can mimic basic neurobehaviors (synaptic functions). The composite is processed into filaments and used in dual-nozzle Fused Deposition Modelling (FDM) 3D printing to fabricate devices. One nozzle extrudes the functional TPU-CB composite, and the other extrudes pure TPU for packaging/insulation. A Wheatstone bridge circuit, incorporated using the 3D printed composite, is used to amplify and visualize the resistance changes via an LED array, creating an artificial visual memory system demonstrator (3x3 pixel array). The purpose is to create a material that intrinsically performs sensing and memory, simplifying device fabrication for artificial visual memory, potentially applicable in flexible/wearable electronics or AI sensory systems.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name             | Value          | Units                     | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :----------------------- | :-------------: | :------------------------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Key parameters defining the material composition and operating conditions are listed. Resistivity is from ESI but referenced explicitly. Resistance change rate is explicitly stated but is condition-dependent.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is light, specifically simulated sunlight used in the experiments.
    *   Value: e.g., 70.3 (specific intensity used in examples)
    *   Units: mW cm⁻²

### **2.2 Energy Transduction**

    *   Content: 1. **Photothermal Conversion:** Incident light energy is absorbed by the Carbon Black (CB) nanoparticles within the TPU matrix. This absorbed optical energy is converted into thermal energy (heat) due to the photothermal effect of CB. 2. **Thermal to Mechanical/Structural:** The generated heat increases the temperature of the TPU matrix. This thermal energy increases the kinetic energy of the TPU polymer chains, leading to increased segmental motion, slippage, and irreversible viscous flow (creep behavior), particularly around the glass transition temperature. 3. **Structural to Electrical:** The viscous flow and rearrangement of TPU chains alter the spatial distribution and connectivity of the conductive CB nanoparticle network. Specifically, increased chain motion tends to increase the average distance between CB particles or disrupt conductive pathways. This change in the CB network structure directly modifies the bulk electrical resistance of the composite material. Energy also flows into the electrical circuit (Wheatstone bridge) when reading the resistance state.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify the energy efficiency of the photothermal conversion or the subsequent resistance change. However, photothermal processes often involve significant heat loss to the environment. The functional output (resistance change) represents a tiny fraction of the input light energy. The primary mechanism relies on heating the bulk material, which is inherently inefficient for information storage compared to electronic methods. Qualitative Assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: The primary dissipation mechanism is heat loss from the TPU-CB composite to the surrounding environment via convection and radiation after the photothermal conversion. The paper notes the temperature recovers to the initial state after irradiation stops (Fig 2b), implying heat dissipation to the surroundings. Internal friction within the polymer during viscous flow also dissipates energy as heat, noted as contributing to the slower temperature rise near Tg (p. 1599). Energy is also dissipated resistively (Joule heating) when current is passed through the composite to measure its resistance, although this is part of the readout, not the primary memory mechanism. Quantification is not provided. Qualitative Assessment: High (due to reliance on thermal processes).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable / Depends on interval time
*    Units: s (Qualitative: Short-to-Long term demonstrated)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Units: % loss per unit time (e.g., s⁻¹)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Recyclability | Resistance stability after melting/cooling cycles | Stable (within 10-20 Ω cm range) | Ω cm | Attribute `recyclability` of `SystemNode` or `MemoryNode` | Fig S3 (ref p.1599) | Explicit | Paper explicitly states and shows (via ESI ref) that resistivity is stable after recycling. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Skipping M4.2-M4.7 as M4.1 is "No")**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Skipping M5.2-M5.4 as M5.1 is "No")**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description                          | Value             | Units | Source        | Implicit/Explicit | Justification                                        |
        | :--------------------------------------------- | :---------------: | :----: | :-----------: | :----------------: | :--------------------------------------------------- |
        | Irradiation Time (Write Pulse Duration)        | 30, 42, 60 (examples) | s     | Fig 4a, 4c, 4f | Explicit          | Explicitly varied parameter in experiments.          |
        | Recovery/Interval Time (Relaxation/Forgetting) | 5, 10, 20, 40, 60, 100 (examples) | s     | Fig 3c, 4c, 5e | Explicit          | Explicitly varied parameter in experiments.          |
        | Temperature Increase Time (to quasi-steady state) | ~100            | s     | Fig 2b        | Explicit          | Time for temperature curve to slow its rise significantly. |
        | Resistance Increase Time (during irradiation)   | ~100            | s     | Fig 2b        | Explicit          | Time for resistance curve to slow its rise significantly. |
        | Signal Attenuation/Forgetting Timescale       | 5-100+          | s     | Fig 4c        | Mixed             | Dependent on interval; significant changes over this range. |
        | LTM-like Behavior Consolidation Time       | Minutes range?     | s     | Fig 4f, 5f    | Implicit          | Relearning is faster; LED stays ON longer implies persistence over minutes. |

    *   **Note:** Relevant timescales related to stimulus application, material response, and memory effects are listed based on experimental parameters and observed behaviors.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism of adaptation is the cumulative and history-dependent nature of the irreversible viscous flow of the TPU polymer chains. Each light pulse induces some degree of unrecoverable chain slippage, altering the CB network and increasing resistance. The final resistance state depends on the integral of past stimuli (intensity, duration, frequency, interval times). Repeated stimuli (especially with short intervals) cause further irreversible changes building upon the previously altered state, leading to a higher baseline retained resistance (potentiation, LTM-like behavior). Longer intervals allow for more relaxation (though not complete recovery due to irreversibility), resulting in less accumulated change (STM-like behavior). The paper invokes the Boltzmann superposition principle (p.1600) to explain the cumulative effect of multiple irradiations, suggesting a linear superposition framework for the stress/strain history related to the viscous flow, which manifests as the adaptive change in resistance.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behaviors described are:
        1.  **Photo-sensing:** The material's resistance changes predictably upon exposure to light (specifically simulated sunlight), driven by the photothermal effect and subsequent polymer chain dynamics (Fig 2b, Fig 3d, e, f).
        2.  **Photo-memorization:** The material retains a change in its resistance state after the light stimulus is removed, representing a memory of the exposure (Fig 2b, Fig 3b, Fig 4). This includes characteristics analogous to:
            *   Memory Potentiation (cumulative increase with repetition).
            *   Short-Term Memory (STM) / Long-Term Memory (LTM) transition based on stimulus timing/repetition.
            *   Learning-Forgetting-Relearning behavior.
        3.  **Visualized Memory Readout:** When integrated into a Wheatstone bridge circuit with LEDs, the stored resistance state (memory) can be visually represented by the activation (ON/OFF state, persistence time) of LEDs in a pixel array (Fig 5).

### **8.2 Behavior Robustness:**

        *   **Material Recyclability:** The composite material itself shows stable resistivity after melting/cooling cycles (Fig S3 ref p.1599), suggesting robustness to reprocessing.
        *   **Parameter Dependence:** Behavior is sensitive to parameters like irradiation intensity, duration, and interval time, which is functional but also implies sensitivity to variations. Thickness showed little influence (Fig 3i).
        *   **Long-term Stability/Endurance:** Not extensively tested. While memory persists, the "forgetting" process (Fig 4c) indicates state drift. Performance over many cycles beyond the initial 10 shown (e.g., Fig 3c, 4b) is not documented.
        *   **Environmental Factors:** Sensitivity to ambient temperature variations is likely high given the thermal mechanism near Tg, but not explicitly tested or compensated for.
        *   **Noise:** Robustness to electrical or optical noise is not discussed. The resistance changes are relatively small (% range), suggesting potential susceptibility to noise.
        The score reflects the demonstrated recyclability but acknowledges the lack of data on long-term stability, environmental robustness, and noise tolerance.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claims of photo-sensing and photo-memorization are validated through quantitative measurements of resistance and temperature changes over time under controlled irradiation conditions (Figs 2b, 3, 4). Statistical analysis (e.g., error bars in Fig 2b, linear fits in Fig 3f, h) is used. The STM/LTM analogies and learning behaviors are supported by experiments varying stimulus parameters (duration, interval time, repetition) and observing the resulting changes in retained resistance (Fig 4). Control experiments differentiating the composite's behavior from pure TPU or direct heating effects are mentioned (Sec S4 in ESI ref p. 1599). Visualization using the 3x3 pixel array provides a qualitative demonstration of the memory effect (Fig 5). Reproducibility is implied by the presentation of consistent results across multiple stimuli (e.g., Fig 3c, 4b). Limitations include the relatively small number of cycles shown in some plots and the lack of quantification for visualization persistence times (Fig 5f).

---

#Key: [wan_artificial_2020]

# Artificial Sensory Memory

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews the field of Artificial Sensory Memory (ASM), which involves electronic devices designed to mimic the biological sensory memory process. ASM devices typically integrate bioinspired sensing components (analogous to biological receptors) with neuromorphic memory components (analogous to synapses) to capture, store, and potentially process sensory information (e.g., haptic, iconic, nociceptive, motor). The purpose is to achieve perceptual intelligence in electronic systems, advancing applications like prosthetics, robotics, and cyborg systems by enabling them to perceive and interact with the environment in a more bio-like, adaptive, and energy-efficient manner than conventional digital systems based on von Neumann architectures. Key components are Sensors (S), optional Pathways (P, analogous to axons), and Memory devices (M). Two main architectures are discussed: SM-ASM (Sensor-Memory) and SPM-ASM (Sensor-Pathway-Memory).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Operating Voltage | Variable (e.g., < 5 V for typical devices) | V | Figs 5-8, 10-12 (Implied from plots) | Implicit | Medium | Inferred from I-V curves and experimental descriptions |

    *   **Note:** Parameters represent typical ranges or specific examples discussed in the review. Sensor sensitivity varies greatly by modality. Memory retention depends on the specific device type (TS, RRAM, Transistor). Thresholds are design-dependent. Power consumption is generalized from Fig 13.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input is typically external stimuli relevant to the sensor modality (e.g., mechanical pressure/strain, light photons, heat/temperature difference) which activate the sensor, and external electrical energy (voltage/current) used to bias the device components (sensor and memory) and drive the memory state changes (write/read/erase operations). Some sensors mentioned are self-powered (triboelectric, piezoelectric), reducing the need for continuous electrical bias for sensing itself.

### **2.2 Energy Transduction**

    *   Content: 1. **Sensing:** Transduction of the input stimulus energy (mechanical, optical, thermal) into an electrical signal (e.g., change in resistance, voltage, current) by the sensor component. Mechanisms include piezoresistivity, photoconductivity, thermoelectric effect, triboelectric/piezoelectric effects. 2. **Memory Operation:** Transduction of electrical energy (voltage/current pulses) into a change in the memory component's state (e.g., resistance, conductance). Mechanisms involve ion migration (Ag filament formation/dissolution in RRAM/TS), electrochemical doping/ion relaxation (synaptic transistors), ferroelectric polarization, phase change, electron trapping/detrapping. The sensor's output signal often directly modulates the voltage/current applied to the memory element in series architectures.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper explicitly states that energy efficiency is a major challenge and that current ASMs have significantly higher power consumption (~µW ON state) compared to biological neurons (~pW-fW estimated), as shown in Fig 13. While some components like enhancement-mode transistors or self-powered sensors are suggested to reduce OFF-state or sensing energy (Sec 5), the overall operational efficiency (especially for writing/state changes in memory components) is implied to be relatively low compared to the biological benchmark. No specific overall efficiency values (e.g., J/operation or %) are consistently provided across different ASM types in the review. The low score reflects the explicitly stated gap compared to biology.

### **2.4 Energy Dissipation**

    *   Content: The paper does not explicitly detail or quantify specific dissipation mechanisms (like Joule heating in resistive elements, dielectric losses, heat generated during ionic movement or phase change). However, energy dissipation is implicitly present, as indicated by the non-zero power consumption (Fig 13) and the general inefficiency compared to biological systems. Dissipation would occur during sensing (e.g., current flow through piezoresistor), signal transmission (pathway resistance), and memory operations (current flow during read/write/erase, ionic movement, polarization switching). Qualitative Assessment: Medium to High (based on µW power consumption for single devices).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceed to M3.2-M3.8)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Seconds to Weeks+ (device dependent)
*    Units: s, min, days, weeks (Qualitative: Short-term, Long-term)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Binary (HRS/LRS) or Multivalued
*   Units: States (distinct resistance/conductance levels)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Slight decay observed over 1 week
    *   Units: % loss / time (Implicit)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Retention | Duration memory state persists | Seconds - Weeks+ | s, days | `MemoryNode.retentionTime` | Sec 2, Sec 2.2, Fig 5h, Fig 6e | Explicit | Explicitly mentioned characteristic and shown in figures. |
    | Reprogrammability | Ability to erase and rewrite memory | Yes | Binary | `MemoryNode.reprogrammable` | Sec 3.1 (Fig 5h), Sec 3.2 (Fig 6e) | Explicit | Demonstrated in examples (erase & rewrite patterns). |
    | Stability | Resistance to decay over time | Slight decay over 1 week | Qualitative | `MemoryNode.retentionStability` | Fig 5h, Fig 6e | Mixed | Figures show slight decay; rate not quantified. |
    | Multivalued States | Capacity beyond binary | Yes (for some types) | States | `MemoryNode.capacity` | Sec 2.2 | Explicit | Explicitly mentioned for ferroelectric synapses. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", proceed to M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Neuromorphic/Analog/Hybrid

### **5.3 Computational Primitive:**

    *   Content: Thresholding (Memory switching), Temporal Filtering (High-pass/Low-pass via short-term plasticity in synaptic transistors), Signal Integration (Temporal summation of inputs in synaptic transistors), Weighted Summation (Implicit in synaptic weight modulation for ANNs).
    *   **Sub-Type (if applicable):** Filtering: High-pass, Low-pass.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Biological Sensory Memory (SM) | ~0.25 - 1 | s | Sec 2 | Explicit | Explicitly stated range. |
        | Biological Short-Term Memory (STM) | ~1 - few dozens | s | Sec 2 | Explicit | Explicitly stated range. |
        | Biological Long-Term Memory (LTM) | Unlimited (Potentially) | s / years | Sec 2 | Explicit | Explicitly stated. |
        | Threshold Switch Relaxation | ns - minutes | s, min | Sec 2.2 | Explicit | Explicitly stated range. |
        | Synaptic Transistor Ionic Relaxation (Short-Term Plasticity) | ms - s | s | Sec 2.2, Sec 4.1 | Mixed | Short-term plasticity is explicit; timescales inferred from typical ionic device behavior & plots (e.g., Fig 10f decay). |
        | Non-Volatile Memory Retention (Demonstrated) | >= 1 week | days | Sec 3.1 (Fig 5h), Sec 3.2 (Fig 6e) | Explicit | Explicitly shown in figures. |
        | Stimulus Frequency (Rate Coding) | Variable | Hz | Sec 4.1 | Explicit | Concept of rate/frequency dependence is explicit. |
        | Device Response Time (Switching/Read) | Variable (ns-ms typical for memory components) | s | Sec 2.2 (Implied) | Implicit | Implied by device types (memristors, transistors) but not specified in detail. |
    *   **Note:** Covers biological benchmarks, specific component dynamics, and demonstrated system-level memory persistence.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The primary mechanisms described are based on mimicking biological synaptic plasticity:
        1.  **Short-Term Plasticity:** Temporal changes in device conductance based on recent stimulus history (frequency, timing). Seen in threshold switches (ionic diffusion dynamics) and synaptic transistors (residual ions/charge trapping at electrolyte/channel interface leading to facilitation or depression). Example: Differentiating touch speed (Sec 4.1).
        2.  **Long-Term Plasticity (Implicit/Contextual):** Persistent changes in conductance (synaptic weight) based on learning rules (like STDP, delta rule - mentioned in context of Fig 3). Implemented via mechanisms like filament growth/dissolution in memristors, phase changes, ferroelectric polarization, or long-lasting charge trapping/ionic configurations in transistors. This underlies supervised learning for pattern recognition tasks (Sec 4.2).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behaviors demonstrated by the reviewed ASM systems include:
        1.  **Sensory Mapping & Storage:** Recording the spatial distribution of stimuli (e.g., haptic patterns like letters 'N', 'T', 'U'; iconic patterns like a butterfly) in the memory array state (Sec 3.1, 3.2).
        2.  **Stimulus Thresholding & Nociception Mimicry:** Responding only when stimulus intensity (e.g., temperature) exceeds a threshold, and exhibiting sensitization (hyperalgesia, allodynia analogues) based on prior "damage" (forming voltage) (Sec 3.3).
        3.  **Motion Monitoring & Memory:** Detecting and storing information about body movements (e.g., elbow flexion, shoulder abduction) using integrated strain sensors and memory (Sec 3.4).
        4.  **Temporal Feature Extraction:** Differentiating stimulus timing/rate (e.g., touch speed) using short-term plasticity (Sec 4.1).
        5.  **Pattern Recognition:** Classifying input patterns (e.g., tactile binary patterns, color-mixed numeric images) after training using machine learning algorithms coupled with device responses (Sec 4.2).
        6.  **Motion Control (Hybrid System):** Using the processed sensory signal (integrated touch information) from an artificial afferent nerve to modulate the actuation of a biological component (cockroach leg) (Sec 4.3).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The behaviors described are validated through standard experimental characterization and functional demonstrations:
        *   **Sensory Mapping:** Applying patterned stimuli (molds, light masks) and reading out the corresponding current/resistance map of the device array (Figs 5g, 6e). Validated by visual comparison of input pattern and output map. Reliability shown by retention tests (Figs 5h, 6e).
        *   **Nociception:** Measuring device current response to voltage pulses after different forming voltages (Fig 7b analogy) and measuring system output voltage vs. input temperature (Fig 7d,e). Validated by comparing responses to biological definitions (threshold, hyperalgesia, allodynia).
        *   **Motion Memory:** Attaching devices to joints, performing actions (flexion), and monitoring sensor/memory state (LED indicator) before and after motion (Fig 8d,e). Validated by correlation between motion and device state change/retention.
        *   **Temporal Filtering:** Applying stimuli at different frequencies/speeds and observing changes in output amplitude and facilitation ratio (Fig 10f,g). Validated by showing frequency-dependent response.
        *   **Pattern Recognition:** Using device responses as features for a machine learning classifier, training the classifier, and evaluating its accuracy on test data (Fig 11d, Fig 11h,i). Validated by classification accuracy/error rate over training epochs.
        *   **Motion Control:** Applying pressure to the artificial nerve and measuring the resulting force/motion of the cockroach leg (Fig 12e,f,g). Validated by correlating input stimulus (pressure, duration) with output actuation force.
        *   **Limitations:** Validation often relies on simplified stimuli or tasks. Robustness testing under diverse conditions, long-term stability, and scalability beyond lab prototypes are generally not extensively covered in the reviewed examples.

---

#Key: [lagasse_future_2023]

# Future medicine: from molecular pathways to the collective intelligence of the body

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews and proposes a framework viewing biological systems (specifically tissues, organs, and cellular collectives) as reprogrammable, information-processing systems exhibiting collective intelligence. It focuses on the multiscale competency architecture where cells and tissues solve problems in physiological, transcriptional, and anatomical spaces. Key examples used are liver regeneration (hepatostat), developmental bioelectricity (pattern control, regeneration induction, cancer suppression), and general principles of anatomical homeostasis and allostasis. The purpose is to advocate for a new approach to biomedicine ('somatic psychiatry') that targets the computational and decision-making capabilities of these cellular collectives (their physiological 'software') rather than solely focusing on molecular 'hardware', aiming to induce self-repair and regeneration by communicating desired anatomical outcomes. Components include cells, ion channels, gap junctions, gene regulatory networks, extracellular matrix, and the resulting tissue/organ structures. The system *does* exhibit problem-solving, adaptation, memory, and goal-directed behavior in morphogenesis and regeneration.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Timescale of Bioelectric Signal Intervention (Regeneration) | ~1 hour (Monensin example) | hours | Section: Tapping into cellular intelligence; Fig 6B, 6C; Ref [90], [78] | Explicit | High | Based on specific experimental examples cited. |

    *   **Note:** The paper is a review, focusing on concepts. Specific quantitative parameters for the *general framework* are often qualitative or illustrative through examples.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source for the biological processes described (cell function, signaling, morphogenesis, regeneration) is metabolic energy derived from biochemical reactions (e.g., ATP hydrolysis). For experimental *manipulations* (e.g., bioelectric interventions), external energy sources like electrical fields (implicit) or light (for optogenetics, Fig 5E, 5G) might be used.

### **2.2 Energy Transduction**

    *   Content: Energy is transduced through multiple pathways. Metabolic energy drives ion pumps maintaining membrane potentials (chemical to electrical). Ion flow through channels represents electrical energy conversion. Gap junctions facilitate electro-chemical signaling. Gene expression and protein synthesis involve conversion of chemical energy. Cell movement and tissue rearrangement convert chemical energy into mechanical work. In optogenetics, light energy is converted to electrical energy (ion flow).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The review does not discuss or provide metrics for the energy efficiency of the biological processes (e.g., morphogenesis, regeneration) or the proposed interventions. This is outside the scope of the conceptual framework presented.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation occurs primarily as heat generated during metabolic processes, electrical resistance during ion flow, mechanical friction during cell movement, and inefficiencies in biochemical reactions. The review does not quantify these mechanisms. Qualitative assessment: Biological processes are inherently dissipative, necessary to maintain non-equilibrium states, but specific rates or dominant mechanisms are not discussed.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceed with M3.2-M3.8)**

### **3.2 Memory Type:**

    1.  **Bioelectric Pattern Memory:** Stable spatial patterns of Vmem encode target morphology (setpoints) for development and regeneration. These can be re-written experimentally (e.g., two-headed planaria). This represents a form of persistent state information guiding future structure (High retention, potentially complex capacity, readout via cell behavior). (Explicit, Sec: Developmental bioelectricity, Fig 5F, Ref [72])
    2.  **Transcriptional/Pathway Memory:** Gene regulatory networks and signaling pathways exhibit learning/memory, changing their response based on prior stimuli (associative learning discussed). (Explicit, Box 1, Refs [7, 24-26])
    3.  **Trophic Memory:** Anatomical memory persisting across regeneration cycles (e.g., deer antlers, planaria). Mechanism less clear but involves non-genetic pattern information. (Explicit, Fig 3D,D', Box 2, Ref [2])
    4.  **Cellular Memory:** Individual cells make decisions based on a history of perceptions. (Explicit, Box 1, Ref [1])
    These mechanisms represent persistent, modifiable states influencing future behavior, justifying a score above simple responsivity but below high-fidelity digital memory due to the biological substrate and potential for noise/drift (though biological systems demonstrate remarkable stability). Readout accuracy is functional (correct anatomy achieved) but not quantified in bits. Capacity is likely high but undefined. Retention can be long-term (lifespan, across regeneration cycles).

### **3.3 Memory Retention Time:**

*    Units: Qualitative Descriptor: "Persistent", "Long-term", "Stable" (across regeneration cycles, developmental stages)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The paper discusses several types of local rules governing cell behavior and interactions:
        1.  **Gene Regulatory Networks (GRNs):** Intracellular logic determining cell state and response to signals (Implicitly assumed based on molecular biology context, mentioned Box 1, Fig 2A).
        2.  **Cell Signaling:** Paracrine and juxtacrine signaling via secreted molecules and receptors (Implicitly assumed, basis of development).
        3.  **Bioelectric Interactions:** Cells communicate Vmem changes via gap junctions, influencing neighbors' states and coordinating collective behavior (Explicit, Section: Developmental bioelectricity, Fig 5B, 5E). Ion channel activity sets individual cell Vmem based on local environment.
        4.  **Physical/Mechanical Interactions:** Cell adhesion, migration guided by ECM, response to physical forces (Implicitly part of morphogenesis, ECM mentioned Box 1).
        5.  **Metabolic/Functional Coupling:** Cells sense and respond to functional demands, e.g., hepatocytes sensing need for function (Explicit, Section: The 'hepatostat', Fig 4).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The emergent global order described is primarily functional anatomical structure at the tissue, organ, and organism level. Specific examples include: correctly patterned and functional organs (eyes, limbs, brain, heart, gut, liver), specific tissue architectures (kidney tubules, liver lobules), whole body plans (frog face, planarian body, bilaterian symmetry), and regenerated appendages (salamander limbs, frog legs, deer antlers). This order involves correct cell types, spatial arrangement, size, and connectivity for proper function.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Functionality | Ability of the emergent structure to perform its intended role | Functional assays (e.g., liver function tests, visual response from ectopic eye) | Assay-specific | Assay-specific | Explicit | Measurement of physiological or behavioral output. | Functional assays, Behavioral tests | Sec: The 'hepatostat', Sec: Tapping into cellular intelligence |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid/Neuromorphic

### **5.3 Computational Primitive:**

    *   Content: The review suggests several computational primitives embodied in the biological substrate:
        1.  **Signal Integration:** Cells integrate multiple cues (chemical, electrical, mechanical) to make decisions (e.g., migration direction, differentiation fate). (Implicit, Ref [1, 11, 12])
        2.  **Thresholding:** Bioelectric states or signaling pathway activities triggering specific cell behaviors (e.g., proliferation, apoptosis) when crossing a threshold. (Implicit, general cell biology)
        3.  **Pattern Matching/Recognition:** Cellular collectives recognizing deviation from a target morphology (setpoint) and initiating corrective actions (homeostasis). Bioelectric networks recognizing specific voltage patterns. (Explicit concept, Sec: Wisdom of body, Developmental bioelectricity; Fig 2A)
        4.  **Memory Storage/Retrieval:** Storing and accessing information about past states or target morphologies (see M3). (Explicit concept)
        5.  **Error Calculation/Minimization:** Comparing current state to target state (setpoint) and acting to reduce the difference (homeostasis loop). (Explicit concept, Fig 2A, Box 2)

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|
    *   **Note:** The review discusses these as components of information processing systems but does not quantify their computational properties.

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Morphogenesis / Regeneration | days - weeks - months | days - weeks - months | Explicit (Examples: Planarian head ~days, Ref[22]; Frog limb weeks-months, Ref[78]; Liver ~weeks/months, Ref[39, 50]) | Explicit | Durations from specific biological examples discussed/cited. | Fig 6B, Sec: The 'hepatostat' |
        | Memory Retention (Trophic/Bioelectric) | years / lifetime | years / lifetime | Explicit (Concept: deer antlers annually Ref[15], lifelong patterns) | Explicit (concept), Implicit (value) | Implied long-term stability from examples. | Fig 3D |

### **6.2 Active Inference:**

    *   Content: Yes/Partial
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:**
        *   **Prediction Error Rate:** Quantify the deviation of the system state (e.g., morphology, physiological parameter) from the inferred target setpoint over time during development or regeneration. Measure the rate at which this error is reduced. (CT-GIN: Attribute of `StateNode` or `BehaviorNode`, edge weight change over `TemporalEvolutionEdge`).
        *   **Model Update Rate:** If internal models (e.g., bioelectric patterns, GRN states) change based on experience/perturbation, quantify the rate and magnitude of these changes. (CT-GIN: Change in `MemoryNode` attributes over time).
        *   **Action Efficiency:** Quantify the relationship between cellular actions (e.g., migration, proliferation rate) and the reduction in prediction error (deviation from target state). (CT-GIN: Attribute of `ActionSelectionEdge`).
        *   **Anticipatory Behavior:** Measure if cellular/tissue responses precede expected environmental changes or internal state transitions, based on learned patterns or inferred goals. (CT-GIN: Temporal analysis of `StateNode` transitions).

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes
        1.  **Physiological Adaptation:** Planaria adapting to barium by altering gene expression to regrow functional heads (Fig 3B, Ref [22]). Hepatocytes adapting to chronic injury by developing cell death resistance (Ref [34]).
        2.  **Morphological Adaptation:** Kidney tubules adjusting cell number/shape to maintain diameter despite altered cell size (Fig 3C, Ref [156]). Slijper's goat developing anatomical adjustments for bipedalism (Box 2, Ref [171]). Salamander tails grafted to flank remodeling into limbs (Box 2, Ref [166]).
        3.  **Learning in Pathways:** Gene regulatory networks and signaling pathways exhibiting associative learning and adapting responses based on training protocols (Box 1, Refs [7, 24-26]).
        These involve persistent changes in structure, function, or behavior based on experience or environmental challenges, going beyond simple stimulus-response.

**(Conditional: M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanisms described vary:
        1.  **Gene Expression Changes:** Altering the levels of specific proteins to cope with stress (e.g., Planaria in barium, Ref [22]). This involves feedback from the physiological state to the GRN.
        2.  **Bioelectric State Modification:** Changes in Vmem patterns or network connectivity potentially underlying altered morphological setpoints or responses (Implicitly underlying anatomical adaptation, explicit link to learning Ref [14, 70]).
        3.  **Cell Behavior Changes:** Altering proliferation, migration, differentiation, or apoptosis rates/patterns based on environmental cues or internal state (e.g., kidney tubule cells changing behavior based on neighbor size, Fig 3C).
        4.  **Pathway 'Learning':** Modification of internal states or connection strengths within signaling pathways/GRNs based on stimulus history, analogous to associative learning (Explicitly discussed for pathways, Refs [7, 24-26]).
        5.  **Allostasis:** Mechanisms that dynamically alter homeostatic setpoints themselves over time (Explicit concept, Box 2, Ref [157-162]).
        The paper suggests these mechanisms allow systems to solve novel problems and reach goals via diverse means.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behaviors discussed are:
        1.  **Anatomical Homeostasis/Pattern Homeostasis:** Maintenance and restoration of a specific target anatomical structure despite perturbations or injury (e.g., regeneration of limbs, organs; twinning). This involves coordinated cell proliferation, migration, differentiation, apoptosis to minimize deviation from a setpoint.
        2.  **Morphogenesis:** The process of development, where complex structures (tissues, organs, body plan) emerge from simpler initial states (e.g., blastomeres) through self-organizing cellular activities.
        3.  **Adaptation:** Adjusting physiological or morphological states to cope with novel environmental challenges or internal perturbations (e.g., barium resistance, kidney tubule size adaptation).
        4.  **Learning:** Modification of system responses (e.g., in signaling pathways) based on experience/training.
        5.  **Collective Decision-Making:** Cell groups collectively deciding "what to build and when to stop" during development and regeneration.
        6.  **Cancer Suppression:** Implicitly, the normal process of maintaining tissue organization prevents or corrects tumorigenesis (viewed as a defect in collective behavior/pattern homeostasis).

### **8.2 Behavior Robustness:**

        *   **Regulative Development:** Split embryos forming complete twins (Fig 2B), kidney tubules maintaining diameter despite huge cell size changes (Fig 3C), scrambled tadpole faces resolving into normal frog faces (Fig 3A), resistance to genetic perturbations (polyploidy in salamanders, Box 2).
        *   **Regeneration:** Accurate restoration of complex structures after major injury (salamander limbs, Fig 2C; planaria regeneration).
        *   **Adaptation:** Successful coping with novel stressors like barium (Fig 3B).
        This robustness stems from the goal-directed, error-correcting nature of the underlying homeostatic/allostatic processes. However, robustness is not absolute; severe perturbations, specific mutations, or conflicting signals can lead to developmental defects, failed regeneration, or disease (e.g., cancer as a failure of homeostasis). The score reflects the high degree of robustness observed in biological systems while acknowledging limits. Quantification not provided.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The review validates claims of emergent behaviors primarily through:
        1.  **Citation of Experimental Evidence:** Referencing numerous experimental studies demonstrating phenomena like regeneration (Fig 2C, 6B, 6C), developmental robustness (Fig 2B, 3A, 3C), adaptation (Fig 3B), bioelectric control (Fig 5F, 6A), and pathway learning (Refs [7, 24-26]).
        2.  **Illustrative Examples:** Providing specific, often visual, examples from the literature (e.g., twinning, scrambled faces, barium adaptation, ectopic eyes).
        3.  **Conceptual Coherence:** Arguing that these diverse phenomena can be understood within a unifying framework of collective intelligence, homeostasis, and bioelectric control.
        Control experiments are implied in the cited primary research but not detailed in the review. Quantitative analysis is limited within the review itself. Reproducibility is implied by the citation of established biological phenomena. Limitations include the complexity of the systems and the reliance on interpreting behaviors within the proposed framework.

---

#Key: [calvino_microcapsulecontaining_2018]

```markdown
# Microcapsule‐Containing Self‐Reporting Polymers

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is a class of self-reporting polymers incorporating microcapsules filled with dye systems. Mechanical damage (e.g., stress, cracking, impact) ruptures the microcapsules, releasing the payload. This release triggers a chemical or physical change involving the dye system, resulting in a perceptible optical signal (color change or fluorescence change/turn-on) that indicates the location and occurrence of damage. The components are a host polymer matrix, microcapsules (typically PUF shell), a solvent core within the capsules, and a chromogenic system (e.g., single dye, pro-dye + activator, donor + acceptor, AIE luminogen, excimer-forming dye). The purpose is to provide materials with built-in damage sensing capabilities for applications like structural health monitoring, tamper-evident packaging, and stress visualization. Several architectures are discussed: simple dye release, cargo-matrix interaction activation, interaction between cargos from two capsule types, and aggregation-induced optical changes upon release.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Microcapsule Diameter | 10 - 1000 | µm | Section 4.1, 4.2 (Ref [26], Fig 6A shows ~50-300µm range); Section 4.5 (Ref [100] mentions 65, 187, 213 µm) | Explicit | Medium | Ranges cited from specific studies/reviews. |
        | Microcapsule Shell Thickness | Few nm - <1 µm | nm / µm | Section 2.1 (nm); Section 4.2 (Ref[85] <1 µm) | Explicit | Medium | Values cited from specific studies. |
        | Microcapsule Content in Matrix | 10-20 (typical) | wt% | Section 4.4 | Explicit | Medium | Typical range stated based on literature. |
        | Agitation Rate (Capsule Size Control) | 200-2000 | rpm | Section 4.2 (Ref [26]) | Explicit | High | Directly cited from a source discussing fabrication. |
        | Response Time (Signal Generation) | Minutes | min | Section 2.4 (Ref [40], [44]) | Explicit | Medium | Qualitative ("within minutes") or example timeframes (Fig 5A) cited from specific studies. |

    *   **Note:** Parameters represent typical ranges or specific examples discussed in the review, reflecting the general state of the art covered. Reliability is Medium as these are cited ranges/examples, not exhaustive measurements across all possible systems.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input is mechanical energy applied to the composite material. This can be in the form of stress, strain, impact, incision, compression, or abrasion, sufficient to cause rupture of the embedded microcapsules.

### **2.2 Energy Transduction**

    *   Content: 1. Applied Mechanical Energy -> Elastic/Plastic Deformation Energy in matrix and capsule wall. 2. Exceeding Capsule Strength -> Fracture Energy (Capsule Rupture). 3. Rupture -> Release of stored chemical potential energy (if reactants mix) or physical state change (solvent evaporation/mixing). 4. Payload Release/Interaction -> Change in electronic state of chromophore (Chemical Reaction, Charge Transfer, Aggregation, Solvatochromism, Excimer Formation). 5. Changed Electronic State -> Change in Optical Properties (Absorption/Fluorescence Spectrum Shift -> visible light energy change, or change in fluorescence quantum yield -> light emission energy change).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The efficiency of converting input mechanical energy into a detectable optical signal is not quantified and likely very low. Most input mechanical energy is dissipated through matrix deformation, friction, and heat. Only a small fraction goes into rupturing the capsules. The subsequent chemical/physical processes might be efficient locally, but the overall mechanical-to-optical transduction efficiency is poor. The primary goal is signal generation upon threshold crossing, not efficient energy conversion. Qualitative Assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include: 1. Viscoelastic/plastic deformation of the polymer matrix (heat). 2. Friction during crack propagation or deformation. 3. Energy consumed in the fracture process of the microcapsule shell (surface energy creation, heat). 4. Non-radiative decay processes following light absorption/excitation of chromophores (heat). 5. Heat released during exothermic chemical reactions (if applicable). Quantification is not provided. Qualitative Assessment: High dissipation through mechanical deformation and fracture seems likely, as this is the primary response to the input energy. Optical and chemical dissipation depends heavily on the specific dye system.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Minutes to Months+
*    Units: Time (min, hours, days, months)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: Variable (Contrast dependent)

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Contrast | Visual difference between damaged/intact states | Variable (System-dependent) | Qualitative / ΔE* / Intensity Ratio | `MemoryNode` attribute | Sections 2.1, 2.3, 2.4 | Mixed | Contrast quality explicitly discussed; quantification is system-specific and implicit in the review. |
    | Stability | Persistence of the optical signal over time | Variable (e.g., >8 months cited) | Time (months) | `MemoryNode` attribute (`retention_time`) | Section 4.3, 4.6 (Ref [34]) | Mixed | Need for stability explicit; specific values cited; general rate implicit. |
    | False Positives | Signal generation without mechanical damage (e.g., leakage, thermal activation) | Undesired; minimized by design | Rate/Probability | `MemoryNode` or `SystemNode` attribute | Section 2.2, 4.3 | Mixed | Mentioned as potential limitation (e.g., unspecific activation, leakage); rates not quantified. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog/Thresholding

### **5.3 Computational Primitive:**

    *   Content: Thresholding / State Change. The basic operation is: IF (Local Mechanical Stimulus > Capsule Rupture Threshold) THEN Change Optical State. This is analogous to a comparator or a simple activation function.
    *   **Sub-Type (if applicable):** Thresholding

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Payload Release/Diffusion | Minutes (for visible signal) | min | Section 2.4 (Refs [40], [44]) | Explicit | Time for aggregation/signal change observed in examples. |
        | Chemical Reaction/Aggregation | Minutes (for visible signal) | min | Section 2.2 (Ref [31]), Section 2.4 (Ref [40], [44]) | Explicit | Time cited for color/fluorescence to develop in specific systems. |
        | Signal Persistence (Memory Retention) | Minutes to Months+ | min, days, months | Section 3.3 (Analysis) | Mixed | Lower bound (min) explicit, upper bound (months+) cited or inferred. |
        | Shelf-Life (Intact Capsules) | Months+ | months | Section 4.3, 4.6 | Mixed | Discussed as important; specific values cited or implied from stability discussions. |
    *   **Note:** Timescales are estimated ranges based on descriptions or specific examples cited in the review.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is mechanochromism: a change in optical properties (color or fluorescence) in response to mechanical stimuli (damage, stress, strain). This serves as a self-reporting or damage indication function, providing a visual or spectroscopic signal when the material experiences mechanical events exceeding the capsule rupture threshold. Different systems exhibit variations like color change, fluorescence turn-on, or fluorescence color change (ratiometric response).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The review cites studies validating the mechanochromic behavior primarily through: 1. Mechanical Testing: Applying controlled damage like scratching, incision, compression, impact, or tensile deformation (Sections 2.1-2.4, 4.5; Figures 2B, 3B, 4C, 4D, 5B). 2. Optical Observation/Spectroscopy: Visually inspecting for color changes or using UV illumination and cameras/spectrometers to detect fluorescence changes (Sections 2.1-2.4; Figures 2B, 3, 4, 5). Quantitative analysis is sometimes performed, such as correlating fluorescence ratios with impact energy (Section 2.4, Fig 5C). Control experiments often involve comparing damaged areas to intact areas or comparing composites with and without capsules/specific components. Robustness is sometimes assessed via shelf-life studies (Section 4.3, 4.6). Limitations include the often qualitative nature of damage assessment and the difficulty in precisely correlating local stress/strain with capsule rupture in complex loading scenarios.

---

#Key: [liao_tunable_2018]

# Tunable surface morphology via patterned cavities in soft materials

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of a soft, nearly incompressible homogeneous material (modeled as neo-Hookean) containing pre-patterned cavities embedded beneath its surface (Cavity-Embedded Soft Materials - CESMs). Applying external in-plane compression (uniaxial or biaxial) causes the initially flat surface to undergo reversible transformations into various controllable surface topographies, such as 1D waves, checkerboard patterns (alternating convex/concave features), and ridges. The components are the soft material matrix and the embedded cavities (circular in 2D, cuboidal or spherical/hemispherical in 3D). The purpose is to design and demonstrate a method for creating tunable and reversible surface morphologies in soft materials via patterned internal cavities, with potential applications in areas requiring controllable surface properties (optics, adhesion, hydrophobicity, flexible electronics).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is mechanical work done on the system via external compression (uniaxial or biaxial) applied by a mechanical tester (experiment) or defined boundary conditions (simulation). This results in stored elastic strain energy within the soft material.
    *   Value: Not explicitly quantified as total energy, but characterized by applied global compressive strain (up to 45% or 0.45). Strain rate is 0.0005 s⁻¹ in experiments.
    *   Units: Strain is dimensionless. Strain rate is s⁻¹. Work done would be Joules (J).

### **2.2 Energy Transduction**

    *   Content: Input mechanical energy (via compression) is stored as elastic strain energy in the soft material. When the compressive strain reaches a critical value (dependent on geometry), this stored strain energy is transduced into kinetic energy briefly during the buckling event and then primarily into potential energy associated with the new, buckled configuration (surface deformation - waves, checkerboard patterns, ridges). The buckling instability allows the system to transition to a lower energy state (for that specific confined geometry) by deforming out-of-plane (surface) or changing cavity shape. Energy is stored in bending and stretching of the material around the cavities and on the surface.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The concept of energy efficiency is not relevant in the context presented. The process is about storing and releasing elastic energy to change morphology. The paper states the process is "reversible," implying that upon removal of the load, the stored elastic energy is released, returning the system to its flat state, suggesting high elastic energy recovery (low hysteresis, typical for elastomers under moderate strain). However, no quantitative efficiency metric (e.g., energy stored vs. dissipated per cycle) is provided or relevant to the core phenomenon studied.

### **2.4 Energy Dissipation**

    *   Content: Potential dissipation mechanisms include viscoelastic losses within the soft material (TangoPlus is a polymer, likely exhibiting some viscoelasticity, though modeled as purely hyperelastic) and friction between the sample and confining plates in the experiment (explicitly mentioned as reduced using mineral oil). The simulations (using a neo-Hookean model) do not inherently account for viscous dissipation but focus on the elastic instability. Dissipation is not quantified. Assessed as Low to Medium, minimized experimentally.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local interactions are governed by the principles of continuum mechanics for a nearly incompressible, homogeneous, hyperelastic (neo-Hookean) material under finite deformation. Key rules include:
        1.  **Material Constitution:** Stress-strain relationship defined by the neo-Hookean model (strain energy potential), with initial shear modulus G = 0.50 MPa.
        2.  **Force Balance:** Equilibrium equations of continuum mechanics (quasi-static assumption).
        3.  **Compatibility:** Strain-displacement relationships ensuring material continuity.
        4.  **Boundary Conditions:** Periodic boundary conditions on lateral sides, no vertical displacement or shear traction on the bottom. Applied global compressive strain (ε) on the top/sides. Free surface boundary condition on the top surface.
        5.  **Geometric Constraints:** Interactions mediated by stress concentrations around pre-defined cavity geometries (shape, spacing α, depth β, aspect ratio γ). Buckling occurs when membrane forces exceed critical values determined by the geometry of the thin layer above/between cavities (approximated by plate buckling theory).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | 1 | Material Consititution (Neo-Hookean) | Shear Modulus (G) | 0.50 | MPa | Numerical Model | Explicit | Stated value for simulation. |

### **4.3 Global Order:**

    *   Content: The emergent global order consists of periodic surface topographies. Specific patterns observed/simulated include:
        *   **2D:** Local bumps, periodic waves (wavelength = 2 * cavity spacing), local ridges (above cavities).
        *   **3D (Cuboidal Cavities):** Checkerboard pattern (alternating convex/concave features, wavelength 2a or a depending on γ and loading), quadrangular bumps.
        *   **3D (Spherical/Hemispherical):** Square array of convex bumps, alternating mutually orthogonal peanuts, alternating mutually orthogonal ellipses, non-symmetric patterns (single row).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Material | Neo-Hookean Hyperelasticity | Shear Modulus (G) | 0.50 | MPa | Explicit | Stated simulation parameter. | Numerical Model |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Pattern Wavelength (2D) | Spacing between peaks/valleys | Wavelength | ~2 * cavity spacing (for small α) or ~cavity spacing (for large α) | mm (Implicit) | Mixed | Explicitly called periodic waves doubling spacing; ridge spacing inferred from geometry. Units derived from mm-scale prototypes. | Measurement from Simulation/Experiment Image | Fig 1, 2 |
| Pattern Amplitude (2D) | Half peak-to-valley distance | Amplitude | Increases with strain | mm (Implicit) | Mixed | Amplitude qualitative description; Ratio plotted in Fig 2b. Units derived from mm-scale prototypes. | Measurement from Simulation/Experiment Image | Fig 1, 2 |
| Pattern Wavelength (3D Checkerboard) | Spacing between convex/concave features | Wavelength | 2a or a | mm (Implicit) | Explicit | Stated dependence on γ. Units derived from mm-scale prototypes. | Measurement from Simulation Image | Fig 4 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", including M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog

### **5.3 Computational Primitive:**

    *   Content: Thresholding / Bifurcation / Pattern Selection. The system undergoes a qualitative change in its state (buckling into a specific pattern) when a control parameter (compressive strain ε, related to internal stress) crosses a critical threshold value (ε_crit). This threshold depends on other system parameters (α, β, γ). The system effectively selects a specific output pattern (lowest energy buckling mode) based on the input parameters exceeding the threshold.
    *   **Sub-Type (if applicable):** Bifurcation (Pitchfork/Symmetry-breaking depending on the specific transition).

### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Experimental Loading Rate | 0.0005 | s⁻¹ | Experimental Methods | Explicit | Stated strain rate used in tests. |
        | Total Experimental Compression Time (approx) | 833 (for 25%), 1500 (for 45%) | s | Calculated from Strain & Rate | Implicit | Calculated as Max Strain / Strain Rate. Actual duration depends on total strain applied. 25% strain takes 0.25/0.0005 = 500s. Paper mentions taking pictures every minute (60s), suggesting tests run for several minutes. 20% strain takes 400s (~6.7 min). The 45% simulation represents a much longer timescale (~900s or 15 min). Let's use the explicit loading rate's inverse timescale ~2000s. |
        | Buckling Event Timescale | Fast/Sudden | s | Text (2D modeling) | Explicit | "suddenly transforms" suggests a fast instability onset compared to loading rate. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behavior is the transformation of the material's initially flat surface into various periodic or localized topographical patterns upon application of sufficient in-plane compression. Specific behaviors include the formation of 1D waves, 2D checkerboard patterns (alternating convex/concave), ridges, bumps, peanuts, and ellipses. The type, wavelength, and amplitude of the pattern are determined by the cavity geometry (α, β, γ) and the applied load (ε, uniaxial/biaxial). This behavior is reversible upon load removal.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors (surface patterns) are validated through:
        1.  **Numerical Simulations:** Finite Element Analysis (FEA) using ABAQUS predicts pattern formation based on material model and geometry (Figs 1b, 4a, 5, 6). Phase diagrams (Figs 1b, 4a) map behavior regimes.
        2.  **Experimental Verification:** 3D printed prototypes made of TangoPlus rubber were mechanically compressed, and surface evolution was imaged (Fig 1a setup, Fig 2a results).
        3.  **Quantitative Comparison:** Critical strain values and amplitude-wavelength ratios from simulations and experiments were compared, showing good agreement (Fig 2, text discussion).
        4.  **Theoretical Modeling:** A simple plate buckling model qualitatively explains the dependence of pattern wavelength on cavity aspect ratio (Fig 4c, text discussion).
        *Limitations:* Experiments were limited to 2D cases and lower strains (25%) to avoid global buckling. 3D patterns were only simulated. Potential dynamic effects and non-ideal boundary conditions in experiments noted as sources of minor deviation. Material model (neo-Hookean) is purely elastic, neglecting potential viscoelastic effects.

---

#Key: [negi_collective_2024]

# Collective behavior of self-steering active particles with velocity alignment and visual perception

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of N self-propelled active Brownian particles (ABPs), specifically termed intelligent ABPs (iABPs), simulated in two dimensions. Each iABP moves with a constant propulsion force/velocity and experiences translational and rotational diffusion. The key feature is self-steering based on two mechanisms: 1) Vision-based steering, where particles adjust orientation towards the center of mass of neighbors within a visual perception cone (VC), weighted by distance. 2) Polar alignment, where particles align their velocity vector with the average orientation of neighbors within a defined radius (PA). Excluded volume interactions are modeled via a Lennard-Jones potential. The purpose is to study the emergent collective behavior (structure formation, dynamics) resulting from the interplay between these steering mechanisms, particle activity (Pe), vision angle (θ), and maneuverabilities ((cid:2)v, (cid:2)a).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These are key parameters varied or kept constant to study the system's behavior. DR is the rotational diffusion coefficient (DR = 8×10⁻²/τ).

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input is the constant self-propulsion force applied to each iABP, represented as `γv₀eᵢ(t)` in Eq. 1, where `v₀` is the characteristic propulsion speed and `γ` is the friction coefficient. This force drives the particles' persistent motion against friction. This is implicitly analogous to chemical energy conversion in biological microswimmers or external power in robotic systems. Stochastic forces `Γᵢ(t)` also input thermal energy.
    *   Units: N (Newtons)

### **2.2 Energy Transduction**

    *   Content: 1. Internal/Environmental Energy to Kinetic Energy: The propulsion force `γv₀eᵢ` performs work against the frictional drag `−γr˙ᵢ` and stochastic forces `Γᵢ`, maintaining the particle's directed kinetic energy. 2. Kinetic Energy to Potential Energy: During collisions, kinetic energy is temporarily stored as potential energy via the Lennard-Jones repulsion (Eq. 2). 3. Information to Mechanical Torque: Information about neighbors' positions (via vision) and orientations (via alignment) is transduced into mechanical torques (Mvᵢ, Maᵢ, Eqs. 4, 7) that reorient the particle, influencing its kinetic energy direction. 4. Thermal Energy to Kinetic Energy: Stochastic forces `Γᵢ` and `Λᵢ` convert thermal energy into random translational and rotational kinetic energy.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify the efficiency of energy conversion (e.g., from the implicit fuel source to directed motion or collective structures). As a model of active Brownian motion in a viscous environment, energy efficiency is expected to be very low due to continuous dissipation through friction, but no metrics are provided.

### **2.4 Energy Dissipation**

    *   Content: Energy is primarily dissipated through: 1. Translational Friction: The term `-γr˙ᵢ` in Eq. 1 represents viscous drag, dissipating kinetic energy into the surrounding medium (heat). 2. Rotational Friction: Implicitly included, as rotational motion (ϕ˙ᵢ in Eq. 8) against the medium causes dissipation, related to the rotational diffusion coefficient DR. Thermal noise (Γᵢ, Λᵢ) represents coupling to a thermal bath, involving continuous energy exchange and dissipation consistent with the fluctuation-dissipation theorem. Energy is also dissipated during inelastic aspects of collisions (though the LJ potential itself is conservative, the surrounding damping makes collisions dissipative overall). Quantification: The translational friction coefficient `γ` is given (relative to mass and thermal energy), but overall dissipation rates for the system or structures are not calculated. Dissipation is High due to the overdamped nature (m/γ = 10⁻²τ << τR).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: ~1/DR for orientation; diffusion-dependent for position.
*    Units: τ (simulation time units)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: ~DR for orientation; related to translational diffusion for position.
    *   Units: 1/τ

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content:
        1.  **Excluded Volume:** Particles interact via a short-range repulsive Lennard-Jones potential (Eq. 2), preventing overlap (`Fᵢ = -∂U/∂rᵢ`). Range `r ≤ 2^(1/6)σ`.
        2.  **Vision-Based Steering Torque (Mvᵢ):** Particles experience a torque directing them towards the weighted center of mass of neighbors `j` within their vision cone (VC) defined by angle `θ` and range `RV` (Eqs. 4, 6). Torque magnitude depends on visual maneuverability `(cid:2)v` and relative angle `φᵢⱼ - ϕᵢ`. Neighbors are weighted by `exp(-rᵢⱼ/R₀)`. See Eq. 8 for the 2D angular form: `((cid:2)v / Ncv,i) * Σ[j∈VC] exp(-rᵢⱼ/R₀) * sin(φᵢⱼ - ϕᵢ)`.
        3.  **Polar Alignment Torque (Maᵢ):** Particles experience a torque aligning their orientation `eᵢ` with the average orientation `eⱼ` of neighbors `j` within a distance `2Rc` (Eq. 7). Torque magnitude depends on alignment maneuverability `(cid:2)a` and relative angle `ϕⱼ - ϕᵢ`. See Eq. 8 for the 2D angular form: `((cid:2)a / Na,i) * Σ[j∈PA] sin(ϕⱼ - ϕᵢ)`.
        4.  **Self-Propulsion:** Constant force `γv₀eᵢ` along the current orientation `eᵢ` (Eq. 1).
        5.  **Stochastic Forces/Torques:** Gaussian white noise terms `Γᵢ(t)` (translational, Eq. 1) and `Λᵢ(t)` (rotational, Eq. 3, implicitly in Eq. 8 via `ξᵢ(t)`) representing thermal fluctuations.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------------------: | :---: | :----------: | :----------------: | :------------: |
    | 1 | Excluded Volume | σ | 1 (by definition) | length units | Section III | Explicit | Particle diameter sets length scale. |
    | 1 | Excluded Volume | ε/kBT | 1 + Pe | dimensionless | Section III | Explicit | Interaction strength, scaled with activity. |
    | 2 | Vision Steering | θ | π/16 - π | radians | Table I, Figs 2-5 | Explicit | Varied parameter. |
    | 2 | Vision Steering | (cid:2)v / DR | 12.5 | dimensionless | Section III | Explicit | Fixed parameter. |
    | 2 | Vision Steering | R₀ | 1.5σ | length units | Section III | Explicit | Characteristic vision range for weighting. |
    | 2 | Vision Steering | RV | 4R₀ (=6σ) | length units | Section II | Explicit | Absolute cutoff for vision. |
    | 3 | Polar Alignment | (cid:2)a / (cid:2)v | 0.1 - 25 | dimensionless | Figs 2, 7, 9 | Explicit | Varied parameter (ratio). |
    | 3 | Polar Alignment | Rc | σ (typically) | length units | Section II, Appendix C | Explicit | Alignment interaction radius (varied in Appendix C). |
    | 4 | Self-Propulsion | Pe | 10 - 200 | dimensionless | Table I, Figs 2-5 | Explicit | Varied parameter representing activity. |
    | 5 | Noise | DR | 8×10⁻² | 1/τ | Section III | Explicit | Rotational diffusion coefficient. |
    | 5 | Noise | DT/DR | σ²/8 | length units² | Section III | Explicit | Ratio related to translational diffusion. |

### **4.3 Global Order:**

    *   Content: Various globally ordered structures emerge depending on parameters:
        *   **Dispersed Clusters:** Small, transient groups of particles (Figs 2, 3, 4).
        *   **Close-Packed Clusters/Aggregates:** Dense, often quasi-circular, nearly immobile clusters (Figs 2, 4, 5). Sometimes hexagonally close-packed (HCP).
        *   **Wormlike Swarms:** Elongated, flexible, highly motile structures of aligned particles (Figs 2, 3, 4, 5, 6). Thickness and length vary.
        *   **Milling:** Ring-like structures where particles circulate collectively (Figs 2, 3, 4, 5, 11). Can be bands or filled disks.
        *   **Dilute Phase:** Homogeneous, disordered state (Figs 2, 3, 4, 5).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| 1 | Excluded Volume | σ | 1 | length units | Explicit | Base length scale | Section III |
| 1 | Excluded Volume | ε/kBT | 1+Pe | dimensionless | Explicit | Collision energy scale | Section III |
| 2 | Vision Steering | θ | π/16 - π | radians | Explicit | Key control parameter | Table I |
| 2 | Vision Steering | (cid:2)v/DR | 12.5 | dimensionless | Explicit | Fixed control parameter | Section III |
| 2 | Vision Steering | R₀ | 1.5σ | length units | Explicit | Vision weighting range | Section III |
| 3 | Polar Alignment | (cid:2)a/(cid:2)v | 0.1-25 | dimensionless | Explicit | Key control parameter ratio | Section IV |
| 3 | Polar Alignment | Rc | σ | length units | Explicit | Typical alignment range | Section II |
| 4 | Self-Propulsion | Pe | 10-200 | dimensionless | Explicit | Key control parameter | Table I |
| 5 | Noise | DR | 8×10⁻² | 1/τ | Explicit | Stochastic influence strength | Section III |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Shape | Asphericity | A | ~0 - 0.8+ | dimensionless | Explicit | Quantifies elongation (Eq 13) | Section IV.B.2 | Fig 7 |
| Size | Radius of Gyration | Rg² | Varies | σ² | Explicit | Measures spatial extent (Eq 12) | Appendix E | Fig 18 |
| Alignment | Global Polarization | P | ~0 - 1 | dimensionless | Explicit | Measures collective alignment (Eq 14) | Section IV.B.3 | Fig 8, Fig 19 |
| Dynamics | Mean Squared Displacement exponent | α | ~1 - 2 | dimensionless | Explicit | Characterizes motion type (ballistic vs diffusive) | Section IV.C.1 | Fig 9, Fig 20 |
| Dynamics | Persistence Length (temporal) | ξp | ~80σ - >300σ | σ | Explicit | Temporal correlation decay length (Eq 17-19) | Section IV.C.2 | Fig 10 |
| Dynamics | Persistence Length (spatial) | ξr | ~80σ - >300σ | σ | Explicit | Spatial correlation decay length (Eq H1-H2) | Appendix H | Fig 21, Table II |
| Milling | Ring Radius | R | ~10σ - 100σ+ | σ | Explicit | Size of milling structure (Eq 23) | Section IV.C.3 | Fig 11a |
| Milling | Angular Frequency | ω | ~0.001 - 0.1 DR | DR (or 1/τ) | Explicit | Rotation rate of milling structure (Eq 22) | Section IV.C.3 | Fig 11b |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | Local Rules -> Global State | Mapping from particle interaction rules (Eqs. 1-8) to emergent phases (swarms, clusters, etc.). | High (Score 8) | 7 | Phase Diagrams, Order Parameters (P, A, MSD slope) | Mixed | The local rules largely predict the global states seen in phase diagrams, but stochasticity and potential complex nonlinearities mean the mapping isn't perfect. Yoneda score reflects good but not total explanatory power. | Section IV.A, Figs 5, 12 |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 7. Rubric: 0 = Local rules completely fail to predict global structure. 5 = Local rules predict general structure types but not boundaries or details well. 7 = Local rules predict phase diagram topology and structure types well, some boundary/stability details might differ slightly from simulation. 10 = Local rules perfectly predict all aspects of emergent global structures quantitatively. The model achieves good prediction of phase types and dependencies (e.g., Fig 5), justifying a score of 7.
    *   **Metrics:** Phase diagram topology matching, qualitative agreement of structure types, quantitative prediction of order parameters (A, P, ξp) based on local parameters (Pe, θ, (cid:2)a/(cid:2)v) via scaling relations (Figs 7 inset, 10, 11, 18 inset).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid (Rule-Based)

### **5.3 Computational Primitive:**

    *   Content: Weighted summation and nonlinear transformation (sine function) of neighbor inputs. Specifically:
        1.  For vision: Calculate weighted average direction to neighbors in VC (weighted by `exp(-rᵢⱼ/R₀)`), compute angle difference (`φᵢⱼ - ϕᵢ`), apply sine function, sum results, normalize (Eq. 8, first term).
        2.  For alignment: Calculate average orientation of neighbors in PA, compute angle difference (`ϕⱼ - ϕᵢ`), apply sine function, sum results, normalize (Eq. 8, second term).
        The primitive operation is the local integration of multiple neighbor signals (position and orientation) via trigonometric functions to determine a resulting torque/orientation change rate.
        *   **Sub-Type (if applicable):** Vector Integration / Nonlinear Mapping

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Simulation timestep (Δt) | 10⁻³ | τ | Section III | Explicit | Integration step. |
        | Inertial relaxation time (m/γ) | 10⁻² | τ | Section III | Explicit | Time for velocity to relax (indicates overdamped). |
        | Rotational relaxation time (τR = 1/DR) | 12.5 | τ | Section III | Explicit | Time for orientation correlation to decay (single particle). |
        | Translational diffusion time (σ²/DT) | 100 | τ | Section III | Explicit | Time to diffuse particle diameter. |
        | Advection time (σ/v₀ = τ/Pe * (σ²/DT/τR)) | 100 / Pe | τ | Eq 9, Section III | Explicit | Time to move particle diameter ballistically. |
        | Swarm persistence time (ξp / v₀) | (ξp/σ) * (100/Pe) | τ | Section IV.C.2, Eq 19 | Implicit | Time for swarm direction to decorrelate. Varies strongly (e.g., if ξp/σ=200, Pe=40 -> 500 τ). |
        | Milling period (2π/ω) | Varies (~Pe) | τ | Section IV.C.3, Eq 24 | Implicit | Time for one rotation in milling state. Varies strongly. |
    *   **Note:** τ = mσ²/(kBT) is the base unit of time.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The system exhibits several distinct collective behaviors emerging from local interactions:
        *   **Disordered Motion:** Particles move randomly with little correlation (Dilute Phase).
        *   **Clustering/Aggregation:** Particles form dense, relatively static clusters (Close-Packed Clusters).
        *   **Swarming/Flocking:** Particles form elongated, coherently moving groups (Wormlike Swarms) exhibiting persistent, superdiffusive motion over long timescales, interrupted by sharp turns.
        *   **Milling:** Particles form stable rotating ring-like structures.
        *   **Phase Separation:** Coexistence of dense clusters/swarms and a dilute gas phase.
        *   **Dynamic Splitting/Merging:** Large swarms can split, and smaller swarms can merge dynamically (mentioned for high Pe).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors are validated through:
        *   **Agent-Based Simulations:** The behaviors are directly observed in numerical simulations of the model (Section III).
        *   **Visualizations:** Snapshots provide qualitative evidence of the structures (Figs 2, 3, 4, 6, 14). Movies are referenced [58].
        *   **Quantitative Analysis:** Order parameters (Polarization P, Asphericity A, Radius of Gyration Rg) are calculated to characterize the structures (Section IV.B, Figs 7, 8, 18). Dynamical properties (MSD, persistence length ξp, milling frequency ω, milling radius R) are measured to quantify behavior (Section IV.C, Figs 9, 10, 11, 20, 21).
        *   **Phase Diagrams:** Systematic exploration of parameter space maps regions where specific behaviors are stably observed (Section IV.A, Figs 5, 12, 15, 16).
        *   **Scaling Analysis:** Relationships between parameters and behavioral metrics are investigated (e.g., A vs (cid:2)a/((cid:2)vθν), ξp vs θ, R/ω vs Pe/N) to reveal underlying principles (Sections IV.B.2, IV.C.2, IV.C.3).
        *   **Control/Comparison:** Implicit comparison to limiting cases (e.g., vision-only, alignment-only/Vicsek) is discussed (Introduction, Section IV.A).
        *Limitations: Validation relies solely on simulations of the proposed model. Experimental validation is suggested via analogies (animal swarms, microbots) but not performed. Robustness tests are limited (mainly N, parameters).

---

#Key: [levin_self-improvising_2024]

# Self-Improvising Memory: A Perspective on Memories as Agential, Dynamically Reinterpreting Cognitive Glue

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper presents a conceptual framework viewing memory not as a static store of high-fidelity data, but as a dynamic, agential process involving continuous reinterpretation ("mnemonic improvisation," "confabulation") to preserve salience and meaning rather than literal details. This "cognitive glue" operates across diverse biological and potentially artificial systems (from cells to societies), enabling agents to adapt to changing internal states and external environments. Key components of the framework include: the concept of agents as processes ("Selves" as dynamical constructs, "Selflets" as temporal slices), memory engrams as messages between temporal selves or even distinct agents requiring interpretation, the "bowtie architecture" (information compression/decompression facilitating generalization and remapping, analogous to autoencoders), and "polycomputation" (multiple observers interpreting the same physical processes differently). The purpose is to unify phenomena across scales (development, evolution, neuroscience, bioengineering) under a lens of dynamic sense-making and to propose that memories themselves might possess a degree of agency ("thoughts are thinkers"). Examples used include memory retention through metamorphosis, regeneration, confabulation in humans, brain plasticity, and evolutionary adaptation.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters are mostly conceptual or illustrative based on examples discussed, not empirically measured within the paper's primary investigation. Reliability is Low as these are not measurements from a specific system implemented/tested by the author in this work.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Not explicitly defined or quantified for the conceptual framework or specific examples. Mentioned abstractly as constraints ("agents under energy and time constraints"). Biological examples implicitly rely on metabolic energy.

### **2.2 Energy Transduction**

    *   Content: Not described. The focus is on information processing, interpretation, and agency, not the underlying energy transformations enabling these processes. Biological examples involve complex biochemical energy transduction, but this is not the focus.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Not discussed. Energy efficiency is not a metric considered in the paper's conceptual framework.

### **2.4 Energy Dissipation**

    *   Content: Not described or quantified. Concepts like "dissipative patterns" are mentioned in relation to self-reification (citing Rosen, Prigogine) but not analyzed within the paper's core argument.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable; potentially very Long-term

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Potentially Very High / High-dimensional

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: Low (for details), High (for salience/meaning)

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: Not explicitly formalized. Implied through biological examples: cell-cell signaling (bioelectric, biochemical) guiding morphogenesis and regeneration; chemical trails and local interactions in ant colonies; neuronal interactions in brains. The "polycomputing" concept suggests local interpretation rules are key: subsystems interpret signals based on their own context/perspective. The bowtie architecture implies compression/decompression rules. The core proposed rule is interpretation for salience/meaning.
    * **Implicit/Explicit**: Implicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The emergent global order includes: Correct anatomical structure and morphology in development and regeneration (e.g., eye, limb, whole body plan); Coherent behavior at the organism level arising from neural activity; Colony-level behavior in social insects; Functional intelligence and Selfhood emerging from interactions of components across time and scale.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog / Hybrid / Reservoir Computing (Implied) / Other (Polycomputing, Interpretive Computation)
    *   Description: The paper emphasizes interpretation, context-dependence, and sense-making over formal symbolic manipulation. "Polycomputing" suggests observer-dependent analog computation. The mention of the "senome" as a high-dimensional state space exploitable for memory hints at Reservoir Computing principles. It contrasts with conventional digital computation's fixed interpretation.

### **5.3 Computational Primitive:**

    *   Content: Interpretation / Sense-Making / Pattern Completion / Compression-Decompression (Abstraction/Generalization & Remapping/Instantiation)
    *   Description: The most basic operation described is the interpretation of signals/engrams/states to extract salient meaning for adaptive action. This involves pattern completion (filling gaps, confabulation), abstraction/compression (bowtie input), and remapping/decompression/instantiation (bowtie output) into a new context. The paper frames these as fundamental acts of intelligence.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | 'Selflet' Duration | ~few hundred | ms | Section 2 | Explicit (text) | Estimate given for cognitive frame |
    *   **Note:** Timescales are mostly qualitative or illustrative estimates based on the examples discussed.

### **6.2 Active Inference:**

    *   Content: Partial
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** CT-GIN metrics could potentially quantify: Rate of convergence of the internal model (e.g., how quickly confabulation produces coherence - measured via information-theoretic measures on graph states), Predictive accuracy gain over time (reduction in 'surprise' analog), Complexity/depth of the internal model graph (`SelfNode` complexity), Cost (energy/time) associated with updating the model (`UpdateEdge` weights). Experiments could involve tracking system state changes in response to predictable vs. unpredictable stimuli.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The primary mechanism proposed is **dynamic reinterpretation and sense-making**. This involves: (1) **Mnemonic Improvisation:** Actively rewriting/remapping memories/information onto new contexts or substrates, preserving salience over fidelity. (2) **Confabulation:** Filling gaps and modifying beliefs/memories to maintain coherence and a functional self-model in the present/future context. (3) **Bowtie Architecture:** Compressing experience into a generalized representation (learning/abstraction) and then decompressing/reinterpreting it for novel situations (application/remapping). (4) **Polycomputing:** Subsystems interpreting signals/states based on their own perspective/needs, enabling functional reuse and hacking. This is framed as an ongoing, interpretive process driven by the need to maintain coherence and achieve goals in a changing world, rather than a specific algorithm like backpropagation. It resonates with principles of active inference (updating models to minimize surprise) and potentially reinforcement learning (adapting interpretations based on utility).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The paper discusses a wide range of behaviors emerging from the proposed framework across different scales:
        *   **Robust Morphogenesis/Regeneration:** Building and repairing complex anatomies correctly despite noise, damage, or component variation (e.g., planaria, salamanders, polyploid newts).
        *   **Adaptive Behavior:** Organisms adjusting actions based on learned information reinterpreted for new contexts (e.g., moth remembering caterpillar learning across metamorphosis).
        *   **Cognitive Coherence (Confabulation):** Maintaining a consistent self-narrative and perception by actively filling gaps and reinterpreting information (e.g., split-brain patients, visual inpainting, color phi phenomenon).
        *   **Problem-Solving Intelligence:** Competency in navigating various problem spaces (anatomical, behavioral, cognitive) by adapting strategies.
        *   **Collective Intelligence:** Coordinated group behavior arising from local interactions (e.g., ant colonies).
        *   **Emergence of Selfhood:** The dynamic persistence of a coherent agent/perspective over time through continuous interpretation and self-construction.
        *   **Agency of Information Patterns:** Speculatively, memories/thoughts themselves acting as minimal agents influencing the cognitive system ("thoughts are thinkers").

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: As a perspective paper, it doesn't present primary experimental validation. It validates its claims through: (1) **Synthesis of Existing Evidence:** Citing and reinterpreting findings from diverse fields (neuroscience [e.g., confabulation, memory reconsolidation], developmental biology [e.g., regeneration, polyploidy], evolution, AI [autoencoders]) to show consistency with the proposed framework. (2) **Conceptual Coherence:** Arguing for the logical consistency and explanatory power of the framework in addressing paradoxes (e.g., persistence paradox). (3) **Illustrative Examples:** Using specific phenomena (metamorphosis memory, planarian regeneration, ectopic eyes, split-brain studies) as strong supporting cases. No new quantitative analysis or control experiments are presented within the paper itself. Reproducibility relies on the cited literature.

---

#Key: [sole_open_2024]

# Open problems in synthetic multicellularity

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews the field of synthetic multicellularity (MC), outlining open challenges and conceptual approaches. It discusses various systems engineered or observed to form multicellular structures, ranging from microbial consortia and programmable cell aggregates to organoids and biobots (Xenobots, Anthrobots). The purpose is to explore the principles underlying MC emergence, complexity, function (like pattern formation, computation, agency, self-repair, movement), and evolution, using synthetic systems as tools for interrogation and engineering. Key components discussed include genetically engineered circuits, adhesion molecules, stem cells, and computational design algorithms, operating through mechanisms like self-organization, developmental programs, and evolutionary dynamics.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** The parameters listed are from the conceptual or theoretical models discussed in the paper to illustrate principles, not specific experimental implementations detailed within this text. Reliability is generally low/medium as specific values aren't provided or are tied to cited works.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The paper implicitly discusses energy requirements for living systems (cells, organisms) through metabolism (mentioned indirectly via concepts like energetic favorability of UC vs MC, cell growth, maintenance). Specific energy sources (e.g., chemical energy from growth media, ATP for cellular processes) are assumed for the biological components but not explicitly detailed or quantified as a focus. For engineered aspects like computation via circuits, energy input is implicit in the metabolic cost of gene expression and cellular function.

### **2.2 Energy Transduction**

    *   Content: Energy transduction is implicit in the biological processes described: chemical energy (metabolism) is converted into mechanical energy (cell movement, cilia propulsion in Anthrobots, heart cell contraction in Xenobots), energy for biosynthesis (growth, protein production for circuits/adhesion), energy for maintaining gradients, and potentially electrical energy (bioelectrical signals, mentioned briefly in Box 2 context). Energy is used to drive self-organization (e.g., adhesion-driven sorting towards lower energy states) and computation (gene expression, signaling).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any quantitative data or qualitative assessment regarding the energy efficiency of the synthetic multicellular systems discussed. Concepts like energetic favorability are mentioned in passing comparing UC and MC life, but not analyzed for the synthetic examples.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation (e.g., as heat) is an inherent aspect of all irreversible biological and physical processes described (metabolism, movement against friction/viscosity, computation, maintenance of non-equilibrium structures), but it is not explicitly addressed or quantified. The concept of energy minimization in adhesion models (Eq. 4, 5) implies dissipation during the rearrangement process towards lower energy states.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**

    The overall score reflects the presence of relatively simple engineered memory (bistability) and theoretical/emerging concepts (network/agential memory) rather than high-fidelity, complex memory systems described in detail.

### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**

*   Units: states / bits

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Units: % / error rate

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: Specific local rules are described for certain systems:
        1.  **Differential Adhesion:** Cells attempt to swap positions with neighbors based on minimizing interfacial energy (Hamiltonian H in Eq. 4), determined by adhesion strengths (matrix W in Eq. 3) between cell types. The swap probability follows a Boltzmann rule (Eq. 5), influenced by energy difference (ΔH) and noise (T).
        2.  **Gene Regulation:** Gene expression changes based on interactions with other genes (activation/inhibition defined by ωij in Eq. 6, 7, Box 2) and potentially diffusion of signaling molecules between neighboring cells (Di term in Eq. 7, Box 2). Mutual inhibition (Eq. 1, Box 1) and dimerization (Eq. 2, Box 1) are specific examples.
        3.  **Biobots/Agential Materials:** Local rules are less explicitly defined but involve cell-cell interactions (signaling, mechanical forces), cell-environment interactions, and intrinsic cellular behaviors (e.g., cilia beating for Anthrobots, contraction for Xenobot heart cells) leading to emergent morphology and movement. Implicitly governed by biophysics and cellular physiology.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: Examples of emergent global order include: segregated cell clusters/layers (from differential adhesion, Fig 3c), spatial patterns like stripes or spots (Turing patterns, reaction-diffusion Fig 1a, g), specific morphologies (organoids resembling mini-organs Fig 1f, biobot shapes Fig 1h, 1l), defined cell fate distributions, branching structures (Fig 1g), collective movement/behavior (biobots, swarms).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes
        1.  **Synthetic Circuits:** Engineered cells performing logic operations (Boolean gates, multiplexers - Fig 2b,c), filtering, oscillations, decision-making (e.g., targeting cancer cells). Computation is embodied in the engineered gene networks.
        2.  **Distributed Computation:** Multicellular consortia performing computations where different cells handle parts of the logic (Fig 2a, c).
        3.  **Implicit Computation in Biology:** Hopfield's idea that computation distinguishes biology from physics is cited. Cells/organisms are described as processing information to navigate environments adaptively. Organoids/biobots implicitly compute morphology/behavior. Polycomputation concept mentioned.

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Digital (Logic Gates), Analog (implicitly, e.g., gradient sensing, dynamic responses), Hybrid (potentially combining digital logic with analog sensing/responses), Neuromorphic (mentioned in Box 2 for neural agents, Hopfield networks analogy in Box 1). Distributed computation is also a key type discussed. Polycomputation concept suggests multiple types coexist.

### **5.3 Computational Primitive:**

    *   Content:
        *   **Explicit:** Logic gates (NOR ref 40, OR ref 63), Multiplexer (MUX Fig 2b,c), Band-pass filter (Fig 1b), Bistable switch (Toggle switch ref 72, Mutual inhibition Box 1).
        *   **Implicit:** Thresholding (inherent in gene regulation Hill functions Eq 1, Box 1; neural models Eq 9, Box 2), Oscillation (mentioned for circuits), Sensing/Filtering (mentioned for circuits), Pattern formation (Reaction-diffusion can perform computation), Learning/Adaptation (discussed as computation, see Box 1 Hopfield analogy, Section "Embodied memory and learning"). Collective decision-making (implicit in collective intelligence).
    *   **Sub-Type (if applicable):** Logic Gate: NOR, OR, MUX; Switch: Toggle, Mutual Inhibition; Filter: Band-pass.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Cell Movement / Sorting (Adhesion) | Hours to Days | time | Fig 3a-c | Mixed | Figure shows evolution over time, implying this scale. Explicit mention of timescale absent. |
        | Evolutionary Dynamics | Days to Years | time | Refs 11, 12 | Mixed | Evolution experiments run over many generations, timescale explicitly stated in cited works but not detailed here. |

    *   **Note:** Specific values are generally not provided in the text; timescales are inferred based on the biological/physical processes involved or mentioned qualitatively.

### **6.2 Active Inference:**

    *   Content: Unclear / Partial
        *   **Agency:** Defined as goal-directed behavior, sensing, responding, adapting, resisting perturbations, projecting actions (Explicit). This aligns with the general idea of agents minimizing surprise/maintaining homeostasis.
        *   **Problem-Solving:** Mentioned for agential materials solving novel problems (Explicit). Could potentially involve predictive internal models.
        *   **Learning:** Discussed as an open problem and potential capability (e.g., associative learning, learning without neurons) (Explicit). Learning often involves updating internal models based on prediction errors.
        *   **Biobots:** Their behavior (movement, self-repair, potential learning) might be interpretable through an Active Inference lens, but the paper doesn't frame it this way.
        Overall, the paper touches upon elements consistent with Active Inference (agency, adaptation, potential goal-directedness), but lacks the formal structure (explicit internal models, prediction error minimization) and evidence to confirm its presence. It remains a potential framework for analyzing some systems discussed.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes
        1.  **Evolutionary Adaptation:** Explicitly shown in experiments evolving multicellularity (refs 11, 12, Fig 1j, k) and *in silico* evolution of Xenobots (ref 13). This involves changes (genetic or structural) over generations leading to improved fitness/function.
        2.  **Learning/Behavioral Adaptation:** Discussed as a key aspect of agency and cognition (Section "Embodied memory and learning", "Synthetic collective intelligence", "Synthetic neural cognition", "Synthetic behaviour"). Examples include associative learning, learning in GRNs without genetic change (refs 101, 102), experience-dependent behavior in C. elegans (ref 130-132) as inspiration. Implies changes within an organism's lifetime.
        3.  **Agential Materials:** Described as solving new problems and exhibiting adaptive structure/behavior in real-time (Refs 14, 26, 27). Xenobots/Anthrobots show self-repair and adaptability.

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: Mechanisms mentioned or implied include:
        *   **Evolutionary Algorithms / Natural Selection:** Changes driven by selection pressures acting on variations (genetic mutations, structural configurations). Used explicitly in evolving Xenobots (Ref 13) and observed in MC evolution experiments (Ref 11, 12).
        *   **Learning Mechanisms:** Associative learning (Ref 89-91), reinforcement learning principles (implied for agency/problem solving, discussed in background context), Hebbian-like mechanisms (potentially in GRN learning or neural contexts, though not specified). Memory mechanisms (Box 1, Section on Memory) underlie learning. Specific algorithms or detailed molecular mechanisms for learning within synthetic MC systems are mostly treated as open problems or cited examples (e.g., Ref 58 theoretical associative learning).
        *   **Dynamic Network Reconfiguration:** Changes in GRN states or potentially synaptic strengths (in neural contexts) driven by experience/signals (Refs 101, 102, 104, 105).
        *   **Bioelectrical/Biochemical/Biomechanical Feedback:** Implicitly driving adaptation in agential materials/regenerating systems (Ref 26, 95-97). Perception-action loops guide navigation of morphospace.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The paper describes a range of functional behaviors:
        *   **Pattern Formation:** Creating spatial arrangements like gradients, stripes, spots, segregated layers, branching structures (Fig 1a, b, d, g).
        *   **Morphogenesis:** Development of specific shapes and structures (Organoids Fig 1f, Biobots Fig 1h, l, Embryos Fig 1e).
        *   **Computation:** Performing logical operations, filtering signals (Fig 1b, Fig 2b, c).
        *   **Movement/Locomotion:** Cilia-driven propulsion (Anthrobots), muscle-driven crawling/swimming (Xenobots), swarming (conceptual).
        *   **Collective Behavior:** Coordinated action in groups (Xenobots pushing objects, conceptual swarms, collective intelligence).
        *   **Self-Repair:** Healing from damage (Xenobots, Anthrobots healing neural wounds).
        *   **Reproduction:** Kinematic self-replication (Xenobots, Ref 118), budding/fragmentation in evolved yeast (Ref 11, Fig 1k), life cycle completion.
        *   **Sensing/Responding:** Reacting to chemical signals, stimuli (implicit in circuits, agency).
        *   **Learning/Adaptation:** Modifying behavior based on experience (discussed as a capability/goal).

### **8.2 Behavior Robustness:**

        *   **Overall:** Robustness varies greatly. Engineered systems face challenges, while emergent systems based on biological materials might leverage inherent biological robustness (e.g., homeostasis, self-repair) but also suffer from biological variability. The score reflects this mixed picture.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper primarily relies on citing experimental results and showing images/diagrams as validation for emergent behaviors (e.g., Figs 1, 3).
        *   **Pattern Formation/Morphogenesis:** Visual observation (microscopy images) of patterns/structures (Fig 1a, c, d, f, g, h, k, l; Fig 3a-c). Comparison with expected patterns from models (implicit for adhesion Fig 3).
        *   **Computation:** Validation via measuring output signals (e.g., GFP) corresponding to input logic states (Truth tables, Fig 2b).
        *   **Movement/Behavior (Biobots):** Observation via video microscopy, tracking movement paths, documenting interactions with objects or environment (Refs 13, 14, 94, 118 references imply this).
        *   **Self-Repair/Reproduction:** Direct observation of healing or replication processes (Refs 14, 118 references imply this).
        *   **Quantitative Analysis:** Generally lacking within this paper, but implied in cited works (e.g., measuring state distributions, motion parameters). Robustness/reliability/reproducibility are not systematically addressed here, though concepts like predictability are discussed. Control experiments are implied by the nature of synthetic biology (comparing engineered vs. wild-type). Limitations often relate to variability and complexity (Fig 4).

---

#Key: [bayat_self-indicating_2024]

# Self-indicating polymers: a pathway to intelligent materials

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The publication reviews "self-indicating polymers," described as a class of smart materials that exhibit detectable changes in physical or chemical properties (e.g., color, fluorescence, conductivity, mechanical properties) in response to various external stimuli (e.g., aggregation state, phase transition, bond cleavage, isomerization, charge transfer, energy transfer, light, pH, temperature, electricity, magnetic field, ion concentration, mechanical force). The purpose of these materials is primarily sensing, monitoring (analytes, environmental parameters, mechanical stress), and enabling functionalities in applications like coatings, drug delivery, food sensors, wearable devices, and molecular switches by visually or otherwise indicating a change in state or environment. Components typically involve a polymer matrix incorporating stimuli-responsive moieties (e.g., chromophores, fluorophores, AIEgens, mechanophores like spiropyran/azobenzene, phase change materials, specific chemical bonds susceptible to cleavage).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters listed are general categories derived from the review's scope or specific examples cited within the excerpt. A review article does not focus on exhaustive parameters for a single implementation. Data reliability is "High" for explicitly stated concepts/examples, "Medium" for general ranges cited.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source varies depending on the stimulus being sensed/responded to. Examples include: Thermal energy (for temperature/phase change response), Light energy (photons for photoisomerization, photochromism, UV activation, FRET excitation), Chemical potential energy (pH change, ion concentration gradients, reactant binding, bond cleavage triggers), Mechanical energy (stress/strain for mechanophores). Electrical energy (for electrochromism). Magnetic energy is mentioned but not detailed.

### **2.2 Energy Transduction**

    *   Content: The review details multiple energy transduction mechanisms where input energy triggers a change in the polymer system, leading to an observable output. Examples:
        *   **Light Energy -> Chemical Energy -> Optical Change:** Photoisomerization (e.g., spiropyran SP->MC, azobenzene trans->cis) involves photon absorption altering molecular structure (bond rotation/cleavage), changing absorption/emission spectra.
        *   **Light Energy -> Electronic Excitation -> Energy Transfer -> Optical Change:** RET (FRET/TBET) involves absorption by a donor, non-radiative transfer to an acceptor via dipole-dipole coupling (FRET) or through bonds (TBET), followed by acceptor emission or quenching.
        *   **Thermal Energy -> Phase Change -> Optical/Thermal Change:** PCMs absorb/release latent heat during solid-liquid or crystalline-amorphous transitions, altering refractive index, transparency, and thermal properties.
        *   **Mechanical Energy -> Chemical Energy -> Optical Change:** Mechanophores (e.g., spiropyran) convert applied stress/strain energy into chemical energy to break bonds (e.g., C-O bond in SP), leading to a structural change (SP->MC) and color change.
        *   **Chemical Energy -> Structural Change -> Optical/Electrical Change:** Binding of analytes (ions, molecules, changes in pH) can trigger conformational changes, aggregation/disaggregation (AIE/ACQ), bond cleavage/formation, or charge transfer events, altering optical or electrical properties.
        *   **Aggregation/Disaggregation -> Optical Change:** Changes in molecular organization (driven by solvent, concentration, binding) affect intermolecular interactions (e.g., RIR, RIV in AIE), altering fluorescence properties (ACQ vs AIE).
        *   **Charge Transfer -> Electrical/Optical Change:** Modulation of electron/hole transfer affects conductivity or redox properties, or changes absorption/emission via ICT/TICT states.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The review does not provide quantitative data on the energy efficiency of the stimulus-to-response transduction for any specific system. Qualitative assessments (e.g., quantum yields for fluorescence, efficiency of FRET listed in Table 2) are mentioned for some mechanisms, but an overall efficiency score cannot be assigned without more system-specific data (e.g., comparing input energy required to trigger a response vs. energy released/modulated in the output signal). FRET efficiency values are provided for specific donor-acceptor pairs (Table 2), ranging significantly (e.g., 49.8% to 99.9%), suggesting high efficiency is *possible* for RET but not universal. AIE efficiency is qualitatively described as high in the aggregated state. Phase change materials focus on latent heat storage, which can be efficient, but efficiency as an *indicator* is not quantified.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation mechanisms are inherent but not explicitly quantified. Examples include:
        *   **Non-radiative decay:** Competing pathway to fluorescence/FRET (e.g., internal conversion, intersystem crossing, vibrational relaxation like RIR/RIV in AIE which *restricts* dissipation). Mentioned implicitly in the context of ACQ and AIE mechanisms. (Qualitative: High in ACQ, Low in AIE).
        *   **Heat loss:** During phase transitions (though PCMs are designed to *store* latent heat, some sensible heat exchange occurs), chemical reactions (bond cleavage/formation), and resistive losses in conductive polymers. (Qualitative: Likely Medium/High depending on process).
        *   **Mechanical dissipation:** Hysteresis during stress-strain cycles involving mechanophores, friction (not discussed). (Qualitative: Present but unquantified).
        *   **Scattering/Absorption:** In optical processes (relevant for LSCs, smart windows), light can be scattered or absorbed non-productively. (Qualitative: Present, potentially significant depending on design, mentioned implicitly in LSC context).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Partial/Weak
        *   **Photochromism (Isomerization):** Spiropyran, azobenzene, diarylethene, etc., can switch between two states (e.g., SP/MC, trans/cis, open/closed ring). The state persists after the initiating stimulus (UV/Vis light) is removed, influencing subsequent responses (e.g., absorption, color, conductivity). This fits the definition of memory. However, the lifetime of the metastable state can vary, and reverse switching can occur spontaneously (thermally) or via another stimulus (visible light, heat). It's often reversible and might not strongly influence *future* distinct behaviors beyond maintaining the current state. (Sec 2.3)
        *   **Phase Change Materials (PCMs):** The solid/liquid or crystalline/amorphous state persists within a temperature range, representing memory of the thermal history. This state determines properties like transparency or heat storage capacity. (Sec 2.2)
        *   **Mechanophores:** Color change (SP->MC) persists after mechanical force is applied and potentially removed (though relaxation occurs, Fig 4b shows fading). This memory indicates past stress. (Sec 2.3)
        These examples show state persistence influencing current properties, but the review doesn't describe systems where this memory actively *modulates complex future decision-making or learning* in the cognitive sense. The memory is often a direct record of the last significant stimulus, not an integrated history influencing diverse future actions.

**(Conditional: M3.1 is "Partial/Weak", proceed with M3.2 and M3.3)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable (system dependent)
*    Units: seconds, minutes, hours, days, potentially years (in dark/stable conditions)
        *   Photochromic pyrrylfulgide: Recorded signals stable "> 3 months" in ambient light, "> 1 year" in darkness (Sec 2.3). This suggests potentially long-term retention is possible for some systems under specific conditions.
        *   Mechanophore SP3 in PMA: Color fades significantly over 300 minutes after failure (Fig 4b), suggesting minutes-to-hours retention for this specific case. (Sec 2.3)
        *   Phase Change Materials: Retention is tied to maintaining the temperature above/below the transition point. (Implicit from Sec 2.2).
        The retention time is highly dependent on the specific chemical system and environmental factors (temperature, light exposure, presence of reactants/catalysts for reversal). It can range from transient (milliseconds/seconds) to potentially very long-term under ideal storage.

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Typically 2 (for bistable systems)
*   Units: distinct states

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Variable / Fatigue Effects Mentioned

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial/Weak

**(Conditional: If M4.1 is "Partial/Weak", include M4.2-M4.7, but note limitations)**

### **4.2 Local Interaction Rules:**

    *   Content: The local rules are generally standard intermolecular forces and thermodynamic principles driving aggregation or phase separation, but are not described in specific operational detail for emergence:
        *   **Aggregation (AIE/ACQ):** Driven by factors like poor solvent interactions (hydrophobicity), pi-pi stacking, hydrogen bonding, electrostatic interactions, and concentration effects. Specific forces depend on the molecule (e.g., TPE derivatives). The key outcome discussed is restriction of intramolecular rotation/vibration (RIR/RIV) upon aggregation. (Sec 2.1)
        *   **Phase Separation:** Driven by thermodynamic immiscibility (e.g., differences in polymer-polymer or polymer-solvent interaction parameters, temperature dependence like LCST/UCST). For smart windows, mismatch/match of refractive indices between phases is key. (Sec 2.2)
        Rules are generally optimizing free energy locally, but not detailed algorithmically.
    * **Implicit/Explicit**: Implicit

### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**

    *   Content: The emergent global order described is relatively simple:
        *   **Aggregates:** Formation of molecular clusters or nanoparticles (e.g., AIEgens, J-aggregates mentioned). The order is primarily local packing or alignment within the aggregate, leading to bulk property changes (fluorescence). (Sec 2.1)
        *   **Phase-Separated Morphologies:** Formation of distinct domains (e.g., crystalline/amorphous, polymerA-rich/polymerB-rich). The scale and geometry depend on composition and processing but complex emergent patterns are not the focus. (Sec 2.2)
        *   **Liquid Crystal Phases:** Mentioned indirectly via azobenzene mesogens (Sec 2.3), implying potential for nematic/smectic order, but not detailed as an emergent outcome in the excerpt.
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    *   **Yoneda Embedding Fulfillment Score [0-10]:** 0

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6.)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Photoisomerization Response | ms to s (typical) | ms, s | Inferred | Implicit | Based on general knowledge of photochromism, not specific values in text. |
        | Mechanophore Activation | ms to s (force application dependent) | ms, s | Inferred | Implicit | Response related to rate of strain/force application; not specified. |
        | Memory Retention (Photochromic - Fulgide) | > 3 months (ambient light), > 1 year (dark) | months, years | Sec 2.3 | Explicit | Values explicitly stated for this example. |
        | Memory Retention (Mechanophore - SP3 relaxation) | ~300 minutes (significant fading) | minutes | Fig 4b, Sec 2.3 | Explicit | Inferred timescale from figure description. |
        | Phase Transition Response (Thermal) | Seconds to minutes (depends on heating/cooling rate, thermal mass) | s, min | Inferred | Implicit | General knowledge of thermal transitions; not specified in text. |
        | FRET/TBET | ps to ns | ps, ns | Inferred | Implicit | Fundamental timescale of excited state energy transfer; not explicitly stated as system dynamic time. |
        | Aggregation/Disaggregation Kinetics | Variable (seconds to hours) | s, min, h | Inferred | Implicit | Dependent on concentration, solvent diffusion, nucleation; not specified. |
    *   **Note:** Most timescales are inferred based on general knowledge of the phenomena, as the review focuses on mechanisms rather than detailed kinetics. Only memory retention examples have explicit (or figure-derived) values.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8.)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior described is **stimulus indication**. This manifests as a detectable change in an observable property upon exposure to a specific stimulus or environmental change. Specific manifestations include:
        *   Color change (photochromism, mechanochromism, pH indication, charge transfer complex formation).
        *   Fluorescence change (turn-on/off via AIE/ACQ, FRET/TBET, bond cleavage, isomerization).
        *   Transparency/Opacity change (phase transitions in smart windows/PCMs).
        *   Conductivity change (isomerization in conductive polymers, charge transfer).
        *   Structural/Mechanical property change (bond cleavage).
        Secondary behaviors enabled by indication include sensing, monitoring, self-reporting (damage), self-healing (when coupled), light harvesting/concentration (LSCs), temperature regulation (smart windows/PCMs), controlled release (drug delivery), and authentication (fingerprints).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: As a review, the paper cites primary literature (implicitly assumed to contain validation). Within the excerpt:
        *   Fig 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 21, 22, 23, 24 provide visual evidence (photos, schematics) for claimed behaviors (color change, fluorescence, transparency change, device operation).
        *   Spectroscopic data (absorption/emission spectra shifts) are mentioned as evidence for mechanisms like isomerization, AIE, FRET/TBET, phase transitions determining optical properties.
        *   Table 2 lists FRET efficiencies, providing quantitative support for energy transfer claims.
        *   Specific concentration/pH ranges or temperature values are sometimes given for responses.
        Validation primarily relies on demonstrating the stimulus-response link via spectroscopic or visual changes, consistent with the proposed mechanism. Robustness/reproducibility validation is not detailed in the excerpt.

---

#Key: [gregor_self-organizing_2020]

# Self-Organizing Intelligent Matter: A blueprint for an AI generating algorithm

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper proposes a theoretical framework called Self-Organizing Intelligent Matter (SIM) aimed at facilitating the emergence of intelligent organisms within an artificial life context. The system consists of an environment populated by atomic 'elements'. Each element contains a neural operation (potentially a mini neural network). These elements interact through physics-like rules (e.g., energy/chemical exchanges, movement) and direct communication (e.g., signal propagation, attention). There is no explicit agent definition; organisms are emergent properties of collections of these elements. The purpose is to create an AI generating algorithm, where evolution, driven by propagation success rather than an explicit objective, leads to the emergence of complexity and intelligence. A simplified grid-world implementation is presented, where elements (RNNs) reside on a grid, interact via signals, manage energy and chemicals, and can move or copy themselves (mutate).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name             | Value           | Units   | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------------------- | :-------------: | :-----: | :------------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters describe the simplified grid implementation. "Max Element Lifetime" is mentioned but not quantified.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: In the grid implementation (Sec 3.2, App A.1), energy enters the system primarily through chemical reactions mediated by enzymes produced by the elements (cells). A reaction Ci -> C(i+1) releases a fixed amount of energy (except the last reaction). Cells can also 'pull' energy from neighbours. In one variation ('pure energy system'), energy increases at every location up to a threshold.
    *   Value: Not specified (Fixed amount per reaction, varies based on element actions)
    *   Units: Arbitrary Energy Units

### **2.2 Energy Transduction**

    *   Content: Energy stored chemically is converted into usable energy for a cell via enzyme-catalyzed reactions (Ci -> C(i+1) releases energy). This energy is then used (transduced) to power cell actions: copying weights (replication), moving, producing enzymes, and performing energy/chemical flows (pulling/pushing quantities). Energy can be transferred between cells via the flow actions. Cells below an energy threshold 'die'.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any quantitative measure or qualitative assessment of energy efficiency (e.g., energy cost per computation, energy cost per replication). The focus is on the mechanisms of energy flow and survival, not optimizing efficiency.

### **2.4 Energy Dissipation**

    *   Content: Energy is explicitly dissipated (consumed) when cells perform actions: 1. Copying weights has a fixed energy cost (App A.1). 2. Energy/chemical flow actions cost energy proportional to the push/pull (Sec 3.2, App A.1). 3. Enzyme production costs energy (Sec 3.2). Implicitly, maintaining the cell state (RNN updates) likely consumes energy, although not stated. Energy leaving the system occurs when cells die (energy below threshold or max lifetime reached) and their energy is effectively removed or reset. Quantification is qualitative (cost mentioned, proportional cost mentioned) but not numerically specified. Assessment: Medium (multiple specified costs).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding with M3.2-M3.8)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable (Short-term for `h`, Long-term/Generational for `W`)
*    Units: Simulation Steps / Generations

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Dependent on n_h and network size
*   Units: Bits (theoretically) / Number of parameters

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", proceeding with M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules primarily govern individual cell behavior and interaction with immediate neighbors or environment within the grid implementation:
        1.  **Neural Update:** Cell's hidden state `h` updates based on inputs `x` (from signals/environment) and previous state `h_{t-1}` using RNN equations: `h_t = tanh(W_x x_t + W_h h_{t-1} + b_h)`. Actions `a_hat` derived from `h_t`: `a_hat = W_a h_t + b_a` (App A.1).
        2.  **Energy/Chemical Dynamics:** Cells interact with local energy/chemical fields. They produce enzymes (`Z_i`) influencing reactions (`C_i -> C_{i+1}`) which release/consume energy. They can pull/push energy and chemicals to/from neighbours, with costs. Energy below threshold causes death (Sec 3.2, App A.1).
        3.  **Actions (derived from `a_hat`):**
            *   **Copy:** If energy sufficient, copy weights+biases (with noise/mutation) to an empty neighbour cell (App A.1).
            *   **Move:** Swap contents with a neighbour cell (Sec 3.2).
            *   **Energy/Chemical Flow:** Control flow to/from neighbours (App A.1).
            *   **Enzyme Production:** Determine enzyme levels `Z_i` (App A.1).
        4.  **Communication:** Cells write state `h` to signal layers; signals propagate; cells read signals as input `x` (Sec 3.2). Alternative: Attention mechanism (Sec 3.2).
        5.  **Death/Birth:** Cells die below energy threshold or max lifetime. New random cells may appear if population low (Sec 3.2).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID               | Description                          | Parameter Name         | Parameter Value Range | Units         | Data Source | Implicit/Explicit | Justification                       |
    | :-------------------- | :----------------------------------- | :--------------------- | :-------------------- | :-----------: | :----------: | :----------------: | :---------------------------------- |
    | Birth                 | Population density for reawakening   | Density Threshold      | 10                    | %             | Sec 3.2     | Explicit          | Explicitly stated.                |

### **4.3 Global Order:**

    *   Content: The emergent global order described consists of:
        1.  **Coexisting Species:** Populations of elements with distinct characteristics (represented by different colors/weights in Fig 2) stably coexisting in the same spatial regions, suggesting niche differentiation.
        2.  **Spatial Niche Construction:** Elements modifying the environment (e.g., chemical densities) creating distinct spatial regions (niches) occupied by different species (Fig 2 top left).
        3.  **Population Dynamics:** Oscillatory behavior in the populations of different species, reminiscent of predator-prey dynamics (Lotka-Volterra like, Fig 2 middle graph).
        4.  **Self-Propagation Behavior:** Emergence of elements that actively collect energy to enable copying, even when survival doesn't strictly require energy (Fig 2 right graph).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID           | Description                   | Parameter      | Value Range       | Units         | Implicit/Explicit | Justification                     | Source   |
| :---------------- | :---------------------------- | :------------- | :---------------- | :-----------: | :----------------: | :-------------------------------- | :------- |
| Birth             | Population threshold to re-seed | Pop. Threshold | 10                | %             | Explicit          | Value given                       | Sec 3.2  |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID             | Description                      | Parameter                    | Value Range        | Units    | Implicit/Explicit | Justification                                       | Protocol                      | Source     |
| :---------------------- | :------------------------------- | :--------------------------- | :----------------- | :------: | :----------------: | :-------------------------------------------------- | :---------------------------- | :--------- |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type                     | Description                      | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification                                            | Source |
    | :---------------------------- | :------------------------------- | :------------- | :----------- | :------ | :----------------: | :------------------------------------------------------- | :----- |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", proceeding with M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Neuromorphic/Hybrid

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operation performed *by the material elements* is the recurrent neural network update cycle. This involves:
        1.  Matrix multiplication and vector addition (linear transformations): `W_x * x_t`, `W_h * h_{t-1}`.
        2.  Element-wise non-linearity: `tanh(...)`.
        3.  State update: Update hidden state `h_t`.
        4.  Output calculation: `W_a * h_t + b_a`.
        Potentially includes softmax for action selection (App A.1). Section 2.1 also mentions basic operations like additions, matrix multiplications, outer products, and non-linearities as fundamental building blocks the system *could* represent.
    *   **Sub-Type (if applicable):** RNN Update Cycle (including Matrix Multiplication, Vector Addition, Non-linear Activation `tanh`).

### **5.4 Embodied Computational Units**
| Unit ID       | Description                             | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source     | Implicit/Explicit   | Justification                                                    |
| :------------ | :-------------------------------------- | :--------------- | :--------------- | :--------------: | :-------: | :-------------- |:-------------------:| :--------------------------------------------------------------- |

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description      | Value | Units    | Source             | Implicit/Explicit   | Justification                                       |
        | :------------------------- | :---: | :------: | :----------------- | :-----------------: | :-------------------------------------------------- |
        | Simulation Step            | 1     | step     | Implicit Assumption| Implicit          | Discrete time steps assumed for simulation.           |
        | RNN Update                 | 1     | step     | App A.1            | Explicit          | "At every point in time... we apply a classic RNN update" |
        | Signal Propagation         | 1     | step     | Sec 3.2            | Explicit          | "At every point in time, each grid moves..."        |
        | Cell Action Execution      | 1     | step     | App A.1            | Implicit          | Actions seem to resolve within one time step.       |
        | Population Dynamics (Osc.) | ~Many | steps    | Sec 4, Fig 2       | Implicit          | Oscillations occur over many simulation steps.      |
        | Evolutionary Timescale     | ~Many | generations| Sec 1.1, Implicit  | Implicit          | Evolution occurs over multiple copy/mutation events. |
    *   **Note:** Most timescales are implicitly 1 simulation step or longer-term emergent dynamics.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", proceeding with M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The primary adaptation mechanism described is objective-free evolution operating on the neural network weights of the elements. When an element successfully performs a 'Copy' action on an empty neighbour, its weights and biases are copied with added noise (mutation) (App A.1). There is no explicit fitness function or objective being optimized. Instead, "those classes of units that propagate [...] keep existing, while those that don’t propagate cease to exist" (Sec 1.1). Propagation depends on survival (energy management) and successful copying. This leads to selection for behaviors that enhance propagation within the specific environmental context, including interactions with other elements (niche construction). The 'ideal' system also mentions evolving the update rule `F` itself or having elements program others (Sec 3.1), representing meta-adaptation or learning to learn, but these are future concepts.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors observed emerging from the interactions of elements include:
        *   **Self-Propagation/Replication:** Elements successfully manage energy and execute copy actions to create copies of themselves (with mutation).
        *   **Niche Construction:** Elements modify the local environment (chemical concentrations) creating spatial heterogeneity that influences the survival and distribution of different element types.
        *   **Coexistence:** Different types ('species') of elements finding ways to stably coexist in the same environment, presumably by occupying different niches.
        *   **Competitive/Predator-Prey Dynamics:** Oscillations observed in the populations of different coexisting species, suggesting dynamic interactions like competition or predation.
        *   **Resource Acquisition:** Elements evolving strategies to collect energy (even when not strictly needed for survival in one variant), as energy enables propagation via copying.
        *   **Aggregation/Swarming (Implicit):** Figure 2 shows clustered elements, suggesting aggregation might occur, although not explicitly analyzed as a behavior.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation primarily relies on computational simulation and qualitative observation of the resulting dynamics. Emergent behaviors like species coexistence, niche construction, and oscillations are identified visually from plots of element weights (color-coded), chemical concentrations, and population graphs over time (Sec 4, Fig 2). The paper mentions accompanying videos for better visualization. There are no formal quantitative metrics defined or measured for emergence itself (e.g., measures of complexity or organization). Control experiments (e.g., comparing with non-evolving systems, different rulesets) are not presented. Robustness/reproducibility is touched upon by mentioning diversity across runs but not systematically studied. Limitations include the qualitative nature of the validation and the focus on one specific grid-world implementation.

---

#Key: [mcmillen_collective_2024]

# Collective intelligence: A unifying concept for integrating biology across scales and substrates

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper presents a conceptual framework centered on "collective intelligence" (CI) as a unifying principle for understanding biological systems across multiple scales (molecular networks, cells, tissues, organs, organisms, swarms/societies) and substrates (conventional neural systems, non-neural cellular collectives, bacterial biofilms, potential subcellular networks). It argues that functionality is nested, with each level capable of problem-solving in distinct spaces (metabolic, physiological, anatomical, behavioral). The core idea is that adaptive functionality percolates from competent subunits to higher organizational levels through collective dynamics, enabling problem-solving (achieving goals via different means) in novel circumstances. The framework aims to bridge behavioral science (swarm intelligence, decision-making) with developmental biology, regenerative medicine, and bioengineering by treating biological phenomena like morphogenesis, regeneration, and immune responses as instances of collective problem-solving by cellular groups. Components include the hierarchical levels of biological organization, the distinct problem spaces navigated at each level, and the communication/interaction mechanisms enabling collective behavior (e.g., bioelectricity, chemical signaling). Its purpose is to provide a unifying perspective, leverage tools across disciplines (e.g., cognitive science tools for non-neural systems), and potentially enable better prediction and control in biomedicine and synthetic biology.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters define the *scope* and *basis* of the conceptual framework presented. Values are categorical or descriptive. Concrete physical values apply to specific biological examples reviewed, not the framework itself.

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Varies Qualitatively (from milliseconds to organism lifetime)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Bistability (Planaria) | Stochastic switching between 1-head and 2-head states during regeneration following bioelectric perturbation. | ~70:30 (2H:1H) | Ratio | `MemoryNode` attribute `stateProbability` | Fig 5f, Ref 130, 131 | Explicit (outcome ratio) / Implicit (as fidelity metric) | Ratio explicitly stated; interpreting as fidelity/robustness metric is implicit. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The paper describes local interaction rules conceptually and through examples, rather than providing formal mathematical equations. Rules include:
        *   **Cell-cell communication:** Via signaling pathways (biochemical, bioelectrical) influencing fate decisions, proliferation, migration (e.g., serotonin signaling in melanocyte conversion, Fig 4c; Notch pathway in segmentation clock, Fig 8; generic signaling implied in embryogenesis/regeneration).
        *   **Bioelectric coupling:** Cells influencing each other's resting potential via gap junctions, creating tissue-level patterns that guide morphogenesis and regeneration (e.g., planarian regeneration polarity via Vmem differences, Fig 5e; eye induction via K+ channel expression, Fig 6e; LR asymmetry via H+/K+-ATPase, Fig 6a-d). The rule is often pattern-based (relative differences matter, Fig 5e).
        *   **Physical interactions:** Implicit in collective cell migration (neural crest, Fig 7), tissue fusion/separation (embryo twinning, Fig 3).
        *   **Environmental sensing:** Cells responding to local cues (e.g., morphogen gradients, electric fields, substrate properties, nutrient levels).
        *   **Genetic network dynamics:** Intrinsic rules governing gene expression within cells based on internal state and external signals (Fig 1e).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Bioelectric Coupling | Resting Potential Difference | Vmem | mV (e.g., differences driving polarity) | Fig 5e, Refs 117, 132 | Explicit (concept) / Implicit (exact range) | Vmem differences mentioned as key; specific ranges required for specific outcomes are implied by cited studies, not detailed here. |
    | Signaling Pathway Activation | Ligand Concentration / Receptor Occupancy | Conc. / % | Varies (e.g., Morphogen gradients) | Molar, % | Fig 5b (concept) | Implicit | Concept of gradients is explicit, but specific concentration ranges for outcomes are background knowledge from cited fields. |
    | Oscillatory Gene Expression | Period / Phase | Varies | minutes/hours | Fig 8, Refs 163-170 | Explicit (concept) / Implicit (exact values) | Oscillations are key; specific period/phase values depend on the system (cited refs). |

### **4.3 Global Order:**

    *   Content: The emergent global order includes:
        *   **Anatomical Structures:** Correctly formed and patterned embryos (Fig 3), organs (eyes - Fig 3e, 6e; hearts, viscera - Sec "Left/right..."), regenerated organisms (planaria - Fig 5), segmented body axes (Fig 8).
        *   **Physiological States:** Coordinated tissue-level states (e.g., all-or-none melanocyte conversion across tadpole - Fig 4), synchronized oscillations (segmentation clock - Fig 8), coordinated metabolism (biofilms - Ref 38).
        *   **Behavioral Patterns:** Coordinated cell migration (neural crest - Fig 7), swarm/flock behavior (mentioned in Intro).
        *   **Functional Integrity:** Achieving target morphologies despite perturbations (regulative development, regeneration), homeostatic setpoints.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Анатоmical Polarity (Planaria) | Head Number & Location | Head Count | 0, 1, 2 | Count | Explicit | Directly observed outcomes of regeneration experiments. | Regeneration Assay | Fig 5e,f |
| Tissue State (Melanocytes) | Conversion Phenotype | % Converted Cells (per animal) | 0 or 100 | % | Explicit | Quantified observation of the all-or-none phenotype. | Microscopy/ Imaging | Fig 4a,b |
| Biofilm Structure/ Behavior | Coordinated Growth Oscillations / Metabolic Coordination | Oscillation Period / Metabolic Rate Synch. | Time / Rate | s / M/s | Explicit (from cited work) | Measures used in cited biofilm studies (Refs 35, 36, 177, 178). | Cited Experimental Methods | Sec: Intelligence in bacterial communities |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid/Neuromorphic (by analogy)

### **5.3 Computational Primitive:**

    *   Content: Primarily **Decision-Making** and **Pattern Recognition/Matching**.
        *   **Decision-Making:** Cells/collectives choosing between discrete fates or outcomes (e.g., Head vs. Tail in planaria Fig 5, Left vs. Right identity Fig 6, Normal vs. Cancerous state Fig 4, Eye vs. Skin Fig 6e,f, Migrate vs. Stay). Often involves thresholding based on local cues or collective states.
        *   **Pattern Recognition/Matching:** Cells/collectives interpreting spatial patterns (morphogen gradients Fig 5b, bioelectric pre-patterns Fig 5e, Fig 6e) to determine identity or action. Achieving target morphology during development/regeneration implies matching current state to a target pattern/attractor (Fig 1f,g).
        *   Other potential primitives implied: **Error detection/correction** (regeneration correcting defects), **Signal Integration** (combining multiple cues), **State Maintenance/Memory** (holding positional information or cell fate).
    *   **Sub-Type (if applicable):** Decision-Making: Threshold-based fate selection; Pattern Recognition: Gradient interpretation, Bioelectric pattern matching.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Bioelectric Signaling (Propagation) | Milliseconds to Minutes | s, min | Implicit (General knowledge) / Refs 117, 132 | Implicit | Standard timescale for ion channel dynamics and intercellular propagation. |
        | Gene Expression Changes | Minutes to Hours | min, hr | Implicit (General knowledge) / Refs 225, 226 | Implicit | Standard timescale for transcription/translation. |
        | Cell Migration | Hours to Days | hr, day | Fig 7 (Neural Crest), Fig 4 (Melanoma) | Mixed | Process duration implied by developmental context/figures. |
        | Morphogenesis / Development | Days to Weeks (or longer) | day, week | Fig 3 (Embryogenesis), Fig 5 (Regeneration) | Mixed | Process duration implied by developmental context/figures. |
        | Cellular Oscillations (Segmentation Clock) | Minutes to Hours | min, hr | Fig 8, Refs 163-170 | Explicit (periodicity is key feature) | Explicitly discussed as oscillatory. |
        | Memory Retention | Milliseconds to Lifetime | s, min, hr, day, year | Sec 3.3 | Mixed | Varies depending on the memory mechanism as discussed in M3.3. |

    *   **Note:** Specific values are often implicit, based on general biological knowledge of the processes reviewed, or require consulting the original cited research. The table reflects the range indicated by the phenomena discussed.

### **6.2 Active Inference:**

    *   Content: Yes/Partial
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** Measuring the rate of convergence towards a target morphology during regeneration after perturbation (Prediction error reduction rate). Analyzing signaling dynamics for predictive signals preceding developmental events (Timescale of anticipation). Modeling the complexity of bioelectric patterns or GRNs required to explain observed robustness/goal-directedness (Complexity of internal models). Experimentally perturbing feedback loops hypothesized to mediate active inference and quantifying the deviation from expected outcomes. CT-GIN could model information flow, prediction error signals, and model updates as distinct edge/node attributes, allowing quantification of information-theoretic measures related to inference.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes
        *   **Regulative Development/Regeneration:** Embryos or fragments adjusting cellular processes to achieve a normal morphology despite damage or altered initial conditions (Fig 1f, Fig 5a, Refs 100-102, 121, 122). This involves changes in cell behavior (migration, proliferation, differentiation) over developmental time.
        *   **Neural Crest Migration:** Cells re-routing their migration paths to compensate for ablation of neighboring arches (Fig 7a, Refs 158, 159). This is a change in behavior based on environmental context (missing cells).
        *   **Learning in GRNs/Cells:** Gene regulatory networks and potentially cells exhibiting learning-like behavior (Pavlovian conditioning) through changes in network dynamics over time based on stimulus patterns (Fig 1e, Refs 225-227).
        *   **Physiological Adaptation:** Implied in homeostasis, where systems adjust internal parameters to maintain stability despite external changes.
        This involves persistent changes driven by experience or context, going beyond fixed stimulus-response.

### **7.2 Adaptation Mechanism:**

    *   Content: The paper suggests diverse mechanisms depending on the scale and system:
        *   **Feedback Control Loops:** Biological systems utilize feedback (e.g., morphogen gradients, bioelectric states, mechanical stress) to compare current state to a target state (homeostatic setpoint, target morphology) and adjust cellular behaviors (proliferation, migration, differentiation) to reduce the discrepancy. This is evident in regeneration and regulative development (Fig 1f,g, Fig 5). Active inference is proposed as a formal framework for this (M6.2).
        *   **Cellular Communication & Collective Decision-Making:** Cells exchange information (chemical, electrical) allowing the collective to sense perturbations or context changes (e.g., missing structures) and coordinate a modified response (e.g., neural crest re-routing Fig 7a; synchronization in segmentation clock Fig 8b).
        *   **Changes in Network Dynamics:** Bioelectric or gene regulatory networks altering their connectivity or activity patterns based on experience/stimuli, leading to changed outputs (e.g., learning in GRNs Fig 1e; stable alternative states in planarian regeneration Fig 5f). This resembles plasticity in neural networks.
        *   **Environmental Influence:** Cells responding to altered local cues (e.g., changes in morphogen gradients after injury, different mechanical environment) trigger adaptive responses.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main emergent behaviors described are manifestations of collective intelligence acting across biological scales:
        *   **Morphogenesis/Pattern Formation:** The coordinated self-assembly of cells into complex, functional anatomical structures like embryos, organs (eyes, hearts), tissues with specific polarity (LR asymmetry, AP axis), and segmented bodies. (Fig 1f,g, Fig 3, Fig 5, Fig 6, Fig 8).
        *   **Regeneration:** The ability of a system (e.g., planarian) to restore missing parts and achieve a complete, correctly patterned organism after injury, involving coordinated cell proliferation, migration, differentiation, and decision-making. (Fig 1g, Fig 5).
        *   **Collective Cell Migration:** Coordinated movement of cell groups to specific targets during development (e.g., neural crest migration, Fig 7). Includes adaptive re-routing.
        *   **Coordinated Physiological States:** Emergence of tissue- or organism-wide states from cellular interactions (e.g., synchronized oscillations in segmentation clock Fig 8; all-or-none cancer conversion Fig 4; coordinated metabolism/growth in biofilms).
        *   **Problem-Solving/Goal Achievement:** The overarching behavior of biological collectives achieving specific endpoints (morphological, physiological) robustly and via diverse means when perturbed (W. James definition).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper itself is a review and does not present primary validation data. However, it *reviews and cites* experimental work that validates the emergent behaviors discussed. Validation methods implied by the cited studies include:
         *   **Observational Studies:** Imaging development, regeneration, cell migration over time (e.g., Figs 3, 4, 5, 7, 8).
         *   **Perturbation Experiments:** Ablating cells/tissues (Fig 7a), grafting tissues (Fig 1g, Fig 7b), altering signaling pathways chemically or genetically (Fig 4, 5e,f, 6), applying electric fields, changing environmental conditions, and observing the system's response (e.g., ability to regulate/regenerate, changes in behavior/phenotype).
         *   **Quantitative Analysis:** Measuring outcomes like morphology, number of structures (heads, eyes), gene expression patterns, cell migration paths, oscillation synchrony, phenotype penetrance (% converted animals, Fig 4b; head ratio, Fig 5f).
         *   **Computational Modeling:** Developing models that reproduce observed behaviors based on hypothesized local rules and validating model predictions against experimental data (e.g., melanocyte conversion model, Fig 4c,c', Refs 119, 120; planarian regeneration models, Ref 121).

---

#Key: [zhao_exploring_2024]

# Exploring Embodied Intelligence in Soft Robotics: A Review

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: This paper reviews the concept of embodied intelligence (EI) within the context of soft robotics. It explores how EI, which emphasizes the synergy of brain, body, and environment, applies to soft robots whose behaviors inherently depend on their physical forms, material properties, and interaction with the environment. The review covers research branches including embodied morphological computation, embodied artificial evolution, and perception, control, and decision-making in soft robotics. It summarizes research progress and discusses related scientific problems, aiming to provide a reference for future work. Key components discussed include soft robot bodies (morphology, materials like soft silicone, hydrogels, SMAs, liquid metals), sensors, actuators, control systems (including RL, CPGs), and the environment itself as an interactive component. The purpose is to synthesize the current understanding and advancements in EI for soft robotics.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters characterize the review's implementation, not a specific material system described within it.

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes (Implicitly, through discussion of learning and adaptation)

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The review mentions several contexts where local rules lead to global outcomes:
        *   **Evolutionary Algorithms (Sec 4.3):** Local rules are embodied in the genetic representation (encoding morphology and/or controller) and the selection criteria (fitness function based on task performance in an environment). The interaction is mutation/crossover operators acting locally on the genotype, and selection acting based on global performance resulting from the phenotype's interaction with the environment. Example: DERL framework [42], CPG evolution with morphology [44].
        *   **Morphological Computation (Reservoir Computing, Sec 4.2):** Local rules are the physical dynamics governing the interactions within the soft body (the reservoir), e.g., mass-spring dynamics [35] or fluid dynamics in pneumatic systems [37]. The input signal perturbs these local dynamics.
        *   **Multi-Robot Organisms (Sec 4.3):** Inspired by Dictyostelium, local rules involve emitting/sensing signaling molecules (e.g., cAMP) leading to chemotaxis and aggregation [46, 47]. Robots dock based on local attraction/connection rules.
        *   **Reinforcement Learning (Sec 4.4.2):** Local rules may exist within the controller (e.g., neural network activations), but the primary "interaction" is the agent-environment loop (state -> action -> reward -> state update), which involves global state observation (often) and reward signals.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The review describes several types of emergent global order:
        *   **Evolved Morphologies/Gaits (Sec 4.3):** Complex robot shapes and coordinated movement patterns (e.g., trotting, galloping, rolling gaits, Fig 6) emerge from evolutionary processes optimizing for task performance (e.g., locomotion).
        *   **Aggregated Structures (Sec 4.3):** Multi-robot organism formation (inspired by slime mold fruiting bodies, Fig 7).
        *   **Learned Control Policies (Sec 4.4.2):** Complex, adaptive behaviors (e.g., grasping [52], swimming [54], manipulation [51, 53]) emerge from RL training.
        *   **Computational States (Sec 4.2):** Higher-dimensional dynamic representations suitable for processing time-series data emerge within the physical reservoir.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Reservoir Computing, Analog (implicitly)

### **5.3 Computational Primitive:**

    *   Content: Nonlinear transformation/mapping (inherent in Reservoir Computing). Reservoir computing's function is to transform input data into higher-dimensional dynamic representations, enhancing nonlinear characteristics (Sec 4.2, para 2). This is the core primitive enabling subsequent linear readout for tasks. Other examples imply primitives like Signal Integration (body dynamics for perception [38]) or potentially basic filtering/feature extraction through physical interaction.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
    *   **Note:** The review discusses processes occurring over vastly different timescales, from fast control loops to slow evolutionary adaptation. Specific values are generally not provided.

### **6.2 Active Inference:**

    *   Content: Unclear

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes
        *   **Evolutionary Adaptation (Sec 4.3):** Robot morphology and controllers adapt over generations to improve fitness in specific environments [41, 42, 43, 44]. The Baldwin effect explicitly links learned adaptations to evolutionary change [42].
        *   **Learning-Based Adaptation (Sec 4.4.2, 4.4.3):** Reinforcement learning enables robots to adapt their control strategies through interaction with the environment to achieve tasks like grasping [52], locomotion [54], manipulation [51, 53], or human-robot collaboration [62].
        *   **Morphological Adaptation (Implicit in Sec 4.2, 4.3):** The physical form itself adapts (through evolution or potentially dynamic morphing [31, 32]) to better suit tasks or environments. Self-repair capabilities [21, 31, 32] also represent a form of structural adaptation.

### **7.2 Adaptation Mechanism:**

    *   Content: The primary mechanisms discussed are:
        *   **Evolutionary Algorithms (Sec 4.3):** Adaptation occurs through selection based on fitness, combined with variation operators (mutation, crossover) acting on genetic representations of morphology and/or controllers. Examples include Genetic Reinforcement Learning (GRL) [41] and DERL combining evolution with DRL [42]. Central Pattern Generators (CPGs) controlled by evolutionary algorithms are also used [44].
        *   **Reinforcement Learning (Sec 4.4.2, 4.4.3):** Agents learn optimal policies (mapping states to actions) by trial-and-error interaction with the environment, guided by reward signals. Algorithms mentioned or implied include Deep RL (DRL) [42, 51, 61], Trust Region Policy Optimization (TRPO) [51], and model-free methods like ELFNet (based on Q-networks) [53]. Fusion with simulation (e.g., FEM) is also used [52]. Adaptation occurs via updating policy/value function parameters (e.g., neural network weights).
        *   **Morphological Computation (Sec 4.2):** Adaptation is less about changing structure and more about leveraging existing physical dynamics for computation/control. However, the *readout* layer in Reservoir Computing is trained (adapted).
        *   **Self-Assembly/Self-Repair (Sec 4.1, 4.3):** Mechanisms involve dynamic aggregation based on local signals [46, 47] or intrinsic material properties enabling dynamic morphing/repair [21, 31, 32].

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The review describes a wide range of functional behaviors resulting from embodied intelligence in soft robotics:
        *   **Locomotion:** Swimming (fish-like [26, 54], jellyfish-like [28], turtle-like [25]), trotting, galloping, rolling (evolved creatures [44]), general movement in complex environments.
        *   **Manipulation:** Grasping [52], object manipulation [42, 51, 53], agile maneuvers [61].
        *   **Perception:** Sensing environmental stimuli (non-contact, contact, shape, material, roughness) [48, 49], object localization using body dynamics [38].
        *   **Computation/Information Processing:** Nonlinear mapping (Reservoir Computing) [34, 36].
        *   **Adaptation/Learning:** Acquiring new skills/gaits through evolution [42, 44] or RL [51, 52, 53, 54].
        *   **Collective Behavior:** Aggregation, self-assembly (Multi-robot organisms) [46, 47].
        *   **Self-Maintenance:** Self-repair [21, 31, 32].
        *   **Navigation/Exploration:** Autonomous mapping and exploration [60], reaching targets [54].
        *   **Human-Robot Interaction:** Air-gesture teaching [48], adaptive collaborative assembly [62].

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The review indicates validation primarily through:
         *   **Simulation:** Many studies involving evolution [42, 44] and RL [52, 55-59] rely heavily on simulation environments (e.g., SofaGym, Elastica, MuJoCo implied in [42]). Fig 6 explicitly shows simulated evolved robots.
         *   **Physical Experiments:** Some studies demonstrate behaviors in real robots, e.g., multimodal sensing [48, 49], swimming robot target reaching [54] (Fig 8 indicates experimental setup), soft arm control [51], octopus-inspired arm interaction [29]. Fig 4 shows various physical bio-inspired robots [24, 25, 26]. Morphological computation is validated via physical systems like pneumatic arms [37] or octopus arms [36].
         *   **Performance Metrics:** Validation often involves measuring task success rates (e.g., grasping success, target reaching accuracy [54], object identification accuracy [49]), efficiency, or comparing learned/evolved behaviors to baselines.
         *   **Limitations:** The review notes the challenge of the reality gap between simulation and physical experiments (implicit). It doesn't systematically analyze the rigor of validation across all studies.

---

#Key: [hu_selfreporting_2024]

# Self‐Reporting Multiple Microscopic Stresses Through Tunable Microcapsule Arrays

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of dye-filled microcapsules with precisely controlled breaking forces, synthesized using microfluidics. Three types of microcapsules, containing different fluorescent dyes (green, yellow, blue) and designed to rupture at distinct stress levels (3.2 MPa, 4.9 MPa, 8.1 MPa respectively), are assembled into chains within micro-traps on a PDMS template using sequential capillarity-assisted particle assembly (sCAPA). These arrays are embedded into a material (or used as a coating). When subjected to mechanical stress (e.g., via indentation), the microcapsules rupture sequentially based on the local stress magnitude, releasing their dye and providing a spatially resolved fluorescent readout of the stress distribution across multiple thresholds. The purpose is real-time, autonomous reporting and recording of localized multiple stress levels and micro-damage in materials.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Microcapsule Radius (r) | ~4.5 (Example) | μm | Section 2.1, Fig 1d, Fig S3 | Explicit | High | Measured from SEM/Optical Microscopy |
        | Microcapsule Shell Thickness (h) | 250-928 | nm | Section 2.1, Fig 1e,f | Explicit | High | Measured from SEM |
        | h²/r² ratio | 0.006, 0.020, 0.038 | Dimensionless | Section 2.2, Fig 2c | Explicit | High | Calculated from measured h and r |
        | Critical Breaking Force (F) | 50-600 | μN | Section 2.2, Fig 2c | Explicit | High | Measured via micro-indentation |
        | Effective Rupture Stress (σ) | 3.2, 4.9, 8.1 | MPa | Section 2.3, Fig 3d, Fig 4e | Explicit | High | Measured via indentation on arrays (F/S) |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is mechanical work done by an external force applied during indentation.
    *   Value: 4.9 (Example load); 0.16 - 8.10 (Calculated stress range)
    *   Units: N (Force); MPa (Stress)

### **2.2 Energy Transduction**

    *   Content: Mechanical energy (input work from indenter) is transduced into elastic strain energy within the microcapsule shell. When the local stress exceeds the critical breaking stress (determined by h²/r²), this stored elastic energy is released rapidly, causing shell fracture (mechanical energy dissipation). The fracture releases the encapsulated fluorescent dye (potential energy stored via encapsulation -> kinetic energy of dye release). The released dye molecules, when excited by an appropriate wavelength (e.g., 365 nm), emit light via fluorescence (light energy input -> electronic excitation -> light energy output).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The efficiency of converting input mechanical work into the desired output (fluorescent signal indicating stress level) is qualitatively very low. Most input energy is dissipated as heat during plastic deformation of the PDMS matrix and capsules, and fracture energy of the capsules. The energy required for fluorescence excitation is separate, and the quantum yield of fluorescence is relevant but not the primary efficiency concern here. No quantitative efficiency value is provided or derivable.

### **2.4 Energy Dissipation**

    *   Content: Energy is dissipated primarily through:
        1.  Elastic/Viscoelastic/Plastic deformation of the PDMS matrix (likely the largest component). (Qualitative: High)
        2.  Fracture energy required to break the microcapsule shells. (Qualitative: Low per capsule, but summed over many capsules).
        3.  Non-radiative decay pathways during fluorescence (heat). (Qualitative: Medium, depends on dye quantum yield).
        4.  Frictional losses during indentation. (Qualitative: Low/Medium).
        Quantification is not provided in the paper.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**

    *   Retention: Long-term (dye persists, Fig S12, S13).
    *   Capacity: Low, limited to the number of distinct capsule types/thresholds (3 levels reported). Each location can store one of 4 states (no rupture, level 1, level 2, level 3).
    *   Read-out accuracy: High (visual fluorescence imaging).
    *   Re-writability: None (rupture is irreversible).
    The score reflects the simple, irreversible, low-capacity nature of the memory. It's a passive recording mechanism.

### **3.3 Memory Retention Time:**

*   Value: Long-term
*    Units: Qualitative Descriptor

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 3 levels (or 4 states per location)
*   Units: Stress levels (or Discrete states)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: High (Qualitative)

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Low (Qualitative)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Skipping M4.2-M4.7 as M4.1 is "No")**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Skipping M5.2-5.4 as M5.1 is "No")**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Fluorescence Lifetime | ns - μs (Typical range) | ns-μs | Inferred | Inferred | Standard timescale for fluorescence processes; not specified for these dyes in the excerpt. |
        | Observation/Recording Time | Minutes - Hours (Assumed) | min-hr | Implicit | Inferred from experimental context (imaging after indentation). |
        | Assembly Speed (sCAPA) | 3-5 | μm/s | Section 4 | Explicit | Explicitly stated in Methods. |
    *   **Note:** Only timescales explicitly mentioned or directly inferable from the core mechanism are included.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Skipping M7.2 as M7.1 is "No")**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behavior is multi-level, spatially resolved mechanical stress reporting/mapping. The system transduces local mechanical stress above specific, discrete thresholds (3.2, 4.9, 8.1 MPa) into localized optical signals (green, yellow, blue fluorescence, respectively) via microcapsule rupture and dye release. This allows visualization of the peak stress distribution experienced by the material.

### **8.2 Behavior Robustness:**

        *   **Threshold Consistency:** Microcapsules show relatively low variability in breaking force within batches (Fig 2c error bars suggest reasonable consistency).
        *   **Signal Clarity:** Fluorescent signals appear distinct and localized (Fig 3d, 4e,f; S13, S15).
        *   **Material Stability:** Arrays show long-term stability (Fig S12).
        *   **Agreement with Simulation:** Experimental stress maps agree qualitatively with simulations (Fig 4f vs S7, S8).
        *   **Limitations:** Assembly yield is 80% (Fig 4d), meaning 20% of locations might lack full functionality, reducing spatial robustness. Performance near the rupture threshold might show variability. Robustness to environmental factors (temperature, chemical exposure) beyond the basic setup is not discussed.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary behavior (multi-level stress reporting) is validated through systematic indentation experiments at controlled loads/stresses (Section 2.3, 4; Fig 3, 4). Optical and fluorescence microscopy visualize the capsule rupture and dye release corresponding to applied stress levels. Single capsule indentation tests quantify the breaking force relationship (Fig 2). The spatial mapping capability is demonstrated via indentation patterns (Fig 4f). Reproducibility is suggested by the consistency within batches (Fig 2c) and yield data (Fig 4d). Comparison with simulation results (Fig S7, S8, Table S2) provides further validation of the stress mapping. Limitations include the 80% assembly yield and lack of quantification of variability very close to the stress thresholds.

---

#Key: [goh_noisy_2022]

# Noisy pursuit and pattern formation of self-steering active particles

**Paper Type:** Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of a target particle (whose trajectory is prescribed - stationary, linear, or circular) and a pursuing agent modeled as an "intelligent" Active Brownian Particle (iABP). The iABP is capable of self-propulsion at a constant speed (v0) and undergoes translational and rotational diffusion (characterized by DT and DR). Crucially, the iABP senses the instantaneous location of the target and adjusts its direction of motion via an adaptive torque proportional to the cross product of its orientation vector and the vector pointing to the target. The purpose is to study the dynamics of pursuit, including success criteria, emergent trajectories (like orbiting), mean first-passage times (MFPT), and potential for pattern formation (like sorting) based on the interplay between self-propulsion (Pe), maneuverability (Ω), noise (DT, DR), and target velocity/trajectory. The study uses analytical methods (Fokker-Planck equations under approximations) and numerical simulations (Langevin dynamics) in two dimensions.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Pe = v0 / (rH * DR), Ω = C0 / DR, α = u0 / v0, rH = sqrt(DT / DR). Values are varied in simulations/analysis. Units depend on the base units chosen for length (rH) and time (1/DR). Reliability is 'High' as these are defining parameters of the theoretical model being studied.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is internal to the iABP, driving its self-propulsion (active velocity v0). This is characteristic of active matter systems, often modeled as consuming fuel from the environment or an internal reservoir, although the specific underlying energy conversion mechanism is not detailed in this model. Noise (thermal or active) also represents energy input from the environment (bath).

### **2.2 Energy Transduction**

    *   Content: Energy is transduced internally (implicitly) to produce directed motion (self-propulsion v0). The sensing of the target's position and subsequent calculation of the adaptive torque (via Ω) involves information processing, which implicitly requires energy, but this is not modeled. The calculated torque performs work to reorient the particle. Energy is also exchanged with the environment via random forces/torques (noise terms).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Energy efficiency is not discussed or quantified in the paper. The model operates in the overdamped regime, typical for microscale active matter, where energy input is continuously balanced by dissipation, making traditional efficiency metrics difficult to define without specifying the exact energy source and task.

### **2.4 Energy Dissipation**

    *   Content: The system operates in the overdamped limit (Eq 1 uses velocity, not acceleration), implying significant energy dissipation primarily through viscous drag with the surrounding medium (implicit friction coefficients related to DT and DR). This dissipation balances the energy input from self-propulsion and noise. The paper does not quantify the dissipation rate. Assessment: High (inherent to overdamped regime).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are fully defined by the coupled Langevin equations (Eqs. 1-5) describing the iABP's motion and reorientation.
        *   **Translational Motion (Eq 1):** `r_dot = v0 * e + sqrt(2*DT) * eta_T`. Velocity is the sum of self-propulsion along orientation `e` and translational noise `eta_T`.
        *   **Rotational Motion (Eq 2, specified in Eq 5 for 2D):** `phi_dot = - Ω * sin(theta - phi) + sqrt(2*DR) * eta_phi`. The change in orientation angle `phi` depends on an adaptive torque term proportional to maneuverability `Ω` and the sine of the angle difference relative to the target (`theta - phi`, which relates to `beta`), plus rotational noise `eta_phi`. The torque acts to align `e` towards the target direction `r_c / |r_c|`.
        *   These rules are local because the pursuer's update depends only on its current state (r, e or φ) and the target's current state (r_T, used to calculate r_c and angles θ, β).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Eq 1    | Self-propulsion | v0 (or Pe)  | Varied (e.g., Pe=0.25-64) | speed (or dim.less) | Eq 1, Eq 6 | Explicit | Model parameter |
    | Eq 1    | Translational Noise | DT (or rH) | Implicit in units/scaling | length^2/time (or dim.less) | Eq 1, Section 2 | Explicit (Definition) | Model parameter |
    | Eq 5    | Steering Torque | C0 (or Ω) | Varied (e.g., Ω=4-128) | 1/time (or dim.less) | Eq 5, Eq 6 | Explicit | Model parameter |
    | Eq 5    | Rotational Noise | DR (or time unit) | Implicit in units/scaling | 1/time | Eq 5, Section 2 | Explicit (Definition)| Model parameter |

### **4.3 Global Order:**

    *   Content: The emergent global order depends on the scenario:
        *   **Stationary Target:** (Quasi-)periodic, rosette-like orbits around the target (noise-free limit, Fig 1b); Stationary probability distributions P(r), P(β) indicating preferred distances and orientations relative to the target (with noise, Section 3.1, Fig 2a).
        *   **Linear Target:** Pursuer trails target at some average distance <r> (Figs 6, 7).
        *   **Circular Target:** Pursuers form distinct circular trajectories ("groups" or "clouds") around the center of the target's path, segregated spatially based on their parameters (α, Ω, Pe), effectively achieving sorting (Section 5, Figs 8, 9). Phase shift δ0 relative to the target also occurs.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Explicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Eq 5    | Steering towards target | Ω         | Varied (e.g., 4-128) | Dimensionless | Explicit | Controls strength of alignment interaction | Eq 6 |
| Eq 1    | Self-propulsion speed | Pe        | Varied (e.g., 0.25-64) | Dimensionless | Explicit | Controls intrinsic particle activity | Eq 6 |
| Eq 1, 5 | Noise Intensity | DT, DR    | Implicit in units | Dimensionless (scaled) | Explicit (Definition) | Controls stochastic fluctuations | Section 2 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| <r>         | Mean Pursuer-Target Distance | ¯r        | Varied (depends on Pe, Ω, α) | Dimensionless | Explicit | Quantifies average proximity in stationary state | Simulation averaging / Analytical Eq 16, 19, 21, 31 | Figs 2c, 6c, 7a |
| <cos β>     | Mean Orientation Alignment | <cos β>   | Varied (depends on Pe, Ω) | Dimensionless | Explicit | Quantifies average alignment towards target | Simulation averaging / Analytical Eq 12, 20 | Fig 2b |
| <τ_fp>      | Mean First Passage Time | <τ_fp>    | Varied (depends on Pe, Ω) | Dimensionless | Explicit | Quantifies time to reach target vicinity | Simulation analysis | Fig 3b |
| ~r          | Mean Pursuer Radius (Circular Target) | ~r        | Pe/ωT or rT/α | Dimensionless | Explicit | Defines radius of pursuer trajectory for circular target | Analytical / Simulation averaging | Section 5, Fig 8, 9a |
| δ0          | Phase Shift (Circular Target) | δ0        | Depends on α, ζ=Ω/ωT | Radians | Explicit | Phase lag/lead relative to target | Analytical Eq 33 / Simulation analysis | Section 5, Fig 8 |


### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | Local Rules -> Global Order | Mapping from Langevin Dyncs (Pe, Ω, α, noise) to steady-state distributions (<r>, <cos β>, P(r), P(β)) and dynamic properties (MFPT, orbits, sorting radii). | High (See M4.4) | 7 | Agreement between analytical prediction (Eqs 16, 19-21, 31, 33) and simulation results (Figs 2, 3, 6, 7, 8, 9). Scaling behaviors observed. | Explicit | The paper successfully derives and validates macroscopic properties from microscopic rules. | Sections 3, 4, 5 |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 7 (Rubric: 0=No connection; 3=Qualitative link; 5=Quantitative link for some aspects; 7=Quantitative link for key average properties, good scaling demonstrated; 9=Predictive power across wide range, including fluctuations; 10=Complete statistical mechanics derivation). The paper shows strong quantitative links for average properties and scaling laws, but a full statistical mechanics treatment covering all fluctuation details might be missing.
    *   **Metrics:** Comparison of analytical formulae (mean values, scaling exponents) with simulation averages (e.g., <r>, <cos β>, <τ_fp>, ~r). Visual inspection of distribution functions and trajectories. Scaling collapse plots (e.g., Fig 3b).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", including M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operation is the calculation of a steering torque/angular velocity based on the relative position vector (r_c) or angle (θ - φ = β + π) to the target. Mathematically, it involves:
        1. Sensing/Calculating the relative vector r_c = r - r_T.
        2. Calculating the sine of the angle difference (sin(θ - φ)) or performing vector cross products (e x (r_c / |r_c| x e)).
        3. Scaling the result by the maneuverability parameter Ω (or C0).
        The primitive function is essentially: `Steering Output = f(Relative Position/Angle, Orientation, Maneuverability Parameter)`. It computes a directional adjustment vector/torque.
    *   **Sub-Type (if applicable):** Directional Control/Error Correction Signal Generation.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Rotational Relaxation Time | 1/DR | time | Implicit in units (Section 2) | Explicit (Definition) | Standard ABP timescale. |
        | Self-propulsion time over rH | rH/v0 = 1/(Pe*DR) | time | Implicit from Pe definition (Eq 6) | Explicit (Definition) | Time to travel characteristic length rH. |
        | Steering/Reorientation Time | 1/Ω = DR/C0 | time | Section 3.2 (τ_Ω) | Explicit | Characteristic time for active reorientation mechanism. |
        | Ballistic Motion Crossover Time | ~1/Pe^2 (scaled by 1/DR) | time | Section 3.2 (τ_0) | Explicit | Crossover from diffusive to ballistic regime (derived). |
        | Target Orbit Period (Circular) | 2π/ωT | time | Section 5 | Explicit | Timescale imposed by external target motion. |
    *   **Note:** Base time unit is 1/DR in dimensionless analysis (Section 2). Values are often discussed relative to each other or via dimensionless ratios like Ω/Pe^2 (Eq 23).

### **6.2 Active Inference:**

    *   Content: Unclear/Partial

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main behaviors observed are:
        1.  **Pursuit:** The iABP moves towards and attempts to reach the target. Success depends on parameters like α, Pe, Ω, and noise.
        2.  **Orbiting:** For a stationary target, the iABP can enter quasi-periodic orbits around it, especially in the low-noise limit (Fig 1b). Noise is required to break these orbits and approach the target.
        3.  **Trailing:** For a linearly moving target (α < α0), the pursuer follows the target at an average distance <r> (Fig 6, 7).
        4.  **Overshooting/U-turns:** For high Pe and low α, the pursuer may overshoot the target and perform U-turns (Fig 6a-II).
        5.  **Sorting/Pattern Formation:** For a circularly moving target, iABPs with different properties (α, Ω, Pe) spontaneously segregate into distinct circular trajectories at different radii (~r = rT/α), forming organized patterns (Fig 8, 9).

### **8.2 Behavior Robustness:**

        *   Pursuit success requires α < α0(Ω) (Fig 5), sensitive near the boundary.
        *   Stationary orbits are marginally stable and easily disrupted by noise (Section 3).
        *   Trailing behavior is stable for α < α0 (Section 4).
        *   Sorting via circular target motion appears robust, with analytical predictions matching simulations well even with noise (Figs 8, 9), although noise increases fluctuations (Fig 9b).
        The system shows qualitatively consistent behaviors under variations in noise and parameters Pe, Ω, α, indicating reasonable robustness, though quantitative bounds are behavior-specific.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors are validated through:
        1.  **Numerical Simulation:** Direct integration of Langevin equations (Eqs 1, 5) shows the emergence of trajectories like orbits (Fig 1b), trailing (Figs 6a, 7), and sorted rings (Fig 8, 9c, 9d).
        2.  **Analytical Modeling:** Fokker-Planck equations are used to derive stationary distributions (P(r), P(β), Eq 11, 15), mean values (<r>, <cos β>, Eq 16, 19, 20, 21, 31), and stability conditions (e.g., fixed points Eq 8, pursuit condition α<α0(Ω)), which predict the emergent statistical properties.
        3.  **Quantitative Analysis:** Simulation results (e.g., averaged distances, MFPT, distributions) are compared quantitatively with analytical predictions (Figs 2, 3, 6, 7, 9a). Scaling laws are identified and tested (Fig 3b).
        4.  **Visualization:** Trajectories and density maps provide visual confirmation of patterns (Figs 1b, 2a, 6a, 8, 9c).
        Limitations: Analytical results often rely on approximations (e.g., limits of Pe, Ω). Simulations are limited by finite time and system size.

---

#Key: [sartori_thermodynamics_2015]

# Thermodynamics of Error Correction

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is a general theoretical framework for template-assisted polymerization, modeling the process of copying information (e.g., DNA, RNA) one monomer at a time. Components include a template strand, a pool of free monomers (e.g., two types, right 'r' and wrong 'w'), and a molecular copying machine (e.g., polymerase) that facilitates monomer incorporation onto a growing copy strand. The machine can transition through a network of intermediate states (potentially complex, e.g., including kinetic proofreading) before finalizing incorporation. The purpose is to derive universal thermodynamic relationships between the copy error rate (η), the entropy production (ΔS_tot), the work dissipated (ΔW), and other physical quantities (energy changes ΔE, chemical driving μ_ij), independent of the specific molecular machinery details.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
    *   **Note:** Key parameters characterizing the system's theoretical framework are listed. Values are generally variable or parameters of the model, derived or assumed within the theory.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source driving the system out of equilibrium and enabling error correction or faster copying is chemical energy, represented by the chemical drivings (μ_ij) of transitions between states. This often corresponds to the hydrolysis of molecules like ATP or GTP in biological systems.
    *   Units: Energy (e.g., k_B T)

### **2.2 Energy Transduction**

    *   Content: Chemical energy (from μ_ij) is transduced into work (ΔW) performed by the copying machine during monomer incorporation. This work drives the system away from thermodynamic equilibrium (characterized by ΔF_eq). The energy is used to bias transitions, potentially moving through intermediate states, facilitating monomer selection and incorporation (correct or incorrect), and driving proofreading reactions (removing incorrect monomers). The net effect is the creation of the polymer chain (storing some energy ΔF_eq + information energy TD(η||η_eq)) and the dissipation of the remaining energy as heat (entropy production TΔS_tot).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not define or calculate a single dimensionless efficiency metric (like work output / energy input). Instead, it focuses on the thermodynamic *cost* of accuracy, relating the copy error (η) to the entropy production (ΔS_tot) and excess work (ΔW - ΔF_eq). Equation (4) η = η_eq * exp[-ΔS_w_tot + (ΔW_w - ΔF_eq)/T] shows how entropy production related to wrong insertions (ΔS_w_tot) reduces error, given a work budget (ΔW_w). Equation (5) η ≥ η_eq * exp[(-ΔW_p + ΔF_eq)/T] bounds the error reduction achievable via proofreading work (ΔW_p). Efficiency is implicitly discussed in terms of minimizing dissipation (ΔS_tot or ΔS_w_tot) for a given level of error reduction below η_eq. High dissipation implies low "efficiency" in using energy for accuracy.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation occurs primarily as entropy production (ΔS_tot) due to the irreversibility of the copying process when driven out of equilibrium (v > 0). The total entropy production per copied monomer is quantified by TΔS_tot = ΔW - ΔF_eq - TD(η||η_eq) ≥ 0 (Eq 2). This dissipation arises from the net flux through reaction cycles driven by chemical potential differences (μ_ij) and energy barriers. Specific contributions to dissipation come from incorporating both right and wrong monomers, and from any proofreading cycles. The entropy production associated specifically with wrong incorporations is ΔS_w_tot = (ΔW_w - ΔF_eq)/T - log(η/η_eq) ≥ 0 (Eq 3). High dissipation occurs particularly in effective proofreading schemes operating far from equilibrium (Sec II.C).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8.)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Inverse Reaction Rate (Forward, k_ij) | 1/k_ij | Time | Fig 3, Sec II.B | Explicit | Time constant for forward transition i->j. |
        | Inverse Reaction Rate (Backward, k_ji) | 1/k_ji | Time | Fig 3, Sec II.B | Explicit | Time constant for backward transition j->i. |
        | Inverse Bare Rate (ω_ij) | 1/ω_ij | Time | Fig 3 | Explicit | Characteristic timescale of reaction i->j neglecting energy barriers/driving. |
        | Polymer Elongation Time per Monomer | 1/v | Time/Monomer | Sec I, Eq (1) | Explicit | Average time taken to add one monomer to the chain. |
    *   **Note:** The fundamental timescales are related to the inverse kinetic rates governing transitions between states. The overall process timescale is related to the inverse elongation speed.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors are:
        1.  **Template-Assisted Polymerization:** The copying machine moves along the template, incorporating monomers to synthesize a copy strand at an average speed v.
        2.  **Error Incorporation:** Wrong monomers are incorporated with a certain probability, resulting in an overall error rate η.
        3.  **Error Correction (in specific regimes/models):** Processes like kinetic proofreading expend energy (e.g., chemical work ΔWp) to preferentially remove wrongly incorporated monomers, reducing the error rate η below its equilibrium value η_eq.
        4.  **Thermodynamic Operating Regimes:** Based on the relationship between work, entropy, and error, the system can operate in distinct regimes for wrong monomer incorporation: Error Amplification (η > η_eq, ΔWw - ΔF_eq > 0), Maxwell Demon (η < η_eq, ΔWw - ΔF_eq < 0), or Error Correction (η < η_eq, ΔWw - ΔF_eq > 0).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The "behaviors" described (polymerization, error rates, thermodynamic regimes) are outcomes of the theoretical model and its analysis. Validation consists of:
        1.  **Mathematical Derivation:** The core results (Eq 2, 3, 4, 5, 16) are derived mathematically from the master equation formulation and thermodynamic principles (Sec IV).
        2.  **Model Application:** The framework is applied to specific models (single-step incorporation, kinetic proofreading) to demonstrate its utility and show that these models can exhibit the predicted behaviors/regimes (Sec II.B, Fig 4; Sec II.C, Fig 5). Numerical simulations are used to generate plots (Fig 4, Fig 5) illustrating the relationships between error, work, and entropy production in these specific cases, consistent with the general theory.
        No experimental validation is presented within the excerpt.

---

#Key: [yang_memristor_2022]

# Memristor Circuits for Colloidal Robotics: Temporal Access to Memory, Sensing, and Actuation

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of arrays of two-terminal memristive elements integrated into micrometer-scale particles (colloidal robots). These arrays, potentially combined with fixed resistors or chemiresistors, are designed using additive fabrication methods (printing, coating, colloidal self-assembly). The system aims to provide microrobots with on-board capabilities for memory, sensing (chemical concentration via chemiresistors), timekeeping (sequential memristor switching), data logging (1D/2D arrays), and actuation (feedback-controlled response, e.g., drug delivery). The core components are memristors, interconnecting resistors (fixed or chemical-sensitive), and a voltage source. The purpose is to enable autonomous sense-think-act cycles in untethered microrobots operating in enclosed or remote environments. Specific validated designs perform tasks like tracking elapsed time, timestamping rare events, cataloging time-indexed data, and feedback-controlled drug release.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters listed are key values used in the simulations or given as representative examples defining the memristor behavior and circuit operation. Reliability is 'Medium' for simulation parameters as they represent chosen values for demonstration, not necessarily experimentally optimized values from this specific work, although based on literature. Example R_ON/R_OFF from Fig 1A caption are explicitly stated as representative experimental values from a cited source.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is an on-board voltage source (V0), which could be supplied by harvested energy (e.g., solar cell, photodiode) or stored energy (e.g., zinc-air battery), miniaturized for sub-100μm applications.
    *   Value: 1 (Typical simulation value)
    *   Units: V

### **2.2 Energy Transduction**

    *   Content: Electrical energy from the voltage source is transduced into current flowing through the memristor/resistor array. This electrical current drives the primary energy transformation: the electromigration of dopants within the memristors, changing their internal state variable (w_n(t)) and thus their resistance (memristance, M_n(t)). This represents a conversion of electrical energy into a change in the material's physical state (stored potential energy related to dopant distribution) and dissipated heat (Joule heating). In chemiresistors, chemical binding energy (analyte interaction) modulates electrical resistance, influencing current flow and subsequent memristor state changes. If an actuator is present (e.g., electroactive polymer), electrical energy reaching the terminus (V_th) is transduced into mechanical work (e.g., polymer swelling/contraction) or electrochemical potential changes to release cargo.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper highlights memristors as potentially "energy-efficient" (Sec 2) due to their non-volatile memory (sustaining information without power) but does not provide quantitative data or analysis regarding the energy efficiency of the switching process, timekeeping, sensing, or actuation within the proposed circuits. A qualitative assessment is difficult without knowing the energy required for switching versus the information stored or computation performed.

### **2.4 Energy Dissipation**

    *   Content: Energy is primarily dissipated as heat (Joule heating) due to current flowing through the resistive components: the memristors (in both ON and OFF states, though much higher current in ON state) and the fixed or chemiresistors. The resistance values (e.g., R_ON = 37.5 kΩ, R_OFF = 10 GΩ, α=100 implies bridging resistors are significant) indicate substantial potential for dissipation depending on the operating current and time. No quantitative analysis of dissipation is provided. Qualitative assessment: potentially Medium to High, especially when memristors are in the ON state or during switching events.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceed with M3.2-M3.8.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: N (discrete), Length/Resolution (continuous)
*   Units: Bits (discrete), Dimensionless/Length (continuous)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: >78% accuracy with 20% measurement error (robustness test)
*   Units: %

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Time Readout Robustness vs V0 Variability | % Correct Arrays | >60% (for 20% σ/μ) | % | `MemoryNode` attribute `robustness` | Fig 1I | Explicit | Measures fidelity of time encoding against voltage source variations. |
    | Time Readout Robustness vs R_ON Variability | % Correct Arrays | >60% (for 20% σ/μ) | % | `MemoryNode` attribute `robustness` | Fig 1I | Explicit | Measures fidelity of time encoding against intrinsic memristance variations. |
    | Time Readout Robustness vs R_R Variability | % Correct Arrays | >80% (for 20% σ/μ) | % | `MemoryNode` attribute `robustness` | Fig 1I | Explicit | Measures fidelity of time encoding against bridging resistor variations. |
    | Time Readout Robustness vs Measurement Error | % Correct Arrays | >56% (for 20% σ/μ) | % | `MemoryNode` attribute `robustness` | Fig 1I | Explicit | Measures fidelity of time encoding against readout errors. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid

### **5.3 Computational Primitive:**

    *   Content: Weighted Current Integration / State Update Rule (Mathematical Description: Eq 2: d(w_n(t))/dt = (μ/L²)*R_ON*I_n(t) defining state change based on current) combined with Resistance Modulation (Mathematical Description: Eq 1: M_n(t) = R_ON*w_n(t) + R_OFF*(1-w_n(t)) defining output based on state). This primitive enables higher-level functions like:
        *   Thresholding (Implicit in sequential switching - memristor effectively 'turns off' when resistance becomes high enough to redirect current; Explicit in actuation trigger V_th)
        *   Temporal Integration (Sequential switching accumulates time)
        *   Comparison (Implicit in timestamping - difference between timers; Implicit in feedback control - comparing sensed state to threshold [G]_th)

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Memristor Switching Time (per element/step) | ~20-30 (Fig 1G), variable (e.g., up to ~26s increment at 400s in Sup. Inf. S2.1) | s | Fig 1G, S2.1 | Implicit | Estimated from simulation results in Fig 1G; Sup. Info analysis provides more detail but is outside main text. |
        | Overall Time Logging Duration | 100s to >400s (simulated examples) | s | Fig 1G/H, Fig 2C, Fig 3F/G | Explicit | Simulation durations shown in figures. |
        | Sensor Event Duration (Example) | 220 | s | Fig 2C, Sec 4 | Explicit | Explicitly calculated from simulation results (Δt = 320s - 100s). |
        | Drug Reservoir Release Half-life (t_1/2) | 6, 12 (simulated) | h | Fig 4B/C, Sec 6 | Explicit | Parameter values used in pharmacokinetic simulations. |
        | Feedback Control Timescale (Glucose Regulation) | ~2 (to bring down BG), 24 (sustained control) | h | Fig 4B, Sec 6 | Explicit | Timescales observed in pharmacokinetic simulation results. |
    *   **Note:** Timescales are crucial for the system's function. Memristor switching time determines the resolution and range of the internal clock. Sensor response time limits event detection speed. Actuator kinetics determine feedback control effectiveness.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content:
        1.  **Timekeeping/Temporal Logging:** Sequential switching OFF of memristors in a 1D array encodes elapsed time (excursion time).
        2.  **Event Sensing & Timestamping:** Modulation of sequential switching speed by chemiresistors allows detection and recording of the onset time and/or duration of exposure to specific analytes.
        3.  **Multivariable Data Cataloging:** A 2D array architecture enables sequential activation of sensing branches, recording time-indexed data (e.g., analyte concentration vs. time).
        4.  **Feedback-Controlled Actuation:** Using the sensed state (e.g., glucose concentration encoded in memristor states) to trigger an electrical output (reaching V_th) that controls an actuator (e.g., releasing insulin from a reservoir).
        5.  **(Collective Behavior - Swarm):** A swarm of GRI microrobots using ergodic diffusion and local feedback control collectively regulates a spatially inhomogeneous glucose distribution.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The behaviors are validated computationally through circuit simulations based on established physical models (classical memristor model, Kirchhoff's laws, cable theory for continuous arrays) and pharmacokinetic modeling (PAMERAH for GRI). Specific validation methods include:
        *   **Timekeeping:** Simulating discrete (Fig 1G) and continuous (Fig 1H) arrays, showing sequential switching encodes time. Robustness validated via Monte Carlo simulations with parameter variations (Fig 1I).
        *   **Timestamping:** Simulating arrays with reversible/irreversible chemiresistors under varying analyte exposure, showing correct onset/duration encoding (Fig 2C).
        *   **Data Logging:** Simulating 2D arrays exposed to time-varying analyte concentrations (sinusoidal, realistic), showing accurate profile recording (Fig 3F, 3G).
        *   **Feedback Control:** Simulating GRI microrobot using circuit model output linked to pharmacokinetic model (PAMERAH), demonstrating effective BG regulation (Fig 4B) and parameter optimization (Fig 4C, D).
        *   **Collective Behavior:** Spatiotemporal simulation of a swarm (100 robots) showing regulation of inhomogeneous glucose distribution (Fig 4E).
        Limitations: Validation is purely computational/theoretical in this paper; experimental verification of these specific integrated circuit designs and their robustness in realistic microrobot contexts is not presented. Reproducibility is high within the simulation framework.

---

#Key: [langton_computation_1990]

```markdown
# Computation at the Edge of Chaos: Phase Transitions and Emergent Computation

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system studied is Cellular Automata (CA), specifically 1D and 2D lattice models. CAs are discrete space-time dynamical systems where each cell's state updates based on a local rule applied to its neighborhood. The paper explores how the dynamics of CAs, specifically their ability to support information transmission, storage, and modification (computation), vary across the space of possible CA rules. The key component is the transition function (rule) A, parametrized by the λ parameter, which biases the rule towards a quiescent state. The purpose is to investigate the conditions under which complex behavior and computation emerge, hypothesizing a connection to phase transitions ("edge of chaos"). The system is presented as a formal abstraction to understand potential principles governing computation emergence in physical systems.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters define the specific CA systems investigated. λ is the primary control parameter varied to explore the rule space. Data reliability is high as these are defining parameters of the computational model.

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable (Rule Dependent)
*    Units: Time steps (or Qualitative: Short-term to Arbitrarily Long)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Potentially High (K^L for L cells)
*   Units: States (or Bits if encoded)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: 100% (Local Rule Application)
*   Units: %

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: 0 (for stable/periodic); Variable/High (for chaotic/transient)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rule is the transition function A: S^N -> S, where S is the set of K cell states and N is the neighborhood size. For each of the K^N possible neighborhood configurations, A specifies the state of the central cell at the next time step. The rules are constructed randomly based on the λ parameter (Sec 2.1, 2.2): a transition maps to the quiescent state s_q with probability (1-λ), or to one of the other (K-1) states (chosen uniformly) with probability λ/(K-1). Additional constraints are strong quiescence and isotropy (Sec 2.3).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The emergent global orders correspond to Wolfram's four classes (Sec 2.6):
        *   Class I: Homogeneous fixed point (often the quiescent state).
        *   Class II: Simple, separated periodic structures (limit cycles).
        *   Class III: Chaotic, aperiodic patterns (strange attractors).
        *   Class IV: Complex patterns of localized, often propagating structures ('edge of chaos').
        The paper shows these emerge as λ is varied (Sec 3). Quantitative measures like entropy (Fig 6) and mutual information (Fig 11) also characterize the global order/disorder.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Entropy_Avg | Average single cell entropy | H | ~0 - log2(K) | bits | Explicit | Measures state uncertainty/disorder. | Shannon Formula Eq(2) | Sec 5.1.1, Fig 6-10 |
| MutualInfo_Avg | Average mutual information (temporal/spatial) | I(A;B) | ~0 - log2(K) | bits | Explicit | Measures correlation between cells. | Formula Eq(3) | Sec 5.1.2, Fig 11-14 |
| Transient_Length | Time steps to reach stable/typical behavior | T | 1 - ~Exponential(Size) | steps | Explicit | Measures time to settle. | Observation/Simulation | Sec 3, 4, Fig 3, 4 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Other (Rule-based, potentially Universal Computation)

### **5.3 Computational Primitive:**

    *   Content: The fundamental computational primitive is the application of the local transition function A: S^N -> S. This single operation, applied synchronously across all cells, performs one time step of the CA evolution. It takes the state of the N cells in the neighborhood as input and outputs the next state of the central cell. Complex computations emerge from the iteration of this local primitive.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | CA Time Step | 1 (by definition) | step | Sec 1.3 | Explicit | Fundamental unit of time progression. |
        | Transient Length | 1 to ~10^7+ (or ~exp(Size)) | steps | Sec 3, 4, Fig 3, 4 | Explicit | Time to reach attractor/typical behavior; diverges near transition. |
        | Period Length | 1 to ~10^4+ | steps | Sec 3 (Fig 1.45) | Explicit | Period of repeating structures (Class II/IV). |
        | Correlation Decay Time | Variable | steps | Sec 5.1.2, Fig 13 | Explicit | Time over which mutual information decreases; short for chaotic, longer near transition. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No (within the primary analyzed system)

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behaviors are the dynamical regimes corresponding to Wolfram's Classes:
        *   Class I: Evolution to a homogeneous fixed state.
        *   Class II: Evolution to simple periodic structures (limit cycles).
        *   Class III: Evolution to chaotic, aperiodic patterns.
        *   Class IV: Evolution to complex patterns with localized, propagating structures, exhibiting long transients and potential for computation.
    Specific behaviors within Class IV include the propagation, interaction, and annihilation of particle-like structures (gliders), as shown in Fig 1.45 and Fig 5. The overall behavior transitions from simple order through complexity to chaos as λ increases.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors (Wolfram classes) are validated through direct simulation and visualization of the CA evolution over time (Figs 1, 2, 5). Qualitative classification is based on observing the resulting space-time patterns. Quantitative validation uses statistical measures like entropy (Figs 6-10) and mutual information (Figs 11-14) averaged over many runs and cells/time steps, showing distinct characteristics for different λ ranges corresponding to the classes (e.g., low entropy for Class I/II, high for III, intermediate peak for MI near transition/Class IV). Transient length measurements also differentiate regimes (Figs 3, 4). Limitations include finite simulation time/size and reliance on specific parameter choices (K, N). Reproducibility seems high given the deterministic nature (for a fixed rule and initial condition).

---

#Key: [lin_intelligent_2021]

# An Intelligent Material with Chemical Pathway Networks

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system proposed is a type of "intelligent material," specifically "intelligent plasma," characterized by programmable Chemical Pathway Networks (CPNs) analogous to Artificial Neural Networks (ANNs). The system comprises plasma species whose interactions (chemical reactions) are governed by rate equations, forming a network. Supramolecular additives (like rotaxane valves or CNT-tweezers) are proposed as components for controlling species availability. The purpose is to create a material with embedded intelligence capable of autonomous decision-making, reacting to dynamic conditions, and accomplishing complex tasks (like automatic workflows or signal processing) based on its pre-programmed CPN topology, requiring only basic data input/output during operation without external control. The CPN acts as an analog computer where species concentrations are neuron values and rate coefficients are weights, enabling computation through chemical kinetics.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** The parameters listed are the core variables defining the CPN dynamics presented in the theory and examples. Specific values are not given as it's a general framework. Units are inferred based on context. Reliability is Medium as they are part of a defined theoretical model, not measured experimental data.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy sources are thermal energy, which determines reaction rates via temperature (influencing Arrhenius factor), and potentially electromagnetic (EM) waves, used for manipulating plasma and/or triggering supramolecular components (as in the signal processing example, Eq 11-15). Energy injections are mentioned as a way to program or input data.

### **2.2 Energy Transduction**

    *   Content: Thermal energy influences the kinetic energy of species, affecting collision rates and overcoming activation energy barriers, thus transducing thermal energy into chemical potential energy changes via reactions (governed by Arrhenius kinetics, Eq 4). In the signal processing example, EM wave energy is transduced into rotational kinetic energy of CNT-tweezers (Eq 11, 13), which overcomes the π-π bond energy (potential energy) to release guest molecules (Eq 15). Released molecules then participate in chemical reactions, altering the chemical state (chemical potential energy).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss or quantify the energy efficiency of the proposed chemical computations or processes. It focuses on the logical structure and possibility rather than thermodynamic efficiency. Qualitatively, chemical reactions in plasma can be complex and involve many non-productive pathways, suggesting potentially low efficiency for specific computational tasks, but this is not analyzed.

### **2.4 Energy Dissipation**

    *   Content: Dissipation mechanisms are inherent in plasma processes (e.g., inelastic collisions not leading to desired reactions, radiative losses from excited species, heat transfer to surroundings) and chemical reactions (exothermic reactions releasing heat). Frictional or viscous effects during supramolecular motion could also occur. However, these are not specifically identified, analyzed, or quantified in the excerpt. Qualitative Assessment: Likely High, characteristic of plasma and complex chemical systems.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*    Units: s (or Qualitative Descriptor: "Short-term" / Dynamic State)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog

### **5.3 Computational Primitive:**

    *   Content: The fundamental computational primitive is the chemical reaction rate calculation, effectively performing a weighted sum/product (based on reactant concentrations and rate coefficients) and integration over time, as described by the rate equations (Eq 1-3). The Arrhenius equation (Eq 4) implements a temperature-dependent, exponential activation function. The paper also conceptually proposes the implementation of logic gates ("if conditions," "while loops") through specific CPN topologies, demonstrated analytically for an "if" condition in the etching example (Eq 5-10 resulting in oscillatory behavior based on conditions). The signal processing example claims molecular-level discrete Fourier Transform capability via selective guest release based on EM frequency.
    *   **Sub-Type (if applicable):** Weighted Sum/Integration (Rate Equations), Activation Function (Arrhenius), Logic Gates (Conditional Reactions, e.g., "If"), Signal Transformation (Fourier Transform).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Reaction Timescale | Variable (1/k[n]) | s (Implied) | Theory (Eq 1-3) | Implicit | Determined by rate coefficients (k) and concentrations (n), not given specific values. |
        | Process Duration (e.g., etching cycle) | Variable (Oscillatory) | s (Implied) | Example (Fig 2, Eq 10) | Implicit | The etching example shows oscillatory behavior, implying characteristic cycle times, but values depend on specific parameters. |
        | EM Response Time (Signal Proc.) | Variable | s (Implied) | Example (Eq 11-15) | Implicit | Depends on EM frequency, CNT-tweezer properties, rotation dynamics. Not quantified. |
    *   **Note:** The system dynamics are governed by chemical reaction rates. Specific timescales are highly dependent on the chosen chemical system and conditions (temperature, pressure, species concentrations) which are not specified generally.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is based on feedback loops inherent in the designed Chemical Pathway Network (CPN). Changes in the concentrations of certain species (internal state variables, e.g., n<sub>P</sub> in the etching example) alter the rates of subsequent reactions (via Eq 1-3), leading to a modification of the system's overall behavior (e.g., adjusting C4F8 release rate, Eq 5b, 7). This is a form of dynamic feedback control embedded within the chemical kinetics, driven by the current chemical state reflecting recent 'experience' (e.g., amount of etching occurred). It is not described as learning that modifies the CPN structure (Φ, Γ matrices or fundamental k values) itself.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors described are: 1) Automatic Workflow Execution: exemplified by the cyclic plasma atomic layer etching process where the material autonomously switches between etching and protection phases based on internal chemical sensing (Fig 2). 2) Signal and Data Processing: exemplified by the molecular-level discrete Fourier transform of an incoming EM wave, where specific frequencies trigger the release of guest molecules from CNT-tweezers, altering plasma properties (Fig 3, 4). General capabilities for implementing "if" conditions and "while" loops are also mentioned.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claims of specific behaviors (automatic etching workflow, signal processing) are supported by theoretical analysis and derivation of governing equations based on chemical kinetics (Eq 5-10 for etching, Eq 11-20 for signal processing). Conceptual figures illustrate the proposed mechanisms (Fig 2, 3, 4). There is no experimental validation presented in the excerpt. The validation relies entirely on the soundness of the chemical kinetics model and the proposed CPN designs. Limitations include the assumptions made in the derivations (e.g., simplified CPNs, specific reaction mechanisms like Arrhenius, approximations in Eq 19) and the lack of experimental confirmation.

---

#Key: [foumthuim_solvent_2024]

# Solvent quality and nonbiological oligomer folding: revisiting conventional paradigms

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of a single meta-substituted poly-phenylacetylene (pPA) foldamer oligomer (12mer, 16mer, or 20mer) simulated using all-atom molecular dynamics (MD). The oligomer is dispersed in one of three explicit solvents: water (H2O), cyclohexane (cC6H12), or n-hexane (nC6H14). The purpose is to investigate the folding behavior (coil-to-helix transition), stability, and driving forces (electrostatics, van der Waals interactions, entropy-enthalpy compensation) of the pPA oligomer in solvents of different polarities and quality, comparing the findings to conventional polymer physics paradigms and analogous polypeptide systems (polyPHE). Additionally, a bead-spring polymer model in a Lennard-Jones fluid is simulated for comparison.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters define the core systems being simulated and compared. Simulation time represents the duration over which dynamics and properties are observed.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input is thermal energy, driven by maintaining the system at a constant temperature (typically 300 K) using a thermostat (Nos`e–Hoover or velocity rescaling). This maintains the kinetic energy of the atoms according to the target temperature.
    *   Value: Corresponds to kT at 300 K (approx. 4.14e-21 J or 2.49 kJ/mol).
    *   Units: K (temperature as control parameter) or J or kJ/mol (thermal energy scale).

### **2.2 Energy Transduction**

    *   Content: Energy constantly transduces between kinetic and potential forms within the MD simulation. Potential energy includes intramolecular bonded terms (bond stretching, angle bending, dihedral/improper torsions - Eq. 4) and intermolecular/non-bonded terms (Lennard-Jones/vdW and Coulomb/electrostatic interactions - Eq. 4). As the polymer folds or atoms move, potential energy minima are sought, converting potential energy to kinetic energy, which is then dissipated to the thermal bath by the thermostat to maintain constant temperature. Work is also done by/on the system via the barostat to maintain constant pressure (in NPT). The calculation of solvation free energy via thermodynamic integration (Section 2.5, Eq. 8) involves monitoring the change in the system's potential energy derivative (`<∂V(r,λ)/∂λ>_λ`) as interactions are switched on/off. Folding is driven by changes in the system's free energy, involving both enthalpic (potential energy changes, Fig 13, Fig 14b) and entropic contributions (Fig 14b).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not define a specific task for which efficiency is measured in the sense of work done per energy input. The study focuses on the thermodynamics and mechanisms of a spontaneous process (folding) driven by free energy minimization. While solvation free energies (ΔG), enthalpies (ΔH), and entropies (ΔS) are calculated (Fig. 14), these relate to thermodynamic favorability and stability, not efficiency in performing external work or computation. The simulation itself consumes computational energy, but this is not the efficiency of the *material system* being studied.

### **2.4 Energy Dissipation**

    *   Content: Energy is dissipated primarily through the coupling to the thermostat (Nos`e–Hoover or velocity rescaling) and barostat (Parrinello–Rahman or Berendsen) which act as interfaces to external heat and pressure baths, respectively. These algorithms remove or add kinetic energy (thermostat) or perform volume changes (barostat) to maintain the target temperature and pressure, mimicking energy exchange with a macroscopic environment. Intrinsic dissipation also occurs within the simulated atomic motions due to the inherent 'friction' represented by the non-bonded interactions (vdW, electrostatics) during atomic collisions and movements, converting directed motion into randomized thermal motion. Quantification is not provided, but dissipation is inherent to maintaining equilibrium/steady-state in canonical (NVT) or isothermal-isobaric (NPT) ensembles. Qualitative assessment: Dissipation is essential and continuous throughout the simulation to maintain target T/P.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: >= 80 ns (in water), Variable/Shorter (in cC6H12, nC6H14)
*    Units: ns

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Low (potentially 2-3 states)
*   Units: distinct states

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Low (in water), High (in cC6H12/nC6H14)
    *   Units: Qualitative assessment

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |


### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Stability | Persistence of folded state in RMSD | Plateau (Water), Fluctuating (Org. Solvents) | Qualitative | `MemoryNode` attribute | Fig. 7 | Explicit | RMSD plots explicitly show stability/instability. |
    | FEL Depth | Free energy barrier separating states | Deep (Water), Shallow/Multiple (Org. Solvents) | kJ/mol (relative) | `MemoryNode` attribute | Fig. 11 | Explicit | FEL plots explicitly show minima depth/separation. |


---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are defined by the terms in the potential energy function (Eq. 4) used in the MD simulation (amber99sb-ildn force field with Gaff2 parameters). These include:
        1.  Bond stretching: Harmonic potential between covalently bonded atoms.
        2.  Angle bending: Harmonic potential based on angles between three bonded atoms.
        3.  Dihedral torsion: Periodic potential based on angles between four bonded atoms.
        4.  Improper torsion: Harmonic potential to maintain planarity/chirality.
        5.  Non-bonded Lennard-Jones (vdW): Pairwise interaction describing short-range repulsion and long-range attraction (`~ A/r^12 - B/r^6`). Parameters `ε_ij` (well depth) and `σ_ij` (zero-potential distance) define the strength and range. Lorentz–Berthelot combining rules used.
        6.  Non-bonded Coulomb (Electrostatic): Pairwise interaction based on partial charges (`~ q_i*q_j / (ε_r * r_ij)`). Partial charges `q_i` derived from AM1-BCC. Long-range interactions handled by PME.
        Solvent-solvent and solvent-polymer interactions follow the same functions (TIP3P for water, united-atom/Gaff2 for organic solvents).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | LJ      | Lennard-Jones inter. | ε_ij (well depth) | Varies (atom type pair dependent) | kJ/mol | Force Field Files (Implicit) | Implicit | Specific LJ parameters for all atom pairs are defined in the force field files (amber99sb-ildn/Gaff2/TIP3P) but not listed individually in the paper. |
    | LJ      | Lennard-Jones inter. | σ_ij (size param.) | Varies (atom type pair dependent) | nm | Force Field Files (Implicit) | Implicit | As above. |
    | Coulomb | Electrostatic inter. | q_i (partial charge) | Varies (atom dependent) | e (elementary charge) | AM1-BCC Calc. (Explicit Method, Implicit Values) | Mixed | Method (AM1-BCC) mentioned (Sec 2.2), specific values are in topology files (implicit). |
    | Bond    | Bond stretch        | k_r (force const.) | Varies (bond type dependent) | kJ/mol/nm² | Force Field Files (Implicit) | Implicit | Force constants are part of the force field definition (implicit). |
    | Angle   | Angle bend          | k_θ (force const.) | Varies (angle type dependent)| kJ/mol/rad² | Force Field Files (Implicit) | Implicit | As above. |
    | Dihedral| Dihedral torsion    | k_φ (barrier height)| Varies (dihedral type dependent)| kJ/mol | Force Field Files (Implicit) | Implicit | As above. |

### **4.3 Global Order:**

    *   Content: The primary emergent global order is a collapsed, helical conformation of the pPA oligomer. The specific geometrical features described (based on prior work by Sen, ref 10, mentioned in Sec 3.3) include a pitch of ~5.5 residues/turn, rise of ~0.69 Å/residue, inner pore diameter ~10 Å, outer diameter ~19 Å. The stability and perfection of this helical order depend strongly on the solvent (stable in water, marginally stable/unstable in n-hexane/cyclohexane) and require sufficient oligomer length (n>=6-8, based on cited literature, though tested lengths here are longer).
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| LJ      | Lennard-Jones inter. | ε_ij (well depth) | Varies | kJ/mol | Implicit | Part of standard force field, not listed per interaction. | Sec 2.2, Eq. 4 |
| LJ      | Lennard-Jones inter. | σ_ij (size param.) | Varies | nm | Implicit | Part of standard force field, not listed per interaction. | Sec 2.2, Eq. 4 |
| Coulomb | Electrostatic inter. | q_i (partial charge) | Varies | e | Mixed | Method explicit (AM1-BCC), values implicit. | Sec 2.2, Eq. 4 |
| Temperature | Thermal fluctuations | T | 300 | K | Explicit | Controls kinetic energy, influences probability of overcoming energy barriers. | Sec 2.3 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Conformation | Overall polymer shape | Radius of Gyration (Rg) | ~0.7 - ~1.3 (dep. on n, solvent) | nm | Explicit | Explicitly calculated and plotted. | Standard MD analysis | Fig. 9 |
| Conformation | Deviation from initial state | RMSD | ~0.5 - ~2.5 (dep. on n, solvent) | nm | Explicit | Explicitly calculated and plotted. | Standard MD analysis | Fig. 7 |
| Structure | Helical characteristics | Dihedral angle (μ) distribution | Peak ~ -10° (Water), Broader (Org. Solvents) | Degrees | Explicit | Explicitly calculated and plotted. | Standard MD analysis | Fig. 8d |
| Structure | Local packing | Pseudo-bond length (b) distribution | Peak ~0.685 nm (Water), Slightly larger (Org. Solvents) | nm | Explicit | Explicitly calculated and plotted. | Standard MD analysis | Fig. 8b |
| Energetics | Stability | Free Energy Landscape (FEL) minimum depth | Deep (Water), Shallow (Org. Solvents) | kT or kJ/mol (relative) | Explicit | Explicitly calculated and plotted. | PMF analysis from Rg/RMSD | Fig. 11 |
| Interface | Solvent Exposure | SASA | Lower (Water), Higher (Org. Solvents) | nm² | Explicit | Explicitly calculated and plotted. | Eisenhaber algorithm | Fig. 10d |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | MD Time Step (dt) | 0.001 - 0.002 (pPA), 0.005 (Bead-Spring) | ps (pPA), LJ time units (Bead-Spring) | Sec 2.3, 2.1, 2.5 | Explicit | Explicitly stated simulation parameter. |
        | Folding Transient Time | ~20 | ns | Fig. 7 | Explicit | Time taken for RMSD to initially plateau in water. |
        | Simulation Length | 100 | ns | Sec 2.3, Table 1 | Explicit | Duration of production runs. |
        | Structural Fluctuation Time | ps - ns | ps-ns | Implicit | Implied by atomic vibrations, bond rotations, and RMSD fluctuations around equilibrium. | Standard MD |
        | Memory Retention (Fold Stability in H2O) | >= 80 | ns | Fig. 7 | Mixed | Duration of stable RMSD plateau observed (explicit), interpretation as memory retention (implicit). |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behavior observed is the conformational change of the pPA oligomer from an initial extended coil state to a collapsed state. Under certain conditions (sufficient chain length, specific solvents like water), this collapsed state takes the form of a relatively stable helix (Coil-to-Helix transition). In other solvents (cyclohexane, n-hexane), the collapse occurs, but the resulting structure is unstable or marginally stable, leading to fluctuations and partial unfolding/refolding. The behavior is essentially structural self-organization driven by thermodynamics.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The emergence of the collapsed/helical state is validated through multiple quantitative analyses of the MD trajectories:
        1.  **RMSD vs. Time (Fig. 7):** Shows the deviation from the initial extended state, indicating collapse when it reaches a plateau significantly lower than a fully extended chain would have. Stability of the plateau indicates persistence of the emergent structure.
        2.  **Radius of Gyration (Rg) vs. Time/Length (Fig. 9):** Quantifies the compactness of the polymer. The scaling of Rg with oligomer length n (Rg ~ n^ν) is used to infer the conformational state (ν≈1/3 for collapsed globule, explicit in Fig. 9 right panel for water).
        3.  **Solvent Accessible Surface Area (SASA) (Fig. 10d):** Lower SASA values in water compared to organic solvents indicate a more compact, collapsed structure, consistent with the emergent folded state.
        4.  **Structural Parameter Distributions (Fig. 8):** Distributions of pseudo-bond length, bending angle, and especially dihedral angles provide evidence for specific local ordering (e.g., dihedral peak at -10° in water indicative of helical p-p stacking).
        5.  **Free Energy Landscapes (FEL) (Fig. 11):** Calculated from Rg and RMSD, these show minima corresponding to the probable conformational states. The location and depth of minima validate the existence and stability of the emergent folded state.
        6.  **Visual Inspection (Snapshots in Fig. 7, ESI):** Provides qualitative visual confirmation of the helical structure.
        Reproducibility is addressed by performing five independent runs for each system (mentioned in Table 1 caption, Sec 2.3). Limitations include the finite simulation time and potential force field inaccuracies.

---

#Key: [wegst_bioinspired_2015]

# Bioinspired structural materials

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews natural structural materials (e.g., bone, wood, shells, nacre, teeth, bamboo) focusing on their hierarchical architectures, constituent phases (hard/soft), and resulting mechanical properties, particularly the combination of strength and toughness. It discusses common design motifs (hierarchy, interfaces, porosity, gradients, self-repair) and the mechanisms (intrinsic/extrinsic toughening, crack deflection/bridging) underlying their performance. The purpose is to understand these natural designs to inspire the development of synthetic materials with improved mechanical properties, while also discussing the significant challenges in mimicking nature's complexity and fabrication processes (e.g., biomineralization, freeze casting, additive manufacturing).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                   | Value                 | Units          | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------------------------- | :--------------------: | :------------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These are representative parameters explicitly mentioned for specific key examples (nacre, bone) discussed in the review. A comprehensive list covering all materials is not feasible from a review article. Data Reliability is High as these are commonly cited values in the field, presented as factual in the review.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: For natural materials: Metabolic energy driving biological processes (e.g., biomineralization, self-assembly, remodeling) at ambient temperatures. For synthetic mimics: Thermal energy (sintering, hot pressing), electrical energy (electrophoretic deposition, additive manufacturing processes), chemical potential energy (solution methods, mineralization), mechanical energy (extrusion, roll compaction). Freeze casting uses thermal energy for directional freezing and sublimation.

### **2.2 Energy Transduction**

    *   Content: Natural: Chemical energy (ATP) transduced into material deposition, structural organization via cellular processes, protein synthesis, mineralization. Mechanical energy from loads transduced into elastic deformation, plastic deformation (fibrillar sliding, molecular uncoiling), crack formation/propagation, and heat (dissipation). Synthetic: Input energy transduced into phase changes (freezing/melting/sublimation), material transport (deposition, extrusion), chemical reactions (sintering, polymerization), densification, and structural ordering (template effects, self-assembly). Mechanical testing transduces applied mechanical energy into material deformation and fracture.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Natural processes are described as occurring at ambient temperature, implying high thermodynamic efficiency for material formation compared to typical high-temperature synthetic routes ("built at ambient temperatures...little energy requirements"). Synthetic processes like freeze casting are noted as "relatively inexpensive", implying reasonable efficiency, but quantitative values are absent. Mechanical energy efficiency (e.g., toughness) is a central theme, but presented as material property values (e.g., J/m², MPa m^(1/2)) rather than a dimensionless efficiency score in terms of energy conversion. Qualitative Assessment: Natural (Formation: High), Synthetic (Formation: Variable, depends on process), Mechanical Response (Toughness metrics provided).

### **2.4 Energy Dissipation**

    *   Content: Primary focus is on energy dissipation during mechanical loading, particularly fracture. Mechanisms explicitly discussed include: Plastic deformation (intrinsic toughening, fibrillar sliding in bone, viscoplasticity in nacre's organic layer), crack deflection and twisting, crack bridging (uncracked ligaments in bone, mineral bridges in nacre), microcracking (bone, dentin), mineral brick pull-out and frictional sliding (nacre), molecular uncoiling/sacrificial bonds (bone), viscoplastic energy dissipation in organic layers (nacre). Quantification is typically via fracture toughness (K, J, G) or work of fracture (area under load-displacement). Dissipation during material formation (heat loss during processing, friction) is not detailed. Qualitative Assessment: Mechanical dissipation mechanisms are High contributors to toughness in discussed materials.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: Natural: Molecular interactions (protein folding, crosslinking, protein-mineral binding), cellular secretion and signaling, templating effects (organic matrix guiding mineral growth), crystal growth kinetics, physical constraints (e.g., space-filling). Examples: Collagen interacts with hydroxyapatite, specific proteins control aragonite growth in nacre, cells secrete matrix components. Synthetic: Particle interactions (colloidal forces), phase change kinetics (ice crystal growth dynamics in freeze casting influenced by temperature gradients, additives, solute rejection), capillary forces, interfacial tension, chemical reactions (polymerization, sintering).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**

    *   Content: Hierarchical structures spanning multiple length scales (nano, micro, meso, macro). Specific examples: Lamellar structure (osteons in bone), brick-and-mortar (nacre), fibrous composites (wood, bamboo), porous networks (cancellous bone, foams), graded structures (bamboo density gradient, enamel/dentin), helicoidal structures (stomatopod club).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description                         | Value      | Units                     | Source          | Implicit/Explicit | Justification                                       |
        | :-------------------------------------------- | :---------: | :------------------------: | :-------------: | :----------------: | :-------------------------------------------------- |
    *   **Note:** The review discusses processes over various timescales but rarely quantifies them. Values are qualitative estimations based on the context provided or general knowledge of the processes.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: Natural: Genetic mutation and natural selection (Evolutionary). Cellular activity (osteoblasts/osteoclasts) responding to mechanical stimuli and biological signals (Bone Remodeling). Cellular processes triggered by damage signals to deposit new material or repair structure (Self-Repair). Synthetic: Embedded microcapsules releasing healing agents, vascular networks delivering agents, stimuli-triggered repair (e.g., temperature), described qualitatively in Box 2. The mechanisms are primarily biological or chemistry-based repair processes, not typically described using learning algorithm terminology.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior discussed is the achievement of superior mechanical properties, specifically combinations of stiffness, strength, and fracture toughness, that often surpass the properties of the constituent materials (rule-of-mixtures exceeded, Fig 1b). Key aspects include high specific strength/stiffness (Fig 1a), exceptional damage tolerance (resistance to crack initiation and growth - R-curve behavior), wear resistance (enamel), lightweight performance (porous structures), and fracture resistance despite brittleness of components (e.g., nacre, bone, teeth). Self-repair is another emergent functional behavior in some natural systems.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The review cites and describes validation methods commonly used in materials science: Mechanical testing (tensile tests, fracture mechanics tests like K<sub>c</sub>/J<sub>c</sub>/R-curve measurements - Box 1), microscopy (SEM, TEM to observe microstructure and fracture surfaces - Figs 2, 4, 5), indentation (hardness), computational modeling (mentioned briefly for understanding mechanisms). These methods validate the *mechanical properties* claimed to emerge from the structures. Validation limitations include difficulties in testing small/complex natural samples, challenges in replicating realistic loading conditions, and complexities in modeling multi-scale interactions. Reproducibility is high for standard material tests, but testing unique biomaterials can be challenging.

---

#Key: [de_nicola_graphene_2020]

# Graphene plasmonic fractal Metamaterials for Broadband photodetectors

**Paper Type:** Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a photodetector metadevice based on a gold/graphene (Au/G) Sierpinski carpet (SC) fractal metamaterial fabricated on a SiO2/Si substrate. The Au SC fractal consists of iteratively placed gold squares on a continuous single-layer graphene sheet. The device is designed for broadband (near-infrared to visible), highly efficient, polarization-insensitive, and gate-tunable photodetection at room temperature. Its components include the Au SC fractal, single-crystal single-layer graphene, SiO2 dielectric layer, Si substrate (acting as back gate), and Au/Ti electrodes for electrical contact. The purpose is to enhance light absorption in graphene via plasmonic effects for improved photodetection performance (responsivity, quantum efficiency, detectivity).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Listed parameters are key for defining the fabricated structure and its operational regime. Reliability is high as they are directly stated/measured.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input is incident light (photons) in the near-infrared to visible range (VIS-MIR, specifically tested from ~0.16 eV to 2.38 eV). A secondary energy input is electrical energy via gate voltage (Vg) for tuning and source-drain voltage (Vsd) for biasing the photodetector.
    *   Value: e.g., 1.94 (for specific tests)
    *   Units: eV (photon energy) / V (electrical bias)

### **2.2 Energy Transduction**

    *   Content: 1. Incident photons excite Localized Surface Plasmons (LSPs) in the Au SC fractal. 2. The enhanced near-fields of the LSPs couple strongly to the graphene layer. 3. Energy is absorbed in graphene, generating electron-hole pairs (photocarriers), potentially involving multiple hot carrier generation via carrier heating. 4. These photocarriers (primarily holes under typical bias) drift under the applied Vsd, creating a measurable photocurrent (Iph). 5. Electrical energy from Vg tunes the graphene Fermi level, modulating LSP resonance and graphene absorption. 6. Electrical energy from Vsd drives the current. Some energy is dissipated as heat (Joule heating, relaxation of hot carriers).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper reports high internal quantum efficiency (IQE) up to 100% at specific wavelengths (ω5 LSP resonance), indicating very efficient conversion of *absorbed* photons into charge carriers contributing to photocurrent. The external quantum efficiency (EQE) is lower (max ~15% in Fig 6c) because it accounts for reflection losses (device doesn't absorb all incident light, A < 1). The overall energy efficiency (light power in to electrical power out) is low, typical for photodetectors (not power generators), but the *quantum* conversion efficiency (IQE) is remarkably high. The score reflects the high IQE, tempered by the EQE and the fact that it's a detector, not a power generator. Metrics: IQE ≈ 100% (at ω5, Vg=-50V), EQE ≈ max 15% (Fig 6c).

### **2.4 Energy Dissipation**

    *   Content: Primary dissipation mechanisms include: 1. Non-radiative decay of LSPs (e.g., Landau damping in Au, absorption). 2. Relaxation of hot carriers in graphene via electron-electron scattering and electron-phonon scattering (heat generation). 3. Joule heating due to the flow of photocurrent (Iph) and dark current under bias (Vsd). 4. Radiative scattering of LSPs (contributes to reflection/transmission, not purely loss within the device). The paper mentions thermal effects influencing photocurrent (Fig 5a discussion) and discusses the bolometric effect, implicitly acknowledging thermal dissipation. Quantitative values for each mechanism are not provided. Qualitative Assessment: Likely Medium to High, given the metallic components, high carrier densities, and currents involved, especially under bias and illumination.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceed with M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: 27 (approx.)
*    Units: ms

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip to Module 6.)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Carrier Transit Time (τ_trans) | 20 (approx.) | ns | Results and Discussion | Explicit | Calculated based on channel length, mobility, bias. |
        | Carrier Trapping Time (τ_trap) | 27 (approx.) | ms | Results and Discussion | Explicit | Calculated based on experimental data (Δn, ħω, IQE, P). |
        | Estimated Bandwidth (Δf_-3dB) | 30 (approx.) | MHz | Results and Discussion | Explicit | Calculated as 3.5 / (2πτ_trans), representing lower bound. |
        | Chopper Frequency (for Resp. Meas.) | 173 | Hz | Methods | Explicit | Parameter of the experimental setup for responsivity measurement. |
        | Graphene Carrier Scattering Time (τ) | 0.1 (typical value used for context) | ps | Results and Discussion | Explicit | Mentioned in discussion to exclude plasma-wave effect (ωτ << 1). |
    *   **Note:** These timescales characterize the speed of different processes within the device, from fundamental scattering to carrier transport and trapping, and overall device response speed.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip to Module 8.)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behavior is broadband, gate-tunable, polarization-insensitive photodetection. This involves converting incident photons (VIS-MIR) into a measurable electrical current (photocurrent). Key aspects include: enhanced light absorption due to multimodal plasmon resonances in the Au/G SC fractal, generation of photocarriers in graphene (potentially involving hot carrier effects leading to gain), and tuning of responsivity and spectral features via gate voltage. Secondary observed behaviors include Surface-Enhanced Raman Scattering (SERS) enhancement due to the plasmonic fields.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary behavior (photodetection) is validated through extensive experimental measurements: spectral responsivity (Fig 6b), quantum efficiencies (IQE, EQE, Fig 6c), detectivity (D*, Fig 6d), current-voltage characteristics (Fig 4a), power dependence (Fig 5a), bias dependence (Fig 5b), and gate tunability (Fig 6a). Control experiments comparing patterned (Au/G SC) vs. unpatterned graphene devices demonstrate the enhancement effect (Fig 6a inset). SERS behavior is validated using Raman spectroscopy and mapping (Fig 3). The measurements appear quantitative and reproducible (error bars mentioned in Fig 4d context). Limitations include the lack of long-term stability tests or analysis under varying environmental conditions beyond vacuum. The term "emergent" is not used by the authors to describe the primary photodetection behavior, which arises from designed plasmonic interactions.

---

#Key: [robin_long-term_2023]

# Long-term memory and synapse-like dynamics in two-dimensional nanofluidic channels

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of two-dimensional (2D) nanofluidic channels embedded in a fluidic cell separating two reservoirs filled with an aqueous electrolyte (KCl, NaCl, LiCl, CaCl2, NiSO4, or AlCl3). Two types of channels were investigated: 1) "Pristine" channels made of MoS2 flakes separated by graphene nanoribbon spacers (height 0.68-86 nm). 2) "Activated" carbon channels made from graphite flakes with a nanoscaletrench milled via EBIE (height 4-13 nm), exhibiting higher surface charge. The channels are fabricated on SiNx membranes. The system's purpose is to investigate ion transport under confinement and demonstrate memory effects (memristance) potentially applicable to neuromorphic computing and bio-inspired iontronics. It functions by measuring ionic current under an applied time-dependent voltage using Ag/AgCl electrodes and a patch-clamp amplifier, revealing conductance hysteresis and memory phenomena. These phenomena are linked to interfacial processes like ionic self-assembly (Wien effect, Bjerrum pairs in pristine channels) or surface adsorption/entry effects (in activated carbon or due to geometric asymmetry). The system demonstrates synapse-like dynamics and Hebbian learning capabilities.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters are crucial for defining the experimental conditions under which memristive behavior was observed. Reliability is high as these are directly stated experimental parameters.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is electrical energy supplied by a patch-clamp amplifier (Keithley 2400 or 2600 Series) applying a time-dependent voltage V(t) across Ag/AgCl electrodes immersed in the electrolyte reservoirs connected by the nanofluidic channel.
    *   Value: Up to 1 (amplitude)
    *   Units: V (Volts)

### **2.2 Energy Transduction**

    *   Content: Electrical energy input (voltage difference) drives the transport of solvated ions (charge carriers) through the nanofluidic channel, constituting an ionic current I(t). This involves the conversion of electrical potential energy into the kinetic energy of ions and overcoming resistive forces within the channel. The memristive effect implies energy is also stored/dissipated through changes in the system's internal state related to ion configurations (e.g., ion pair formation/breaking in the Wien effect, ion adsorption/desorption, ion accumulation/depletion near channel entrances). Energy associated with these configurational changes affects the channel's conductance.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify energy efficiency. The primary purpose is information processing (memory, learning emulation), not energy conversion or storage in the traditional sense. Most input electrical energy is likely dissipated as heat due to ionic friction and overcoming channel resistance (Joule heating), and potentially during the internal state changes (ion pairing/unpairing, adsorption/desorption). Efficiency for the intended *computational* task might be considered differently, but energy efficiency in the thermodynamic sense is expected to be very low. Score justification: Low efficiency expected for energy transduction; primary function is computation/memory where efficiency metrics are different and not provided.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation primarily occurs through resistive losses (Joule heating) as ions move through the electrolyte and the confined channel. Friction between ions and solvent, and ions and channel walls contributes. Additional dissipation occurs during the processes underlying memory: energy is required to break ion pairs (Wien effect) or desorb ions from surfaces, and energy is released upon pairing or adsorption. The hysteresis loop itself represents energy dissipated per voltage cycle. The paper notes that capacitive effects are negligible at the frequencies used (0.1-200 mHz), suggesting minimal dissipation through dielectric losses in this regime. Quantification is not provided, but dissipation is inherent to ionic conduction and the observed hysteresis. Qualitative assessment: High overall dissipation relative to energy potentially stored in memory states.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding with M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: 50 - 400 (typical experimental range); Qualitative: Minutes to Hours
*    Units: s (seconds)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial/Unclear

**(Conditional: M4.1 is "Partial/Unclear", proceeding with M4.2-M4.7 with appropriate caveats)**

### **4.2 Local Interaction Rules:**

    *   Content: The local "rules" are the physical laws governing ion behavior under confinement and electric fields:
        1.  **Electrostatics:** Coulomb interactions between ions (leading to pairing/clustering like Bjerrum pairs, especially under strong confinement where dielectric constant might be lower), interactions between ions and charged channel walls (surface charge effects, ion exclusion/accumulation), ion interaction with the applied electric field (driving force).
        2.  **Ion-Surface Interactions:** Adsorption/desorption dynamics of ions onto channel walls (relevant for stop-and-go transport and bipolar memristors in activated carbon). Chemical affinity between specific ions and surface sites/defects influences this.
        3.  **Hydrodynamics:** Confined fluid flow and ion mobility, potentially affected by electroosmosis and slippage (mentioned for activated carbon).
        4.  **Diffusion:** Random thermal motion of ions.
        Equations from Ref 17 (cited for Wien effect/pairing) or standard electrokinetic models would provide mathematical descriptions, but are not detailed in this excerpt. Eq 1 (G ~ |V|^a) and Eq 2 (G(V>0) ~ b_Rect * G(V<0)) describe macroscopic consequences of these local interactions (non-linear conductance) rather than the rules themselves. Eq 3 (tm ~ tdiff * td/ta) models memory time based on adsorption dynamics.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**</h3>

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The primary "global order" described is the collective ion transport behavior resulting in a specific, history-dependent macroscopic conductance state G(t) of the entire channel. This manifests as the shape and area of the I-V or G-V hysteresis loops under periodic voltage forcing. It is a functional order (conductance state) rather than a spatial structural order.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Adsorption | Ion adsorption/desorption on walls | td/ta ratio | High (est. Du~10^2-10^3) | Dimensionless | Mixed | Mechanism explicit, ratio estimated via Dukhin number (Du), suggesting ta << td. | Eq 3, Text (Pg 4) |
| Rectification | Asymmetric entry/exit resistance | Rectification factor (b_Rect) | 1 - 5 | Dimensionless | Explicit | Mechanism proposed, factor range explicitly measured. | Eq 2, Text (Pg 3), SM Sec 3 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Conductance | Macroscopic channel conductance | G(t) | Varies (e.g., Fig 1C shows ~0.1-10 nS) | S (Siemens) | Explicit | Primary measured quantity defining the system state. | Current/Voltage meas. | Eq G=I/V, Fig 1 |
| Hysteresis | I-V loop characteristic | Loop Area | Varies with freq. (Fig 3C) | A⋅V | Explicit | Quantifies the memory effect and energy dissipation per cycle. | I-V curve integration | Fig 1, Fig 3 |
| Memory | Persistence of conductance state | Memory time (tm) | 50 - 400 | s | Explicit | Timescale over which the conductance state is retained. | Loop area vs freq. analysis | Fig 3C, Text (Pg 4) |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 1
    *   **Metrics:** Qualitative model agreement (Fig 3), Reproducibility implies consistent mapping.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", proceeding with M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Neuromorphic / Analog

### **5.3 Computational Primitive:**

    *   Content: The basic computational operation demonstrated is the **history-dependent modulation of conductance**, analogous to synaptic weight update in neuromorphic systems. Specifically, it implements a form of **Spike Timing-Dependent Plasticity (STDP)**. The change in conductance (ΔG) depends on the relative timing (Δt) of input voltage pulses (pre- and post-synaptic analogues). ΔG = f(Δt), where f is approximated by the curve in Fig 5D (potentiation for pre-before-post, depression for post-before-pre within a time window). This relies on the bipolar memristor behavior where positive/negative voltage pulses increase/decrease conductance, and the duration of these pulses is controlled by the spike timing protocol.
    *   **Sub-Type (if applicable):** STDP / Synaptic Weight Update

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Input Voltage Frequency | 0.1 - 200 | mHz | Para 4, Pg 1 | Explicit | Experimental parameter. |
        | Characteristic Memory Time (tm) | 50 - 400 | s | Text (Pg 4), Fig 3C | Explicit | Extracted from hysteresis loop analysis. |
        | Short-Term Memory Relaxation | ~120 (~2 min) | s | Fig 4B | Explicit | Visual estimate from graph. |
        | Long-Term Memory Retention | >120 (>2 min) to hours | s | Fig 4B, Text (Abstract, Pg 2) | Explicit | Lower bound from Fig 4B, upper range stated in text. |
        | Write Pulse Duration | 10 | s | Text (Pg 5), Fig 4 Caption | Explicit | Experimental parameter for Hebbian learning. |
        | Erase Pulse Duration | 10 | s | Text (Pg 5), Fig 4 Caption | Explicit | Experimental parameter for Hebbian learning. |
        | Read Pulse Duration | 5 | s | Text (Pg 5), Fig 4 Caption | Explicit | Experimental parameter for Hebbian learning. |
        | Diffusion Time (tdiff, theoretical) | ~1 | s | Estimated in Text (Pg 4) | Explicit | Theoretical estimate for comparison (0.1s corrected to ~1s based on units). |
        | Adsorption/Pairing Event (microscopic) | << tm (e.g., ms) | s | Implicit/Inferred | Theoretical basis for long memory time. | Text (Pg 4) | Implicit | Microscopic times inferred from theory discussion. |

### **6.2 Active Inference:**

    *   Content: Partial/Unclear

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", proceeding with M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism of adaptation is the voltage-induced change in the channel's internal state, which determines its conductance. For **bipolar memristors** (used for Hebbian learning), the mechanism is attributed to polarity-dependent processes, likely related to asymmetric entry/exit effects coupled with surface charge, leading to ion accumulation (conductance increase) or depletion (conductance decrease) depending on voltage polarity (Fig 2B). Positive voltage pulses ('write', pre-before-post potentiation) drive cations in a way that accumulates charge/increases conductance, while negative pulses ('erase', post-before-pre depression) reverse this. The persistence (memory) is linked to slow "stop-and-go" ion transport possibly mediated by surface adsorption/desorption (Eq 3, Fig 2D). For **unipolar memristors**, adaptation would involve changing the balance of paired vs. unpaired ions via the Wien effect (Fig 2A), driven by voltage magnitude, with persistence potentially linked to clustering dynamics (Fig 2C). The Hebbian learning protocol leverages the polarity dependence of bipolar devices to implement STDP.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors described are:
        1.  **Memristive Effect:** History-dependent conductance, manifesting as pinched hysteresis loops in I-V curves and conductance loops in G-V curves under periodic voltage driving (Fig 1 B-E). Exists in unipolar and bipolar forms.
        2.  **Synaptic Plasticity Analogue:** Ability to reversibly modify conductance using voltage pulses (write/erase), mimicking synaptic potentiation and depression (Fig 4 A, C). Includes both short-term and long-term memory components (Fig 4B).
        3.  **Hebbian Learning (STDP):** Implementation of spike timing-dependent plasticity, where the change in channel conductance depends on the relative timing of voltage pulses mimicking pre- and post-synaptic neuron firing (Fig 5).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content:
        *   **Memristance:** Validated experimentally by applying periodic voltage waveforms (sinusoidal, triangular) and measuring current. Plotting I vs V reveals pinched hysteresis loops, and G (=I/V) vs V shows characteristic loops (twisted for unipolar, simple for bipolar), which are the defining signatures ("hallmark") of memristors (Fig 1 B-E). Control experiments implied by stating capacitive effects are negligible at these frequencies. Reproducibility suggested by consistency across figures and robustness claims.
        *   **Synaptic Plasticity:** Validated by applying discrete voltage pulses (+1V write, -1V erase, +0.1V read) and measuring conductance changes over time. Demonstrated potentiation/increase with positive pulses, depression/decrease with negative pulses, short-term relaxation, and long-term retention (Fig 4A, B). Reversibility shown by returning to initial state after balanced write/erase cycles (Fig 4C).
        *   **Hebbian Learning (STDP):** Validated by implementing a specific protocol mimicking pre- and post-synaptic spike timing using voltage pulses applied to the channel. The resulting percentage change in conductance was measured as a function of the relative spike timing (Δt). The characteristic STDP curve (potentiation for Δt > 0, depression for Δt < 0 within a window) was experimentally obtained (Fig 5D).
        Limitations: Statistical validation (e.g., number of devices tested, variability) is not detailed in the excerpt (likely in SM). Long-term endurance/stability over many cycles not reported.

---

#Key: [obyrne_how_2022]

# How critical is brain criticality?

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is the brain, specifically its global neuronal dynamics. The paper reviews the "critical brain hypothesis," which posits that the healthy brain operates near a critical phase transition boundary between order and disorder. Components discussed include neurons, neuronal ensembles, and large-scale brain networks. The purpose of operating near criticality is hypothesized to confer advantageous information-processing capabilities, such as maximal sensitivity, enriched state repertoire, high information storage/transfer capacity, and an optimal balance of integration and segregation, ultimately supporting complex cognitive functions and adaptation to the environment. Two main types of criticality discussed are Avalanche Criticality (AC) and Edge of Chaos (EOC).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters listed are key theoretical concepts discussed. Specific values are often context-dependent or not precisely given in this review excerpt. Reliability reflects how consistently these parameters are defined vs. measured empirically in diverse systems mentioned.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Not explicitly discussed in terms of metabolic or physical energy (e.g., Joules). The "input" driving the system dynamics is framed as sensory input, internal physiological drives, or general neuronal activity/excitation. The level of input/drive is mentioned as relevant for distinguishing driven vs. equilibrium systems and affecting the precise operating point (subcriticality in driven systems).

### **2.2 Energy Transduction**

    *   Content: Not discussed in terms of physical energy conversion. The paper focuses on the propagation and transformation of *neuronal activity* (spikes, LFP, BOLD signals) through networks, governed by neuronal interactions (synaptic weights, E/I balance). This can be viewed as information transduction/transformation rather than energy transduction in a physical sense. Mechanisms include synaptic transmission, neuronal firing, and propagation of activity cascades (avalanches).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss the metabolic energy efficiency of brain operation in the context of criticality. While criticality is linked to optimal *information* processing, its relation to thermodynamic or metabolic efficiency is not addressed in the excerpt.

### **2.4 Energy Dissipation**

    *   Content: Not discussed. Mechanisms of physical energy dissipation (e.g., heat loss from metabolic processes) are not mentioned in the context of criticality within this excerpt. The concept of activity "dying out" in the subcritical phase relates to signal dissipation, not physical energy loss.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-range / Power-law decay
*    Units: Seconds to Minutes (Qualitative Descriptor based on LRTC context)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Maximized at criticality; Large state repertoire

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Power-law decay (slow)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local rules are the interactions between constituent elements (neurons or neuronal ensembles). Explicit examples derived from the Ising model analogy include:
        1.  **Ordering Interaction:** Tendency for elements (spins/neurons) to align their state (spin orientation/firing state) with neighbors, modeled as synaptic weights or excitatory interactions.
        2.  **Disordering Force:** Intrinsic tendency for elements to adopt random/idiosyncratic states, modeled as variance of neuronal excitabilities or temperature effects.
        3.  **Response to Global Parameter:** Influence of a control parameter (external magnetic field, E/I balance, neuromodulator concentration) on the overall state.
        4.  **Activity Propagation (Avalanches):** An activated node potentially activating neighbors based on coupling strength, leading to cascades. The average number of secondary activations (branching parameter) depends on the balance of interactions.
        5.  **Coupling Heterogeneity (EOC):** For Edge of Chaos, the rules involve both positive (excitatory) and negative (inhibitory) couplings between nodes, with the *distribution* of these couplings being key.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | 1 | Ordering Interaction | Synaptic Weight / Coupling Strength | Tuned for criticality | Arbitrary / Biological Units | Main Text | Explicit (concept) | Value is system-dependent, relates to achieving branching=1 |
    | 2 | Disordering Force | Excitability Variance / Temperature (analogy) | Tuned for criticality | Arbitrary / Biological Units | Main Text | Explicit (concept) | Value is system-dependent |
    | 4 | Activity Propagation | Branching Parameter (σ or m) | ~1 (critical), <1 (subcritical), >1 (supercritical) | Dimensionless | Fig 1, Box 1, Main Text | Explicit | Defines the dynamical regime |
    | 5 | Coupling Heterogeneity | Standard Deviation of Coupling Distribution | Tuned for EOC | Arbitrary / Biological Units | Main Text | Explicit (concept) | Value is system-dependent, determines stable/chaotic phase |

### **4.3 Global Order:**

    *   Content: The emergent global order is the **critical state** itself. This state is characterized by specific macroscopic properties:
        1.  **Scale-Invariance/Scale-Freeness:** Statistical properties (e.g., avalanche size/duration distributions, power spectra) are similar across different scales (power-law distributions).
        2.  **Long-Range Correlations:** Correlations in activity extend across large spatial distances and long time intervals (LRTCs).
        3.  **Specific Dynamical Regime:** Poised between ordered (e.g., synchronized, stable, activity dies out quickly) and disordered (e.g., noisy, chaotic, activity explodes) phases. Examples include the AC point (branching parameter ≈ 1) or the EOC point (boundary between stable and chaotic dynamics).
        4.  **Maximized Complexity/Variability:** The system exhibits a rich repertoire of states and high variability.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| AC-Balance | Balancing ordering (coupling) vs disordering forces | Control Parameter (e.g., E/I balance, temp) | Tuned to Critical Value (C*) | System-specific | Explicit (concept) | Explicit description in AC section | Main Text, Fig 1 |
| EOC-Balance | Tuning coupling heterogeneity (std dev) | Std Dev of Couplings | Tuned to Critical Value | System-specific | Explicit (concept) | Explicit description in EOC section | Main Text |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| AC-State | Avalanche Criticality | Branching Parameter (σ or m) | ~1 | Dimensionless | Explicit | Defines the critical point for AC | Box 1 | Main Text, Fig 1, Box 1 |
| AC-State | Avalanche Size/Duration Distribution | Power Law Exponent (e.g., τ) | Specific to universality class | Dimensionless | Explicit | Signature of scale-free avalanches | Box 1 | Main Text, Fig 1, Box 1 |
| AC/EOC-State | Temporal Correlations | LRTC Exponent (e.g., DFA exponent) | > 0.5 (persistent) | Dimensionless | Explicit | Signature of long memory | Box 1 | Main Text, Box 1, Box 3 |
| EOC-State | Edge of Chaos | Lyapunov Exponent / 0-1 Chaos Test | ~0 / Near transition value | 1/time / Dimensionless | Explicit | Distinguishes stable/chaotic regimes | Box 2 | Main Text, Box 2 |
| EOC-State | Edge of Chaos | Lempel-Ziv Complexity (LZC) | Maximized | Dimensionless | Explicit | Proposed signature correlating with EOC | Box 2 | Main Text, Box 2 |
| General | Power Spectrum | 1/f slope (β) | ~1 | Dimensionless | Explicit | Consistent with critical dynamics | Main Text | Main Text |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | Local Rules -> Global State | Mapping from neuron interactions (E/I balance, coupling) to macroscopic critical state (power laws, LRTCs, branching=1) | High (Theoretically, via universality) | 7 | Critical exponents, Branching parameter, LRTC exponent, Power law fit (R^2), κ, DCC | Mixed | Predictability based on universality; Score reflects theoretical strength vs empirical noise/deviations | Main Text |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 7. Rubric: Score reflects how well the global behavior (critical state characteristics) can be predicted solely from the local interaction rules, ignoring microscopic details. 0=No relation; 5=Qualitative relation; 7=Quantitative relation for key parameters (like branching ratio), robust due to universality but with deviations; 10=Perfect prediction of all global properties from local rules. Example: Tuning E/I balance (local rule parameter) predictably shifts the system towards/away from criticality (global state with branching ratio ~1 and power laws). Universality ensures this mapping holds across different microscopic implementations within the same class. Deviations occur empirically (near-criticality).
    *   **Metrics:** Branching parameter, Power law exponents (τ), Scaling relations between exponents, Shape collapse parameter, LRTC exponents (DFA), κ, DCC, Power spectrum slope (1/f).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Neuromorphic / Reservoir Computing / Analog / Hybrid

### **5.3 Computational Primitive:**

    *   Content: The paper suggests criticality enables several fundamental operations intrinsically:
        1.  **Signal Integration:** Neurons integrate inputs; long memory/LRTCs imply integration over extended periods.
        2.  **Amplification/Filtering:** Avalanches represent signal propagation; subcritical systems dampen signals (filtering small inputs), supercritical systems amplify excessively. Criticality provides balanced propagation, potentially maximizing dynamic range (sensitivity to a wide range of input intensities). Divergent susceptibility implies high sensitivity (amplification) to relevant perturbations.
        3.  **Information Storage:** See Memory section (M3); dynamical persistence of information.
        4.  **Information Transfer:** Maximized between system components due to long-range correlations.
        5.  **Nonlinear Transformation:** Explicitly mentioned for Edge of Chaos models ("rich repertoire of nonlinear transformations on inputs").

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Neuronal Firing | ms | ms | General Background | Implicit | Timescale of individual neuron action potentials |
        | Avalanche Duration | Power-law distributed (up to seconds) | Seconds (s) | Main Text, Fig 1C, Box 1 | Explicit | Characteristic of avalanche criticality |
        | LRTCs / Long Memory | Seconds to minutes or longer (Power-law decay) | Seconds (s) / Minutes (min) | Main Text, Box 1, Box 3 | Explicit | Defines the persistence of dynamical memory |
        | Critical Slowing Down | Long / Divergent at C* | Seconds (s) / Arbitrary | Main Text | Explicit | System response time slows near critical point |
        | Intrinsic Timescale | Long near criticality | Seconds (s) / Arbitrary | Main Text (ref [86-88]) | Explicit | timescale emerging from network reverberations |
        | Oscillation Periods (Theta, Alpha, Beta, Gamma) | ~125ms (theta) down to ~10-25ms (gamma) | Milliseconds (ms) | Main Text, Box 3 | Explicit | Timescales of brain rhythms sometimes linked to criticality |

    *   **Note:** Criticality introduces dynamics across a wide range of timescales (characteristic of power laws) rather than a single dominant timescale.

### **6.2 Active Inference:**

    *   Content: Unclear / Partial
        1.  **Internal Models (Implicit):** The brain's dynamics reflecting environmental statistics or task demands (e.g., adapting to input statistics [43], optimizing for task demands [106, 132]) suggests an internal representation or expectation.
        2.  **Prediction/Surprise (Implicit):** The need to balance structure (order/predictability) and flexibility (disorder/novelty) to deal with predictable and novel aspects of the environment aligns with minimizing prediction error/surprise. High sensitivity at criticality could relate to detecting surprising events.
        3.  **Action Selection (Implicit):** The idea that the brain dynamically shifts its distance to criticality based on task demands (e.g., shifting subcritical for focused attention/exploitation vs. critical for exploration/resting state) implies action/state selection to optimize performance according to context.
        The link is conceptual and not formally developed in the text.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes
        1.  **Evolutionary Adaptation:** The "strong" critical brain hypothesis suggests evolution selected for critical dynamics due to their information processing advantages.
        2.  **Homeostatic Adaptation:** Mechanisms like SOC or homeostatic tuning (e.g., ref [24]) actively adjust parameters to maintain the (near) critical state despite perturbations.
        3.  **Task-Dependent Adaptation:** The brain potentially adapts its operating point (distance to criticality) dynamically based on current cognitive demands (e.g., exploration vs. exploitation, rest vs. task).
        4.  **Sensory Adaptation:** Adaptation to sensory input statistics can tune the cortex towards criticality (ref [43]).
        These involve persistent changes (evolutionary timescale, homeostatic setpoints, shifts in operating regime) beyond simple stimulus-response.

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: Several mechanisms are mentioned or implied:
        1.  **Homeostatic Regulation:** Explicitly mentioned for SOC and supported by specific studies (e.g., ref [24] finding homeostatic tuning to criticality). This could involve mechanisms like synaptic scaling or adjusting E/I balance based on activity levels.
        2.  **Neuromodulation:** Mentioned as a potential physiological variable corresponding to a control parameter which could shift the system's operating point. Neuromodulators (like dopamine, acetylcholine) are known to reconfigure network dynamics based on behavioral state or task demands.
        3.  **Synaptic Plasticity (Implicit):** While not the focus, underlying synaptic plasticity rules (like Hebbian learning, STDP) are the basis for structural changes in neural networks over longer timescales, which could contribute to establishing or tuning the network dynamics towards criticality during development or learning.
        4.  **Parameter Tuning (Theoretical):** In models, adaptation involves tuning control parameters (E/I balance, coupling strength/heterogeneity) towards the critical point.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior is the **critical dynamics** itself (AC or EOC), characterized by scale-invariance, LRTCs, etc. This dynamical state is argued to *underlie* or *enable* higher-level functional behaviors:
        1.  **Complex Information Processing:** Efficient storage, transfer, and transformation of information (as detailed in M5).
        2.  **Optimal Balance:** Achieving balances like integration/segregation and order/flexibility.
        3.  **Cognitive Functions:** Supporting cognitive flexibility, fluid intelligence, potentially consciousness states.
        4.  **Adaptation:** Enabling robust adaptation to changing environments.
        The observable behaviors are the specific statistical signatures of criticality (power laws, exponents) in neurophysiological measurements (EEG, MEG, fMRI, spikes).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper reviews various methods used to validate the presence of critical dynamics (the emergent behavior) in brain data:
        1.  **Power Law Fitting:** Assessing if distributions (avalanche size/duration) or spectra (1/f slope) follow power laws (Figs 1C, Text). Limitations: Power laws can arise from non-critical mechanisms; fits must be statistically rigorous (ref [183]). Metrics: Exponents, goodness-of-fit.
        2.  **Scaling Relations:** Checking if different critical exponents measured in the system obey theoretical interrelations expected for a specific universality class (Box 1, ref [16]). Metric: Deviation from predicted relations (e.g., DCC).
        3.  **Shape Collapse:** Verifying if the temporal profiles of avalanches of different sizes collapse onto a single universal shape after rescaling (Box 1, ref [16]). Metric: Quality of collapse.
        4.  **Branching Parameter Estimation:** Calculating the average number of subsequent events triggered by preceding events; should be ≈ 1 at criticality (Box 1, ref [13, 79]). Metric: Branching ratio or MR parameter value.
        5.  **LRTC Analysis:** Measuring long-range temporal correlations using methods like Detrended Fluctuation Analysis (DFA) (Box 1, Box 3, ref [1]). Metric: DFA exponent.
        6.  **Eigenspectrum Analysis:** Examining eigenvalues of the covariance matrix; crowding near the unit circle suggests EOC (Box 2, ref [54, 60, 69]). Metric: Eigenvalue distribution, largest eigenvalue.
        7.  **Chaos Metrics:** Applying tests like the 0-1 chaos test or Lyapunov exponents (if feasible) (Box 2, ref [27]). Metric: Chaos test output (K value), Lyapunov exponent.
        8.  **Information-Theoretic Measures:** Assessing if quantities like information storage (AIS), transfer, LZC are maximized near the suspected critical point (Box 2, ref [27, 34]). Metric: Value of the measure vs. system parameters.
        Operational definitions involve thresholding signals, defining events/avalanches, choosing time bins (Box 1). Control experiments involve comparing states (e.g., wake vs. sleep/anesthesia, task vs. rest) or using pharmacological manipulations. Robustness is partially assessed by universality checks. Reproducibility is addressed by citing multiple studies across different labs, species, and modalities finding consistent (though sometimes differing in detail) evidence. Limitations mentioned include distinguishing true criticality from alternatives, effects of subsampling, and choice of analysis parameters.

---

#Key: [buckley_natural_2024]

# Natural Induction: Spontaneous Adaptive Organisation without Natural Selection

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is a theoretical/computational model of a physical system composed of a network of masses connected by viscoelastic springs and subjected to occasional disturbances (perturbations or shocks). The viscoelasticity means the springs' natural lengths (internal structure/parameters, θ) can change slowly under stress, constituting "physical learning". The relaxation of masses to positions minimizing potential energy given the current spring parameters constitutes "physical optimization". The interaction and feedback between these two processes (fast state relaxation and slow parameter adaptation over many disturbance-relaxation cycles) is termed "natural induction". The system's purpose is to demonstrate that this natural induction can lead to spontaneous adaptive organization, specifically an improved ability to find low-energy configurations (solve optimization problems) over time, without natural selection or external design/supervision. The components are masses (state variables, x), springs (defining interactions and potential energy V(x, θ)), and dampers (determining relaxation dynamics). The system's core function is to find low-energy states (optimization), and adapt its structure (learning) based on the states it visits, thereby improving its future optimization performance.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Values vary across scenarios/experiments presented. Units are not explicitly stated but can be inferred based on context (e.g., k depends on choice of force/length units). Reliability is 'High' as parameter values used in simulations are explicitly stated.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input comes from the occasional disturbances or perturbations applied to the system state (mass positions). These disturbances displace the masses from their equilibrium positions, increasing the potential energy stored in the springs. This stored potential energy then drives the subsequent relaxation/optimization dynamics.

### **2.2 Energy Transduction**

    *   Content: 1. **Disturbance -> Potential Energy:** External disturbances input kinetic energy which is rapidly damped, resulting primarily in increased potential energy stored in the stressed springs (elastic energy). 2. **Potential Energy -> Kinetic Energy -> Heat (via damping):** During relaxation (optimization phase), the stored potential energy is converted into kinetic energy of the masses, which is then dissipated as heat due to viscous damping (both γ and γ_m). The system moves towards a local minimum of the potential energy function V. 3. **Potential Energy -> Structural Change (Memory):** Stress (stored potential energy) in a spring also drives the slow change in its natural length (l_ij), according to Eq. (1) (Maxwell model, learning phase). This represents energy being used to modify the system's structure/parameters, effectively storing information about the states visited.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The system is primarily dissipative. The goal isn't energy efficiency in a traditional work-output sense. Energy is input via disturbances and then dissipated during relaxation (optimization) and structural change (learning). The 'useful' outcome is information storage (structure change) and finding low-energy states. From a physics perspective, the energy eventually dissipates as heat. The efficiency relates more to how effectively the energy landscape guides the system to good solutions and how effectively stress induces useful structural changes. No quantitative efficiency metric is provided or relevant in the standard sense. The score reflects the high dissipation inherent in the damped relaxation process.

### **2.4 Energy Dissipation**

    *   Content: Energy is dissipated primarily through viscous damping associated with the movement of masses (parameter γ) and the internal rearrangement of the viscoelastic springs (parameter γ_m, Maxwell element). This converts kinetic energy and the energy associated with structural changes into heat. The paper assumes a heavily damped regime (Sec 4.2), implying significant dissipation. Quantification is not provided, but dissipation via damping γ (faster timescale) and γ_m (slower timescale, during learning/creep) are the main mechanisms. Qualitative Assessment: High.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rule governing the self-organization (change in structure) is the viscoelastic update rule for the natural length of each spring, l_ij:
        `l_dot_ij = (k / γ_m) * (r_ij - l_ij)` (Equation 1)
        Where `l_dot_ij` is the rate of change of the natural length of the spring between mass i and j, `k` is the spring constant, `γ_m` is the damping constant of the Maxwell element (controlling the learning timescale), `r_ij` is the current distance between masses i and j, and `l_ij` is the current natural length. This rule states that the natural length changes slowly in proportion to the current stress (extension or compression, `r_ij - l_ij`) in the spring. Additionally, the standard damped harmonic motion rules govern the fast state dynamics (mass positions `x_i`), driven by the gradient of the potential energy V:
        `F_i = -∇_i V - γ * x_dot_i` (Implicit from Sec 2 description and standard physics)
        `V = Σ (1/2) * k * (r_ij - l_ij)^2` (Explicit potential energy, Sec 2)
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | NewtonianRelaxation | Spring constant | k | 10, 1, 0.1 | Force/Length | Sec 3.1, 3.2 | Explicit | Values given for different scenarios. |

### **4.3 Global Order:**

    *   Content: The emergent global order is the specific, non-uniform configuration of the natural spring lengths (l_ij) across the network. This configuration represents an implicit, learned model of the system's own low-energy states. This learned structure alters the overall energy landscape, specifically enlarging the basins of attraction for very low-energy configurations, leading to the emergent function of improved optimization capability.
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Functional | Enhanced optimization capability | Minimum Energy Found (V_obj) | Significantly Lower | Energy | Explicit | Key result shown in simulations, compared before/after learning. | Natural Induction | Fig 4C/D, 5C/D |
| Dynamical | Dominant attractor(s) | Attractor State(s) | Specific low-energy configurations | Position Vector | Explicit | System converges predominantly to specific states after learning. | Natural Induction | Fig 3C, 4B |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | Global Structure -> Global Function | How the network {l_ij} enables enhanced optimization | High | 8 | V_min (Energy) | Explicit | The paper's main claim and simulated result: the learned structure leads to finding lower energy states. | Sec 3, Fig 4/5 |
    | Global Structure -> Global Dynamics | How the network {l_ij} determines attractor landscape | High | 8 | Attractor Basins | Explicit | Explicitly stated and shown (Fig 3C) that learning modifies attractor basins. | Sec 3.1, Fig 3 |
        * Rubric: 0 - No mapping; 3 - Local rules qualitatively influence global behavior; 5 - Statistical correlation between local state and global outcome; 7 - Quantitative model predicts global state probability from local rules; 9 - Global behavior uniquely determined by aggregation of local perspectives; 10 - Formal proof of Yoneda Lemma applicability.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog / Neuromorphic / Other (Physical Annealing/Relaxation)

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operation performed by the material is **Local Energy Gradient Descent / Relaxation**. Each mass moves in the direction that locally reduces the potential energy function V, subject to damping. This collective local relaxation leads the system towards a local minimum of the global energy function V.
        `x_dot_i ∝ -∇_i V` (Ignoring damping for simplicity)

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
    *   **Note:** Specific values depend on simulation parameters, but the relative ordering (Relaxation << Disturbance << Learning) is crucial and explicitly stated as a condition. Units depend on simulation setup.

### **6.2 Active Inference:**

    *   Content: Partial
        (1) *Prediction:* The learned structure (l_ij configuration) implicitly encodes a model of the system's own low-energy states. The dynamics guided by this structure can be seen as implicitly predicting likely future low-energy configurations. Generalization (finding novel low-energy states, Sec 3.1) suggests the model goes beyond mere memorization.
        (2) *Action Selection:* The relaxation dynamics (physical optimization) can be viewed as action selection (moving towards predicted lower energy states) to minimize the 'surprise' of being in a high-energy state.
        (3) *Internal Model Update:* The physical learning mechanism (Eq 1) updates the internal model (l_ij configuration) based on experience (visited states).
        However, the model is implicit (emergent structure) rather than an explicit generative model used for prediction error calculation in the formal Active Inference sense. The 'surprise' being minimized is simply the potential energy V. It lacks explicit prediction error signals driving both action and learning in the typical Bayesian formulation of Active Inference.
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:**
        *   Prediction Error Reduction Rate: Measure the rate at which the system finds lower energy states over time (Fig 4C, 5D).
        *   Timescale of Anticipation: Could potentially be assessed by how quickly the system finds good solutions after a disturbance, comparing early vs late stages of learning.
        *   Complexity of Internal Model: Could be quantified by analyzing the learned l_ij network structure (e.g., using graph metrics, information content).
        *   Free Energy Minimization Rate: Track dV/dt during relaxation and how it changes with learning.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism is "physical learning" via viscoelasticity, specifically modeled using the Maxwell model (Appendix A). The natural length l_ij of each spring adapts according to the rule: `l_dot_ij = (k / γ_m) * (r_ij - l_ij)` (Eq 1). This is a form of unsupervised learning where the parameters (l_ij) adjust to reduce the stress (`r_ij - l_ij`) imposed by the states (`r_ij`) the system frequently occupies (typically local energy minima visited between disturbances). This process effectively learns an associative memory or an implicit model of the distribution of visited low-energy states. It resembles energy minimization on the parameters (Appendix A), which is mathematically related to Hebbian learning in Hopfield networks (Sec 1.2). The adaptation is driven by the physical stress within the system components and occurs implicitly through the feedback loop between state dynamics (optimization) and parameter dynamics (learning), facilitated by disturbances.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main emergent functional behavior is **enhanced optimization capability**. Specifically, after undergoing the natural induction process (repeated cycles of disturbance, relaxation/optimization, and slow structural adaptation/learning), the system becomes significantly better at finding configurations (states) that correspond to very low values of the *original* potential energy function. It learns to find solutions that are "exceptionally low energy" (Abstract) or of "exceptionally rare quality" (Sec 1.6), which were statistically very unlikely to be found through simple relaxation (local optimization) from random starting points in the original system. This includes finding solutions better than any sampled by the original dynamics within the same number of trials (Sec 3.1, Fig 4C/D). A secondary emergent behavior is the formation of a **stable, associative memory** of the system's own dynamics, manifested as highly concentrated attractor basins around specific low-energy states (Fig 3C, Fig 4B).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claims of emergent behavior (enhanced optimization) are validated through numerical simulations of the described physical model. The validation primarily involves:
        1.  **Operational Definition:** Defining "better optimization" as finding states with lower potential energy (V) in the *original* problem landscape.
        2.  **Control Comparison:** Comparing the energy of states found by the adapted system (after natural induction) to the energy of states found by the original system (simple relaxation from the same/similar initial conditions or random sampling) over the same number of trials/disturbances (Fig 4C/D, Fig 5C/D).
        3.  **Quantitative Analysis:** Plotting the energy found over time/disturbances (Fig 4C, 5D) and showing histograms of the energy distributions before and after learning (Fig 4D, 5C). Statistical analysis (comparing means and STDs) is used to quantify the improvement (Sec 3.1, 3.2a, 3.2b).
        4.  **Reproducibility:** Mentioning results over multiple runs (e.g., 10 runs in Sec 3.1, 3.2a, 3.2b) suggests reproducibility.
        Limitations: Validation is purely computational; no physical experiment is performed. Robustness testing across a wide parameter range is limited.

---

#Key: [howard_evolving_2019]

# Evolving embodied intelligence from materials to machines

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is Multi-Level Evolution (MLE), an algorithmic framework for the automatic design of robots, specifically tailored to tasks and environmental conditions. It operates bottom-up across multiple hierarchical levels, typically proposed as three: Material, Component, and Robot. At the lowest level, materials are discovered or selected. Components are then created by combining materials into specific geometries. Finally, robots are assembled by integrating these components into body plans and equipping them with controllers. The core mechanism involves parallel search processes (suggested: evolutionary algorithms, specifically illumination/quality diversity algorithms) operating at each level, generating libraries of diverse, high-performing candidate solutions (materials, components, robots) characterized by their physical properties and fitness. Higher levels draw upon the libraries generated by lower levels. The purpose is to harness the vast space of possible materials and manufacturing techniques to automatically evolve specialized, embodied robots capable of operating in complex, unstructured environments, moving beyond traditional engineering approaches that often favor generalist designs. Key components include: hierarchical levels, search algorithms per level (e.g., illumination algorithms), representations (e.g., CPPNs for geometry/body plans), genotype databases, simulation/modeling tools, physical property characterization methods, fitness functions, and the concept of 'hybridization' (seamless integration of real and virtual components/testing).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters describe the proposed conceptual framework. Specific values for dimensions, bin sizes, algorithm settings etc., are not provided as it's not a detailed implementation report.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source for the MLE *framework* itself is computational power (electricity) required to run the search algorithms, simulations, data management, and potentially control physical experimentation/fabrication robots. For the *robots designed by MLE*, energy sources are diverse and depend on the evolved design (e.g., solar, biomass, wind - mentioned in Fig. 2 examples).

### **2.2 Energy Transduction**

    *   Content: For the MLE framework: Electrical energy is transduced into computational operations (CPU/GPU processing for algorithms, simulations) and potentially mechanical work (if controlling physical robots for synthesis/testing). For the designed robots: Energy transduction depends on the evolved design, e.g., photovoltaic effect (solar), chemical reactions (biomass), mechanical coupling (wind), conversion to actuation (e.g., electrical to mechanical in motors, chemical to mechanical in artificial muscles). The paper emphasizes integrating sensing, actuation, structure, and potentially power generation within the designed components/robots.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The energy efficiency of the MLE *framework* (computational efficiency) is mentioned as a concern (combinatorial issues, need for simulation) but not quantified. The paper suggests evolutionary algorithms are "relatively efficient" across levels (Sec: Inspiration...). Efficiency is a potential *fitness criterion* or characteristic for the *designed* materials/components/robots (e.g., actuator efficiency, sensor signal-to-noise implicitly relate to energy use), but no overall efficiency values for generated systems are provided. Table 1 lists 'energy requirements' as a desired property for materials.

### **2.4 Energy Dissipation**

    *   Content: For the MLE framework: Energy dissipates primarily as heat from computational hardware during algorithm execution and simulation. If physical processes are involved (synthesis, testing), dissipation would occur through friction, heat loss, chemical reactions, etc. For the designed robots: Dissipation mechanisms would depend heavily on the evolved design (e.g., heat from actuators, friction during locomotion, energy loss in sensing). The paper does not quantify dissipation for the framework or potential products.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Large / Scalable
*   Units: Number of stored solutions (genotypes/phenotypes) or Information Content (bits, if quantifiable)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: High

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Low / Negligible

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are primarily those of the evolutionary illumination algorithm operating within each level/library (as described in Fig. 1b):
        1.  Selection: Randomly select an occupied bin (parent solution).
        2.  Variation: Apply random mutations (and potentially crossover) to the parent's genotype.
        3.  Evaluation: Determine the fitness (e.g., cost, performance) and features (physical properties) of the resulting offspring phenotype (via simulation or physical test).
        4.  Competition/Placement: Identify the bin corresponding to the offspring's features. Compare the offspring's fitness to the current best solution (elite) in that bin. If the offspring is fitter (or the bin is empty), the offspring becomes the new elite for that bin.
        These rules operate locally on individual solutions based on their fitness and features. Communication between levels involves passing candidate solutions (phenotypic properties) upwards.

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The emergent global order is the populated multi-dimensional library (archive) of diverse, high-performing solutions (materials, components, robots) at each level. This "order" is characterized by the distribution of solutions across the feature space (which bins are filled) and the fitness landscape within that space (the quality of the solution in each bin). The goal is not a single ordered structure but a diverse "illuminated" search space.

### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| EA-Eval | Evaluation of offspring fitness & features | Fitness Function | User-Defined | Task-Specific | Explicit | Text | Section 'A conceptual MLE architecture' |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| LibraryCoverage | Fraction of bins filled in the library | % Filled Bins | 0-100 | % | Explicit | Mentioned implicitly via goal of filling bins, visualization in Fig 1a/b | Monitor bins over time | Fig 1a/b, Text |
| LibraryQuality | Fitness of solutions in the library | Average/Max Fitness per Bin/Overall | Task-Specific | Explicit | Fitness is core concept | Track elite fitness values | Fig 1a/b, Text |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No (for the framework); Yes (for the designed robots)

### **5.2 Computation Type:**

    *   Content: Hybrid (Framework utilizes evolutionary algorithms, potentially neural networks (CPPNs), simulation physics engines. Designed robots would likely utilize Analog computation via physical dynamics / Morphological Computation).

### **5.3 Computational Primitive:**

    *   Content: For the Framework: The core operations are variation (e.g., mutation, crossover operating on genotypes), selection (based on fitness/features, e.g., tournament, truncation, elite replacement as in Fig 1b), evaluation (running simulations or physical tests to get fitness/features), and potentially geometric pattern generation (CPPNs). For the Designed Robots: Primitives would be physical processes like elastic deformation, stress propagation, fluid dynamics, chemical reactions, depending on the evolved morphology and materials, harnessed for behavioral control (Morphological Computation).
    *   **Sub-Type (if applicable):** Framework: Variation (Mutation), Selection (Elite Replacement), Evaluation (Simulation), Pattern Generation (CPPN). Designed Robots: Physical Dynamics (e.g., Non-linear elasticity).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
    *   **Note:** The paper discusses the need to reduce time (e.g., via simulation, autonomous experiments) but doesn't quantify specific timescales for the framework or its products.

### **6.2 Active Inference:**

    *   Content: Partial

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The primary adaptation mechanism is artificial evolution, specifically population-based algorithms like evolutionary illumination / quality diversity algorithms. Key steps involve:
        1.  Maintaining populations (libraries/archives) of candidate solutions (genotypes mapping to phenotypes).
        2.  Evaluating solutions based on fitness (performance on task) and features (physical properties).
        3.  Selecting promising parent solutions (e.g., elites from library bins).
        4.  Applying variation operators (e.g., mutation, recombination/crossover) to parent genotypes to create offspring.
        5.  Replacing less fit or older solutions in the libraries with better/novel offspring based on their evaluated fitness and features.
        The adaptation occurs over generations, driven by selection pressure towards higher fitness and/or greater diversity across the defined feature dimensions. The use of CPPNs adds a layer of developmental mapping from genotype to phenotype.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behavior of the MLE *framework* is the **automated design and discovery of diverse, specialized materials, components, and robots**. It explores a vast design space across multiple levels to generate novel solutions tailored to specific tasks and environments. For the *robots produced by MLE*, the desired emergent behaviors are **task-specific actions** like locomotion (sliding, crawling), gripping, carrying, etc., achieved through the interaction of their evolved body (materials, morphology) and potentially a controller with the environment (Embodied Behavior mentioned in Table 1). The paper emphasizes the emergence of useful behaviors facilitated by the framework's multi-level search and diversity generation (e.g., "spontaneous emergence of component–material combinations that facilitate useful behaviour").

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: As a perspective paper, it does not present experimental validation of emergent behaviors from a full MLE system. It relies on:
        1.  **Conceptual Argument:** Explaining how the multi-level structure and quality-diversity search are *expected* to lead to emergent specialization and complex behaviors.
        2.  **Reference to Prior Work:** Citing successes in related fields like evolutionary robotics (discovering unconventional designs), quality diversity algorithms (finding diverse high-performers), materials discovery via EA, and CPPNs (generating complex morphologies).
        3.  **Illustrative Examples:** Providing conceptual examples of how MLE *could* produce different robots for different niches (Fig. 2).
        Limitations: No empirical demonstration of the full MLE producing emergent behaviors is provided. Operational definitions of emergence specific to MLE are not given. Reproducibility/robustness are discussed as goals/challenges, not demonstrated results.

---

#Key: [marcucci_theory_2020]

# Theory of Neuromorphic Computing by Waves: Machine Learning by Rogue Waves, Dispersive Shocks, and Solitons

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a theoretical/computational model termed a "single wave-layer feed-forward network" (SWFN). It uses nonlinear wave propagation, governed by the nonlinear Schrödinger equation (NLSE: ı∂ζψ + ∂ξ²ψ + χ|ψ|²ψ = 0), as a computing reservoir. The system consists of three conceptual layers: 1) An input encoding layer where an input vector x is mapped onto the initial state of a complex-valued wave field ψ₀(ξ, x), often by modulating plane waves (Eq. 3, 4) and adding a bias wave |b⟩. 2) A wave layer where the initial wave ψ₀ propagates over a distance L according to the NLSE, resulting in a final state ψL(ξ, x). 3) A decoding readout layer where the modulus square of the final wave |ψL(ξ, x)|² is sampled at NC discrete points (channels) ξ¯j. These sampled values form a vector g(x). The final output o(x) is a linear combination of these channel values: o(x) = Σ βj * gj(x) = β · g(x). The purpose is to perform machine learning tasks, such as function approximation, learning datasets, and implementing Boolean logic gates, by training the readout weights β using standard methods like least squares based on a training set {x(t), T(t)}. The system explores the role of nonlinearity (χ) and emergent wave phenomena (solitons, rogue waves, shocks) in computation.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters are taken from the examples provided. The NLSE is likely used in dimensionless form. Data reliability is High as these are input parameters for the theoretical simulations described.

## M2: Energy Flow
    *   Content: The paper is purely theoretical/computational, focusing on the mathematical model (NLSE) and its computational properties. It does not discuss physical energy sources, transduction, efficiency, or dissipation in the context of a specific physical realization (like optics or hydrodynamics). While the NLSE itself conserves certain quantities (like power |ψ|² integrated over ξ), the paper does not analyze the energy aspects of the proposed computational scheme.

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip to Module 4.)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rule is implicitly defined by the nonlinear Schrödinger equation (NLSE): ı∂ζψ + ∂ξ²ψ + χ|ψ|²ψ = 0. This partial differential equation describes how the complex wave field ψ changes infinitesimally in the propagation direction ζ based on its local curvature (∂ξ²ψ, representing linear dispersion/diffraction) and its local intensity (|ψ|², representing nonlinear self-interaction/Kerr effect). The parameter χ scales the strength of the nonlinear self-interaction term relative to the linear term. The interplay between dispersion/diffraction and nonlinearity governs the emergence of structures like solitons and shocks.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | NLSE | Nonlinear Schrödinger Eq. | Nonlinearity Strength (χ) | 0 - 1000 | Dimensionless | Text (p. 2, 3, 4), Fig. 2f, 3e, 4 | Explicit | Parameter controlling the strength of the local |ψ|² interaction term. |
    | NLSE | Nonlinear Schrödinger Eq. | Dispersion/Diffraction Coefficient | 1 (implicit) | Dimensionless | Eq. 2 | Implicit | Coefficient of the ∂ξ²ψ term is implicitly 1 in the dimensionless form used. |

### **4.3 Global Order:**

    *   Content: The emergent global order includes spatially and temporally localized structures such as solitons (stable wave packets maintaining shape), complex patterns arising from soliton interactions (soliton gases), potentially extreme events like rogue waves (waves with unusually high amplitude), and sharp gradients characteristic of dispersive shock waves. These represent organized structures emerging from the underlying continuous wave field dynamics. Figures 2b, 3a, 3b, 4a, 4b visually depict these complex spatiotemporal patterns.
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| NLSE-Nonlin | Kerr nonlinearity | χ | 0 - 1000 | Dimensionless | Explicit | Controls strength of local intensity-dependent phase shift. | Eq. 2, Text |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Rank (r) | Rank of Transmission Matrix H | r | 0 - NT | Dimensionless | Explicit | Quantifies functional complexity/learning ability. | Calculation from simulation results | Fig 2f, 3e, Text (p.2, 3) |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Reservoir Computing

### **5.3 Computational Primitive:**

    *   Content: The core computational primitive embodied in the wave layer is the propagation according to the nonlinear Schrödinger equation (NLSE). This involves a complex, nonlinear transformation of the input wave function ψ₀(ξ, x) into the output wave function ψL(ξ, x) over the propagation distance L. This transformation implicitly performs high-dimensional feature mapping, leveraging the interplay of linear dispersion/diffraction (∂ξ²ψ term) and nonlinear self-interaction (χ|ψ|²ψ term). While not a simple standard function like a logic gate, the NLSE propagation *itself* is the fundamental computational operation performed by the material/medium analog. The overall system then uses linear combination (readout) on samples of the result. The paper demonstrates this primitive enables approximation of functions (like sin(x)/x) and implementation of Boolean logic (NAND gate), suggesting the NLSE propagation acts as a universal function approximator basis when combined with the linear readout.
    *   **Sub-Type (if applicable):** Nonlinear Partial Differential Equation Evolution (NLSE) acting as a feature map.

### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Propagation "Time" (Distance) | L = 1 | Dimensionless | Text (p. 3), Fig. 2, 4 | Explicit | The evolution coordinate ζ acts like time; the total "duration" of computation is L. |
        | Characteristic Nonlinear Time | ~1/χ (or related scale) | Dimensionless | Eq. 2 | Implicit | The timescale over which nonlinear effects become significant is inversely related to χ, inferred from NLSE theory. |
    *   **Note:** The paper uses a spatial coordinate ζ for evolution, analogous to time. Values are based on the dimensionless NLSE formulation. Physical timescales would depend on the specific physical realization (e.g., ps in optics, seconds in hydrodynamics).

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip to Module 8.)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors demonstrated are:
        1.  **Function Approximation:** Learning to approximate a target mathematical function (e.g., y = sin(x)/x) given input samples (Example 1, Fig. 2).
        2.  **Dataset Learning/Classification:** Learning to map input vectors from a dataset (e.g., abalone dataset) to corresponding target values (Example 2, Fig. 3). This implies classification or regression capabilities.
        3.  **Boolean Logic Implementation:** Learning to implement universal Boolean logic gates (e.g., NAND gate) using binary encoded inputs (Example 3, Fig. 4).
        These behaviors emerge from the combination of the complex, nonlinear feature mapping performed by the NLSE wave propagation and the linear readout layer trained on specific tasks.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claims of emergent behaviors (function approximation, dataset learning, Boolean logic) are validated through numerical simulations of the SWFN model.
        *   **Operational Definitions:** The tasks are standard machine learning benchmarks (approximating sin(x)/x, learning abalone dataset, implementing NAND gate). Success is defined as the SWFN output o(x) matching the target T(x) for the training data points.
        *   **Control Experiments:** The role of nonlinearity is investigated by comparing performance at different χ values (Fig. 2f, 3e), showing learning failure at low/zero χ (linear case), supporting the claim that nonlinearity is essential. The role of NC is shown by varying it (Fig. 2e, 3d), demonstrating the need for sufficient channels (NC >= NT) for perfect learning, consistent with theory.
        *   **Quantitative Analysis:** Learning performance is quantified by the training error (||H·β - T||), shown to drop significantly when conditions (sufficient NC and χ) are met (Fig. 2e, 3d). The rank (r) of the transmission matrix H is calculated and shown to correlate with learning ability (r = NT needed for zero error, Fig. 2f, 3e).
        *   **Robustness/Reproducibility:** Not explicitly addressed; assumed reproducible given the deterministic model and specified parameters.
        *   **Limitations:** Validation is purely computational. It doesn't account for physical noise, imperfections, or energy constraints. Generalization performance (on unseen data) is not assessed, only training error.

---

#Key: [fraxedas_collective_2024]

# Collective motion of Nafion-based micromotors in water

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of self-propelling micromotors fabricated from Nafion, a perfluorinated polymer with pendant sulfonic acid groups, structured into asymmetric rods capped with passive beads (polystyrene modified with amidine or Al2O3-coated). These micromotors are designed to move autonomously in water when fueled by innocuous salts (e.g., NaCl). Propulsion is driven by ion exchange between the Nafion (releasing H+) and the surrounding salt solution (uptaking cations like Na+). The asymmetry in shape and/or material composition (Nafion rod vs. passive cap) breaks symmetry, generating tangential components of self-generated electric fields due to differing ion mobilities. These fields interact with the surrounding fluid and surface charges (zeta potentials) to induce electrokinetic/diffusio-osmotic flows, propelling the micromotor. The system exhibits collective motion, forming clusters where velocity increases with size. The purpose is to study self-propulsion based on ion exchange and explore potential applications like water remediation by trapping micro/nano-objects. Components include Nafion rods, passive capping beads (polystyrene/Al2O3), water, and fuel salts (NaCl).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Simulation parameters are explicitly stated for the model system. Experimental dimensions are typical values stated. Velocities are measured averages. Zeta potentials are given for simulation and related pump experiments. Reliability is High for direct experimental measurements/conditions, Medium for simulation parameters or values cited from pump experiments (relevance depends on exact experimental conditions matching simulations).

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the chemical potential difference associated with the ion exchange process between the Nafion polymer and the surrounding aqueous salt solution. Specifically, the difference in chemical potential drives the exchange of protons (H+) from the Nafion's sulfonic acid groups with cations (e.g., Na+) from the bulk solution.

### **2.2 Energy Transduction**

    *   Content: Chemical energy from the ion exchange process is transduced into electrical energy and then into kinetic energy (fluid motion and motor propulsion).
        1.  **Chemical to Electrical:** The difference in diffusion coefficients between the exchanged ions (H+ and Na+) leads to charge separation near the Nafion interface, creating local concentration gradients and associated electric fields (diffusion potential). Eq. 1 (Poisson's) and Eq. 3 (Nernst-Planck) describe the coupling between ion concentration and electric potential.
        2.  **Electrical to Kinetic:** The self-generated electric field interacts with the net charge in the electrical double layer (EDL) adjacent to the micromotor surfaces (including the passive cap and surrounding fluid). The tangential component of this electric field exerts an electrical body force on the fluid within the EDL, inducing electro-osmotic flow along the surface (described by Stokes' equation, Eq. 2, including the electrical body force term `r_e * E * V4`). The asymmetry of the motor ensures that these flows do not cancel out, resulting in a net fluid flow relative to the motor, which propels the motor via reaction forces (Newton's third law). Diffusio-osmosis (fluid flow driven by solute gradients interacting with the surface) is also implicitly involved, coupled with electro-osmosis.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any quantitative measurement or calculation of energy efficiency (e.g., chemical energy input vs. kinetic energy output). Efficiency for chemically propelled micromotors is typically very low (<<1%), but this is not quantified here. Qualitative assessment: Likely Low.

### **2.4 Energy Dissipation**

    *   Content: Energy is primarily dissipated through viscous drag as the micromotor moves through the fluid (water) and as internal fluid flows occur. The Stokes' equation (Eq. 2) implicitly accounts for viscous dissipation via the viscosity term (eta). Heat generation due to ionic currents (Joule heating) associated with the electric fields and ion movement might occur but is likely negligible at these low concentrations and field strengths. Brownian motion also represents thermal energy randomization. Qualitative assessment: Viscous dissipation is High (dominant).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rules stem from the phoretic and hydrodynamic fields generated by each active micromotor.
        1.  **Phoretic Interaction:** Ion exchange creates concentration gradients (H+, Na+, Cl-, OH-) and electric fields around each motor (governed by Poisson-Nernst-Planck, Eq 1, 3). These fields exert forces on other nearby charged objects (other motors, passive particles) and influence their motion (electrophoresis/diffusiophoresis). The exact forces depend on the local field gradients and the surface properties (zeta potential) of the interacting objects.
        2.  **Hydrodynamic Interaction:** The self-generated fluid flow (pumping) around each motor (governed by Stokes, Eq 2) advects nearby objects. The paper highlights an attractive effect ("pumping matter nearby towards the collective motile structure", Abstract; "dragging fluid towards them and attracting objects nearby", Sec 2.2.3). The simulations (Fig 3b) show inflow towards the Nafion end, predicting attraction along that axis. The collective clusters act as stronger mobile pumps (Fig 8).
        3.  **Short-Range Forces:** Standard colloidal forces (van der Waals, electrostatic repulsion/attraction based on surface charges) likely also play a role once motors are very close, determining adhesion/stability of clusters, although these are not explicitly detailed as drivers of the initial *formation*.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Phoretic | Electro-/Diffusiophoresis | Zeta Potential (motor components) | -73 to +17 (Exp. proxy); -35 to +17 (Sim.) | mV | Sec 2.1, 2.2.1 | Explicit | Values given for components, interaction strength depends on these. |
    | Phoretic | Ion Exchange Rate | k_ie | 10^-4 (Sim.) | m/s | Sec 2.2.1 | Explicit | Simulation parameter defining gradient strength. |
    | Phoretic | Ion Diffusion Coefficients | D_i (H+, Na+, Cl-, OH-) | See Sec 2.2.1 | m^2/s | Sec 2.2.1 | Explicit | Simulation parameters defining gradient formation/decay. |
    | Hydrodynamic | Pumping Velocity (Single motor proxy) | ~1.5 (relative motion) | µm/s | Sec 2.2.3 | Explicit | Characterizes flow strength generated by single units. |
    | Hydrodynamic | Pumping Velocity (Cluster) | up to ~33.6 | µm/s | Sec 2.2.3, Fig 8 | Explicit | Characterizes enhanced flow strength of collective structures. |
    | Hydrodynamic | Fluid Viscosity | eta | 10^-3 | Pa·s | Sec 2.2.1 | Explicit | Simulation parameter affecting flow propagation. |

### **4.3 Global Order:**

    *   Content: The emergent global order consists of dynamic, mobile clusters or aggregates ("Nafion assemblies", "ensembles") of varying sizes and shapes formed by the individual micromotors. These clusters themselves exhibit collective motility, often with enhanced speed compared to individual swimmers, and act as mobile pumps attracting other objects. The specific shape and internal arrangement of motors within clusters are described qualitatively (e.g., "elongated" vs. "more symmetrical", Fig 6, 7).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Phoretic | Chemi/Electro-phoretic attraction/repulsion | Gradient Strength (proxy: k_ie, D_i) | See Sec 2.2.1 | m/s, m^2/s | Explicit | Governs strength of concentration/electric fields. | Sec 2.2.1 |
| Phoretic | Chemi/Electro-phoretic attraction/repulsion | Zeta Potentials | -73 to +17 | mV | Explicit | Determines response of objects to fields. | Sec 2.1, 2.2.1 |
| Hydrodynamic | Fluid Pumping Attraction | Pumping Velocity | 1.5 - 33.6 | µm/s | Explicit | Strength of advective flow generated by motors/clusters. | Sec 2.2.3, Fig 8 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| ClusterSize | Number of aggregated motors or spatial extent | Cluster Dimensions (L x W) | ~few to ~60x30 | µm x µm | Explicit | Measured extents of observed clusters. | Image Analysis | Fig 6, 7, 8 |
| ClusterShape | Qualitative description or aspect ratio | Aspect Ratio (L/W) | Variable (Implicitly ~1 to ~2) | - | Mixed | Shapes described as "elongated" or "more symmetrical"; aspect ratio calculable from dimensions. | Image Analysis | Fig 6, 7, 8 |
| ClusterVelocity | Average speed of the cluster centroid | <V> | ~1.5 to ~23 | µm/s | Explicit | Measured average velocities of different clusters. | Particle Tracking | Fig 6 |
| ClusterDirectionality | Persistence of motion direction | Straightness of Trajectory | Qualitative (zigzagging vs. directional) | - | Explicit | Described qualitatively and shown in Fig 7b. | Particle Tracking | Fig 7b, Sec 2.2.3 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | Individual to Cluster | How individual motor properties (zeta potential, shape, ion exchange rate) determine cluster formation probability and characteristics (size, shape, velocity). | Low (4/10 Score from M4.4) | 2 | Cluster Velocity vs Size/Shape (Fig 6), Trajectory Shape (Fig 7b) | Mixed | While clustering occurs, the specific resulting global order (size, shape, velocity) is highly variable and sensitive to fluctuations, indicating a low-fidelity mapping from local rules to predictable global states. Metrics exist but show complex, non-monotonic relationships. | Fig 6, 7 |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 2
        *   **Rubric:** 0: No relation between local and global. 2: Local rules lead to emergent global states, but specific states are highly variable/unpredictable. 4: Qualitative predictability, trends observable. 6: Quantitative trends, but significant variance. 8: Reliable quantitative prediction of global state from local rules. 10: Perfect deterministic mapping.
    *   **Metrics:** Cluster velocity vs size/shape (Fig 6), Qualitative trajectory analysis (Fig 7b).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-M5.4)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Motor Velocity Timescale (Length/Velocity) | ~4 µm / 1.5 µm/s = ~2.7 | s | Sec 2.2.3 | Implicit | Characteristic time for a motor to travel its own length. |
        | Cluster Velocity Timescale (Size/Velocity) | e.g., ~60 µm / 23 µm/s = ~2.6 | s | Sec 2.2.3, Fig 6 | Implicit | Characteristic time for a large cluster to travel its own length. |
        | Cluster Shape Fluctuation Timescale | ~seconds | s | Fig 7a | Implicit | Estimated from the x-axis of the instantaneous velocity plot showing shape changes. |
        | Pumping Effect Timescale (Distance/Pumping Vel) | e.g., 35 µm / 33 µm/s = ~1 | s | Fig 8 | Implicit | Characteristic time for an object to be attracted to a cluster over a certain distance. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main observed behaviors are:
        1.  **Individual Self-Propulsion:** Asymmetric Nafion rods move autonomously in salt solutions via ion-exchange driven phoretic mechanisms. (Velocity ~1.5 µm/s).
        2.  **Collective Motion / Clustering:** Individual swimmers self-assemble into dynamic, mobile clusters of varying sizes and shapes.
        3.  **Enhanced Cluster Motility:** Clusters, particularly larger and more elongated ones, exhibit significantly higher average velocities (up to ~23 µm/s) and increased directionality compared to individual swimmers.
        4.  **Mobile Pumping / Attraction:** Clusters act as mobile pumps, attracting nearby micro/nano-objects (including other swimmers) towards them via self-generated flows (pumping velocities up to ~33.6 µm/s).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Emergent behaviors (clustering, collective motion, enhanced velocity) are validated primarily through direct experimental observation and quantitative analysis using optical microscopy and particle tracking (Sec 2.2.3).
        *   **Operational Definitions:** Behaviors like velocity and clustering are operationally defined through measurement (average velocity from tracking, visual identification of aggregates).
        *   **Quantitative Analysis:** Average velocities are measured and plotted against cluster size (Fig 6), instantaneous velocity is tracked (Fig 7a), trajectories are visualized (Fig 7b, 8a), and pumping velocity is measured vs distance (Fig 8b).
        *   **Control:** The behavior requires the salt fuel; implicit control is the absence of fuel (no motion). Simulations of symmetric vs asymmetric motors provide a computational control experiment supporting the mechanism (Fig 3).
        *   **Reproducibility:** Implied by presentation of typical results and averages, but quantitative reproducibility tests are not shown.
        *   **Limitations:** Robustness to parameter variations (concentration, temperature) or fabrication tolerances not fully explored. Statistical analysis of cluster size/shape distributions not provided. Long-term stability/dynamics not fully characterized.

---

#Key: [srinivasa_criticality_2015]

# Criticality as a Set-Point for Adaptive Behavior in Neuromorphic Hardware

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper proposes designing large-scale neuromorphic hardware systems that leverage criticality as a dynamic set-point to achieve adaptive behavior, mimicking observed brain dynamics. The core idea is that networks operating at criticality exhibit optimal information processing, learning, and adaptability. The system described conceptually, and referencing specific models (e.g., Stepp et al., 2015), consists of a recurrent network of leaky integrate-and-fire (LIF) neurons (excitatory and inhibitory) connected with a fixed probability. Key components include biologically inspired mechanisms: two types of synaptic dynamics (AMPA for excitation, GABA for inhibition), short-term plasticity (STP) affecting instantaneous synaptic efficacy, and spike-timing-dependent plasticity (STDP) for long-term synaptic strength modification. Inhibitory plasticity is highlighted as a potential homeostatic mechanism to regulate E/I balance and maintain criticality. The purpose is to create scalable, autonomous, and adaptive intelligent systems using neuromorphic hardware that self-tune to an optimal operating regime (criticality) without requiring manual, application-specific parameter tuning, thus embracing network complexity rather than minimizing it.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters relate to the referenced computational model (Stepp et al., 2015) used to support the perspective, and the target scale for hardware implementation. Reliability is Medium for model parameters as they are cited, and Low for target hardware scales as they represent goals.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The paper discusses energy efficiency as a key driver for neuromorphic computing, contrasting with biological systems and CMOS limitations. However, it does not specify the primary energy source for the conceptual hardware or the referenced simulations. For hardware, it would implicitly be electrical power. For simulations, it's computational resources.

### **2.2 Energy Transduction**

    *   Content: The paper does not detail energy transduction mechanisms. In the conceptual hardware, electrical energy would be transduced into spiking activity, synaptic events, and plasticity-related changes via CMOS circuits (potentially including memristors, as cited in Cruz-Albrecht et al., 2013). In the LIF simulation, energy concepts are abstracted; computations transform input parameters into simulated neuronal dynamics.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper *motivates* the work by highlighting the goal of achieving energy efficiencies comparable to biological systems and overcoming limitations of current architectures. It cites energy-efficient circuit designs (Cruz-Albrecht et al., 2012). However, it provides no specific efficiency metrics or quantitative assessment for the proposed criticality-based system or the referenced model. The *aim* is high efficiency, but the achieved efficiency is not evaluated. Qualitative assessment: High (Goal).

### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*    Units: Qualitative Descriptor: "Long-term"

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The primary local rules driving self-organization towards criticality are described as:
        1.  **Leaky Integrate-and-Fire (LIF) Dynamics:** Neurons integrate inputs over time and fire a spike when their membrane potential crosses a threshold. (Implicit standard LIF model assumed).
        2.  **Synaptic Transmission:** Excitatory (AMPA-like) and Inhibitory (GABA-like) currents are generated postsynaptically upon presynaptic spike arrival.
        3.  **Short-Term Plasticity (STP):** Synaptic efficacy changes dynamically based on recent presynaptic activity (e.g., depression or facilitation). Mechanisms from Markram & Tsodyks (1996), Tsodyks et al. (1998) are cited.
        4.  **Spike-Timing-Dependent Plasticity (STDP):** Long-term synaptic weights are modified based on the precise timing difference between pre- and post-synaptic spikes (strengthened for pre-before-post, weakened for post-before-pre). Mechanisms from Markram et al. (1997), Bi & Poo (1998) are cited.
        5.  **Inhibitory Plasticity:** Presented as a key homeostatic mechanism regulating the E/I balance, potentially driving the system towards criticality. Vogels et al. (2011) is cited for the mechanism; Magnasco et al. (2009), Cowan et al. (2013) are cited for its role in criticality.
        6.  **Homeostasis:** More generally, the system strives to maintain variables (like firing rates or E/I balance) around a set-point, drawing parallels to Ashby's homeostat.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
### **4.3 Global Order:**

    *   Content: The primary global order that emerges is **criticality**. This is manifested as spontaneous, scale-invariant cascades of neural activity known as **neuronal avalanches**. This critical state is associated with optimized network properties: maximal information capacity and transmission, maximized number of metastable states, and optimized dynamic range. It represents a dynamic balance between quiescence and uncontrolled excitation ("decay and explosion").
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Criticality | Scale-invariant activity cascades | Avalanche size/duration distribution exponent (τ, α) | Typically ~1.5 (size), ~2.0 (duration) for τ or α respectively | - | Implicit | These are standard measures for criticality (neuronal avalanches), mentioned conceptually but values/protocols are not in *this* paper; inferred from cited works (e.g., Beggs & Plenz 2003). | Refs like Beggs & Plenz 2003, Stepp et al. 2015 | Section 2 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Neuromorphic

### **5.3 Computational Primitive:**

    *   Content: The fundamental operations are:
        1.  **Temporal Integration:** LIF neurons integrate synaptic inputs over time.
        2.  **Thresholding:** Neurons fire action potentials (spikes) when membrane potential crosses a threshold.
        3.  **Synaptic Filtering/Transmission:** Synapses transmit signals, potentially modulated by STP, transforming presynaptic spikes into postsynaptic currents (AMPA/GABA kinetics).
        4.  **Correlation Detection/Weight Update:** STDP modifies synaptic weights based on the temporal correlation (timing) of pre- and post-synaptic spikes.
        5.  **Homeostatic Regulation:** Inhibitory plasticity adjusts weights to maintain E/I balance or target firing rates.
        Collectively, these primitives enable higher-level computations like pattern discrimination and optimized information transmission, particularly near criticality.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Synaptic Dynamics (AMPA/GABA) | Typically ~ms | ms | Section 2 (Implicit) | Implicit | Standard neuroscience values inferred, mechanism mentioned. |
        | Neuronal Integration/Firing | Typically ~ms-10s ms | ms | Section 2 (Implicit) | Implicit | Standard LIF dynamics inferred, mechanism mentioned. |
        | Short-Term Plasticity (STP) | ~10s ms to ~s | ms-s | Section 2 (Explicit mention, Implicit value) | Implicit | Standard STP timescales inferred, mechanism mentioned. |
        | Spike-Timing-Dependent Plasticity (STDP) | ~10s ms (timing window), minutes to hours+ (consolidation) | ms, min, hr+ | Section 2 (Explicit mention, Implicit value) | Implicit | Standard STDP timescales inferred, mechanism mentioned. |
        | Neuronal Avalanches | ms to s | ms-s | Section 2 (Implicit) | Implicit | Typical avalanche durations inferred, mechanism mentioned. |
        | Homeostatic Regulation | seconds to minutes+ | s, min+ | Section 2 (Implicit) | Implicit | Typical homeostasis timescales inferred, mechanism mentioned. |
    *   **Note:** Specific values are not given in the paper but are inferred from the mentioned mechanisms (LIF, AMPA, GABA, STP, STDP, Avalanches, Homeostasis) based on typical neuroscience ranges.

### **6.2 Active Inference:**

    *   Content: Partial/Unclear
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** Measuring the rate at which the system returns to critical dynamics after perturbation; quantifying the 'prediction error' as deviation from target E/I balance or criticality metrics and tracking its minimization via plasticity; analyzing the information-theoretic 'complexity' of the plasticity rules as an internal model. CT-GIN could model the feedback loop involving local activity sensing, plasticity rule computation (internal model prediction/action), and weight update (action).

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The primary mechanisms are:
        1.  **Spike-Timing-Dependent Plasticity (STDP):** Synaptic weights between neurons are potentiated or depressed based on the relative timing of pre- and post-synaptic spikes within a specific time window (typically milliseconds). This allows the network to learn temporal correlations and patterns in activity. Cited mechanisms: Markram et al. (1997), Bi & Poo (1998).
        2.  **Inhibitory Plasticity:** Synaptic weights, particularly inhibitory ones, are adjusted based on neuronal activity levels (e.g., postsynaptic firing rate) to maintain a homeostatic balance, such as a target firing rate or a balance between excitation and inhibition (E/I balance). This stabilizes network activity and contributes to self-tuning towards criticality. Cited mechanisms: Vogels et al. (2011).
        3.  **Short-Term Plasticity (STP):** While primarily affecting short-term dynamics, STP interacts with longer-term plasticity mechanisms and influences overall network adaptation. Cited mechanisms: Markram & Tsodyks (1996), Tsodyks et al. (1998).
        The interplay of these plasticity rules, driven by network activity resulting from internal dynamics and external inputs, allows the system to adapt its structure (weights) and function.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors discussed as emerging from the system (particularly when operating near criticality) are:
        1.  **Adaptive Behavior:** The overarching goal, enabling the system to function effectively in complex, changing environments without explicit reprogramming.
        2.  **Optimal Information Processing:** Maximized information capacity, transmission, and dynamic range, associated with the critical state.
        3.  **Learning:** Ability to modify behavior based on experience, demonstrated through pattern discrimination in cited work (Srinivasa and Cho, 2014). Enabled by STDP.
        4.  **Stable Dynamics (Self-Tuned Criticality):** Maintenance of robust, non-pathological network activity (neuronal avalanches) through self-organization and homeostasis.
        5.  **Fault Tolerance:** Implied robustness to component failure or variations due to the self-tuning nature.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation primarily relies on citations to experimental neuroscience (e.g., Beggs & Plenz 2003; Petermann et al. 2009; Shew et al. 2011) demonstrating criticality (avalanches) and its functional benefits in biological systems, and computational modeling studies (e.g., Stepp et al. 2015; Srinivasa and Cho, 2014; unpublished work mentioned). Evidence cited includes avalanche statistics (scale invariance), E/I balance, measures of information capacity/transmission, and successful pattern discrimination in models. The parameter search mentioned in Stepp et al. (2015) validates the existence of parameter regimes leading to self-tuned criticality in the model. Limitations include the reliance on cited work and the conceptual nature of the hardware link. Figure 1 conceptually illustrates the self-tuning approach. Figure 2 shows a proposed hardware parameter search setup.

---

#Key: [adamatzky_collision-based_2002]

# Collision-Based Computing

__Paper Type:__ Theoretical/Computational/Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper describes Collision-Based Computing (CBC), a paradigm where computation is implemented through the interactions (collisions) of localized, mobile patterns (signals, localizations) within a homogeneous medium. Information is encoded in the presence/absence or state of these localizations (e.g., gliders in Cellular Automata, solitons in optical systems, wave-fragments in excitable chemical systems, billiard balls). Collisions alter the trajectories or states of these localizations, implementing logical operations or other computational primitives. Key features include architecture-less design (no fixed wires/gates) and the use of various media like 1D/2D Cellular Automata (CA), continuous excitable media (e.g., Belousov-Zhabotinsky reaction), and abstract geometrical spaces (AGC). Components include the medium, mobile localizations (representing signals/data), stationary localizations (e.g., eaters, mirrors, used for routing or memory), and collision rules defining the interactions. The purpose is to implement logical circuits, Turing machines, and other information processing devices. Specific examples discussed include the Billiard Ball Model (BBM), Spiral Rule CA, BZ reaction simulations, 1D CA for Turing machines/CTS, and Abstract Geometrical Computation (AGC).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Varies by system. Billiard Ball Model (BBM): Kinetic energy of the balls. Cellular Automata (CA) & Abstract Geometrical Computation (AGC): Abstract concept related to state transitions or signal initiation; energy is not explicitly modeled physicochemically. Belousov-Zhabotinsky (BZ): Chemical potential energy stored in the reactants. The paper explicitly mentions Fredkin & Toffoli's goal for BBM was non-dissipative computation, implying kinetic energy conservation.

### **2.2 Energy Transduction**

    *   Content: BBM: Elastic collisions ideally conserve kinetic energy but change momentum vectors. CA: Abstract state transitions based on rules; energy not explicitly tracked. BZ: Chemical energy is converted via reaction-diffusion processes into propagating wave patterns (excitation/inhibition cycles); collisions modify these patterns. AGC: Abstract signal interactions based on rules; energy not modeled.

### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 6 (for E6); Potentially infinite (for TM tape)
*   Units: bits (E6); symbols (TM tape)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: Spiral Rule CA: Explicitly defined by the update rule `xt+1 = f(σI(x)t, σA(x)t, σS(x)t)` based on counts of neighbors in states I, A, S, and represented by the transition matrix M (provided in Sect 3.1). Sandpile Model (mentioned): Implicit rule - "Each time a pile has at least two more grains than the next one, a grain falls" (Sect 4.3.1). FSS: Solutions involve specific state transition rules designed to propagate signals and trigger synchronization (rules not fully detailed in excerpt, but their existence is explicit). BZ Reaction: Governed by the Oregonator partial differential equations (provided in Sect 3.2), defining local reaction-diffusion dynamics. AGC: Abstract collision rules define signal interactions (Sect 5).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID      | Description                   | Parameter Name | Parameter Value Range | Units         | Data Source | Implicit/Explicit | Justification                   |
    | :----------- | :---------------------------- | :------------- | :-------------------- | :------------ | :---------- | :---------------- | :------------------------------ |
    | Oregonator   | BZ reaction-diffusion       | ε              | 0.03                  | dimensionless | Sect 3.2    | Explicit          | Parameter value given           |
    | Oregonator   | BZ reaction-diffusion       | f              | 1.4                   | dimensionless | Sect 3.2    | Explicit          | Parameter value given           |
    | Oregonator   | BZ reaction-diffusion       | q              | 0.002                 | dimensionless | Sect 3.2    | Explicit          | Parameter value given           |
    | Oregonator   | Light intensity parameter   | φ              | φ₀ + A/2 (...)     | dimensionless | Sect 3.2    | Explicit          | Parameter definition given      |
    | Sandpile     | Grain falling condition       | Height diff    | >= 2                  | grains        | Sect 4.3.1  | Explicit          | Rule definition provided        |

### **4.3 Global Order:**

    *   Content: Spiral Rule CA: Emergence of spiral wave glider guns emitting periodic streams of gliders, and stable stationary patterns (eaters) (Fig 2). Sandpile Model: Characteristic slopes and patterns formed by grain distribution (Fig 19a). FSS: Globally synchronized state (all cells firing simultaneously). BZ Reaction: Sustained propagating wave fragments (Fig 12a), potentially complex patterns from interactions (implicitly).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID         | Description                     | Parameter      | Value Range           | Units         | Implicit/Explicit | Justification                        | Source   |
| :-------------- | :------------------------------ | :------------- | :-------------------- | :------------ | :---------------- | :----------------------------------- | :------- |
| Spiral Rule     | CA neighbor state counts        | Neighbor States| {S, A, I}             | State         | Explicit          | Rule defined by matrix M based on counts | Sect 3.1 |
| Oregonator      | Reaction/diffusion rates        | ε, f, q, φ     | Specific values given | dimensionless | Explicit          | Parameters explicitly listed             | Sect 3.2 |
| Sandpile        | Height difference threshold     | Height diff    | >= 2                  | grains        | Explicit          | Rule explicitly described            | Sect 4.3.1 |
| FSS             | CA state transition function    | Cell States    | e.g., 8 states        | State         | Mixed             | Existence explicit, details implicit     | Sect 4.3.2 |
| AGC Collision   | Signal interaction outcome    | Meta-signals   | Finite set            | Type          | Explicit          | Concept of rules is explicit         | Sect 5   |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID        | Description                   | Parameter             | Value Range | Units       | Implicit/Explicit | Justification                            | Protocol               | Source   |
| :----------------- | :---------------------------- | :-------------------- | :---------- | :---------- | :---------------- | :--------------------------------------- | :--------------------- | :------- |
| Glider Gun         | Emits periodic gliders        | Emission Frequency    | e.g., 1/6   | gliders/step| Explicit          | Fig 2 description mentions frequency   | Visual/Simulation      | Fig 2    |
| Eater              | Stable stationary pattern     | Size/Configuration    | Fixed       | cells       | Explicit          | E3, E6 described and shown             | Visual/Simulation      | Fig 3    |
| Synchronization    | Simultaneous state change     | Firing Time           | e.g., 3n, 4n| steps       | Explicit          | Time complexities mentioned            | Theoretical Analysis   | Sect 4.3.2|
| BZ Wave Fragment   | Sustained propagating wave    | Size/Velocity/Lifetime| Stable/Const| cells/step  | Explicit          | Described as stable, lifetime given    | Simulation            | Sect 3.2 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type        | Description                     | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification                                      | Source |
    | :--------------- | :------------------------------ | :-------------:| :-----------:| :------:| :---------------- | :------------------------------------------------- | :----- |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 0

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Digital (Logic Gates, Turing Machines, CTS), Universal (Turing Machines, CTS, potentially BBM/Life), Analog (AGC encoding reals), Hybrid (Eater-glider FSM combines discrete states with continuous interactions implicitly).

### **5.3 Computational Primitive:**

    *   Content: Signal Collision resulting in: Logic gate operation (AND, OR, NOT, XOR, interaction gate, switch gate - Fig 1, 7, 13, 14), Signal Annihilation, Signal Reflection/Routing (Fig 6, 12d), Signal Transformation (glider type change - Fig 4, 5, 6), Signal Generation (glider guns - Fig 2), Signal Splitting/Fanout (Fig 12b, 12c), State Transition (Finite State Machine via eater-glider - Fig 9), Symbol Read/Write (TM simulation - Fig 24).
    *   **Sub-Type (if applicable):** Logic Gate: AND, OR, NOT, XOR, Interaction, Switch; Signal Operation: Annihilation, Reflection, Transformation, Generation, Splitting; State Machine: Transition; TM: Read/Write.

### **5.4 Embodied Computational Units**
| Unit ID          | Description                                   | Processing Power | Energy/Operation | Freq/Resp. Time                 | Bit-Depth   | Data Source          | Implicit/Explicit | Justification                                   |
| :--------------- | :-------------------------------------------- | :--------------- | :--------------- | :------------------------------ | :---------- | :------------------- |:-----------------:| :---------------------------------------------- |

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description       | Value          | Units               | Source                | Implicit/Explicit | Justification                                        |
        | :-------------------------- | :------------- | :------------------ | :-------------------- | :---------------- | :--------------------------------------------------- |
        | CA Time Step              | 1              | step/cycle          | Sect 3.1, 4         | Explicit          | Discrete nature of CA model                          |
        | BZ Simulation Time Step (dt)| 10⁻³           | time units (model)  | Sect 3.2              | Explicit          | Value given for Euler integration                    |
        | BZ Wave Interaction Time    | ~0.5 - few     | time units (model)  | Figs 13, 14 captions  | Explicit          | Time intervals between snapshots showing interaction |
        | BZ Wave Lifetime            | 4 - 10         | time units (model)  | Sect 3.2              | Explicit          | Stated range for wave persistence in simulation      |
        | Glider Gun Period (Spiral)  | 6              | steps / glider      | Fig 2 caption         | Explicit          | Stated frequency is 1 glider per 6 steps         |
        | FSS Synchronization Time    | O(n) (e.g. 3n, 4n) | steps               | Sect 4.3.2, Fig 21 cap| Explicit          | Stated time complexity solutions                   |
    *   **Note:** Provides key timescales relevant to the dynamics described in different models.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: Skip M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: Implementation of Boolean logic gates (AND, OR, NOT, XOR, interaction, switch). Universal computation (simulation of Turing machines, Cyclic Tag Systems). Finite state machine behavior (eater-glider system). Memory storage and retrieval (eater E6). Signal processing (reflection, routing, annihilation, fanout, transformation). Pattern generation (glider streams, spiral waves). Synchronization (Firing Squad Synchronization). Arithmetic (e.g., multiplication in 1D CA). Geometric computation (AGC, e.g., scaling, potentially beyond Turing computation via singularities). Prime number generation.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Behaviors are validated primarily through: 1) **Computer Simulation:** Space-time diagrams for CA (Figs 17, 19b, 20, 21, 22), simulation snapshots for Spiral Rule CA (Figs 2-6) and BZ reaction (Figs 12-14, 16), AGC diagrams (Figs 23-28). 2) **Theoretical Proofs:** Claims of Turing universality for Life (cited [11]), CTS (cited [14]), AGC (Sect 5.1), and potential for hypercomputation in AGC (Sect 5.3). 3) **Mathematical Construction:** Design of logic gates (Fig 1, 7, 13), multipliers (cited [8], Fig 22a), FSS solutions (Fig 21). Reproducibility is implied for the deterministic models. Limitations include reliance on idealized models (BBM, AGC) and simulations that may not capture all physical constraints or noise. Experimental validation in BZ lab conditions is mentioned as existing elsewhere but not detailed here.

---

#Key: [huang_self-regulation_2018]

# Self-regulation in chemical and bio-engineering materials for intelligent systems

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews the concept and applications of self-regulation in various chemical and bio-engineering materials, positioning them as components for intelligent systems. Self-regulation is defined as the ability of a material/system to regulate itself without external intervention in response to internal or environmental changes. The review covers examples across porous materials (oil/water separation via capillary action), biomaterials (bacteria-triggered disinfectant release, infection-responsive H2O2 generation), polymeric materials (controlled radical polymerization, structure control via annealing), photoelectric materials (charge self-regulation in TM compounds, tunable emission), mechanochemical systems (SMARTS for controlled reactions), and energy materials (yolk-shell structures for battery volume changes). The purpose is to highlight recent discoveries and applications, demonstrating the viability and utility of self-regulation for creating integrated intelligent components.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                      | Value                                   | Units              | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :---------------------------------- | :--------------------------------------: | :----------------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
    *   **Note:** As a review, the paper mentions many parameters specific to the cited works. This table lists a few representative examples explicitly mentioned quantitative or key qualitative parameters related to the self-regulation mechanisms discussed. Reliability is generally Medium as values are cited from other works.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Varies significantly depending on the specific self-regulating system discussed. Examples include: Chemical energy (e.g., bacterial lipases, H2O2 decomposition, battery reactions, oscillating reactions), Thermal energy (e.g., TCWVA for silk), Mechanical energy/Pressure gradients (e.g., suction in PHOMs, mechanochemical systems), Light energy (e.g., photoelectric materials, photo-cross-linking), Electrical energy (e.g., charge regulation in TM compounds, electrospinning).

### **2.2 Energy Transduction**

    *   Content: Energy transduction is central to self-regulation, converting input energy/stimuli into a regulated response. Examples:
        *   PHOMs: Mechanical energy (suction) overcomes capillary pressure -> Fluid flow (Kinetic energy).
        *   Biomaterials (Nielsen): Chemical energy (lipase activity) -> Chemical bond breaking -> Release of fungicide.
        *   Biomaterials (Tegl): Chemical energy (lysozyme activity -> COS) + Chemical energy (CDH reaction) -> Chemical energy (H2O2 production).
        *   Polymerisation (Steenbock): Thermal energy -> Radical formation/decomposition -> Chemical bond formation.
        *   Photoelectric (Raebiger): Photon energy/Electrical doping -> Electronic state change -> Charge redistribution (Negative feedback).
        *   SMARTS: Chemical energy (reaction) -> Mechanical energy (gel reconfiguration) -> Chemical energy (modulated reaction rate).
        *   Batteries (Li): Chemical potential energy -> Electrical energy + Mechanical strain energy (during charge/discharge volume change).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide quantitative data or qualitative assessments regarding the energy efficiency of the reviewed self-regulating processes. Efficiency is mentioned implicitly for LIBs/SIBs in Section 7 ("high-efficiency... energy storage systems"), but not quantified for the self-regulation aspect itself (e.g., efficiency of the yolk-shell mechanism in mitigating energy loss due to degradation).

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation mechanisms are inherent but not explicitly quantified or discussed in detail. Potential mechanisms include: Viscous dissipation (fluid flow in PHOMs), Heat generation (chemical reactions, polymerization, battery cycling, resistance in photoelectric materials), Mechanical damping/friction (SMARTS reconfiguration, battery volume changes), Irreversible side reactions (polymerization). Assessment is qualitative: likely Medium to High depending on the specific system (e.g., battery cycling involves significant heat dissipation).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes
        *   Photoelectric materials (Sec 5): The charge state (occupancy of levels) persists after doping/excitation and influences subsequent photovoltaic properties. This fits the definition of memory influencing future behavior.
        *   Mechanochemical systems (SMARTS, Sec 6): The configuration of the microstructures (bent/straight) is a state that persists and determines whether the catalytic reaction is 'on' or 'off', influencing future chemical output based on past stimuli (that caused bending/straightening).
        *   Polymer structure (Sec 4): The crystallinity/molecular chain packing state achieved via annealing or solvent tuning persists and determines properties like conductivity or mechanical response.
        *   Energy materials (Sec 7): The structural integrity maintained by the yolk-shell architecture due to accommodation of past expansion/contraction influences the capacity retention and stability in future cycles.

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable (Qualitative)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Low (Qualitative)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes
        *   Porous materials (Sec 2): The formation of desired pore structures (e.g., honeycomb scaffolds via 'breath figure' method, potentially PHOM structure) can involve self-organization driven by physical processes (evaporation, phase separation, surface tension). While the *use* of PHOMs involves applied suction (external control), the *material structure itself* might arise from self-organization. The honeycomb scaffold formation is explicitly linked to a method often associated with self-assembly ('breath figure').
        *   Biological systems (mentioned in Intro/Sec 3): The paper acknowledges self-regulation is common in biology, often involving self-organization (e.g., cell structures, biofilm formation triggering response).
        *   Polymer structure (Sec 4): Chain packing, crystallization (e.g., silk annealing) involves molecules organizing based on local interactions (intermolecular forces, thermodynamics) influenced by global conditions (temperature, solvent), aligning with self-organization principles.
        The "self-regulation" described is often a *consequence* of a structure formed potentially by self-organization, or a dynamic process involving local feedback rules that lead to a stable, regulated state (emergent dynamic order).

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: Specific local rules are implied rather than explicitly formalized.
        *   PHOMs (Capillary Action): Local interaction between liquid (oil/water/air), solid (pore walls), governed by surface tension, contact angles, and pore geometry (Laplace pressure: ΔP = 2γcosθ/r). Changes in local curvature (r) due to suction alter local pressure balance.
        *   Silk Annealing (TCWVA): Local interactions between water molecules and silk protein chains (e.g., hydrogen bonding) influenced by temperature, leading to conformational changes (α-helix to β-sheet) and crystallization. Interactions favour lower energy states at given T.
        *   TM Compound Charge Regulation: Local quantum mechanical interactions (orbital overlap, Coulomb interactions) between TM atom d-orbitals and ligand orbitals, governed by crystal field theory and QM principles. Doping changes local electron occupancy, triggering shifts in energy levels (bonding/anti-bonding) based on these interactions (negative feedback).
        *   Polymerization (Triazolinyl): Local chemical reactions involving monomer, growing polymer chain radical, and triazolinyl radical (initiation, propagation, termination, decomposition of triazolinyl). Rate constants govern interactions.
    * **Implicit/Explicit**: Implicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID                       | Description                         | Parameter Name         | Parameter Value Range | Units    | Data Source   | Implicit/Explicit | Justification                                         |
    | :---------------------------- | :---------------------------------- | :--------------------- | :-------------------- | :------- | :------------ | :---------------- | :---------------------------------------------------- |
    | Silk Annealing (TCWVA)        | Water-protein interaction control   | Temperature            | 4 to 100              | °C       | Section 4     | Explicit          | Explicitly stated range controls structure.         |

### **4.3 Global Order:**

    *   Content: The emergent global order is the self-regulated state or structure.
        *   PHOMs: Stable oil flow / separation state (dynamic equilibrium based on capillary pressures and applied suction). Or, the porous structure itself if formed via self-assembly.
        *   Silk Annealing: Specific bulk crystallinity and dominant secondary structure (Silk I or Silk II) across the material.
        *   TM Compounds: Overall charge neutrality or specific defect concentration equilibrium resulting from local charge self-regulation.
        *   Polymerization: Controlled molecular weight distribution and low polydispersity in the resulting polymer batch.
        *   SMARTS: Synchronized oscillation or stable on/off state of the reaction across the device (depending on coupling).
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |


### **4.6. Globally Emergent Order and Order Parameters**
| Property ID        | Description                         | Parameter                 | Value Range     | Units    | Implicit/Explicit | Justification                                     | Protocol                   | Source    |
| :----------------- | :---------------------------------- | :------------------------ | :-------------- | :------- | :---------------- | :------------------------------------------------ | :------------------------- | :-------- |
| Silk Structure     | Dominant secondary structure/phase  | Crystallinity (% β-sheet) | ~0 to ~60       | %        | Explicit          | Quantified outcome of TCWVA process.              | TCWVA at different Temps   | Section 4 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 0

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes (Partial/Potential)
        *   SMARTS (Sec 6): The chemo-mechanical feedback loop acts as a switch ('on'/'off' for catalysis based on microstructure state). This is a thresholding operation, a basic computational primitive, embodied in the material's configuration and reaction kinetics. The paper explicitly mentions the C1→M→C2 cascade, resembling signal transduction pathways which can perform computation.
        *   Photoelectric Charge Regulation (Sec 5): The negative feedback mechanism maintaining charge neutrality acts like a regulatory control circuit, arguably performing an analog computation (balancing charge inputs/outputs) embodied in the material's electronic structure.
        *   Biological Systems (Sec 3): Bacteria-triggered release involves sensing (lipase presence) and thresholding (sufficient lipase to trigger release). Lysozyme-responsive system also involves sensing and conditional action (H2O2 production). These are basic sense-act computations embodied in the biochemical network.

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid (Potentially Threshold Logic)

### **5.3 Computational Primitive:**

    *   Content: Thresholding / Switching / Regulation (Feedback Control)
    *   **Sub-Type (if applicable):** Thresholding (e.g., SMARTS on/off, bacterial trigger); Regulation (e.g., Charge self-regulation in TM compounds)
        *   Thresholding: Systems activate/deactivate based on a condition exceeding a threshold (lipase concentration, lysozyme activity, mechanical state in SMARTS).
        *   Regulation/Feedback: Systems adjust internal state to maintain equilibrium or follow a setpoint (charge regulation).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description                       | Value                      | Units    | Source                  | Implicit/Explicit | Justification                                                         |
        | :------------------------------------------ | :-------------------------: | :------- | :---------------------- | :---------------- | :-------------------------------------------------------------------- |
    *   **Note:** The paper mentions dynamic processes but rarely quantifies the timescales involved.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes
        *   Biomaterials adapt release rate based on bacterial presence (Sec 3).
        *   PHOMs adapt flow based on oil presence and suction (Sec 2).
        *   TM compounds adapt charge distribution based on doping (Sec 5).
        *   SMARTS adapts reaction rate based on mechanical state (Sec 6).
        *   Energy materials adapt structure (within limits) to volume changes (Sec 7).
        *   Polymers adapt structure based on annealing temp/solvent (Sec 4).
        This adaptation allows the system to maintain function or achieve a regulated state despite perturbations. It's primarily reactive adaptation rather than learning over multiple experiences to improve performance (except perhaps implicitly in battery materials surviving cycles).

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanisms are varied and based on the physics/chemistry of each system, generally involving feedback loops:
        *   **Physico-chemical feedback:** (PHOMs) Capillary pressure changes provide feedback adjusting flow resistance based on interface position/suction. (TM compounds) Change in electron occupancy triggers energy level shifts via Coulomb/quantum interactions, providing negative feedback on charge accumulation. (Batteries) Mechanical stress from volume change might implicitly affect reaction kinetics or transport, mitigated by yolk-shell structure (structural adaptation).
        *   **Biochemical feedback:** (Nielsen/Tegl) Presence of bacterial enzymes (lipase/lysozyme) acts as input signal triggering a chemical cascade (bond cleavage/H2O2 production) - an open-loop response to a specific stimulus, or closed-loop if the product inhibits bacteria, reducing the trigger.
        *   **Chemical kinetic feedback:** (Steenbock Polymerization) Concentration of stable free radicals potentially influences initiation/termination rates, creating a self-regulating cycle for radical concentration.
        *   **Chemo-mechanical feedback:** (SMARTS) Chemical reaction drives mechanical change, which in turn gates the chemical reaction.
        *   **Thermo-structural feedback:** (Silk Annealing) Temperature dictates favorable intermolecular interactions, driving the system towards a specific equilibrium structure (β-sheet content).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is **Self-Regulation**. This manifests differently depending on the system:
        *   Selective fluid transport / Separation (PHOMs)
        *   Stimulus-responsive release / Generation of active agents (Biomaterials)
        *   Controlled polymer synthesis (Polymerization)
        *   Stable charge state / Tunable optical properties (Photoelectric materials)
        *   Switchable / Oscillatory chemical reactions (SMARTS)
        *   Stable energy storage cycling / Structural integrity maintenance (Energy materials)
        *   Controlled material structure / properties (Silk annealing, PANI doping)

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

---

#Key: [elowitz_synthetic_2000]

# A synthetic oscillatory network of transcriptional regulators

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a synthetic genetic circuit, termed the "repressilator," constructed in *Escherichia coli*. It consists of three transcriptional repressor genes (lacI from E. coli, tetR from Tn10, cI from lambda phage) arranged in a negative feedback loop: LacI represses tetR transcription, TetR represses cI transcription, and CI represses lacI transcription. The purpose is to create a synthetic biological oscillator that functions independently of natural cellular clocks. The system's state is monitored by measuring the fluorescence of Green Fluorescent Protein (GFP), whose expression is driven by a promoter (P<sub>L</sub>tetO1) repressed by TetR. Components include the genes, their corresponding promoters (P<sub>L</sub>lacO1, P<sub>L</sub>tetO1, P<sub>R</sub>), plasmids (low-copy for repressilator, higher-copy for reporter), ssrA degradation tags on repressor proteins, and the *E. coli* host cell machinery. It experimentally demonstrates sustained oscillations in GFP expression in individual cells, with periods longer than the cell division time.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name          | Value       | Units   | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :---------------------- | :---------: | :------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Protein Half-life (Model) | 10          | min     | Box 1                    | Explicit          | Medium                          | Model parameter                   |
        | mRNA Half-life (Model)  | 2           | min     | Box 1                    | Explicit          | Medium                          | Model parameter                   |

    *   **Note:** Parameters listed are key characteristics of the system's behavior (period) or design model (half-lives, Hill coefficient). Experimental period is directly measured. Model parameters are explicitly stated but represent idealized simulation values, hence Medium reliability for reflecting *in vivo* reality.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the chemical energy derived from the bacterial growth medium (minimal media supplemented with glycerol and casamino acids) metabolized by the *E. coli* host cells. This provides ATP and GTP required for cellular processes including transcription, translation, and protein degradation, which drive the oscillator circuit.

### **2.2 Energy Transduction**

    *   Content: Chemical energy (ATP/GTP) from cellular metabolism is transduced into:
        1.  **Transcription:** Energy is used by RNA polymerase to synthesize mRNA molecules from the DNA templates of the repressor genes (lacI, tetR, cI) and the reporter gene (gfp).
        2.  **Translation:** Energy is used by ribosomes to synthesize repressor proteins and GFP from their respective mRNA templates.
        3.  **Degradation:** Energy (likely ATP-dependent proteases like ClpXP/ClpAP mentioned implicitly via the ssrA tag mechanism) is used to degrade the tagged repressor proteins and GFP, as well as mRNA molecules.
        The cyclical nature involves the transduction of chemical energy into the synthesis of specific proteins, which then regulate the synthesis of other proteins, creating oscillations in protein concentrations.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any information or metrics to assess the energy efficiency of the repressilator circuit (e.g., ATP consumed per oscillation cycle, or chemical energy input vs. oscillatory output). Biological systems are generally complex in terms of energy accounting. A qualitative assessment (likely Low efficiency compared to dedicated physical oscillators) would be purely speculative based on general biological knowledge, not the text.

### **2.4 Energy Dissipation**

    *   Content: Energy is dissipated primarily as heat through the metabolic processes driving transcription, translation, protein folding, protein degradation, and mRNA degradation. These are thermodynamically irreversible processes inherent to biochemical reactions within the cell. Quantifying these losses specifically for the repressilator circuit versus basal cellular metabolism is not possible from the provided text. Dissipation occurs during every synthesis and degradation event within the oscillatory cycle.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: 95 +/- 10 (Sibling decorrelation half-time)
*    Units: min

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: ~95 (half-time for sibling decorrelation)
    *   Units: min

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------------------- | :------------------------------------------- | :----------: | :------: | :---------------: | :-------------: |:-----------------:| :-----------------:|
    | Sibling Decorrelation | Rate at which sibling cell states diverge     | 95 +/- 10   | min (HT) | MemoryNode.decay | Text (Results) | Explicit          | Explicitly measured |
    | Period Variability    | Standard deviation of oscillation period     | 40          | min      | MemoryNode.noise | Text (Results) | Explicit          | Explicitly measured |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are the biochemical reactions governing the concentration of each mRNA (m<sub>i</sub>) and protein (p<sub>i</sub>) species:
        1.  **Transcription:** mRNA m<sub>i</sub> is transcribed from its corresponding promoter. The transcription rate is inhibited by the repressor protein p<sub>j</sub> (where j is the preceding node in the cycle: j=cI for i=lacI, j=lacI for i=tetR, j=tetR for i=cI). The model uses a Hill function: rate = α / (1 + p<sub>j</sub><sup>n</sup>) + α<sub>0</sub>. (α: max rate, α<sub>0</sub>: leakiness, n: Hill coefficient).
        2.  **mRNA Degradation:** mRNA m<sub>i</sub> decays at a constant rate (modeled implicitly with lifetime in equations dm<sub>i</sub>/dt = ... - m<sub>i</sub>, assuming time is scaled by mRNA lifetime).
        3.  **Translation:** Protein p<sub>i</sub> is translated from mRNA m<sub>i</sub> at a rate proportional to m<sub>i</sub> (rate = β * m<sub>i</sub>, where β is translation efficiency scaled into m<sub>i</sub> units in the simple model, or explicitly β in dp<sub>i</sub>/dt = β*m<sub>i</sub> - p<sub>i</sub>).
        4.  **Protein Degradation:** Protein p<sub>i</sub> decays at a constant rate (scaled by β relative to mRNA decay in the simple model `β` in dm/dt eq, or explicit rate `1` in dp/dt eq, assuming protein lifetime scaling). The ssrA tag enhances this rate.
        These rules operate locally at the level of individual molecules and genes within each cell.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID       | Description                | Parameter Name | Parameter Value Range | Units   | Data Source | Implicit/Explicit | Justification                             |
    | :------------ | :------------------------- | :------------- | :--------------------: | :------: | :----------: | :----------------: | :---------------------------------------- |
    | Transcription | Leakiness                  | α<sub>0</sub>    | ~0, 10<sup>-3</sup>       | ratio/dim'less | Box 1, Fig 1b | Explicit          | Model parameter, ratio to max rate (α)  |
    | Transcription | Max Rate (rescaled)        | α              | ~a (see Fig 1b)       | ratio/dim'less | Box 1, Fig 1b | Explicit          | Model parameter                           |
    | Degradation   | Protein/mRNA decay ratio | β              | ~b (see Fig 1b)       | ratio/dim'less | Box 1, Fig 1b | Explicit          | Model parameter                           |
    | Degradation   | mRNA lifetime              | ~2             | min                   | Box 1         | Explicit          | Value used in stochastic simulation model |
    | Degradation   | Protein lifetime           | ~10            | min                   | Box 1         | Explicit          | Value used in stochastic simulation model |

### **4.3 Global Order:**

    *   Content: The emergent global order is sustained temporal oscillation in the concentrations of the three repressor proteins (and consequently, the GFP reporter). This order is characterized by a relatively consistent period (mean ~160 min) but with significant noise and variability in amplitude and phase. It's a dynamic, non-equilibrium pattern.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID       | Description        | Parameter       | Value Range           | Units     | Implicit/Explicit | Justification                       | Source        |
| :------------ | :----------------- | :-------------- | :--------------------: | :--------: | :----------------: | :---------------------------------- | :------------ |
| Repression    | Protein binding    | n (Hill Coeff)  | 2, 2.1                | unitless  | Explicit          | Model parameter                    | Box 1, Fig 1b |
| Repression    | Leakiness          | α₀             | ~ 0, 10⁻³              | unitless  | Explicit          | Model parameter (ratio to max α) | Box 1, Fig 1b |
| Degradation   | Relative lifetime  | β               | variable (see Fig 1b) | unitless  | Explicit          | Model parameter ratio Prot/mRNA     | Box 1, Fig 1b |
| Degradation   | mRNA half-life     | τ<sub>mRNA</sub>      | ~2                    | min       | Explicit          | Used in stochastic model           | Box 1         |
| Degradation   | Protein half-life  | τ<sub>protein</sub>   | ~10                   | min       | Explicit          | Used in stochastic model           | Box 1         |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID   | Description                     | Parameter               | Value Range   | Units   | Implicit/Explicit | Justification                                | Protocol                                     | Source        |
| :------------ | :------------------------------ | :---------------------- | :------------: | :------: | :----------------: | :------------------------------------------- | :------------------------------------------- | :------------ |
| Oscillation   | Temporal Periodicity            | Period (Mean ± SD)      | 160 ± 40      | min     | Explicit          | Measured peak-to-peak interval distribution  | Time-lapse microscopy, manual tracking, FFT | Text (Results) |
| Oscillation   | State Persistence               | Sibling Decorr. HT      | 95 ± 10       | min     | Explicit          | Measured divergence of sibling fluorescence  | Time-lapse microscopy, sibling comparison    | Text (Results) |
| Network State | Stability (Deterministic Model) | Stability Boundary      | See Fig 1b    | unitless | Explicit          | Calculated from model equations             | Linear stability analysis (Box 1)           | Fig 1b, Box 1 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type                 | Description                                     | Predictability | Yoneda Score | Metrics                                 | Implicit/Explicit | Justification                                                              | Source        |
    | :------------------------ | :---------------------------------------------- | :------------- | :----------- | :-------------------------------------- | :----------------: | :------------------------------------------------------------------------- | :------------ |
    *   **Metrics:** Predictability assessed based on model vs. experiment agreement and observed noise/variability (Period SD, Sibling Decorrelation Time).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operation embodied is signal inversion with a time delay and thresholding, implemented by each repression step (e.g., high LacI -> low TetR transcription after a delay). The combination of three such delayed inversions in a loop results in the higher-level function of Oscillation. The Hill function (α / (1 + p<sub>j</sub><sup>n</sup>) + α<sub>0</sub>) represents the thresholded inversion.
    *   **Sub-Type (if applicable):** Thresholded Signal Inversion / Delayed Negative Feedback.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description           | Value     | Units | Source         | Implicit/Explicit | Justification                                   |
        | :------------------------------ | :--------: | :----: | :------------- | :----------------: | :---------------------------------------------- |
        | Oscillation Period (Mean)       | 160       | min   | Text (Results) | Explicit          | Directly measured mean peak-to-peak interval    |
        | mRNA Half-life (Model / Typical) | ~2        | min   | Box 1 / Text   | Explicit          | Stated model parameter and typical E. coli value |
        | Protein Half-life (Model)       | ~10       | min   | Box 1          | Explicit          | Stated model parameter for tagged proteins      |
        | Protein Half-life (GFP)         | ~30-40    | min   | Text (Intro)   | Explicit          | Cited reference value for tagged GFP variant   |
        | Cell Division Time (Typical)    | 50-70     | min   | Text (Results) | Explicit          | Stated typical time under experimental conditions|
        | Sibling Decorrelation Half-time | 95 ± 10   | min   | Text (Results) | Explicit          | Measured timescale for state divergence        |
        | GFP Half-Life (Reporter Plasmid)| ~90       | min   | Methods        | Explicit          | Stated half-life of GFPaav variant used       |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behavior is sustained temporal oscillation of protein concentrations (specifically, the GFP reporter concentration is measured as the output). This oscillatory behavior emerges from the designed negative feedback loop.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The emergence of oscillations is validated through:
        1.  **Direct Observation:** Time-lapse fluorescence microscopy of individual cells growing into microcolonies, tracking GFP intensity over time (Fig 2, Methods).
        2.  **Quantitative Analysis:** Manual tracking of cell lineages, measurement of fluorescence intensity, calculation of peak-to-peak intervals to determine period distribution (mean 160 ± 40 min), and calculation of sibling decorrelation time (95 ± 10 min) (Text-Results, Methods). Fourier analysis was used to classify cells as oscillatory (≥40% met criterion) (Methods).
        3.  **Control Experiments:** Demonstrating disruption of oscillations with IPTG (Fig 3e) and absence of oscillations with reporter plasmid alone (Fig 3f) or other control constructs (Methods).
        4.  **Theoretical Modeling:** Use of deterministic and stochastic models (Box 1, Fig 1b/c) showing that the network topology can produce oscillations under plausible parameter regimes, supporting the design principle.
        *Limitations:* Observation time limited (~10 hours), potential artifacts from 2D microcolony growth, manual tracking limitations, arbitrary threshold for FFT classification. Reproducibility demonstrated across three microcolonies.

---

#Key: [ke_three-dimensional_2012]

# Three-Dimensional Structures Self-Assembled from DNA Bricks

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system utilizes short, synthetic DNA strands called "DNA bricks" to self-assemble complex three-dimensional (3D) nanostructures. The primary component is a 32-nucleotide (nt) single-stranded DNA designed with four consecutive 8-nt domains. These bricks interact through programmed Watson-Crick base pairing between complementary domains of neighboring bricks during a one-step thermal annealing process. Each brick binds to four local neighbors, forming a voxel (8 base pairs, ~2.5x2.5x2.7 nm). The purpose is to create arbitrary, finite-size 3D shapes with high complexity, including surface features, internal cavities, and tunnels, by selecting specific subsets of bricks from a predefined "molecular canvas" (e.g., a 10x10x10 voxel cuboid). System components include full 32-nt bricks, 16-nt half-bricks (boundary), protector bricks (with poly-T ends to mitigate aggregation), and merged 48-nt boundary bricks. The system demonstrates modularity, as bricks can be added or removed independently in the design phase to create different shapes from a master collection of strands.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name             | Value             | Units   | Source (Fig/Table/Section)   | Implicit/Explicit   | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit)   |
        | :------------------------- | :---------------: | :------: | :-------------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Voxel Dimensions           | 2.5 x 2.5 x 2.7   | nm      | Abstract, Fig. 1F, Design   | Explicit            | Medium (Measured/Designed)      | TEM/AFM measurements, design model |

    *   **Note:** Values for optimal conditions are explicitly stated as yielding the best results in the characterization section (referencing supplementary figures). Voxel dimensions combine design intent and experimental measurement.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input is thermal energy supplied during the one-step thermal annealing process. This involves ramping down the temperature over an extended period (e.g., 72 hours) to allow DNA strands to find their thermodynamically favorable, complementary binding partners and fold into the target structure.

### **2.2 Energy Transduction**

    *   Content: Thermal energy drives the system dynamics. At higher temperatures, DNA strands have high kinetic energy, preventing stable hybridization. As the temperature decreases (annealing), thermal energy is transduced into the potential energy landscape favoring specific hybridization events between complementary 8-base domains on different DNA bricks. This binding energy release drives the formation of double helices and the ordered assembly of bricks into the target 3D structure. The energy allows exploration of conformational space and overcoming kinetic barriers to reach the designed low-energy state.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper reports highly variable gel yields (estimated monomer incorporation ratio), ranging from <1% to ~80%, with complex structures often showing lower yields (e.g., 4% for the 6H x 10H x 128B cuboid initially). This suggests that a significant fraction of the input strands (material) does not incorporate into the desired final structure under optimal conditions found, implying low thermodynamic or kinetic efficiency in reaching the target state compared to off-pathway products or unassembled components. The efficiency is assessed qualitatively as Low to Medium, depending heavily on the specific structure's size and complexity. No direct energy efficiency metric (e.g., energy input vs. binding energy of final structure) is provided.

### **2.4 Energy Dissipation**

    *   Content: The primary energy dissipation mechanism is heat loss to the surroundings during the slow cooling (annealing) process. Energy released during DNA hybridization (exothermic process) is dissipated as heat. Entropy also increases in the surrounding solvent. Unwanted side reactions or formation of partially assembled structures also represent dissipative pathways that trap energy in non-target states. Quantification is not provided, but given the slow annealing over days in a thermal bath, heat dissipation to the environment is the dominant mechanism. Qualitative assessment: High.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rule is Watson-Crick base pairing (hybridization) between complementary 8-nucleotide sequences. Specifically, an 8-nt "tail" domain (domain 1 or 4) of one brick binds to a complementary 8-nt "head" domain (domain 2 or 3) of a neighboring brick. Each brick is designed to bind specifically to four local neighbors in a pre-defined relative orientation (forming ~90° angles between connected bricks based on DNA helix geometry over 8 bp). Sequence specificity dictates that each brick binds only to its intended neighbors at its designated position within the target structure. Protector bricks (poly-T domains) are designed to prevent non-specific blunt-end stacking or undesired interactions at boundaries. Merged 48-nt strands enforce specific boundary conditions.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID         | Description         | Parameter Name          | Parameter Value Range | Units   | Data Source        | Implicit/Explicit   | Justification                      |
    | :-------------- | :------------------ | :---------------------- | :-------------------- | :------: | :-----------------: | :----------------: | :--------------------------------- |
    | Hybridization   | Head-Tail Binding   | Domain Length           | 8                     | nt/bp   | Fig 1A, Abstract    | Explicit            | Stated as design principle.        |
    | Geometry        | Inter-brick Angle   | Dihedral Angle          | ~90                   | degrees | Fig 1B, Design     | Explicit            | Derived from 8bp DNA twist.      |
    | Stability       | Boundary Nucleation | Merged Strand Length    | 48                    | nt      | Boundary Bricks Section | Explicit | Designed to improve yield.     |

### **4.3 Global Order:**

    *   Content: The globally ordered structures are finite-size, discrete 3D shapes with precisely defined geometries at the nanoscale. These include cuboids of various sizes and aspect ratios, as well as 102 custom shapes derived from a 10x10x10 voxel canvas, exhibiting features like complex surfaces, patterns (letters, numbers), enclosed cavities, and intricate tunnels. Alternative global orders like single-layer 2D rectangles, honeycomb lattices, and hexagonal lattices were also demonstrated using modified brick designs or rules.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID       | Description                                   | Parameter                 | Value Range | Units   | Implicit/Explicit | Justification                             | Source                 |
| :------------ | :-------------------------------------------- | :------------------------ | :---------- | :------: | :----------------: | :---------------------------------------- | :--------------------- |
| Geom R1       | Relative orientation between bound bricks   | Dihedral Angle            | ~90         | degrees | Explicit          | Dictated by 8bp DNA twist.              | Design section, Fig 1B |
| Condition R1  | Favorable hybridization conditions          | MgCl2 Concentration       | 10-80       | mM      | Explicit          | Tested range for optimal yield.       | Fig S14, Assembly sec|

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID   | Description                      | Parameter          | Value Range                        | Units      | Implicit/Explicit | Justification                                    | Protocol                 | Source          |
| :------------ | :------------------------------- | :----------------- | :--------------------------------- | :--------- | :----------------: | :----------------------------------------------- | :----------------------- | :-------------- |
| Shape P1      | Overall structure geometry       | Shape Class        | Cuboid, Custom, Lattice, etc.      | Category   | Explicit          | Designed and observed shapes.                   | Design, TEM, AFM         | Figs 2, 3, 4    |
| Shape P2      | Structure dimensions             | Length, Width, Height | e.g., 13x22x29 (HC), varies widely | nm         | Explicit          | Measured from TEM/AFM images.                   | TEM, AFM Measurement     | Figs 2, S28, 4  |
| Fidelity P1   | Correctness of assembly         | Gel Yield          | <1 - 80                            | %          | Explicit          | Quantifies strand incorporation into product band. | Gel Electrophoresis      | Figs S20C, S28  |
| Fidelity P2   | Intactness of structures         | % Intact Particles | ~55 (for 6Hx10Hx128B), varies      | %          | Explicit          | Visual inspection of TEM images.                | TEM Image Analysis       | Fig S16         |
| Lattice P1    | Lattice parameter (if periodic) | Helix spacing      | ~2.5                               | nm         | Explicit          | Measured/Designed for square lattice.           | TEM/AFM/Design           | Fig 2C          |
| Lattice P2    | Lattice parameter (if periodic) | Bp per helical turn| 10.67 (3D), 10.5 (2D), 10.8 (HC/HL) | bp/turn    | Explicit          | Design parameters for different lattices.       | Design Section, Fig 4    | Design details  |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type         | Description                                | Predictability   | Yoneda Score | Metrics                        | Implicit/Explicit | Justification                                       | Source          |
    | :---------------- | :----------------------------------------- | :--------------- | :----------- | :----------------------------- | :----------------: | :-------------------------------------------------- | :-------------- |
    | Local-to-Global | Mapping local binding rules to final shape | High (if formed) | 1            | Shape Fidelity, Dimensional Accuracy, Yield | Mixed            | The design directly dictates the global structure via local rules. If assembly works, the mapping is faithful. The complexity lies in achieving assembly, not in an emergent, unpredictable mapping. Yoneda concept not explicitly applicable/invoked. | Design, Figs 2-4 |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 1. (Rubric: Score reflects the complexity and non-triviality of the local-to-global mapping captured by the Yoneda lemma concept. 0 = No mapping/irrelevant. 1 = Direct/designed mapping. 5 = Some emergent complexity in mapping. 10 = Highly complex, non-obvious emergent mapping from local rules). The DNA brick system represents a highly programmed system where the global structure is explicitly encoded by the sum of local interactions; the mapping is direct by design, not emergent in the complex sense implied by invoking Yoneda.
    *   **Metrics:** Yield (probabilistic success of mapping), TEM/AFM (structural verification of mapping).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description           | Value       | Units      | Source                  | Implicit/Explicit   | Justification                                     |
        | :------------------------------ | :---------: | :--------: | :----------------------: | :-----------------: | :------------------------------------------------ |
        | Thermal Annealing Duration      | 24, 72      | hours      | Assembly sec, Fig S14     | Explicit            | Tested annealing times.                           |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is the bottom-up self-assembly of hundreds of unique DNA strands into precisely defined, complex, finite-size 3D nanostructures according to a pre-programmed design. This includes the formation of basic shapes like cuboids, complex arbitrary shapes with internal features (cavities, tunnels) and surface patterns (letters, numbers), and structures with different lattice geometries (square, honeycomb, hexagonal).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation relies primarily on experimental characterization:
        1.  **Agarose Gel Electrophoresis:** Used to assess assembly success (presence of a distinct product band near the expected molecular weight) and estimate yield (mass of product band relative to total strands). Controls often involve comparing different conditions or designs (Figs 2B, S14, S20, S36, S37).
        2.  **Transmission Electron Microscopy (TEM):** Used to directly visualize the assembled nanostructures, confirm their morphology matches the design, measure dimensions, and assess structural integrity (% intact particles). Multiple projection views are often analyzed (Figs 2C, 2D, 3E, 4B, 4F, 4I, S16, S21-S27, S38-S54).
        3.  **Atomic Force Microscopy (AFM):** Used for characterizing 2D structures (Fig 4C), providing dimensional measurements.
        * Limitations: Gel yield is an estimate of monomer incorporation, not necessarily functional yield. TEM provides 2D projections of 3D objects, and visualization of fine internal features can be difficult. Assessment of "% intact" can be subjective. Some subtle features were not resolved (fig S55B discussion). Reproducibility across labs is not demonstrated within the paper.

---

#Key: [ziepke_acoustic_2024]

# Acoustic signaling enables collective perception and control in active matter systems

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of N self-propelled polar agents (swarmers) moving in a 2D plane. Each swarmer has an internal oscillator emitting acoustic waves into 3D space, acting as both emitter and detector. Swarmers interact locally via polar alignment and short-range repulsion. They interact globally via the collective acoustic field: they synchronize their internal oscillators to the field and align their motion towards regions of higher sound amplitude. The purpose is to study how wave-based communication (acoustic signaling) enables self-organization, emergent collective behaviors (blobs, larvae, snakes, rings, volvoxes), and functionalities like collective sensing, robustness (self-healing, passing constrictions), and external control in active matter systems. Components include the swarmers (position, orientation vector, complex oscillator state), the acoustic field (governed by the wave equation with swarmers as sources), and the environment (2D habitat plane within 3D space for sound). Both agent-based (discrete) and continuum field theory models are used for investigation.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value             | Units          | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---------------: | :------------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
    *   **Note:** Units for `Ξ` (acoustic susceptibility) and `λ` (acoustic coupling) are not explicitly defined in terms of fundamental units within the excerpt but are dimensionless combinations arising from the equations. Values are taken directly from the provided tables for simulations. Data Reliability is High as these are input parameters for the theoretical/computational model.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Energy is input at the level of individual agents (swarmers). This allows self-propulsion (persistent motion at speed `v0`) and sustains the internal oscillations (Stuart-Landau oscillator dynamics). The paper refers to active matter consuming energy (Sec V: "constant input of energy at the level of individual agents"). This is characteristic of active matter systems.

### **2.2 Energy Transduction**

    *   Content: The primary energy transformations are:
        1.  Internal energy source -> Kinetic energy of directed motion (self-propulsion). The mechanism is abstracted by the parameter `v0`.
        2.  Internal energy source -> Energy stored in the internal oscillator (maintaining limit cycle oscillations, amplitude `|a|`).
        3.  Oscillator energy -> Acoustic wave energy (emission into the 3D medium, governed by Eq 1d and 3c). The mechanism involves the oscillator state `a` acting as a source term in the wave equation.

### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**

    *   Content: Dissipation mechanisms are not explicitly quantified but are inherent in the model:
        1.  Viscous drag/friction implicitly limits agent speed (`v0` is constant, implying a balance between driving and dissipation). Mentioned implicitly via comparison to biological systems like actin motility assays (Sec V). (Qualitative: High).
        2.  Energy loss during acoustic wave propagation in the medium (spreading in 3D, though absorption is neglected in App A). (Qualitative: Medium/High depending on system size/geometry).
        3.  Internal oscillator dynamics may involve dissipative terms (inherent in Stuart-Landau model, but not detailed). (Qualitative: Medium).
        4.  Inelastic interactions (collisions/repulsion `f_lj`) between agents. (Qualitative: Low/Medium).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*    Units: Qualitative Descriptor: "Short-term" / Time required for recovery.

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Low (qualitative)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Passing Fraction | Fraction of agents passing constriction (robustness measure) | Varies (see Fig 7a) | % | Relates to `MemoryNode` robustness| Fig 7a (right panel) | Explicit | Explicitly plotted metric related to snake integrity/memory. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are defined by the governing equations:
        1.  **Motion & Repulsion (Eq 1a):** Agents move with velocity `v0` along orientation `n_l`, subject to repulsive forces `f_lj` from neighbors `j` within distance `2rp`. `dr_l/dt = v0 * n_l + sum(f_lj)`. (Continuum: Advection term `-v0 * partial_i(p_i)` in Eq 3a, Pressure term `P'(rho)` in Eq 3b).
        2.  **Polar Alignment (Eq 1b):** Agents align orientation `phi_l` with neighbors `j` within radius `rc` at rate `Γ`. `d(phi_l)/dt = -Γ * sum(sin(phi_l - phi_j) / |r_l - r_j|) + ...`. (Continuum: Alignment terms `sigma(rho-1)pi`, `-delta p_j p_j p_i`, elastic term `kappa*nabla^2 p_i`, self-advection `chi p_j partial_j p_i` in Eq 3b).
        3.  **Acoustic Gradient Alignment (Eq 1b):** Agents align orientation `phi_l` towards higher sound amplitude `|u|` at rate `Ξ`. `d(phi_l)/dt = ... + Ξ * sin(phi_s - phi_l) + xi_l`, where `phi_s = angle(nabla |u|)`. (Continuum: `rho * Ξ * partial_i |u|^2` term in Eq 3b).
        4.  **Oscillator Dynamics & Coupling (Eq 1c):** Agent's internal oscillator state `a_l` follows Stuart-Landau dynamics influenced by the local acoustic field `u(r_l, t)` at rate `λ`. `d(a_l)/dt = (1+iω)a_l - (1+ib)|a_l|^2*a_l + λ*u(r_l,t)`. (Continuum: Eq 3d includes diffusion and advection).
        5.  **Acoustic Field Generation (Eq 1d):** The collective acoustic field `u` is generated by all oscillators `a_j` acting as sources in the wave equation. `(1/c^2) * partial_t^2 u = nabla^2 u + sum(w(r-r_j) * a_j * delta(z))`. (Continuum: Eq 3c uses density `rho*a`).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :-------------------- | :---- | :---------- | :---------------- | :------------ |
    | 1 | Repulsion | `rp` | 0.25 | [Length] | Table I | Explicit | Defined agent radius. |
    | 2 | Polar Alignment | `rc` | 4*rp = 1 | [Length] | Eq 1b, Table I | Explicit | Defined interaction radius. |
    | 2 | Polar Alignment | `Γ` | 0.1 - 0.2 | [Time]^-1 | Table I | Explicit | Alignment rate parameter. |
    | 3 | Acoustic Alignment | `Ξ` | 0.01 - 5000 | [?] | Tables I, II | Explicit | Acoustic susceptibility parameter. |
    | 4 | Oscillator Coupling | `λ` | 1e-5 - 1000 | [?] | Tables I, II | Explicit | Oscillator-field coupling rate. |
    | 4 | Oscillator Dynamics | `ω` | 0.1 - 0.5 | [Frequency] | Tables I, II | Explicit | Oscillator base frequency. |
    | 4 | Oscillator Dynamics | `b` | 0.05 - 0.5 | [?] | Tables I, II | Explicit | Oscillator non-linear frequency coupling. |
    | 5 | Field Generation | `c` | 5 - 50 | [Length]/[Time] | Tables I, II | Explicit | Speed of sound parameter. |

### **4.3 Global Order:**

    *   Content: The emergent global order includes various distinct collective structures/phenotypes:
        1.  **Blobs:** Localized aggregates, often circular, with a central polar defect and synchronized oscillators (potentially target waves).
        2.  **Larvae:** Elongated, slowly migrating aggregates with an asymmetrically positioned polar defect (pacemaker) emitting phase waves.
        3.  **Snakes:** Rapidly moving, elongated structures without internal polar defects, exhibiting phase waves along the body and high persistence.
        4.  **Ouroboroi:** Closed, rotating ring-like structures formed from larvae, with propagating phase waves.
        5.  **Volvoxes:** Large blobs with a synchronized core and outer layers exhibiting desynchronized or metachronal waves (chimera-like states).
        These states are characterized by morphology, motility, internal oscillator phase patterns, and defect structure. (Described in Sec III.A, Fig 2). Large-scale synchronization patterns spanning multiple clusters are also observed in continuum simulations (Sec III.B, Fig 5).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| M4.2.1 | See Section M4.2.1 | See Section M4.2.1 | See Section M4.2.1 | See Section M4.2.1 | Explicit | Parameters defining local interactions are explicitly given. | Tables I, II |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Motility | Collective speed | Collective Velocity | 0 - ~`v0` | [Length]/[Time] | Explicit/Mixed | Zero for blobs, low for larvae (~0.2*v0, Fig 7b), high for snakes (~v0). | Simulation Measurement | Sec III, Fig 7b |
| Synchronization | Oscillator phase coherence | Phase Distribution / Avg. Freq. | Varies | Hz | Explicit | Spectrograms show frequency peaks/spread. | Fourier Analysis | Fig 2, Fig 4 |
| Cluster Size/Number | Size/count of aggregates | Aggregate Size / N_cluster | Varies | [Length] / Count | Explicit | Discussed in coarsening (Fig 5). | Simulation Measurement | Sec III.B, Fig 5 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | Agent-based -> Collective States | Mapping local agent rules (Eq 1) to emergent structures (Fig 2) | Medium (Fig 2a) | 6 | Phase Diagram (Fig 2a), Structure Type, Acoustic Signature (Fig 2) | Explicit | Phase diagram shows mapping, but stochasticity exists. Yoneda score reflects ability to predict global state from local rules partially. | Sec III.A, Fig 2 |
    | Continuum -> Collective States | Mapping continuum fields (Eq 3) to emergent structures (Fig 3) | Medium | 5 | Structure Type (Blob, Snake), Acoustic Signature (Fig 4) | Explicit | Captures blobs and snakes but misses larvae, indicating imperfect mapping. | Sec III.B, Fig 3, Fig 4 |
    | Continuum Coarsening | Mapping local model to large-scale dynamics (Fig 5) | High (Qualitative) | 7 | Cluster Number N(t), Synchronization Patterns | Explicit/Mixed | Shows accelerated coarsening and saturation, qualitatively predicted, suggesting reasonable mapping. | Sec III.B, Fig 5 |

    *   **Yoneda Embedding Fulfillment Score [0-10]:** 6 (Overall assessment. The models capture the emergence of distinct global states from local rules fairly well, especially the agent-based model via the phase diagram. However, discrepancies exist (e.g., continuum model missing larvae) and the precise quantitative mapping beyond structure type is complex, indicating the functor (mapping) isn't perfectly faithful or complete.)
    *   **Metrics:** Phase diagrams relating parameters to structure type (Fig 2a), comparison of emergent structures between models (Sec III.A vs III.B), analysis of large-scale dynamics like coarsening rates (Fig 5).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid

### **5.3 Computational Primitive:**

    *   Content: The basic computational operations performed by the material/agents include:
        1.  **Gradient Ascent/Following:** Agents align towards `∇|u|` (Eq 1b, continuum Eq 3b), effectively computing and following the direction of steepest ascent in sound amplitude.
        2.  **Phase Synchronization (Phase Locking):** Oscillators adjust their phase and frequency based on the collective field `u` (Eq 1c, continuum Eq 3d), performing distributed synchronization akin to Kuramoto oscillators, but mediated dynamically by the acoustic field.
        3.  **Signal Integration/Averaging:** The acoustic field `u` itself represents a spatially and temporally integrated sum of emissions from all agents (Eq 1d, 3c, Solved in Eq 2/A15), performing a type of collective signal integration.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Acoustic Propagation | L/c (very short) | [Time] | Eq 1d, `c` value | Mixed | Sound speed `c` is large (5-50) compared to `v0`, quasi-static approx. used (App A), implying fast signal propagation relative to agent motion. Explicit `c`, implicit timescale comparison. |
        | Agent Reorientation (Acoustic) | ~1/Ξ | [Time] | Eq 1b | Implicit | Inferred from alignment term rate constant Ξ. Varies widely. |
        | Agent Reorientation (Polar) | ~1/Γ | [Time] | Eq 1b | Implicit | Inferred from alignment term rate constant Γ (0.1-0.2). |
        | Agent Oscillation Period | ~2π/ω_eff | [Time] | Eq 1c, Fig 2 freq. | Mixed | Base freq ω given. Effective freq ω_eff emerges (Fig 2), often higher (e.g., 1.8 Hz vs ~0.07 Hz baseline). Explicit ω, Explicit emergent ω_eff. |
        | Agent Single Step (Sim) | dt | [Time] | Sec II.A | Explicit | Simulation timestep mentioned qualitatively. |
        | Structure Formation/Coarsening | 100s - 1000s | [Time] | Fig 3, Fig 5 | Explicit | Observed directly from simulation snapshots/time axes. |
        | Collective Response (Sensing/Control) | 10s - 100s | [Time] | Fig 6, Fig 7 | Explicit | Observed directly from simulation time axes during events. |
        | Memory Recovery | 10s - 100s | [Time] | Fig 7a, 7b | Explicit | Estimated from simulation time axes during recovery events. |
    *   **Note:** Specific numerical values depend heavily on chosen parameters. Coarsening/Response/Recovery times are estimates from figures' time axes. Units are relative simulation time units unless specified.

### **6.2 Active Inference:**

    *   Content: Partial
        1.  **Prediction/Internal Model:** Agents' dynamics (especially oscillator phase) are influenced by the expected collective acoustic field. Synchronization implies a form of local prediction/anticipation of neighbors' states mediated by the field. The collective structures themselves might represent a stable "model" of interaction. (Implicit/Partial Evidence).
        2.  **Action Selection:** Agents actively change their orientation (`phi_l`) based on the acoustic gradient (Eq 1b) to move towards higher amplitude regions, which often correspond to synchronized or structured states. This action aims to maintain coupling/cohesion, potentially interpretable as minimizing a form of prediction error or surprise related to the acoustic environment they expect/prefer. Response to reflections (Fig 6) involves changing behavior (morphology) based on altered sensory input, suggesting action to account for environmental changes. (Implicit/Partial Evidence).
        3.  **Model Update:** Adaptation/learning isn't framed as updating an explicit internal model in the paper. Shape recovery (Fig 7) restores a previous state rather than learning a new one based on prediction error.
        Overall: Agents act based on sensory input (acoustic field) potentially to maintain a preferred state (synchronized, aggregated), but the framework isn't explicitly cast as minimizing free energy or prediction error based on a generative model.
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** Information flow rates between agent states and acoustic field; Correlation between agent actions (reorientation) and reduction in local phase/amplitude discrepancies; Timescale of response to unexpected field perturbations (e.g., external source, reflection); Complexity/stability of emergent structures as a proxy for model evidence.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes
        1.  **Phenotype Selection:** The system settles into different collective states (blobs, snakes etc.) depending on parameters like `v0` and `Ξ` (Fig 2a), showing a form of adaptation to system-level conditions.
        2.  **Response to Environment:** Larva and blob structures change morphology (disassemble, shed agents) in response to an approaching object (reflected signals, Fig 6), adapting the collective structure to the perceived environmental change.
        3.  **Robustness/Recovery:** Snakes adapt their shape to pass through narrow constrictions (Fig 7a) and then recover their form. Larvae recover their structure and function after significant perturbation (defect destruction, Fig 7b). This self-healing/recovery is a form of adaptation ensuring functional persistence.
        These changes are persistent alterations in structure/behavior driven by interaction with the environment or internal dynamics, going beyond simple, immediate stimulus-response.

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism of adaptation is primarily emergent self-organization guided by the physical interaction rules and feedback via the acoustic field. It is not a pre-programmed learning rule like Hebbian learning or explicit reinforcement learning.
        1.  **Environmental Adaptation (Fig 6):** Reflected acoustic signals alter the input `u` to agents' oscillators (Eq 1c) and the gradient field they align with (Eq 1b). This change in local input, processed through the existing dynamics, leads to changes in synchronization and motion, resulting in the observed morphological shifts (destabilization, agent shedding). The system adapts by reconfiguring itself according to the modified acoustic landscape.
        2.  **Structural Adaptation (Fig 7a, 7b):** When perturbed (collision, defect destruction), the agents continue to follow their local rules (alignment, repulsion, acoustic response). The interplay of these rules within the perturbed configuration drives the system back towards a stable attractor state (the original snake/larva phenotype or a related stable state), demonstrating self-healing/shape recovery.
        The adaptation arises from the inherent dynamics and stability properties of the self-organized system operating under varying conditions or after perturbation.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The system exhibits several distinct emergent collective behaviors:
        1.  **Aggregation/Structure Formation:** Spontaneous formation of stable or dynamic aggregates (Blobs, Larvae, Snakes, Ouroboroi, Volvoxes) with specific morphologies and internal dynamics (Sec III).
        2.  **Collective Motility:** Coordinated movement of aggregates (Larvae, Snakes) with varying speeds and persistence (Sec III).
        3.  **Synchronization:** Synchronization of internal oscillators within aggregates, leading to coherent acoustic emission and specific frequency signatures (Sec III, Fig 2, Fig 4).
        4.  **Collective Sensing:** Ability of aggregates (Larvae, Blobs) to detect and respond (change morphology) to external objects via reflected acoustic waves (Sec IV.A, Fig 6).
        5.  **Robustness/Self-Healing:** Ability of aggregates (Snakes, Larvae) to maintain or recover their structure and function after significant perturbations or deformations (e.g., passing constrictions, defect destruction) (Sec IV.B, Fig 7a, 7b).
        6.  **Inter-aggregate Interaction:** Distance regulation between aggregates (Volvoxes) mediated by the interference of their emitted acoustic waves (Sec IV.C, Fig 7c).
        7.  **Response to External Control:** Ability of aggregates (Snakes) to be captured, transported, and released using external acoustic fields (Sec IV.D, Fig 7d).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation is primarily through computational simulations:
        1.  **Agent-Based Simulations (ABS):** Direct numerical integration of stochastic equations (Eq 1) demonstrates the emergence and dynamics of collective states (Blobs, Larvae, Snakes etc.) from individual agent rules (Sec III.A, Fig 2, Fig 6, Fig 7). Parameter sweeps generate phase diagrams (Fig 2a) confirming parameter dependence.
        2.  **Continuum Field Simulations (CFS):** Numerical solution of deterministic partial differential equations (Eq 3) shows emergence of similar structures (Blobs, Snakes) and large-scale dynamics (coarsening) (Sec III.B, Fig 3, Fig 4, Fig 5).
        3.  **Comparison:** Comparison between ABS and CFS results provides cross-validation for shared phenomena (Blobs, Snakes), while discrepancies highlight limitations (e.g., CFS missing Larvae) (Sec III.B).
        4.  **Acoustic Signatures:** Analysis of frequency spectra and signal amplitudes from simulations provides quantitative characterization linked to emergent states (Fig 2, Fig 4).
        5.  **Perturbation Studies:** Specific simulations applying perturbations (object reflection, defect removal, constrictions, external control fields) validate claims of sensing, robustness, and control (Sec IV, Fig 6, Fig 7).
        Limitations: Lack of experimental validation. Validation relies solely on computational models and their internal consistency. Robustness/reproducibility demonstrated within simulation framework by showing consistent structure formation in parameter regimes (Fig 2a).

---

#Key: [ceron_programmable_2023]

# Programmable self-organization of heterogeneous microrobot collectives

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of a collective of heterogeneous microrobots (circular ferromagnetic microdisks of varying radii, typically 2-3 discrete sizes) operating at a fluid-air interface within a circular arena. These microrobots are driven by an external, oscillating magnetic field generated by Helmholtz coils. Each microrobot responds by spinning or oscillating about its central axis. The system utilizes local physical interactions – attractive magnetic dipole-dipole forces and repulsive hydrodynamic forces, both modulated by disk size and magnetic field frequency – to achieve programmable self-organization. The purpose is to study and exploit heterogeneity (specifically size) to enable diverse collective behaviors not possible in homogeneous systems, including organized aggregation, dispersion, locomotion, morphology reconfiguration (circular to ellipse under compression), and caging/expulsion of passive objects. Components include: magnetic microdisks (Cobalt/Gold coated, fabricated via 3D microprinting), fluid-air interface (water in a circular arena), Helmholtz coils (generating controlled oscillating magnetic fields), and potentially passive objects for manipulation tasks.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Magnetic Moment per Unit Area (ρm) | ~0.1 | A | Results, Eq. 4 | Explicit (Estimated) | Medium | Estimated from material properties (Co thickness) |
        | Disk Number Ratio (N12=N1/N2) | 0.23 - 1.46 | Dimensionless | Results, Fig. 2, Fig. 4 | Explicit | High | Calculated from experimental conditions |

    *   **Note:** Parameters describe the core components and driving forces. Radii and frequencies cover the heterogeneity and control aspects. Magnetic field amplitude sets the driving strength. Magnetic moment density connects material properties to forces. Number ratio quantifies the heterogeneity mix. Reliability is high for directly controlled/measured parameters, medium for estimated material properties.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the external oscillating magnetic field generated by the Helmholtz coils.
    *   Value: Field Amplitude: 10 mT; Frequency: 15-100 Hz.
    *   Units: mT, Hz

### **2.2 Energy Transduction**

    *   Content: 1. Electromagnetic energy from the oscillating field exerts a torque on the magnetic moment of each microdisk. 2. This torque causes the microdisk to spin or oscillate about its axis (Magnetic -> Mechanical/Rotational energy). 3. The spinning/oscillating disk interacts with the surrounding fluid, generating hydrodynamic forces (repulsive and azimuthal flow fields) (Mechanical/Rotational -> Fluid Kinetic Energy). 4. Magnetic dipole moments of the disks also lead to attractive forces between them (Potential Magnetic -> Mechanical energy). The interplay of these transduced forces (hydrodynamic, magnetic) dictates the collective behavior and self-organization. Capillary forces due to the interface also play a role (mentioned but less detailed).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify energy efficiency. Given the nature of microscale systems operating in viscous fluid driven by external fields, efficiency is expected to be very low. Most input energy is likely dissipated as heat through viscous drag in the fluid and potentially within the magnetic material. The score reflects the lack of quantification and the likely high dissipation common in such systems. Qualitative Assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: The primary dissipation mechanism is viscous drag as the microrobots spin/oscillate within the fluid (water). This converts the mechanical energy of the disks into heat within the fluid. Magnetic hysteresis losses within the ferromagnetic material (Cobalt) might also contribute, though likely less significant compared to viscous losses at these frequencies. Quantification is absent. Qualitative Assessment: High (dominated by viscous dissipation).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*    Units: s

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Low (Order of 2-3 distinct states)
*   Units: states

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rules are:
        1.  **Repulsive Hydrodynamic Force (F_hydro):** Arises from the spinning/oscillating disks. Strength depends on disk radii (Ri^4 * Rj^3), angular velocity (ωj^2, controlled by field frequency Ω), and inverse cube of distance (d^-3). It is asymmetric between disks of different sizes (larger disks exert stronger repulsion). See Eq. 2.
        2.  **Attractive Magnetic Dipole-Dipole Force (F_magdp):** Angle-averaged force between the magnetic moments (mi, mj) of the disks. Depends on magnetic moments (proportional to disk area, Ri^2, Rj^2) and inverse fourth power of distance (d^-4). Also asymmetric due to size dependence of magnetic moment. See Eq. 3.
        3.  **Azimuthal Flow Field Interaction:** Spinning disks create tangential flows, causing nearby disks/objects to orbit. Strength depends on disk radius cubed (R^3). This contributes to collective rotation and interaction dynamics (e.g., object caging/expulsion).
        4.  **Boundary Interactions:** Repulsion from the arena boundary (due to concave interface/meniscus) drives disks towards the center. Described qualitatively and included in models.
        5.  **Contact Forces:** Implicitly present (excluded volume), preventing overlap.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Eq. 2 | Hydrodynamic Repulsion | Disk Radii (Ri, Rj) | 50-200 | μm | Methods, Results | Explicit | Stated disk sizes used. |
    | Eq. 2 | Hydrodynamic Repulsion | Angular Velocity (ωj) | Proportional to Field Frequency (Ω) 15-100 Hz | rad/s or Hz | Results | Mixed | Frequency (Ω) is explicit, ωj is proportional below step-out. |
    | Eq. 3 | Magnetic Attraction | Magnetic Moment (mi, mj) | Proportional to R^2 * ρm (~0.1 A) | A·m² | Eq. 4, Results | Mixed | ρm estimated, R explicit. |

### **4.3 Global Order:**

    *   Content: The primary emergent global order is **spatial segregation by size**. Typically, at optimal frequencies below step-out, larger disks congregate towards the center, and smaller disks form an outer ring (radial separation, Fig 2A-C). In the step-out regime (high frequency), the order inverts, with larger disks moving towards the periphery and smaller (stepped-out) disks remaining nearer the center (organized dispersion, Fig 3G-K). Other global patterns include collective rotation (Movie S1) and organized static aggregation (Fig 3C). Under compression, the collective deforms into an ellipse while maintaining size segregation (Fig 5A).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Eq. 2 | Hydrodynamic Force | R_i, R_j | 50-200 | μm | Explicit | Disk sizes used | Results |
| Eq. 2 | Hydrodynamic Force | ω_j | ~Ω (15-100 Hz) | rad/s | Mixed | Proportional to field frequency below step-out | Results |
| Eq. 2 | Hydrodynamic Force | d (distance) | Variable | μm | Explicit | Distance between disks | Eq. 2 |
| Eq. 3 | Magnetic Dipole Force | m_i, m_j | Proportional to R^2 | A·m² | Mixed | Calculated from R and ρm | Eq. 4 |
| Eq. 3 | Magnetic Dipole Force | d (distance) | Variable | μm | Explicit | Distance between disks | Eq. 3 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| SO1 | Size Separation | Separation Order Parameter (Zs) | ~0.5 - 0.9 | Dimensionless | Explicit | Measures degree of same-size neighbors | Eq. 5 | Fig 2G, H, Fig 3B, Fig 4H |
| SO2 | Local Packing | Local Hexatic Order (ψ6Local) | ~0.4 - 0.8 | Dimensionless | Explicit | Measures local hexagonal packing order | Eq. 9 | Fig 3B |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | Local Forces -> Global Order | Mapping from pairwise interactions (Eqs 2,3) and boundary effects to emergent size-segregated patterns (Zs). | Medium-High (Score 7 from M4.4) | 2 | Zs, Visual Inspection, Simulation Agreement | Mixed | Predictability assessed qualitatively (M4.4). Yoneda embedding not discussed; low score reflects lack of formal category-theoretic analysis of the mapping. | Figs 2, 4, Text |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 2. Rubric: 0-1: No mention or relevance. 2-3: System shows local rules leading to global order, but no formal local-global mapping analysis. 4-5: Qualitative or partial analysis of how local component states/interactions map to global properties. 6-7: Quantitative analysis using order parameters, structure factors, or similar, demonstrating a clear link between local rules and global structure. 8-9: Explicit (potentially category-theoretic inspired) analysis of the local-to-global functor, demonstrating preservation of structure/relationships. 10: Full Yoneda Lemma application demonstrating isomorphism between local interaction patterns and global emergent structure representation. This paper shows local rules yield global order (Score 2-3 level), but lacks the formal mapping analysis required for higher scores.
    *   **Metrics:** Separation Order Parameter (Zs), Visual Comparison (Experiments vs. Simulations), Parameter Space Heatmaps (Zs vs Ω, N12).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog

### **5.3 Computational Primitive:**

    *   Content: Physical Sorting / Thresholding. The system sorts particles based on size, driven by the frequency-dependent balance of asymmetric forces. This acts like a physical sorting algorithm embodied in the material interactions. Alternatively, the step-out frequency acts as a threshold: below it, disks follow the field synchronously; above it, smaller disks behave differently. This thresholding determines the interaction dynamics and resulting global state (e.g., organized dispersion vs. synchronous rotation).
    *   **Sub-Type (if applicable):** Thresholding: Frequency-dependent force balance / Step-out behavior.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Driving Field Period | 10 - 67 | ms | Calculated from Freq (15-100 Hz) | Mixed | Frequency range is explicit, period is calculated. |
        | Relaxation to Ordered/Disordered State | Order of seconds | s | Movies S2, Text (transient dynamics) | Implicit | Inferred from descriptions of state transitions and movie timescales. |
        | Collective Rotation Period | Variable (Depends on Freq, Size) | s | Movie S1, Text | Implicit | Rotation is observed, but period not systematically quantified. |
        | Response Time to Compression | Order of seconds | s | Fig 5B (transient period) | Mixed | Transient period after water removal shown explicitly (~5s), represents response time. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism is driven by the frequency-dependent balance between local attractive (magnetic dipole-dipole) and repulsive (hydrodynamic) forces, which are asymmetric for different disk sizes. As the frequency (Ω) changes, the strength of the hydrodynamic repulsion (proportional to ω^2 ~ Ω^2, Eq. 2) changes relative to the magnetic attraction (Eq. 3). This shift in the net force balance causes the disks to rearrange spatially to find a new stable or quasi-stable configuration. At specific frequencies, this leads to size segregation (self-organization). At higher frequencies, the step-out behavior of smaller disks alters their effective hydrodynamic repulsion, leading to a different adapted state (organized dispersion). The adaptation is essentially the system relaxing into a configuration that balances the forces dictated by the current driving frequency.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The system exhibits several primary functional behaviors arising from the collective interactions and heterogeneity:
        1.  **Programmable Self-Organization:** Spatially segregating into ordered structures based on disk size, controlled by magnetic field frequency.
        2.  **Organized Aggregation/Dispersion:** Transitioning between tightly packed, static, ordered states and dispersed, dynamic states (either disordered or ordered dispersion in step-out regime).
        3.  **Organized Locomotion:** Moving as a cohesive, organized group under an applied magnetic field gradient (Fig 5C, D).
        4.  **Morphology Reconfiguration:** Deforming anisotropically (e.g., circle to ellipse) under isotropic compression while maintaining internal size organization (Fig 5A, B).
        5.  **Object Manipulation (Caging/Expulsion):** Selectively trapping or repelling passive objects based on object size and the collective's frequency-dependent state (ordered/disordered) (Fig 5E-J).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors (self-organization, aggregation, dispersion, deformation, manipulation) are primarily validated through:
        1.  **Direct Observation:** Experimental videos (referenced movies S1-S15) and snapshots (Figs 2, 3, 5) visually document the behaviors.
        2.  **Quantitative Analysis:** Order parameters (Zs for size separation, Fig 2G,H; ψ6Local for packing, Fig 3B) are used to quantify the degree of emergent order. Particle tracking is used to analyze trajectories (Fig 2A,D) and velocities (Fig 5D). Object positions are tracked relative to the collective centroid (Fig 5I, J). Major axis length quantifies deformation (Fig 5B).
        3.  **Parameter Space Exploration:** Systematically varying key parameters (frequency Ω, number ratio N12) demonstrates the conditions under which different behaviors emerge (Figs 2G, H).
        4.  **Control Conditions:** Implicit comparison exists between heterogeneous and homogeneous systems (mentioned in Intro/Discussion), and between different frequency regimes leading to different states (ordered vs disordered).
        5.  **Simulation Corroboration:** Both physics-based and abstract (swarmalator) models are used to replicate the self-organization behavior (Fig 4), providing theoretical support for the emergence from local rules.
        Limitations: Robustness validation against noise or significant environmental fluctuations is limited. Reproducibility across different labs/setups is not addressed.

---

#Key: [he_peptide-induced_2018]

# Peptide-Induced Self-Assembly of Therapeutics into a Well-Defined Nanoshell with Tumor-Triggered Shape and Charge Switch

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of a self-assembling peptide conjugate, PSP-DPMI, designed for anticancer therapy. The core component is the PSP peptide (VVVVVHHRGDC), which has a hydrophobic tail (VVVVV), a pH-responsive segment (HH), and a hydrophilic head with an RGD targeting motif and a Cys residue (RGDC). PSP is conjugated via a bifunctional linker to a D-peptide p53 activator, DPMI. PSP-DPMI monomers self-assemble into hollow nanoshells (~80-90 nm) under physiological pH (7.4). The system's purpose is to deliver the macromolecular therapeutic DPMI specifically to tumor cells. It achieves this through: (1) Enhanced Permeability and Retention (EPR) effect due to nanoshell size, (2) pH-triggered disassembly in the acidic tumor microenvironment (TME) (~pH 6.5) causing shape and charge switch (negative-to-positive), leading to smaller units (~3.6 nm), and (3) RGD-mediated targeting and uptake into integrin-expressing cancer cells. Once inside, the PSP moiety is expected to degrade, releasing DPMI to activate p53 signaling.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | PSP-DPMI Zeta Potential (pH 7.4) | ~ -1.6 (PSP value, inferred similar for conjugate) | mV | Fig 2A (PSP), Fig 3G (conjugate trend) | Mixed | Medium | Inferred similarity PSP/PSP-DPMI |

    *   **Note:** Parameters reflect the physical characteristics governing the system's primary stimulus-response mechanism. Reliability is generally high due to direct experimental measurements via DLS and Zeta potential analysis. PSP-DPMI Zeta potential at pH 7.4 is inferred based on PSP data and the observed trend at pH 6.5.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy inputs are thermodynamic driving forces for self-assembly (hydrophobic interactions, H-bonding) and chemical potential associated with pH gradients (protonation of His residues driving disassembly). No external energy source (light, electricity) is used for the core mechanism.

### **2.2 Energy Transduction**

    *   Content: 1. Chemical energy (hydrophobic effect, H-bonding) is transduced into structural potential energy during self-assembly into ordered nanoshells. 2. Chemical potential energy from the pH gradient (proton concentration difference) is transduced into electrostatic potential energy (charge repulsion upon His protonation) and kinetic energy during disassembly, overcoming the interactions holding the shell together. 3. Binding energy (RGD-integrin interaction) facilitates cellular uptake (transduction into mechanical work for endocytosis, though this process involves cellular energy sources not detailed here).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any information or metrics to quantitatively or qualitatively assess the energy efficiency of the self-assembly, disassembly, or delivery processes. Calculating efficiency would require knowing free energy changes, heat loss, etc., which are not measured or discussed.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation likely occurs as heat during the exothermic process of self-assembly (driven by hydrophobic effect and H-bonding) and potentially during the conformational changes associated with disassembly. Frictional losses during diffusion/movement in vivo are also present but not quantified. The paper does not explicitly discuss or quantify dissipation mechanisms. Assessment: Low (Qualitative, based on typical molecular processes).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local interactions driving self-assembly and disassembly are:
        1.  **Hydrophobic Interactions:** Between the N-terminal VVVVV segments, driving them to aggregate away from water, likely forming the core of the shell wall (Fig 1, Fig 2D).
        2.  **Hydrogen Bonding:** Between peptide backbones, potentially forming β-sheet structures (mentioned for PSP tail rationale, Fig S1E, inferred for PSP-DPMI) that stabilize the assembly ("interlocking the adjacent molecule").
        3.  **Electrostatic Interactions:** Repulsion between protonated His residues (HH segment) at low pH (<6.5) overcomes attractive forces, leading to disassembly. Reduced repulsion/potential weak attraction at neutral pH (7.4) allows assembly (Figs 1, 2A, 2D, 3G).
        4.  **Hydrophilic Interactions:** The C-terminal RGDC segment (and conjugated DPMI) faces outwards towards the aqueous environment (Fig 1, Fig 2D).
        5.  **RGD-Integrin Binding:** Specific molecular recognition between the RGD motif on the disassembled monomers (or potentially exposed on shell surface) and integrin receptors on cancer cells (Figs 1, 4A). This is relevant post-disassembly or for targeting.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Electrostatic | pH-dependent charge | Zeta Potential | -1.6 to +49.5 (PSP-DPMI range based on PSP/conjugate data) | mV | Fig 2A, 3G | Mixed | Explicitly measured for endpoints, inferred for conjugate at pH 7.4. Represents net surface charge effect. |
    | Hydrophobic/H-Bonding | Critical Aggregation Conc. (CAC) | Concentration | <0.05 (PSP), <0.2 (PSP-DPMI) | mg/mL | Fig S1A, Section "Integration of DPMI..." | Explicit | Minimum concentration observed for nanoparticle formation. |

### **4.3 Global Order:**

    *   Content: The emergent global order is a well-defined, hollow, spheroidal nanoshell structure with a diameter of approximately 80-90 nm (for PSP-DPMI) formed from the self-assembly of the peptide-drug conjugate monomers. TEM images show a distinct core-shell morphology.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| R3 | Electrostatic Interaction (pH-dependent) | Zeta Potential / pKa (His) | See M4.2.1 | mV / pH units | Mixed | Explicit measurements (Zeta), Implicit (pKa). Governs assembly/disassembly switch. | Fig 2A, 3G, Intro |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| P1 | Nanoshell Size | Hydrodynamic Diameter (Z-average) | 86.3 | nm | Explicit | Measured property defining the global structure size. | DLS | Fig 3F |
| P4 | Surface Charge (pH 7.4) | Zeta Potential | ~-1.6 (Inferred) | mV | Mixed | Overall charge of the assembled structure. | ZetaCAD | Fig 2A, 3G |
| P5 | Disassembled Size | Hydrodynamic Diameter | 3.6 | nm | Explicit | Size of the basic units after disassembly. | DLS | Fig 3F |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    *   **Metrics:** DLS (Size, Size Distribution vs pH, Concentration), TEM (Morphology), Zeta Potential (Surface Charge vs pH).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | pH Response Time (Disassembly) | <= 30 (Partial disassembly observed) | min | Fig 3H text | Explicit | Time given for observation of disassembly effects at pH 6.5. Full kinetics not measured. |
        | Cellular Uptake Time | 6 | hours | Fig 4B text | Explicit | Time point used for CLSM and flow cytometry experiments showing significant uptake at pH 6.5. |
        | Blood Half-life (PSP-DPMI) | 7.3 | hours | Fig S8 | Explicit | Measured pharmacokinetic parameter reflecting circulation time. |
        | Blood Half-life (DPMI) | <2 | hours | Fig S8 | Explicit | Measured pharmacokinetic parameter for free peptide. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors are:
        1.  **Self-Assembly:** Spontaneous formation of stable, hollow nanoshells (~80-90 nm) from monomers at physiological pH (7.4).
        2.  **pH-Triggered Disassembly:** Rapid structural breakdown of nanoshells into smaller units (~3.6 nm) accompanied by a surface charge switch (negative to positive) upon exposure to acidic pH (~6.5).
        3.  **Targeted Cellular Uptake:** Enhanced internalization into integrin-expressing cancer cells, particularly after pH-triggered disassembly, mediated by the RGD motif.
        4.  **Drug Release & Action:** Intracellular release of the DPMI peptide (following PSP degradation, inferred) leading to activation of p53 signaling (p21 increase, apoptosis, cell cycle arrest).
        5.  **Prolonged Circulation:** Nanoshell formation significantly extends the in vivo blood circulation time compared to the free peptide.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content:
         *   **Self-Assembly:** Validated by DLS (size distribution, Fig 3F), TEM (morphology, Fig 3E), concentration dependence study (Section "Integration of DPMI..."). Control: Monomer state at pH 6.0/6.5.
         *   **pH-Triggered Disassembly/Charge Switch:** Validated by DLS (size change, Fig 3F), TEM (morphology change, Fig 3H), Zeta potential measurements (charge change, Fig 3G). Control: Stable state at pH 7.4.
         *   **Targeted Cellular Uptake:** Validated by CLSM (visualization, Fig 4B), Flow Cytometry (quantification, Fig 4C). Controls: DPMI alone, normal cells (Fig S6), pH 7.4 vs 6.5 comparison.
         *   **Drug Release & Action:** Validated by Western Blot (p53, p21 levels, Fig 5E, S11), MTT assay (cell viability, Fig 5A-D), FACS (apoptosis, cell cycle, Figs 5F, 5G, S12, S13), IHC (p53, p21, Ki67 in vivo, Fig 7B-E), TUNEL assay (apoptosis in vivo, Fig 6E). Controls: PSP alone, DPMI alone, PBS, p53-/- cells (Fig 5D), Nutlin3/DOX (positive controls).
         *   **Prolonged Circulation:** Validated by pharmacokinetic study measuring blood concentration over time (Fig S8), ex vivo imaging showing tumor accumulation over time (Figs 4D, 4E, S7, S9). Control: DPMI alone.

---

#Key: [kukushkin_massed-spaced_2024]

# The massed-spaced learning effect in non-neural human cells

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of two immortalized non-neural human cell lines (SH-SY5Y neuroblastoma and HEK293 embryonic kidney cells) stably transfected with a reporter construct. This construct uses a CREB-dependent promoter to drive the expression of a short-lived (PEST-tagged) luciferase. The system is used to study the massed-spaced learning effect, typically associated with neural memory, in non-neural cells. Repeated pulses of chemical agonists (forskolin and/or phorbol ester TPA), mimicking training stimuli, are applied. Luciferase expression, measured after stimulation, serves as a proxy for cellular "memory" or persistent transcriptional response. The purpose is to demonstrate that canonical features of memory formation, like the massed-spaced effect, can occur in non-neural cells via conserved signaling cascades (PKA, PKC, ERK, CREB).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input driving the specific process studied (luciferase expression as memory proxy) is the chemical potential energy supplied by the agonists (forskolin, TPA) binding to their targets and initiating signaling cascades. The cells also rely on baseline metabolic energy from the culture medium (serum-free media mentioned, specific composition not detailed) for survival and basic function, including transcription and translation.

### **2.2 Energy Transduction**

    *   Content: Chemical potential energy from agonist binding is transduced into intracellular signals via conformational changes in target proteins (adenylate cyclase for forskolin, PKC for TPA). This initiates phosphorylation cascades (PKA/PKC -> ERK -> CREB), converting chemical signals and metabolic energy (ATP hydrolysis) into post-translational modifications. Phosphorylated CREB binding to the CRE promoter transduces this signal into transcriptional activation, utilizing metabolic energy for RNA polymerase activity. mRNA translation into luciferase protein utilizes further metabolic energy (GTP, ATP). Light emission during the luciferase assay involves the enzymatic conversion of chemical energy (luciferin oxidation, requiring ATP) into photons. Cellular machinery also uses energy for protein degradation (PEST sequence targets luciferase to proteasome).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Extremely low. Biological signaling cascades are not optimized for energy efficiency in terms of converting input stimulus energy into the final measured output (luciferase light). The goal is signal amplification and information processing, which involves many dissipative steps. Most input energy (chemical potential of agonists, cellular ATP) is dissipated as heat during the numerous enzymatic reactions, protein turnover, and maintenance of cellular homeostasis. No efficiency metrics are provided or relevant in this context.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation occurs as heat loss during:
        1.  ATP hydrolysis by kinases (PKA, PKC, ERK pathway kinases).
        2.  GTP/ATP hydrolysis during transcription and translation.
        3.  ATP hydrolysis during the luciferase assay reaction.
        4.  ATP hydrolysis by the proteasome during luciferase degradation.
        5.  Maintenance of ion gradients and other homeostatic processes.
        Quantification is not possible from the provided text (Qualitative assessment: High).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: > 24
*    Units: h (hours)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Low (likely graded, not discrete states)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Variable; Slower for spaced pulses vs single pulse.
    *   Units: Relative decrease in luciferase level over time (e.g., comparing 4h vs 24h).

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: Skipped M4.2-M4.7 as M4.1 is "No")**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid

### **5.3 Computational Primitive:**

    *   Content: Temporal Pattern Integration / Filtering. The core computation demonstrated is the cell's ability to differentiate between massed and spaced patterns of stimulation applied over minutes, leading to a stronger, more sustained output for spaced stimuli with an optimal interval. This suggests a form of temporal filtering or integration where the timing between events is critical. Specific operations involved likely include signal integration, thresholding (e.g., for ERK/CREB phosphorylation), and feedback/feedforward loops within the cascades that create temporal sensitivity.
    *   **Sub-Type (if applicable):** Temporal Integration / Band-pass filtering (tuned to specific ITIs).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Stimulus Pulse Duration | 3 | min | Results / Fig 2, 3 | Explicit | Explicitly stated. |
        | Inter-Trial Intervals (Spaced) | 10, 20, 30 | min | Results / Fig 3 | Explicit | Explicitly tested ITIs. |
        | Signaling Cascade Activation (ERK/CREB phosphorylation) | minutes to hours | - | Results / Fig 3D, S4 | Mixed | Phosphorylation measured immediately after and up to 24h post-stimulus. Precise activation/deactivation kinetics not fully resolved but occur on these timescales. |
        | Transcriptional Response (Luciferase mRNA production) | hours | - | Results / Fig 2, 3 | Implicit | Inferred from luciferase protein levels appearing within hours and persisting. |
        | Protein Accumulation/Turnover (PEST-Luciferase) | hours | - | Results / Fig 2, Discussion | Mixed | Luciferase measured at 2, 4, 24h. PEST tag implies rapid turnover (implicit), allowing levels to reflect ongoing transcription; persistence suggests ongoing activity. |
        | Memory Persistence | > 24 | h | Results / Fig 2, 3 | Explicit | Elevated luciferase detected at 24h. |
    *   **Note:** Signaling activation and transcriptional response timescales are typical for these processes, supported by measurements at various timepoints.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism involves the dynamic properties of intracellular signaling cascades, particularly involving ERK and CREB. Spaced stimuli lead to stronger and more sustained activation/phosphorylation of ERK and CREB compared to massed stimuli (Fig 3D, E, H; Fig S4). This differential activation pattern translates into differential CRE-dependent transcription (luciferase expression). The paper suggests that the timing of phosphorylation/dephosphorylation cycles within the cascades (especially ERK) underlies this temporal sensitivity, potentially involving feedback loops or integration properties within the network. Inhibition of ERK or CREB blocks the effect, confirming their role. Specific feedback loops or precise kinetic details are not fully elucidated but are hypothesized based on existing literature (ref 20-25, 37, 48, 53-55). The adaptation tunes the transcriptional output based on the input timing.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior observed is a cellular analog of the massed-spaced learning effect. This manifests as a differential, persistent transcriptional response (measured via luciferase reporter) dependent on the temporal spacing of identical input stimuli (chemical agonist pulses). Specifically, multiple stimuli spaced by an optimal interval (e.g., 10-20 min) produce a significantly stronger and more lasting transcriptional output compared to the same stimuli delivered in a single continuous pulse (massed) or with suboptimal spacing. A secondary related behavior is the differential "forgetting" rate, where the response to spaced stimuli decays more slowly than the response to a single stimulus.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claims of observing the massed-spaced effect are validated through:
        1.  **Operational Definition:** Clear definition of massed (single long pulse) vs. spaced (multiple short pulses with specific ITIs) protocols.
        2.  **Quantitative Measurement:** Luciferase activity measured at defined time points (2, 4, 24h).
        3.  **Control Experiments:** Comparison between spaced, massed, single pulse, and vehicle controls. Use of specific pathway inhibitors (U0126 for ERK, 666-15 for CREB) to confirm mechanistic links.
        4.  **Statistical Analysis:** Paired t-tests and ANOVA used to demonstrate significant differences between conditions (p-values reported).
        5.  **Replication:** Experiments repeated multiple times (N values > 3 for most key findings) and observed in two different cell lines.
        **Limitations:** Validation is within the context of specific cell lines and artificial stimuli. Direct extrapolation to *in vivo* learning or behavior requires caution. The behavior is a molecular analog, not cognition itself.

---

#Key: [rus_design_2015]

# Design, fabrication and control of soft robots

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews the field of soft robotics, defining "soft robots" as systems capable of autonomous behaviour primarily composed of materials with moduli in the range of soft biological materials (approx. 10^4–10^9 Pa). These robots utilize compliant materials (e.g., silicone rubbers, elastomers, hydrogels) for their bodies, enabling continuous deformation, high degrees of freedom, safe human interaction, adaptation to environments/objects, and bio-mimicry. Key components discussed include soft bodies, actuation systems (pneumatic/hydraulic FEAs, PAMs, EAPs, tendon-driven), sensing (stretchable electronics, curvature sensors, exteroception), power sources (soft batteries, chemical), computation/control (often external/rigid, but with potential for morphological computation and stretchable electronics), and algorithms specific to soft bodies. The purpose is to create robots with enhanced adaptation, sensitivity, agility, resilience, and safety for applications like manipulation, locomotion in confined/rough terrains, medical devices, and human-robot interaction.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** The review discusses ranges and typical values rather than specific operational parameters for a single system. Reliability is "High" for explicitly stated ranges based on literature consensus presented.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The review discusses several energy sources: Pressurized fluids (air or liquid) for pneumatic/hydraulic actuators (FEAs, PAMs), often supplied externally via tethers or via onboard sources like miniature compressors, compressed gas cylinders, or chemical reactions (e.g., H2O2 decomposition, combustion). Electrical energy is used for EAPs, control electronics, sensors, and potentially onboard compressors/pumps. Chemical energy stored in fuels (H2O2, combustible fuels, batteries - including soft/stretchable ones). Thermal energy for phase-change materials.

### **2.2 Energy Transduction**

    *   Content: Pneumatic/Hydraulic: Chemical/Electrical energy -> Fluid Pressure (via pump/compressor/reaction) -> Mechanical Work (deformation via pressurized channels/bladders, e.g., FEAs, PAMs). Electrical (EAP): Electrical energy -> Mechanical Work (deformation via electroactive effect). Tendon-driven: Electrical/Chemical energy -> Actuator (motor/SMA) Work -> Tension -> Mechanical Work (bending). Chemical (Combustion): Chemical energy -> Rapid Gas Expansion -> Mechanical Work (jumping). Thermal (Phase Change): Electrical energy -> Heat -> Phase Transition -> Stiffness Change. Material's potential energy is stored during deformation and released.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The review explicitly mentions that "Miniature compressors use valuable electrical energy inefficiently" and highlights the challenge of efficient power sources. EAPs are noted for potentially higher efficiency by directly using electrical potential but face other challenges. Pneumatic/hydraulic systems often involve losses in compressors/pumps and control valves. Combustion can be powerful but inherently involves significant heat loss. Overall efficiency is generally low compared to rigid robots, though specific values are not given. Score reflects the stated inefficiency and challenges.

### **2.4 Energy Dissipation**

    *   Content: Dissipation mechanisms are inherent but not explicitly quantified. Mechanisms include: Viscoelastic damping within the soft materials during deformation cycles, friction (internal material friction, friction with environment), heat loss during chemical reactions (combustion, battery operation, H2O2 decomposition), heat loss in electrical components (resistive losses in wires, EAPs, controllers, potentially soft electronics), inefficiency in energy conversion (compressors, pumps), fluid leakage in pneumatic/hydraulic systems. Qualitative assessment: Likely High, given material properties and actuation methods.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable (Potentially Long-term for Phase Change/Jamming)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Low (typically 2 states)
*   Units: distinct states

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Partial

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid

### **5.3 Computational Primitive:**

    *   Content: The review doesn't detail specific computational primitives performed *by the material itself* through morphological computation. It speaks generally about simplifying control algorithms and impedance matching. Potential primitives (inferred, not explicit) could include physical filtering (damping), impedance modulation (physical adaptation to loads), or non-linear dynamics exploitation for locomotion patterns. The paper mentions evolutionary algorithms *designing* soft robots (Refs 63-65), but this computation is external design computation, not embodied operational computation.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Rapid Maneuver (Fish Escape) | ~100 (or hundreds) | ms | Fig. 5 / Section "Locomotion" / Ref 11 | Explicit | Explicitly stated for robotic fish example. |

    *   **Note:** The review mentions timescales qualitatively or gives specific examples, but doesn't provide a systematic list of operational timescales for a general soft robot.

### **6.2 Active Inference:**

    *   Content: Unclear

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The primary mechanism described is leveraging inherent material compliance ("passive adaptation"). The soft body physically conforms to the environment or object (e.g., grasping, locomotion on uneven terrain). This can be combined with control strategies: impedance control (matching body dynamics), feedback control using sensors (e.g., curvature sensors for closed-loop control - Ref 30), and potentially learning control (e.g., iterative learning for dynamic maneuvers - Ref 7). Variable stiffness mechanisms (phase change, jamming - Refs 40-44) allow active adaptation of mechanical properties. Agonist-antagonist actuation allows adaptable compliance via co-contraction (mentioned in "Actuation"). The review doesn't specify standard learning algorithm types (Hebbian, RL etc.) being widely implemented *within* the material itself, but mentions optimization and learning *control* algorithms running on external hardware.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: Key functional behaviors reviewed include: Locomotion (crawling, walking, rolling, swimming, undulating, jumping - Fig 1), Manipulation (grasping diverse/unknown objects, dexterous manipulation - Fig 3), Bending/Twisting with high curvature (confined spaces), Continuous deformation (bio-mimicry), Collision energy absorption (safety in HRI), Shape adaptation (environmental interaction), Rapid maneuvers (escape response - Fig 5), Potential wearable applications (rehabilitation, assistance - Fig 3i, Refs 32, 37).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The review validates cited behaviors primarily through descriptions of experimental prototypes and demonstrations mentioned in the referenced papers (often accompanied by Figures 1, 3, 5). Validation involves showing that the fabricated robots successfully perform the intended behaviors (e.g., walking, grasping, swimming). Quantitative analysis is sometimes mentioned (e.g., escape maneuver time in Ref 11). Control experiments comparing soft vs. rigid approaches are implied. Reproducibility is suggested by multiple groups achieving similar behaviors (e.g., pneumatic grasping). Limitations include the reliance on specific prototype success rather than systematic parameter space exploration or rigorous robustness testing across all claims. Validation relies heavily on the cited primary sources.

---

#Key: [banda_online_2013]

# Online Learning in a Chemical Perceptron

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a simulated artificial chemistry (AC) model implementing a two-input binary perceptron, termed a "chemical perceptron". It utilizes chemical reactions governed by Michaelis-Menten kinetics (and mass action) to perform linear integration of inputs and weights, apply a threshold function, and implement online supervised learning (Hebbian-like adaptation). Two variants are presented: the weight-loop perceptron (WLP) and the weight-race perceptron (WRP). The system's components are molecular species (representing inputs X, weights W, output Y, desired output D, fuel E for WLP, intermediate processed weights W for WLP) and chemical reactions (catalysis, conversion, annihilation, decay). Its purpose is to autonomously learn and classify all 14 linearly separable two-input logic functions through simulated chemical interactions, demonstrating reusable, programmable, and adaptable chemical computing.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Rate Constants (k) | See Table 6 | Various (e.g., 1/(M*s), 1/s) | Table 6 | Explicit | Medium | Optimized via GA |

    *   **Note:** Units for rate constants are inferred based on reaction types (mass action, Michaelis-Menten), assuming standard concentration (M) and time (s or simulation steps). Rate constants are GA-optimized, providing medium reliability for specific values, though the system shows robustness.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the chemical potential energy stored in the reactant species. Specifically, for the Weight-Loop Perceptron (WLP), an external fuel species 'E' is consumed in the weight processing reactions (W -> W' + Y). For the Weight-Race Perceptron (WRP), the input species 'X' themselves act as the fuel/energy source as they are consumed in the X -> Y reactions. The introduction of input species (X) and desired output species (D) via AC actions represents energy injection into the system.

### **2.2 Energy Transduction**

    *   Content: Energy is transduced through chemical reactions.
        1.  **WLP:** Chemical potential energy in W and E is converted into chemical potential energy of W' and Y (computation/output production). Energy in W' is converted back to W (state restoration). Energy in D is converted to W (learning/adaptation). Energy in Y0/Y1 is dissipated via annihilation (Y0+Y1->E). Energy in X, Y, D is dissipated via decay (->E).
        2.  **WRP:** Chemical potential energy in X is converted into chemical potential energy of Y (computation/output production). Energy in D is converted to W (learning/adaptation). Energy in Y0/Y1 is dissipated via annihilation (Y0+Y1->E). Energy in X, Y, D is dissipated via decay (->E). Energy in W⊕/W⊖ is dissipated via annihilation.
        The key transduction is the conversion of input signals (presence/absence encoded chemically) and weight states (concentrations) into an output signal (relative concentrations of Y0/Y1) via catalyzed reaction pathways. Learning involves transducing the error signal (concentration difference between Y and D) into changes in weight concentrations (W).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any quantitative information on energy efficiency (e.g., thermodynamic efficiency, computational efficiency per energy unit). A qualitative assessment is difficult; while the WRP avoids the explicit fuel 'E' making it seem simpler, both involve dissipative steps (decay, annihilation). The optimization via GA targeted functional correctness (learning performance), not energy efficiency.

### **2.4 Energy Dissipation**

    *   Content: Energy is dissipated primarily through:
        1.  **Annihilation Reactions:** Y0 + Y1 → E (Groups 5 in WLP, 6 in WRP) and W⊕ + W⊖ → E (Group 4 in WRP, Group 5 in WLP). These convert the chemical potential energy of the reactants into an inert byproduct 'E' (presumably representing heat or dispersed chemical species). Quantification: Rate depends on concentrations and rate constants (Tables 3 & 6). Qualitative Assessment: High rate specified for rapid decision/reset.
        2.  **Decay Reactions:** X → E, Y → E, D → E (Various groups in both WLP and WRP, e.g., Groups 7, 8, 9 in WLP; Groups 2, 7 in WRP). These represent the removal of transient species to reset the system, dissipating their chemical potential energy. Quantification: Rate depends on concentration and rate constants (Tables 3 & 6). Qualitative Assessment: Rates are set by GA, likely balancing stability and cleanup speed.
        3.  **Production of Byproduct 'E':** Explicitly mentioned as the product of annihilation and decay instead of 'nothing' to conceptually maintain mass conservation (Section 4.2). 'E' represents dissipated energy/matter.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term (Potentially indefinite within simulation scope)
*    Units: simulation steps / iterations

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Analog (continuous range)
*   Units: M (concentration) per weight component (W0±, W1±, W2±)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: High (approaching 100% for learned functions)
*   Units: % Correct Classification

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Low / Dependent on Annihilation
    *   Units: M / simulation step

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Learning Accuracy | Final % correct classification after 200 iterations | 100 | % | `BehaviorArchetypeNode`.`accuracy` | Section 7.1, Fig 10 | Explicit | Measures overall system fidelity including memory readout. |
    | Robustness (Rate Perturbation) | Final % correct classification with 50% rate constant perturbation | 98.98 (WLP), 99.34 (WRP) | % | `BehaviorArchetypeNode`.`robustness` | Section 7.2, Fig 11 | Explicit | Measures stability of learned state/readout against parameter noise. |
    | Weight Dynamics | Visual representation of weight concentration changes during learning | Fig 9 shows convergence | M | `MemoryNode`.`dynamics` | Figure 9 | Explicit | Qualitative view of memory writing fidelity/stability. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Neuromorphic

### **5.3 Computational Primitive:**

    *   Content: The core computational primitives embodied by the chemical reactions are:
        1.  **Weighted Summation (Linear Integration):** Achieved implicitly by having input species (X) and weight species (W) jointly influence the rate of production of output precursors (Y). In WLP, inputs catalyze W consumption; in WRP, weights catalyze X consumption. The rates (following Michaelis-Menten or mass-action) depend on reactant/catalyst concentrations, effectively summing weighted contributions. Mathematically analogous to z = ∑ w_i * x_i (including bias w0).
        2.  **Thresholding (Activation Function):** Implemented via competing reactions producing Y0 (output 0) and Y1 (output 1), followed by rapid annihilation (Y0 + Y1 -> E). Whichever species (Y0 or Y1) has a higher net production rate (determined by the weighted sum) dominates after annihilation, resulting in a binary output decision (max[Y1] > max[Y0]). Approximates a step function (`sgn`).
        3.  **Weight Update Rule (Learning):** Implemented by reactions that convert desired output species (D) into weight species (W±), catalyzed by the presence of input (X) and the error signal (mismatch between actual output Y and desired D). Embodies w_i(t+1) = w_i(t) + α(d - y(t))x_i.
    *   **Sub-Type (if applicable):** Weighted Summation, Thresholding (Step Function via Annihilation), Supervised Learning Rule (Hebbian-like)

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

*   **Note**: Processing power, energy/operation, and frequency are not quantified in standard computational units (e.g., FLOPS, Joules/Op, Hz). They are implicitly determined by the chemical kinetics (reaction rates, concentrations). Bit-depth is analog for concentrations.

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Reaction Dynamics | Variable (sub-step) | steps | Section 2 (ODE integration) | Implicit | Determined by rate constants (Table 6) and ODE solver step size (not specified, assumed << S). |
        | Input Processing / Output Production | ~100 | steps | Section 5.2 (Delay for D injection) | Explicit | Time allowed for Y production before D injection. |
        | Annihilation/Thresholding | Fast (relative to processing) | steps | Sections 4.3, 4.4, 5.3 | Implicit | Crucial for decision making; rate constants likely high (Table 6, Groups 5/6). |
        | Transient Species Decay | Variable | steps | Table 3 (Decay reactions) | Implicit | Rates set by GA (Table 6), likely fast enough for reset before next iteration. |
        | Action Interval (S) | 5000 | steps | Section 5.1 | Explicit | Time between consecutive input injections. |
        | Learning Iteration (Single) | 5000 (Action) + ~100 (Compute) + Learning Reaction time | steps | Sections 5.1, 5.2 | Mixed | Combined explicit interval and implicit reaction times. |
        | Total Learning Process (Lp) | 10^6 (200 * 5000) | steps | Section 5.2 | Explicit | Total simulation time for learning performance analysis. |
        | Memory Retention (Weights) | >> 10^6 | steps | Sections 3.3, 4.2 | Implicit | Weights persist indefinitely in the absence of learning/annihilation. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is a chemical implementation of the perceptron learning rule (a form of supervised, Hebbian-like learning). Specifically, when a mismatch occurs between the actual output (Y, determined by Y0 vs Y1 concentration) and the desired output (D, injected species D0 or D1), reactions are triggered to adjust the weights (W). If Y=0 but D=1 is desired, D1 species react (catalyzed by relevant input X species and the incorrect output Y0) to produce positive weight species (W⊕). If Y=1 but D=0 is desired, D0 species react (catalyzed by relevant X and incorrect Y1) to produce negative weight species (W⊖). (See Table 3a Group 10-11, Table 3b Group 8-9). This chemically implements the rule: Δw_i ∝ α(d - y)x_i, where the error signal (d-y) gates the conversion of D into W, and the presence of input x_i (via X species) directs which weights are modified. The learning rate α is embedded in the concentration of injected D species (Section 5.2).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behavior is **online supervised learning and classification of linearly separable binary functions**. The system takes sequential inputs (pairs of binary values encoded as X0/X1 species concentrations) and, based on its internal state (weight species W concentrations), produces a binary output (Y0 vs Y1 dominance). During training, it also receives a desired output signal (D species) and adjusts its weights to minimize classification errors. It successfully learns all 14 possible linearly separable two-input logic functions (e.g., AND, OR, NAND, IMPLIES). A secondary behavior is **robustness maintenance**, where it continues to classify correctly despite perturbations to its internal parameters (rate constants). Another behavior is **self-resetting/reusability**, where transient species decay, allowing the system to process the next input without external reset (Section 4.2).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claims of learning and classification behavior are validated through extensive numerical simulations of the ODE system representing the artificial chemistry.
        *   **Operational Definitions:** Learning is defined as achieving correct classification of input patterns according to a target logic function. Output is defined operationally as max[Y1] > max[Y0] at a specific time point (step 99) (Section 5.2).
        *   **Control/Comparison:** Two different models (WLP and WRP) are implemented and compared (Section 7.3). Learning performance is evaluated against chance/initial state (starts from random weights or CONST0, Fig 9).
        *   **Quantitative Analysis:** Learning performance is quantified by the percentage of correct classifications averaged over 104 runs for each of the 14 functions (Section 7.1, Fig 10). Robustness is quantified similarly under varying levels of rate constant perturbation (Section 7.2, Fig 11).
        *   **Reproducibility:** The simulation setup (species, reactions, rates, actions) is described in detail, allowing for potential reproduction. GA optimization adds some stochasticity to finding optimal rates, but the best found rates are provided (Table 6).
        *   **Limitations:** Validation is purely computational (simulation-based). It assumes the deterministic ODE model accurately captures relevant dynamics (ignoring stochasticity at low molecule counts). Potential issues with numerical integration (Euler method) are mentioned but not deeply analyzed. The specific choice of parameters (S, D concentration, delays) was determined experimentally within the simulation context.

---

#Key: [lowen_towards_2025]

```markdown
# Towards Intelligent Active Particles:

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews the application of artificial intelligence (AI), particularly machine learning (ML), to active matter systems. Active matter consists of self-propelling agents (particles), such as biological microorganisms or synthetic microswimmers/robots. The review discusses approaches to enhance the "intelligence" of synthetic active particles, either through internal programming (robots) or external feedback control systems. It focuses on using ML for navigation (steering single agents through complex environments optimally) and communication/cooperation problems (coordinating groups of agents for tasks like collective target collection). It describes different levels of particle sophistication/intelligence, from passive particles to fully autonomous sensor-processor-actuator systems. Various ML techniques like reinforcement learning (Q-learning, actor-critic, neural networks) are discussed in the context of optimizing swimming gaits, finding targets via chemotaxis, discovering fastest paths, predator-prey interactions, and coordinating group behaviors like swarming, clustering, and nutrient foraging. It also touches on ML for identifying phase transitions and learning governing equations in active matter.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Particle Size Scale | micron/nano | m | Section 1 / Implicit | Mixed | Medium | Inferred from context (colloidal regime) |

    *   **Note:** Parameters listed are characteristic of the *review itself* and the *types* of systems discussed, as the paper does not focus on a single implementation. Specific system parameters would need to be found in the cited references.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: For the active particles discussed: Energy is converted from the environment (e.g., chemical fuel leading to self-phoresis, light for light-driven swimmers, external fields for actuation). For AI/ML components: Electrical energy for computation (external computers or onboard processors in robots).

### **2.2 Energy Transduction**

    *   Content: For particles: Chemical/light/etc. energy is transduced into directed mechanical motion (self-propulsion). Mechanisms include self-phoresis (diffusiophoresis, thermophoresis) for colloids or motor mechanisms for robots. For controlled particles (level b, Fig 1), external field energy is transduced into particle motion. For AI/ML: Sensory input energy (light, chemical concentration) is transduced into electrical signals for processing; computational energy is used for information processing (algorithm execution); output signals transduce information into actuation commands (changing particle direction/gait).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The review does not provide specific energy efficiency values or sufficient information for a qualitative assessment applicable across all discussed systems. Efficiency would vary greatly depending on the specific particle type, propulsion mechanism, and AI implementation (e.g., external vs. internal computation). Mentions optimization problems (e.g., optimal swimming speed) which relates to efficiency, but doesn't quantify it.

### **2.4 Energy Dissipation**

    *   Content: Primary dissipation mechanism for particle motion is viscous drag from the surrounding fluid (implicit, inherent to low Reynolds number swimming). Computational processes (internal or external) dissipate heat (implicit). Chemical reactions for self-propulsion may release heat. Other mechanisms depend on the specific system (e.g., friction in robots). Quantification is not provided. Assessment: Medium to High dissipation expected for microswimmers due to viscous fluid environment.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes
        1.  Particles in non-Newtonian solvents exhibiting memory effects via the environment (Fig 1d).
        2.  Particles equipped with internal memory storing past sensations influencing future actions (Fig 1f).
        3.  Reinforcement learning algorithms inherently involve memory (e.g., Q-tables, learned policies stored in neural network weights) to improve strategies based on past experience/training. Memory of past states/actions/rewards influences future decisions.
        4. Specific mention of particle dynamics being influenced by history/memory in non-Newtonian fluids (Section 1).

**(Conditional: M3.1 is "Yes", proceed with M3.2-M3.8)**

### **3.2 Memory Type:**

    The overall score of 6 reflects the presence of functional algorithmic memory in many key examples (RL-based navigation/cooperation) which is re-writable and influences future actions based on past information, though the physical embodiment and fidelity vary greatly or are external.

### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :------------------------------ | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The review describes several types of local interaction rules:
        1.  **Communication via signaling:** Agents emit and sense chemical signals (quorum sensing, Fig 3). The learned rule involves balancing movement up the nutrient gradient (local sensing of environment) versus movement up/down the signaling molecule gradient (local sensing of other agents' signals/presence) determined by coefficients predicted by a neural network (Section 4, Ref [69]).
        2.  **Implicit communication/interaction:** In MIPS (Section 5), interactions are primarily volume exclusion and persistent motion, leading to aggregation without explicit signaling.
        3.  **Predator-prey interactions:** Interactions based on sensing (type unspecified, but likely local) leading to chasing or escaping maneuvers (Section 3).
        4.  **Flocking rules:** Often involve alignment, attraction, and repulsion based on neighbors within a certain range (mentioned conceptually in Section 4, Ref [65]).
        5.  **Hydrodynamic interactions:** Mentioned as a factor making collective tasks difficult (Section 1), implying local fluid-mediated interactions influence particle motion.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID             | Description                                          | Parameter Name        | Parameter Value Range   | Units   | Data Source     | Implicit/Explicit   | Justification        |
    | :------------------ | :--------------------------------------------------- | :-------------------- | :---------------------- | :------ | :-------------- | :------------------ | :------------------- |
    | Nutrient Cons. [69] | Consumption rate (k), Agent density (Nl²/L²)         | k dt₀/l₀², Nl₀²/L² | See Fig 5               | dimless | Fig 5, Sec 4    | Explicit          | Given in Fig 5 axes  |

### **4.3 Global Order:**

    *   Content: The review describes several types of emergent global order:
        1.  **Spatial Patterns:** Clustering (dense aggregates), Spreading (uniform distribution), Adaptive spatial distribution (Fig 4, Section 4). Phase separation into dense and dilute regions (MIPS, Section 5).
        2.  **Dynamical Patterns:** Swarming/Flocking (coherent collective motion, Section 4). Rotating rod (collective rotational motion, Section 4). Predator-prey dynamics (chasing/evasion patterns, Section 3). Trajectories in motility landscapes (Fig 2, Section 2 - emergent path from local rules, though arguably goal-directed rather than purely self-organized).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID             | Description                                          | Parameter                     | Value Range             | Units            | Implicit/Explicit   | Justification         | Source          |
| :------------------ | :--------------------------------------------------- | :---------------------------- | :---------------------- | :--------------- | :------------------ | :-------------------- | :-------------- |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID         | Description                 | Parameter                     | Value Range             | Units         | Implicit/Explicit   | Justification                    | Protocol            | Source          |
| :------------------ | :-------------------------- | :---------------------------- | :---------------------- | :------------ | :------------------ | :------------------------------- | :------------------ | :-------------- |
| Nutrient Strat. [69]| Collective foraging strategy| Strategy Type                 | Clustering/Adaptive/Spreading | Categorical   | Explicit          | Identified & mapped in Fig 5     | Reinforcement Learn.| Fig 4, Fig 5    |
| MIPS [General]      | Phase Separation            | Global Order Parameter (Ψ)    | e.g., 0 to 1            | dimensionless | Implicit          | Standard MIPS analysis           | Simulation/Theory   | Sec 5           |
| Flocking [General]  | Collective Alignment        | Polarization (Magnitude of avg vel)| 0 to 1                | dimensionless | Implicit          | Standard flocking analysis       | Simulation/Theory   | Sec 4           |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type            | Description                                                  | Predictability   | Yoneda Score | Metrics | Implicit/Explicit | Justification                                  | Source          |
    | :------------------- | :----------------------------------------------------------- | :--------------- | :----------- | :------ | :---------------- | :--------------------------------------------- | :-------------- |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes
        1.  **Internal Computation (Fig 1g):** Proposed concept of particles with onboard sensor-processor-actor systems. Computation is fully embodied. Realized in cm/mm scale robots, but stated as challenging at microscale.
        2.  **External Computation with Feedback (Fig 1b, Section 1):** Intelligence/computation resides in an external computer that controls particle behavior via fields. Computation is *not* embodied in the particle itself, but the *system* (particle + controller) performs computation.
        3.  **Learning Algorithms (Sections 2, 4):** Reinforcement learning, neural networks involve computation. If implemented externally (most cases discussed for microswimmers), computation is not embodied in the particle. If implemented internally (robots, Fig 1g), it is.
        4.  **Primitive Computation (Fig 1c, 1e):** Particles reacting to environmental gradients (e.g., chemotaxis, sensor-based reaction) can be seen as performing a rudimentary analog computation (gradient sensing/comparison driving motion). This computation is embodied in the particle's physical/chemical response mechanism.

**(Conditional: M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Hybrid (Discusses approaches ranging from analog-like physical responses (Fig 1c, 1e) to digital/algorithmic computation (RL, NNs) implemented externally or potentially internally (Fig 1g, robots)). Neuromorphic approaches are implied by the use of Neural Networks (Section 2, 4).

### **5.3 Computational Primitive:**

    *   Content: Varies depending on the system:
        *   **Gradient Sensing/Following:** (Fig 1c, 1e, Section 2 - chemotaxis) Basic operation is measuring local gradient and setting velocity proportional/aligned to it. (Analog thresholding/response).
        *   **Policy Execution (RL):** Applying a learned mapping from state (sensory input) to action (movement decision). Implemented via Q-table lookup or neural network forward pass. (Algorithmic function approximation/decision making).
        *   **Neural Network Operations:** Weighted sums, activation functions (e.g., in Refs [50, 47, 69]). (Neuromorphic).
        *   **Optimization:** Finding optimal paths (min time), gaits (max speed), or strategies (max reward). (Algorithmic optimization).
        *   **Equation Discovery (Sparse Regression):** Identifying terms in a governing equation that best fit data. (Algorithmic symbolic regression).
    *   **Sub-Type (if applicable):** Gradient Following, Policy Evaluation, Function Approximation, Optimization, Symbolic Regression.

### **5.4 Embodied Computational Units**
| Unit ID          | Description                                     | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification                             |
| :--------------- | :---------------------------------------------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :---------------------------------------- |

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description               | Value                    | Units      | Source              | Implicit/Explicit   | Justification                                     |
        | :---------------------------------- | :----------------------- | :--------- | :------------------ | :------------------ | :------------------------------------------------ |
        | Ballistic Motion (Microswimmer)     | Short                    | s (<< reorientation time) | Sec 1               | Implicit          | Mentioned as initial part of trajectory           |
        | Reorientation Time (Microswimmer)   | τ_R                      | s          | Implicit            | Implicit          | Characteristic time for active diffusion          |
        | Long-time Diffusion (Microswimmer)  | >> τ_R                   | s          | Sec 1               | Explicit          | Crossover to Brownian motion mentioned            |
        | Environmental Memory Decay (Fig 1d) | Varies                   | s          | Sec 1               | Mixed             | Non-Newtonian memory effects mentioned            |
        | Control Loop Frequency (External)   | Varies                   | Hz         | Sec 1               | Implicit          | Feedback control implies characteristic frequency |
        | Decision Time (Agent)             | Varies                   | s / steps  | Sec 4 (Fig 3)       | Mixed             | Neural network computation time implied         |
    *   **Note:** Most timescales are discussed qualitatively or implicitly. Specific values depend heavily on the system.

### **6.2 Active Inference:**

    *   Content: Partial/Unclear

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes
        1.  **Reinforcement Learning:** Agents adapt their behavior (policy) over time through training (trial and error) to improve performance (e.g., find faster paths, better gaits, escape predators, maximize nutrient intake). The learned policy (e.g., Q-table, NN weights) represents a persistent change based on experience. (Sections 2, 3, 4).
        2.  **Genetic Algorithms:** Used to find optimal parameters/strategies through simulated evolution, representing adaptation over generations (Section 2).
        3.  **Learned Collective Strategies:** Groups of agents adapt their collective behavior (e.g., clustering, adaptive, spreading in Fig 4) based on learned communication rules optimized via RL (Section 4).

**(Conditional: If M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The primary adaptation mechanisms discussed are based on Machine Learning:
        1.  **Reinforcement Learning (RL):** Agents learn through trial and error, receiving rewards or penalties for their actions. Algorithms like Q-learning update value estimates for state-action pairs, while policy gradient methods (e.g., Actor-Critic) directly adjust the policy (often represented by Neural Network parameters) to favor actions leading to higher rewards. Adaptation occurs by updating internal parameters (Q-values, NN weights) based on experience (state transitions, rewards). (Sections 2, 3, 4).
        2.  **Neural Networks (NNs):** Used within RL frameworks (e.g., Deep RL) to represent policies or value functions. Adaptation involves adjusting network weights via backpropagation or similar optimization algorithms based on the RL objective function. (Sections 2, 4).
        3.  **Genetic Algorithms (GAs):** Adaptation occurs through simulated evolution, involving selection, crossover, and mutation of parameters or strategies representing individuals in a population over generations. (Section 2).
        4.  **Supervised/Unsupervised Learning:** Mentioned for phase identification (Section 5), where models adapt internal parameters to classify data based on labeled examples (supervised) or find structure in unlabeled data (unsupervised).
        5.  **Sparse Regression:** Used for learning governing equations (Section 5, Fig 6). Adapts by selecting relevant terms and coefficients from a library to best fit observed data.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The review describes a range of functional behaviors achieved or targeted through AI/ML in active matter:
        *   **Navigation/Pathfinding:** Steering a single agent optimally (e.g., fastest path, target finding) through complex environments or motility landscapes (Section 2, Fig 2).
        *   **Optimized Locomotion:** Finding optimal gaits or shapes for self-propulsion (speed maximization) (Section 2).
        *   **Target Search/Chemotaxis:** Finding static or moving targets (e.g., food sources, prey) using learned strategies (Section 2, 3).
        *   **Predator-Prey Interaction:** Learned chasing (predator) or evasion (prey) strategies (Section 3).
        *   **Collective Motion:** Swarming, flocking, clustering, rod rotation (Section 4, Fig 4a).
        *   **Collective Task Performance:** Coordinated nutrient collection (Section 4, Fig 3, 4), potentially cargo transport or group chasing (Section 3).
        *   **Adaptive Collective Distribution:** Spreading or adaptive spatial arrangements based on environmental conditions (Section 4, Fig 4b, 4c).
        *   **Phase Behavior Analysis:** Classification of phases (e.g., MIPS) or swarming patterns (Section 5).
        *   **Model Discovery:** Learning governing equations from trajectory data (Section 5, Fig 6).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The review primarily relies on citing results from other papers (simulations and some experiments). Validation methods implied or shown include:
        *   **Simulation Results:** Trajectory plots (Fig 2), snapshots of collective states (Fig 4), state diagrams based on simulation parameters (Fig 5). Comparison with optimal solutions (e.g., Dijkstra's algorithm in Fig 2).
        *   **Quantitative Analysis:** Performance metrics from cited works (e.g., path time, nutrient consumption rate, classification accuracy), though specific values often not reproduced in the review itself. E.g., Fig 5 presents a state diagram based on quantitative parameters (consumption rate, density). Fig 2 compares learned paths to optimal paths.
        *   **Conceptual Demonstration:** Fig 1 illustrates different levels of intelligence conceptually. Fig 3 illustrates the setup for collective learning. Fig 6 illustrates the equation discovery workflow.

---

#Key: [lagzi_maze_2010]

# No-brainer: Morphological Computation driven Adaptive Behavior in Soft Robots

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of virtual, voxel-based soft robots simulated in the 2D EvoGym environment. The goal is to demonstrate adaptive behavior (locomotion direction change) driven solely by morphological computation, without a separate neural controller ("brain"). Robots are composed of different voxel materials: two passive (soft, rigid), two active (sinusoidally actuating for locomotion), and four "sensory" types that expand or contract in response to the presence or absence of two binary environmental stimuli. Robot morphologies (voxel arrangements) are encoded by Compositional Pattern-Producing Networks (CPPNs) and optimized using the MAP-Elites evolutionary algorithm to achieve specific locomotion patterns (Left/Right movement) under four different combinations of the two binary stimuli. The system demonstrates adaptive behavior through stimulus-induced shape changes affecting locomotion, and explores combining evolved robots into swarms to implement more complex functions like logic gates (e.g., a D-type latch).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters define crucial aspects of the robot's composition and the experimental setup.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the predefined sinusoidal actuation signal applied to the two "active" voxel types (orange and teal). These voxels cyclically expand and contract horizontally with a 180° phase offset. This simulated actuation provides the energy for locomotion.

### **2.2 Energy Transduction**

    *   Content: The energy from the sinusoidal actuation signal is transduced into mechanical work. The cyclical expansion and contraction of active voxels generate internal forces and deformations within the robot's body. These physical forces propagate through the connected voxel structure, interacting with the environment (simulated ground friction/physics) to produce net displacement (locomotion). Sensory voxels modulate this transduction process by changing the robot's overall shape and stiffness distribution in response to stimuli, thus altering how the actuation energy results in movement.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify energy efficiency. Qualitatively, locomotion in evolved soft robots, especially those relying on simple oscillating actuators without coordinated control, is generally highly inefficient. Much energy is likely lost to internal damping within the soft material and non-productive deformation or friction. The focus is on achieving adaptive behavior, not optimizing energy use. Score is low based on general knowledge of the field and lack of optimization focus.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation mechanisms are not explicitly quantified. Qualitatively, dissipation occurs through:
        1.  Internal Damping: Viscoelastic properties of the simulated soft voxels would lead to energy loss during deformation cycles (Medium/High).
        2.  Friction: Interaction with the simulated ground surface during locomotion (High).
        3.  Non-productive Deformation: Actuation energy causing shape changes that do not contribute to net motion (Medium/High).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No (for individual evolved robots); Yes (for the hand-designed swarm)

**(Conditional: M3.1 is "No" for individual robots, skip to Module 4.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Hybrid (Analog & Threshold-based) / Logic Gates (for swarm)

### **5.3 Computational Primitive:**

    *   Content: Stimulus-Response Mapping via Physical Dynamics. The basic operation is the mapping of binary stimulus inputs (patterns of presence/absence of Stimulus 1 and 2) to a binary locomotion output (Left or Right movement). This mapping is physically implemented by how the stimulus-induced shape changes (expansion/contraction of specific voxels) interact with the cyclic actuation and the overall body mechanics.
    *   **Sub-Type (if applicable):** Thresholding (implicit in mapping binary stimuli to binary output), Logic Gate (explicit for swarm: AND, NAND). The paper highlights achieving XOR/XNOR-like behavior patterns (XYYX), which are non-linearly separable, suggesting complexity beyond simple linear thresholding.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Actuation Cycle Period | 1 | Actuation Cycles (relative unit) | Section 2 (Evaluation) | Explicit | The unit "actuation cycle" is used throughout evaluation descriptions. |
        | Stimulus Change Interval | 10 | Actuation Cycles | Section 2 (Evaluation) | Explicit | Stated in evaluation description. |
        | Single Stimulus Simulation | 10 | Actuation Cycles | Section 2 (Evaluation) | Explicit | Stated in evaluation description. |
        | Variable Stimuli Simulation | 40 | Actuation Cycles | Section 2 (Evaluation) | Explicit | Stated in evaluation description. |
        | Total Evolution Time | 3000 | Generations | Section 2 (Parameters) | Explicit | Total generations run. |
    *   **Note:** Absolute time in seconds is not provided, but relative timescales based on "actuation cycles" and "generations" are given.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip to Module 8.)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors are:
        1.  **Adaptive Locomotion:** Individual robots locomote (move Left or Right along the x-axis) in a direction determined by the current pattern of two binary environmental stimuli. The specific mapping (e.g., LLLL, LRLR, LRRL, etc., across the four stimulus patterns) is determined by the robot's evolved morphology.
        2.  **Logic Gate Implementation (Swarm):** Collections of robots, where the behavior (Left/Right movement) of one robot influences the stimulus input of another, are shown to implement Boolean logic functions (specifically AND and NAND gates demonstrated as components of a D-type latch).

### **8.2 Behavior Robustness:**



### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation relies entirely on simulation results within the EvoGym environment.
        *   **Adaptive Locomotion:** Validated by simulating evolved robots under varying stimulus conditions and observing/plotting their center of mass trajectory over time (spacetime diagrams, Figs 2, 3). Performance is quantified by displacement (Fig 4). Control/consistency is checked by comparing behavior in combined vs. single stimulus simulations (Section 2).
        *   **Logic Gate Implementation:** Validated by simulating the hand-designed swarm, tracking the behavior (movement direction dianggap output 0/1) of each robot in response to its immediate stimuli (derived from swarm inputs or other robots' outputs) and demonstrating the overall desired function (D-latch memory behavior) by plotting the state of a specific robot relative to the global swarm inputs (Fig 5).
        *   **Limitations:** Validation is purely computational; no physical hardware realization. Robustness to real-world noise, manufacturing variations, or unmodeled physics is not assessed. Reproducibility is supported by code availability.

---

#Key: [puy_selective_2024]

# Selective social interactions and speed-induced leadership in schooling fish

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system studied is collective motion (schooling) in fish (black neon tetra, *Hyphessobrycon herbertaxelrodi*). The work investigates the social interaction rules governing this behavior, specifically alignment forces. It uses a combination of experimental data analysis (tracking N=39 and N=8 fish in a tank), agent-based modeling (standard model with attraction-repulsion, alignment, friction-propulsion, noise; explicit anti-alignment model; selective interactions model), and a "force map" technique to infer effective forces from acceleration patterns relative to neighbors' position and velocity. The purpose is to understand the mechanisms of movement coordination and emergent leadership in fish schools, proposing that fish selectively interact with (align to) faster neighbors and effectively anti-align with (ignore) slower ones, linking this to speed-based leader-follower dynamics. Components include individual fish agents, their positions, velocities, accelerations, social interaction forces (attraction, repulsion, alignment/anti-alignment), individual forces (friction, propulsion, noise), and the tank environment.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Standard Model: *d*<sub>0</sub> (Equilibrium distance) | 5.8 | cm | Table I (Methods) | Explicit | Medium | Selected to match experimental data (Fig 4b) |
        | Standard Model: *v*<sub>0</sub> (Preferred speed) | 11 | cm/s | Table I (Methods) | Explicit | Medium | Chosen from average experimental speeds (Supp. Fig S10a) |

    *   **Note:** Table I provides several model parameters. Only a subset is listed here as key examples characterizing both experiment and model implementation. Data reliability for model parameters is "Medium" as they are chosen/fitted rather than directly measured fundamental constants.

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**

    *   Content: The model includes a friction term (-v_i / τ) within F_fric-prop, which represents energy dissipation due to hydrodynamic drag. Noise terms (F_noise) also imply energy input/dissipation related to maintaining non-equilibrium movement. However, these are not quantified in terms of energy units or detailed physical mechanisms in the excerpt. Qualitative assessment: Medium (inherent to movement in a fluid).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip to Module 4.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The paper investigates several sets of local interaction rules:
        1.  **Standard Model:**
            *   Attraction/Repulsion (Eq. 1): `F_att-rep = -k(d_ij - d0) * (x_i - x_j) / d_ij`, where `k=k_rep` if `d_ij <= d0` and `k=k_att` if `d_ij > d0`. Acts towards/away from equilibrium distance `d0`.
            *   Alignment (Eq. 2): `F_alig = µ(v_j - v_i)`. Acts to equalize velocities.
            *   Interaction Scope: Averaged over Voronoi neighbors, weighted by inverse distance `w_ij = 1/d_ij`.
            *   Individual Forces: Friction/Propulsion `F_fric-prop = -(v_i - v0*v_hat_i) / τ` and Noise `F_noise = σ_v*ξ_v(t)*v_hat_i + σ_φ*ξ_φ(t)*φ_hat_i`.
        2.  **Explicit Anti-alignment Model:** Replaces standard alignment `F_alig` with a force interpolated from the experimental alignment force map (Fig 2b), which includes alignment with faster neighbors and anti-alignment with slower neighbors. Other forces remain the same.
        3.  **Selective Interactions Model:** Standard model rules apply, but social interactions (attraction, repulsion, alignment) are *only* considered with neighbors `j` moving faster than the focal individual `i` (`v_j > v_i`). Interactions with slower neighbors are ignored (effectively switched off). Weights `w_ij` recalculated considering only faster neighbors.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The emergent global order described is fish schooling, characterized by collective polarization (individuals moving globally in the same direction) and group cohesion (maintaining proximity). Other patterns like milling (rotation around a core) are mentioned as possibilities in general models but not the focus for the studied species. The paper also studies emergent leader-follower structures based on relative speed.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| AttRep | Attraction/Repulsion | k_rep, k_att, d0 | See M4.2.1 | Various | Explicit | Defined in Methods/Table I | Eq.1, Table I |
| Align | Standard Alignment | µ | See M4.2.1 | s<sup>-1</sup> | Explicit | Defined in Methods/Table I | Eq.2, Table I |
| FricProp | Friction/Propulsion | v0, τ | See M4.2.1 | Various | Explicit | Defined in Methods/Table I | Methods, Table I |
| Noise | Active Fluctuations | σ_v, σ_φ | See M4.2.1 | cm/s<sup>3/2</sup> | Explicit | Defined in Methods/Table I | Methods, Table I |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Polarization | Group alignment level | ϕ | [0, 1] | Dimensionless | Explicit | Defined in Results | Eq. in text | Fig 4a |
| Cohesion | Avg. proximity | <d_NN> | ~2-7 (peak ~3-4) | cm | Explicit | Defined in Results | Averaging NN distances | Fig 4b |
| Spatial Extent | Normalized group area | s | PDF shown | cm<sup>2</sup> | Explicit | Defined in Results | Convex hull area / N | Fig 4c |
| Neighbor Stability | Voronoi neighbor persistence | t_V | PDF shown | s | Explicit | Defined in Results | Contact duration time | Fig 4d |
| Leadership | Info flow direction (speed corr.) | Max Corr Time Delay τ | ~ -0.1 to +0.1 | s | Explicit | Defined in Results/Fig 6 | Pearson correlation | Fig 6 |
| Leadership | Info flow direction (orient. corr.) | Max Corr Time Delay τ | ~ -0.05 to +0.05 | s | Explicit | Defined in Results/Fig 6 | Dot product correlation | Fig 6 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
 Rubric: Assess how well the global behavior (Hom(C(A,-), B)) can be predicted solely from the local interaction rules applied to individual components (Hom(A, B)), where A is an individual agent state, B is the global state/observable, and C(A,-) probes the local neighborhood. High score requires faithful representation of local interactions determining global state.
    *   **Metrics:** Yoneda embedding not assessed in paper. Predictability metrics are qualitative comparisons of PDFs and force maps.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip to Module 6.)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Experimental Frame Interval | 1 / 50 = 0.02 | s | Experimental data | Explicit | Inverse of frame rate. |
        | Correlation Time Delay (τ) | approx -0.2 to +0.2 | s | Fig 5, Fig 6 | Explicit | Range observed in leadership correlation plots. |
        | Model Time Step (Δt) | 0.02 | s | Methods | Explicit | Simulation parameter. |
        | Model Speed Relaxation Time (τ) | 1.6 | s | Table I | Explicit | Model parameter. |
        | Voronoi Neighbor Contact Time (t_V) | Median ~1-10 (PDF peak) | s | Fig 4d | Explicit | Measured observable characterizing neighbor stability timescale. |
        | Burst-and-Coast Oscillation Period | ~2-5 (visual estimate) | s | Supp. Fig S15c | Implicit | Estimated from oscillations in speed plots; not explicitly quantified. |

### **6.2 Active Inference:**

    *   Content: Partial
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:**
        *   **Prediction Error Dynamics:** Measure the rate at which a follower's velocity converges to a leader's future velocity after a behavioral change by the leader. `Metric`: Time constant of error decay (`TemporalEvolutionEdge` property).
        *   **Model Complexity/Evidence:** Quantify the statistical evidence (e.g., Bayesian model comparison) favoring the selective interaction model over models with uniform attention, interpretable as evidence for the 'faster is informative' internal model. `Metric`: Bayes Factor (`CognitiveMappingEdge` weight).
        *   **Anticipation Timescale:** Refine the correlation analysis (Fig 6) to precisely measure the average time delay (τ) at which follower velocity best predicts leader velocity, representing the timescale of predictive alignment. `Metric`: Peak correlation delay (`TemporalEvolutionEdge` property).

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip to Module 8.)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behaviors studied are:
        1.  **Collective Schooling:** Coordinated movement of the group, maintaining cohesion and often polarization (alignment of velocities).
        2.  **Alignment/Anti-alignment Patterns:** Specific patterns observed in the alignment force map (Fig 2b), where fish tend to align with faster neighbors and effectively anti-align (turn away) from slower ones.
        3.  **Speed-Induced Leadership:** Dynamically changing leader-follower relationships where faster individuals tend to lead and slower individuals tend to follow, inferred from time-delayed velocity correlations (Fig 6).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation methods include:
        1.  **Force Map Analysis:** Inferring effective forces (alignment/anti-alignment patterns) from experimental trajectory data (Results, Fig 2). Technique validated using a standard model first (Fig 1).
        2.  **Agent-Based Modeling:** Implementing different interaction rules (standard, explicit anti-alignment, selective) and comparing simulation results (force maps, statistical observables) with experimental data (Results, Fig 1, 3, 4, Supp Figs). The selective interaction model provided better qualitative and quantitative agreement (Fig 4).
        3.  **Correlation Analysis:** Calculating time-delayed correlations in velocity (orientation and speed) between neighbors to identify leader-follower relationships based on relative speed (Results, Fig 6), supporting the selective interaction hypothesis.
        4.  **Robustness Checks:** Repeating force map analysis under various conditions (different N, distances, wall proximity, etc.) to ensure the main finding isn't an artifact (Results, Supp Figs S5-S11).
        5.  **Surrogate Data Analysis:** Using time-delayed data of the *same* individual to simulate pure follower/leader scenarios and comparing resulting force maps (Fig 5) to experimental ones.
        Limitations: Force map inferences might not be unique; model parameters are chosen, not rigorously fitted; biological complexity likely exceeds models. Reproducibility depends on access to data/code (links provided).

---

#Key: [shaberi_optimal_2025]

# Optimal network sizes for most robust Turing patterns

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper investigates the robustness of Turing pattern formation in reaction-diffusion systems using network models of varying sizes (N nodes, representing molecular species). It employs random matrix theory (RMT) to analyze the statistical properties of the Jacobian matrices derived from linearized reaction-diffusion equations (specifically focusing on stability via eigenvalues). The system purpose is to determine how network size (N) influences the likelihood and robustness of Turing instabilities (conditions where a system stable without diffusion becomes unstable with diffusion, leading to spatial patterns), particularly Turing Type I. The core components are the reaction network (nodes and interaction links, implicitly defined by the Jacobian matrix structure) and the diffusion process (modeled by diffusion constants for typically two species). The key finding is an optimal network size (N_opt ≈ 5-8) that maximizes the probability of finding robust Turing patterns, arising from a trade-off between stability without diffusion (favored in small N) and instability with diffusion (favored in large N). The study also explores the effect of sparsity and the reduced importance of differential diffusion in larger networks.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters from the initial Hill-function models (b, V, K, n, μ) are also key but are sampled widely rather than set to fixed values for the main RMT analysis. The focus here is on parameters controlling the RMT setup and findings.

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Skipping M3.2-M3.8 as M3.1 is "No")**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The local interactions are governed by the reaction-diffusion equations (Eq. 1): ∂xᵢ/∂t = fᵢ(X; θ) + Dᵢ ∇²xᵢ.
        1.  **Reactions (fᵢ):** These describe how the concentration of species `i` changes due to interactions with other species `X` according to specific kinetics (modeled explicitly with Hill functions in Eq. 2 for small networks, and implicitly via random Jacobian elements `Jᵢⱼ = ∂fᵢ/∂xⱼ` for large networks in Eq. 6). `Jᵢⱼ > 0` implies activation, `Jᵢⱼ < 0` implies inhibition. Diagonal elements `Jᵢᵢ` often include self-degradation (e.g., the `-I` term in Eq. 6, representing `-μᵢxᵢ` kinetics).
        2.  **Diffusion (Dᵢ ∇²xᵢ):** This describes the movement of species `i` down its concentration gradient, governed by its diffusion coefficient `Dᵢ`. Only species 1 and 2 are typically assumed to diffuse (D=diag(D₁, D₂, 0, ..., 0)).
        The analysis linearizes these rules around a steady state X* (Eq. 4) and examines the stability based on the Jacobian matrix `J` modified by diffusion (Eq. 5: J(k) = J₀|ₓ∗ − k²D).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :-------------------- | :---: | :----------: | :----------------: | :------------: |
    | Reaction (Small N) | Hill Function Max Rate | Vᵢ | (0.1, 10) | conc/time | Methods | Explicit | Parameters used for small network simulations based on Hill functions. |
    | Reaction (Small N) | Hill Function Threshold | Kᵢⱼ | (0.01, 1) | conc | Methods | Explicit | Parameters used for small network simulations based on Hill functions. |
    | Reaction (Small N) | Hill Coefficient | nᵢⱼ | 2 | dimensionless | Methods | Explicit | Parameters used for small network simulations based on Hill functions. |
    | Reaction (Small N) | Basal Rate | bᵢ | (0.001, 0.1) | conc/time | Methods | Explicit | Parameters used for small network simulations based on Hill functions. |
    | Reaction (Small N) | Degradation Rate | μᵢ | (0.01, 1) | 1/time | Methods | Explicit | Parameters used for small network simulations based on Hill functions. |
    | Reaction (Large N - RMT) | Jacobian Element Variance | σ² | Varied (e.g., 0.01-0.5) | (conc/time)² or dimensionless | Results, Figs 5, 7, 8 | Explicit | Key parameter controlling interaction strength variability in RMT. |
    | Reaction (Large N - RMT) | Jacobian Element Mean | 0 | (conc/time) or dimensionless | Results (Eq. 6 context) | Explicit | Assumed mean zero for off-diagonal elements in the random matrix G. |
    | Reaction (Large N - RMT) | Self-Interaction (Diagonal) | -1 (Degradation) | 1/time or dimensionless | Results (Eq. 6) | Explicit | Assumed uniform degradation rate for stability in the RMT model J₀ = G - I. |
    | Diffusion | Diffusion Coefficient | D₁, D₂ | Varied (e.g., D₁=1, D₂=10 or D₁, D₂ ∈ [0, 10²]) | space²/time | Figs 5, 8, Methods | Explicit | Controls spatial coupling strength. |

### **4.3 Global Order:**

    *   Content: The emergent global order consists of stationary, periodic spatial patterns in the concentrations of the chemical species (Turing patterns). These patterns have a characteristic wavelength determined by the wave number `k_max` at which the instability is strongest (peak of the dispersion relation Re(λ_max(k)) in Fig. 2A for Turing I).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| R1 | Reaction Kinetics (Large N RMT) | σ² (Jacobian variance) | 0.01 - 0.5 | dimensionless or (conc/time)² | Explicit | Controls interaction strength variability. | Results, Figs 5, 7, 8 |
| R3 | Diffusion (2 species) | D₁, D₂ | 0 - 100 | space²/time | Explicit | Control spatial coupling strength. | Fig 8 |
| R4 | Connectivity/Sparsity | C or p | 0.5, 0.75, 1.0 | dimensionless | Explicit | Fraction of non-zero off-diagonal Jacobian elements. | Results, Figs S2, S3 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| P1 | Turing Pattern Presence | Probability (%) | 0 - ~14% (for Turing I) | % | Explicit | Frequency of Turing I instability in sampled random matrices. | Figs 6, 7, 8 | Linear Stability Analysis, Matrix Diagonalization | Results, Figs 6, 7, 8 |
| P3 | Characteristic Wavelength | k_max | > 0 (for Turing I) | 1/space | Explicit | Wave number at the maximum of Re(λ_max(k)) for Turing I. | Fig 2A, Results | Linear Stability Analysis | Fig 2A, Results |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    *   **Metrics:** Percentage occurrence of Turing patterns.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Skipping M5.2-5.4 as M5.1 is "No")**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Inverse Max Eigenvalue Growth Rate (Re(λ_max(k>0))⁻¹) | > 0 (Instability) | time | Results, Fig 2 | Explicit | Represents the characteristic time for pattern amplitude to grow initially (inverse of the positive real part of the dominant eigenvalue). |
        | Inverse Max Eigenvalue Decay Rate (abs(Re(λ_max(k=0)))⁻¹) | > 0 (Stability) | time | Results, Fig 2 | Explicit | Represents the characteristic time for homogeneous perturbations to decay (inverse of the magnitude of the negative real part of the dominant eigenvalue at k=0). |
        | Species Degradation Timescale (μᵢ⁻¹) | (1, 100) for small N; 1 for Large N RMT | time (arbitrary units) | Methods (small N), Eq. 6 (large N) | Explicit | Characteristic lifetime of a molecule before degradation. Large N model assumes uniform timescale. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Skipping M7.2 as M7.1 is "No")**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior described is the spontaneous formation of stationary, periodic spatial patterns (Turing patterns) from an initially homogeneous state due to diffusion-driven instability. Specifically, the focus is on Turing Type I patterns, which exhibit a characteristic wavelength (Fig 2A). The paper also identifies Turing Type II and Turing-Hopf instabilities as possible outcomes.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claims of emergent behavior (Turing patterns) and their robustness are validated through theoretical analysis and computational simulations.
        1.  **Operational Definition:** Turing instability is operationally defined via linear stability analysis: Re(λ_max(k=0)) < 0 and Re(λ_max(k>0)) > 0. Turing I requires Re(λ_max(k)) to have a maximum at k_max > 0 and become negative for large k (Fig 2A).
        2.  **Methodology:** The study uses established methods: reaction-diffusion PDEs, Jacobian matrix construction, eigenvalue analysis (dispersion relations), and random matrix theory sampling.
        3.  **Quantitative Analysis:** Robustness is quantified by the percentage of sampled random matrices exhibiting the defined Turing instabilities under varying parameters (N, σ², D₁, D₂, sparsity) (Figs 6, 7, 8, S2-S7). Statistical significance is assessed for Jacobian element differences (Fig 4A).
        4.  **Reproducibility:** The methodology is clearly described, and code is made available (Methods section), allowing for reproducibility.
        5.  **Limitations:** The validation relies on linear stability analysis (predicts onset of instability, not final pattern) and RMT assumptions (e.g., Gaussian distribution of Jacobian elements, independence, uniform degradation). The connection to specific biological systems relies on the plausibility of the RMT assumptions. The analysis assumes an infinite domain.

---

#Key: [nakajima_information_2015]

```markdown
# Information processing via physical soft body

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system utilizes the physical dynamics of a soft silicone arm immersed in water as a computational resource, specifically as a reservoir for reservoir computing. The purpose is to emulate desired nonlinear dynamical systems (NARMA tasks) in real-time. The system components include a soft silicone arm (cone-shaped, 44.7 cm long), 10 embedded bend sensors (Flexpoint Sensor Systems), a servo motor (Dynamixel RX-64) for base actuation, a water tank, and a PC for control, data processing (input/output mapping, readout weight training), and sensing/actuation loop execution. Input motor commands `m(t)` control the base rotation, generating complex arm dynamics influenced by water interaction. Sensor readings `s_i(t)` reflect the arm's posture. The computational output `O(t+1)` is generated by a linear weighted sum of sensor values plus a bias, where weights `w_i` are trained using linear regression (Moore-Penrose pseudo-inverse) on collected sensor data and target NARMA outputs.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters defining the arm dimensions and sensor count are crucial for the physical reservoir. The time step determines the operational speed.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Electrical energy supplied to the servo motor (Dynamixel RX-64) to induce rotation at the base of the soft arm.

### **2.2 Energy Transduction**

    *   Content: 1. Electrical energy is converted to mechanical rotational energy by the servo motor. 2. Mechanical rotational energy at the base is transferred to the soft arm, causing deformation (kinetic and potential elastic energy). 3. Interaction between the moving arm and water converts some mechanical energy into fluid motion (kinetic energy of water) and dissipative forces (viscous friction, generating heat - thermal energy). 4. Arm deformation causes mechanical strain in the bend sensors. 5. Bend sensors convert mechanical strain into changes in electrical resistance (mechano-electrical transduction). 6. The PC consumes electrical energy for processing sensor data, calculating outputs, training weights, and sending motor commands.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Low. The primary goal is information processing, not efficient energy use for actuation or computation. Significant energy is likely lost in the servo motor, arm deformation (viscoelasticity), and especially through viscous dissipation in the water. The computational aspect requires a PC, which is orders of magnitude less efficient than specialized hardware. No efficiency metrics are provided in the paper. The score reflects the likely high energy dissipation in multiple steps without optimization for efficiency.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include: 1. Electrical resistance heating in the servo motor windings. 2. Mechanical friction within the servo motor gearbox. 3. Viscoelastic damping within the silicone arm material during deformation cycles. 4. Viscous drag and friction between the moving soft arm and the surrounding water (likely the dominant factor, converting mechanical energy to heat in the water). 5. Electrical resistance heating in sensor circuitry. 6. Heat generated by the PC during computation. Quantification is not provided, but dissipation due to water interaction is qualitatively described as significant ("damping effect provided by the underwater environment", Discussion). Assessment: High.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding with M3.2-M3.8)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", proceeding with M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Reservoir Computing (Physical Implementation)

### **5.3 Computational Primitive:**

    *   Content: Nonlinear state transformation and temporal integration. The input motor command signal `I(t)` is transformed nonlinearly by the physical dynamics of the actuated soft arm in water into a high-dimensional state represented by the sensor readings `s_i(t)`. This process inherently involves temporal integration due to the system's memory properties (fading memory from viscoelasticity and fluid dynamics). This complex, spatio-temporal transformation serves as the basis for computation, similar to the function of a kernel or the reservoir dynamics in standard RC. The final step is a linear combination (weighted sum) performed externally.
    *   **Sub-Type (if applicable):** Physical Reservoir Dynamics (Nonlinear mapping + Fading Memory)

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Sensing/Actuation Loop | ~0.03 | s | Results | Explicit | Stated as the unit of timestep 't'. |
        | Input Signal Periodicity (controlled by T) | 100*T | timesteps | Results (Eq 4 Note) | Explicit | Input `I(t)` period derived from `T` parameter which controls phase velocity. `T` varied from 100 to 400. |
        | Washout Phase | 1000 | timesteps (~30s) | Results | Explicit | Duration specified for initial transient dynamics to settle. |
        | Training Phase | 5000 | timesteps (~150s) | Results | Explicit | Duration specified. |
        | Evaluation Phase | 5000 | timesteps (~150s) | Results | Explicit | Duration specified. |
        | Memory Retention (effective for NARMA20) | >= 20 | timesteps (~0.6s) | Results (Fig 3a) | Implicit | System successfully emulates NARMA20, which requires memory of at least 20 steps. Actual retention may be longer but weaker. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No (within the core physical system during operation)

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is the real-time emulation of target nonlinear dynamical systems (specifically, NARMA systems of varying orders from 2 to 20). The system takes a time-varying input signal `I(t)` (scaled motor command) and generates a time-varying output signal `O(t+1)` that approximates the output `y(t+1)` of the target NARMA system. This involves leveraging the complex, nonlinear dynamics and fading memory of the actuated soft arm in water. A secondary behavior involves multitasking, simultaneously emulating five different NARMA systems using the same physical reservoir dynamics but different linear readouts.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary behavior (NARMA emulation) is validated quantitatively using Normalized Mean Squared Error (NMSE, Eq. 6) between the system's output and the target NARMA output during a dedicated evaluation phase (5000 timesteps distinct from training). Performance is averaged over 20 trials for statistical significance (Results, Fig 3a). Control comparisons are made against a simple Linear Regression (LR) model (significant improvement shown, p<0.01) and a standard Leaky Integrator Echo State Network (LESN) simulation (system performs comparably to average LESN, worse than optimal LESN). Reproducibility is demonstrated through repeated trials. Limitations include testing only on NARMA tasks and sinusoidal inputs, and lack of analysis on sensitivity to physical parameters. (Results, Fig 2, Fig 3, Supp Table S1).

---

#Key: [ebbens_active_2016]

# Active colloids: Progress and challenges towards realising autonomous applications

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews active colloids, which are micron or sub-micron scale materials capable of autonomous or enhanced motion in fluid environments, distinguishing them from passive Brownian particles. The primary purpose discussed is achieving autonomous function (e.g., transport, remediation) without external intervention. Components vary but typically involve a colloidal body (sphere, rod, tube) with asymmetric properties (e.g., catalytic coating like Platinum, Iridium, Manganese, Zinc; shape asymmetry; mass asymmetry) that interact with a fuel source (e.g., hydrogen peroxide, hydrazine, water/acid, seawater) or environment to generate propulsion via mechanisms like self-diffusiophoresis, self-electrophoresis, or bubble detachment. Key functionalities explored are autonomous motion, guidance (via chemotaxis, gravitaxis, physical structures), solution compatibility, manufacturing methods, and performing tasks like cargo transport, environmental remediation, lab-on-a-chip transport, and potential in vivo drug delivery.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Propulsion Speed | Variable (µm/s range implied) | m/s | General context, e.g., Sec 1.2, 2.1 | Implicit | Low | Inferred from context and goal of enhanced motion. |

    *   **Note:** Values are often ranges or typical examples cited in the review context. Reliability is Medium as they are cited from other works. Propulsion speed is highly variable and not specified generally.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is chemical potential energy stored in dissolved fuel molecules (e.g., hydrogen peroxide, hydrazine) or derived from reactions with the environment (e.g., Zn in acid, Mn in seawater).
    *   Units: J (or J/mol)

### **2.2 Energy Transduction**

    *   Content: Chemical energy is transduced into kinetic energy (motion) via several mechanisms depending on the colloid type:
        1.  **Phoretic Mechanisms:** Catalytic decomposition of fuel (e.g., H2O2 by Pt) creates local gradients (solute concentration, charge/ions). These gradients interact with the colloid surface, generating fluid flow relative to the particle (self-diffusiophoresis) or electrophoretic forces due to self-generated electric fields (self-electrophoresis), resulting in motion. Discussed for nanorods and non-bubbling Janus spheres (Sec 1.2).
        2.  **Bubble Propulsion:** Catalytic reactions produce gas products (e.g., O2 from H2O2, H2 from Zn/acid or Mn/seawater). Bubbles nucleate, grow, and detach asymmetrically (due to Janus structure, tube opening, or even stochasticity on symmetric surfaces), providing momentum transfer that propels the colloid. Discussed for larger Janus spheres, symmetrical swimmers, and nanotubes (Sec 1.2).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The review doesn't provide quantitative efficiency values. However, it implies low efficiency by mentioning the need for relatively high fuel concentrations (1-10% H2O2 for Pt, Sec 2.2) for many systems and highlighting efficiency improvements with Ir/N2H4 (Sec 2.2) or fuel depletion issues (Sec 2.2). Low Reynolds number hydrodynamics are inherently inefficient for propulsion. Qualitative assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: The primary energy dissipation mechanism is viscous drag within the fluid environment, inherent to motion at low Reynolds numbers (mentioned implicitly via context, e.g., Sec 1.1). Thermal energy is also dissipated due to the exothermic nature of the catalytic reactions, though this is not the primary focus. For bubble propulsion, energy is lost in bubble formation (surface energy) and acoustic emissions during detachment (not mentioned). Quantification is not provided. Qualitative assessment: High (due to low Re).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Skipping M3.2-M3.8 as M3.1 is "No")**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The review doesn't detail the local interaction rules for collective behavior. It mentions steric and hydrodynamic interactions in the context of physical guidance around obstacles (Sec 2.1), and chemotaxis involves interactions with chemical gradients (Sec 2.1). For collective behavior, rules would involve hydrodynamic coupling, phoretic field interactions, and potential steric forces, but these are not elaborated upon.
    * **Implicit/Explicit**: Implicit

### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**

    *   Content: The review mentions potential outcomes like accumulation (chemotaxis, physical trapping), circling around obstacles, and unspecified "collective behaviour" or "self-assembly". No specific emergent patterns (like swarms, clusters with defined structure, lanes) are described in detail.
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Skipping M5.2-5.4 as M5.1 is "No")**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Brownian Rotational Diffusion (Janus Spheres) | ~10s | s | Sec 2.1 | Explicit | Time over which orientation randomizes. |
        | Motion Persistence (Rods/Tubes) | >10s (Qualitative) | s | Sec 2.1 | Implicit | Stated as greater persistence than spheres, but value not given. |
        | Bubble Cycle Time (Bubble Swimmers) | Variable (ms to s range likely) | s | Implied by Figs 1c, 1d | Implicit | Inferred from visual nature of bubble propulsion. |
        | Reaction Kinetics | Fast (µs to ms likely) | s | General Chemical Knowledge | Inferred | Typical timescale for catalytic reactions, not stated. |
        | Fuel-free Motion Duration (Mn example) | ~1 | minute | Sec 2.2 | Explicit | Duration before colloid is consumed. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Skipping M7.2 as M7.1 is "No")**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behavior is **autonomous propulsion** (enhanced, directed, or random motion compared to Brownian diffusion). Secondary functional behaviors enabled by this motion include:
        *   **Guidance/Navigation:** Directed movement in response to gravity (gravitaxis), chemical gradients (chemotaxis), or physical structures.
        *   **Cargo Transport:** Attaching and moving cargo (molecules, particles, cells).
        *   **Environmental Remediation:** Neutralizing/collecting contaminants (e.g., chemical warfare agents, oil droplets).
        *   **Mixing:** Enhancing fluid mixing rates.
        *   **(Potential) Drilling:** Penetrating soft materials (mentioned for spiral shapes).
        *   **(Potential) In vivo delivery:** Enhanced residence time/delivery in biological environments (e.g., stomach).

### **8.2 Behavior Robustness:**

        *   **Sensitivity to Solution:** Phoretic swimmers (rods, some Janus) are sensitive to ionic strength (salt quenches motion, Sec 2.2). Bubble swimmers may require surfactants (Sec 2.2, 3.1). Fuel-free swimmers can be consumed (Sec 2.2). Performance sensitive to fuel concentration. Some systems affected by fouling (Sec 3.3).
        *   **Directional Robustness:** Often poor due to Brownian motion and chaotic bubble release (Sec 2.1), requiring specific guidance strategies.
        *   **Operational Lifetime:** Limited by fuel depletion (Sec 2.2) or reactant consumption (Sec 2.2).
        Overall, many systems are sensitive to environmental conditions and exhibit limited directional control or lifetime, indicating moderate to low robustness without specific engineering/strategy.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The review cites experimental work validating the primary behavior (motion) typically via video microscopy and particle tracking to measure speeds and trajectories (implied by figures like 1c, 1d, 2a, 3). Functional behaviors like remediation (Fig 4), cargo transport (Fig 5, Sec 2.4), and in vivo delivery (Fig 6) are validated through specific assays (e.g., degradation measurements, fluorescence tracking, comparing outcomes with inactive controls). Guidance mechanisms are validated by observing trajectories under specific conditions (gradients, fields, structures - Figs 2, 3). Robustness issues (e.g., salt effects) are validated by measuring speed vs. concentration (Sec 2.2). Reproducibility/reliability are generally not discussed at the review level.

---

#Key: [heyman_supermind_2024]

# Supermind Ideator: How Scaffolding Human-AI Collaboration Can Increase Creativity

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is "Supermind Ideator," a web application designed to assist users in creative problem-solving, specifically idea generation. It leverages a Large Language Model (LLM, specifically GPT-3.5 Turbo at the time of the study) augmented with specialized prompts (zero-shot, few-shot), fine-tuning based on a corpus of case studies (~1600 examples), and a structured user interface (UI). The UI provides scaffolding, guiding users through conceptual "moves" based on the Supermind Design methodology (e.g., Zoom In/Out, Analogize, Groupify, Cognify, Technify) organized into "movesets" like "Explore Problem" and "Explore Solutions". Its purpose is to enhance human creativity by generating potentially novel ideas for users to consider, rate, bookmark, and iterate upon, ultimately aiming to produce more innovative solutions compared to unaided human ideation or standard LLM interaction (like ChatGPT). It particularly focuses on generating ideas for designing "superminds" (collectively intelligent systems). Components include: User Interface (React framework), API, LLM (GPT-3.5 Turbo), Prompting Logic (Zero-shot, Few-shot), Fine-tuned LLM versions, Case Study Corpus, User Interaction Features (Rating, Bookmarking).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
    *   **Note:** These parameters relate directly to the AI component's implementation and configuration described in the paper. Other parameters like UI response times or API latency are not discussed.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The system is a software application running on computer hardware. The primary energy source is electricity powering the servers running the LLM, API, and the user's client machine running the web UI.

### **2.2 Energy Transduction**

    *   Content: Energy transformations involve electrical energy being converted into computational processing within the CPU/GPU/TPU (performing LLM inference, API logic, UI rendering) and data transmission over networks. A significant portion is dissipated as heat.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any information on the energy efficiency of the Supermind Ideator system or the underlying LLM computations. Assessing this would require hardware specifications and power consumption measurements, which are outside the scope of the study. Qualitatively, LLM inference is known to be energy-intensive.

### **2.4 Energy Dissipation**

    *   Content: The primary dissipation mechanism is heat generated by the computational hardware (servers, user's computer) during operation. Quantification is not possible based on the paper. Qualitatively likely High due to LLM usage.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: LLM Weights: Long-term; Fine-tuning Data: Long-term (encoded in model); User Bookmarks: Medium-term (session/account bound); Session History: Short-term (session bound).
*    Units: Qualitative Descriptors

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | User Interaction Turn | Variable (seconds to minutes) | s/min | Implicit | Implicit | Inferred from the nature of human-computer interaction in creative tasks. |
        | LLM Response Time | Variable (seconds) | s | Implicit | Implicit | Typical LLM API response times, not specified in paper. |
        | Experimental Task Duration | Variable (not limited) | min/hr | Section 5.1.1 | Explicit | Participants had no time limit. |
    *   **Note:** The paper focuses on the *outcome* (idea quality) rather than detailed temporal dynamics of the interaction, though conversational turns were measured (Fig 6).

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is the generation of textual ideas intended to aid human creative problem-solving. The system takes a user's problem statement and a selected "move" (or moveset) as input and produces one or more relevant textual suggestions (ideas, problem reformulations, analogies, etc.) as output, leveraging an LLM. Users interact with these outputs (reading, rating, bookmarking) to further their own ideation process. A key observed outcome of the human-AI coupled system is the generation of ideas rated significantly higher in innovativeness compared to humans working alone or using un-scaffolded ChatGPT.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary emergent outcome claimed is increased innovativeness of generated ideas. This was validated through a controlled experimental study (Section 5) with three conditions (Ideator, ChatGPT, Human Only). Participants generated ideas for two specific problems. These ideas (N=611 for baseline, N=603 for experimental phase) were then rated for innovativeness (1-5 scale) by a separate group of evaluators (N=505), with good inter-rater reliability (ICC=0.81). Statistical analysis (Welch's ANOVA, Games-Howell post-hoc) showed significantly higher innovativeness ratings for the Ideator condition (M=3.21) compared to ChatGPT (M=2.96) and Human Only (M=2.81) (Figure 4, Section 5.2.1). The validation relies on subjective human ratings of innovativeness as the key metric. Reproducibility relies on access to the system and replicating the study design. Limitations include the specific problems tested and the specific population (Prolific workers).

---

#Key: [reiter_memorizing_2020]

# The memorizing capacity of polymers

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is polymeric materials, specifically focusing on how their properties are influenced by their history and deviation from equilibrium conformational states. It posits that this history dependence constitutes a form of "memory." The key components are polymer chains and their conformations (arrangements in space). The purpose is to conceptually frame how non-equilibrium states in polymers, induced by processing (e.g., drying, cooling, shear, crystallization), act as stored information ("memory") that dictates macroscopic properties (e.g., modulus, transition temperatures, shape) and allows for responsiveness and adaptability. It proposes relating memory capacity to conformational entropy reduction compared to the equilibrium random coil state.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                   | Value                     | Units       | Source (Fig/Table/Section)   | Implicit/Explicit   | Data Reliability (High/Medium/Low)   | Derivation Method (if Implicit)   |
        | :------------------------------- | :------------------------ | :---------- | :--------------------------- | :------------------ | :----------------------------------- | :-------------------------------- |
        | Relaxation Time (τ)              | Varies (e.g., >> τ_reptation) | s           | Section II, Fig 3b, 3c       | Mixed               | Medium (Examples cited)              | Inferred significance from context|

    *   **Note:** Parameters listed relate to the *concept* of polymer memory as discussed, drawing from cited examples (Fig. 3) and conceptual arguments. Conformational entropy is presented conceptually as the key parameter, though not quantified directly in the excerpt. Relaxation times and residual stresses are exemplified via cited data.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Energy is input during processing to drive the polymer system out of equilibrium. Specific sources mentioned include: mechanical forces (shear, drawing, extrusion, load application), thermal energy changes (rapid cooling/quenching, annealing steps, controlling crystallization temperature), and implicitly, energy associated with solvent removal (changes in chemical potential during drying).
    *   Units: J (or related units like Pa for stress work, J/s for power during shear)

### **2.2 Energy Transduction**

    *   Content: Input energy (mechanical, thermal) is transduced into stored potential energy associated with non-equilibrium polymer chain conformations (e.g., stretched chains, strained segments, kinetically trapped states in glasses or crystals). This stored energy corresponds to a reduction in conformational entropy compared to the equilibrium state. During relaxation/equilibration, this stored potential energy is dissipated as heat and/or work done on the surroundings as the system increases its conformational entropy.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss the efficiency of storing energy in non-equilibrium states. The focus is on the *existence* and *persistence* of these states, not the thermodynamic efficiency of creating them. Efficiency would depend heavily on the specific process (e.g., mechanical deformation vs. quenching) and material. Qualitatively, many processes leading to glassy or meta-stable crystalline states are likely inefficient, involving significant dissipation.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation occurs during the relaxation or equilibration process as the polymer "forgets" its history and moves towards equilibrium conformations. Mechanisms include viscous flow (segmental motion, reptation), heat release during structural relaxation (e.g., enthalpy recovery in glasses), and potentially mechanical work if the relaxation drives macroscopic shape changes. The paper emphasizes that relaxation towards equilibrium (dissipation of stored energy/increase in entropy) can be extremely slow ("exceedingly long equilibration times"), allowing the memory state to persist.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceed with M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable, potentially "exceedingly long"
*    Units: s (or Qualitative Descriptor: "Long-term", potentially > service life)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Conceptual ("Immense variability", "tremendous number")

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Inverse of Relaxation Time (τ)
    *   Units: 1/s

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes (Implicitly, in some examples)

**(Conditional: M4.1 is "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: Based on general polymer physics principles (not explicitly detailed as 'rules' in the paper):
        *   **Crystallization:** Monomer/segment attachment to a growing crystal face, governed by thermodynamic driving force (undercooling), surface energy penalties, chain mobility/diffusion, chain folding energetics, topological constraints (entanglements).
        *   **Shear Alignment/Stretching:** Balance between chain elasticity (entropic restoring force), friction with surroundings (solvent/melt), and applied shear force. Includes excluded volume interactions and entanglement effects.
        *   **Glass Formation:** Slowing down of segmental dynamics upon cooling, leading to kinetic arrest governed by local free volume and cooperative rearrangements.
        *   **Chain Coupling:** Implicitly mentioned (Section III) potential alignment leading to effective coupling via summed weak interactions, requiring cooperative relaxation. Governed by intermolecular forces (van der Waals) and topological constraints.
    * **Implicit/Explicit**: Implicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID           | Description                           | Parameter Name           | Parameter Value Range | Units       | Data Source   | Implicit/Explicit   | Justification                                           |
    | :---------------- | :------------------------------------ | :----------------------- | :-------------------- | :---------- | :------------ | :------------------ | :------------------------------------------------------ |
    | Crystallization   | Segment attachment kinetics           | Activation Energy (ΔE_a) | Variable              | J/mol       | General Phys. | Implicit          | Standard Arrhenius/nucleation theory parameter          |
    | Crystallization   | Thermodynamic driving force           | Undercooling (ΔT)      | Variable              | K           | Section II.C  | Explicit          | Mentioned as condition                            |
    | Shear Alignment   | Flow strength vs. relaxation        | Weissenberg Number (Wi)  | Variable              | Dimensionless | General Phys. | Implicit          | Standard parameter characterizing flow                  |
    | Chain Coupling    | Intermolecular interaction strength | Interaction Energy (ε)   | Variable              | J           | General Phys. | Implicit          | Underlying VdW forces                             |
    | Glass Formation   | Segmental mobility                    | Activation Energy (E_a)  | Variable              | J/mol       | Fig 3b        | Mixed             | Cited Arrhenius behavior implies this parameter exists |

### **4.3 Global Order:**

    *   Content:
        *   **Crystalline Structures:** Lamellae, spherulites, single crystals (potentially faceted or branched), shish-kebab structures. Characterized by degree of crystallinity, crystal orientation, lamellar thickness, morphology. (Section II.C)
        *   **Oriented Structures:** Alignment of polymer chains along a preferred direction, induced by shear, drawing, or potentially rapid drying. Characterized by order parameters (e.g., Hermans orientation function). (Section II, III)
        *   **Glassy State:** Amorphous solid with frozen-in density fluctuations and potentially non-equilibrium conformations/stresses. (Section II.B)
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID           | Description                           | Parameter                | Value Range   | Units       | Implicit/Explicit   | Justification                                        | Source        |
| :---------------- | :------------------------------------ | :----------------------- | :------------ | :---------- | :------------------ | :--------------------------------------------------- | :------------ |
| Crystallization   | Segment attachment/detachment rates   | Rate Constants (k<sub>a</sub>, k<sub>d</sub>) | Variable      | 1/s         | Implicit          | Fundamental to kinetic models of crystallization     | General Phys. |
| Crystallization   | Chain folding energy barrier          | Folding Energy (ε<sub>f</sub>)   | Variable      | J           | Implicit          | Key factor in determining lamellar thickness         | General Phys. |
| Shear Alignment   | Monomer Friction Coefficient          | Friction (ζ)           | Variable      | kg/s        | Implicit          | Represents interaction with surrounding medium       | General Phys. |
| Chain Coupling    | Alignment-dependent interaction       | Coupling Strength (J)    | Variable      | J           | Implicit          | Conceptual parameter for cooperative behavior        | Section III   |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID       | Description                     | Parameter               | Value Range     | Units         | Implicit/Explicit   | Justification                                | Protocol                | Source        |
| :---------------- | :------------------------------ | :---------------------- | :-------------- | :------------ | :------------------ | :------------------------------------------- | :---------------------- | :------------ |
| Crystallinity     | Fraction of crystalline material| Degree of Crystallinity (X<sub>c</sub>)| 0 - 1           | Dimensionless | Explicit          | Standard polymer characterization parameter  | DSC, XRD, Density       | General Phys. |
| Orientation       | Average chain alignment         | Order Parameter (S)     | -0.5 to 1       | Dimensionless | Explicit (Concept)| Mentioned implicitly via alignment/stretching| Birefringence, XRD, NMR | Section III   |
| Lamellar Struct.  | Thickness of crystal lamellae   | Lamellar Thickness (l<sub>c</sub>)| Variable        | nm            | Explicit          | Mentioned in context of crystals           | SAXS, TEM, AFM          | Section II.C  |
| Residual Stress   | Stored mechanical stress        | Stress (σ<sub>res</sub>)    | Variable        | Pa            | Explicit (Fig 3a) | Example given                             | Film Curvature, XRD     | Fig 3a        |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type                          | Description                                                                 | Predictability   | Yoneda Score   | Metrics   | Implicit/Explicit   | Justification                               | Source   |
    | :--------------------------------- | :-------------------------------------------------------------------------- | :--------------- | :------------- | :-------- | :------------------ | :------------------------------------------ | :------- |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip to Module 6.)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description             | Value                              | Units   | Source                    | Implicit/Explicit   | Justification                                                         |
        | :-------------------------------- | :--------------------------------- | :------ | :------------------------ | :------------------ | :-------------------------------------------------------------------- |
        | Processing Timescale              | Variable, potentially short        | s       | Section II                | Explicit          | Mentioned relative to relaxation times (e.g., rapid cooling/drying) |
        | Segmental Relaxation (α-relaxation) | Variable (T-dependent)             | s       | Fig 3b (Implied)          | Implicit          | Related to cited activation energy                            |
        | Chain Relaxation (Reptation Time) | Variable (MW-dependent)            | s       | Fig 3b, 3c (Comparison) | Explicit (Mentioned)| Used as benchmark equilibrium timescale                       |
        | Memory Retention/Relaxation Time  | Variable, potentially >> Reptation | s       | Section II, Fig 3b, 3c    | Explicit          | Central concept, examples given showing very long times               |
        | Annealing/Aging Time              | Variable                           | s       | Section I, II, Fig 3      | Explicit          | Experimental parameter controlling degree of relaxation               |
        | Crystal Growth Rate               | Variable (ΔT-dependent)          | m/s     | Section II.C (Implied)    | Implicit          | Underlies crystallization kinetics discussion                       |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes (In the sense of history-dependent state modification)

**(Conditional: M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The "adaptation" mechanism is the physical relaxation or evolution of the non-equilibrium conformational/morphological state over time, driven by thermodynamic forces (seeking lower free energy/higher entropy) and governed by kinetics (activation barriers, mobility). Processing steps (cooling, shear, drying, crystallization) induce the initial non-equilibrium state. Subsequent annealing or aging allows the system to explore accessible states, relaxing towards equilibrium at a rate determined by temperature and material properties (e.g., segmental mobility, entanglement constraints, crystal stability). In shape memory polymers, deformation above Tg followed by cooling below Tg/Tcrystallization traps strained conformations; reheating allows relaxation back to the original shape. This is not learning in the machine learning sense, but rather physical state evolution influenced by history and environment.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary "behaviors" discussed are the history-dependent macroscopic properties resulting from the stored memory (non-equilibrium states). These include:
        *   Altered mechanical properties (e.g., modulus, yield stress).
        *   Modified thermal properties (e.g., glass transition temperature, melting behavior, nucleation rates - Fig 3c, 3d).
        *   Presence of residual stresses (Fig 3a).
        *   Shape memory effects (contraction/re-elongation upon temperature change).
        *   Altered relaxation dynamics (e.g., slower stress relaxation - Fig 3b).
        *   Modified optical properties (e.g., anisotropy).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper functions as a perspective, citing experimental results from other studies (Fig. 3) to validate the *existence* of history-dependent properties (memory). Validation methods in the cited works likely include standard materials characterization techniques:
        *   Stress measurement (e.g., film curvature for residual stress - inferred for Fig 3a).
        *   Thermal analysis (DSC for Tg, melting points).
        *   Microscopy (optical, AFM, TEM for morphology, crystal counting - inferred for Fig 3c, 3d).
        *   Mechanical testing (DMA, tensile tests for modulus, relaxation - inferred for Fig 3b).
        *   Scattering (SAXS, WAXS for structure).
        The paper itself provides a conceptual synthesis rather than primary validation. The link between observed macroscopic behavior and the underlying hypothesized non-equilibrium *conformations* is often inferred or supported by simulations/theory in the broader literature, not always directly proven experimentally in the cited examples.

---

#Key: [milkowski_morphological_2018]

# Morphological Computation: Nothing but Physical Computation

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The "system" under analysis is the concept of Morphological Computation (MC) itself. The paper argues against the claim that MC is a substantially different or novel kind of physical computation. It deconstructs MC by analyzing its definitions and purported examples, classifying them into three kinds based on Müller & Hoffman [7]: (1) morphology facilitating control (e.g., passive dynamic walkers), (2) morphology facilitating perception (e.g., cricket ears), and (3) morphological computation proper (hybrid systems, e.g., reservoir computing). The paper's purpose is to demonstrate that cases of MC are either not genuinely computational (type 1) or are simply instances of physical computation (types 2 & 3) that don't warrant a special category, criticizing associated concepts like "offloading" and morphology as direct environmental models (Friston critique).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters here represent key conceptual distinctions or classifications central to the paper's argument about Morphological Computation, rather than physical measurements of a specific system. Data Reliability is 'High' in the context of reflecting the author's explicitly stated arguments and conclusions.

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper argues *against* the notion that MC inherently offers a "free computational resource" or superior efficiency (Sec 3). It notes that efficiency gains might occur in specific cases (e.g., direct physical simulation replacing digital simulation) but that other MC implementations (e.g., mechanical XOR gate) could be less efficient than electronic counterparts. It also mentions trade-offs involving energy costs. However, no specific efficiency values or systematic analysis for a defined MC system are provided. Assessment is qualitative and focused on refuting a general claim.

### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", including M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Other (Analyzes Multiple Types)

### **5.3 Computational Primitive:**

    *   Content: Other (Discusses General Computational Capabilities) The paper doesn't focus on a single computational primitive embodied by morphology. It discusses:
        * General computation in the context of physical systems (implementing FSMs/Turing machines - Sec 2).
        * Specific logic gates (XOR gate robot example - Sec 3).
        * Complex functions potentially computed by hybrid systems like reservoir computing (Sec 2).
        * The core argument is about *whether* morphology performs computation distinguishable from standard physical computation, rather than identifying a specific primitive unique to it.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Cricket ear frequency sensitivity | 4-5 | kHz | Sec 3 (via [45]) | Explicit | Specific frequency range cited from reference [45]. |

    *   **Note:** Timescales are discussed primarily in comparative or conceptual terms (faster/slower, complexity classes) rather than specific measured durations.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The paper discusses behaviors associated with purported examples of MC:
        *   Locomotion (Passive Dynamic Walker - argued non-computational)
        *   Perceptual Filtering/Directionality (Cricket Ears - argued as MC facilitating perception, = physical computation)
        *   Logical Operation (XOR Gate - argued as MC proper, = physical computation)
        *   Complex Function Computation (Reservoir Computing - argued as MC proper, = physical computation)
        The paper's main point is *not* describing these behaviors in detail, but classifying the underlying mechanism WRT computation.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

---

#Key: [rouleau_multiple_2023]

# The Multiple Realizability of Sentience in Living Systems and Beyond

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The "system" discussed is the abstract concept of cognitive functions (including intelligence, decision-making, learning, and sentience) being multiply realizable and substrate-independent. The paper argues that these functions are not exclusive to brains but can be implemented by diverse physical systems, including non-neural cells/tissues, plants, fungi, tissue-engineered cultures, bio-robotic hybrids (hybrots), synthetic intelligences (AI), and potentially unconventional materials. The purpose is to advocate for a unified framework ("diverse intelligence") that recognizes these commonalities, overcomes neuro-centric biases, and facilitates cross-disciplinary research and ethical considerations. Components are the various agents exhibiting (or potentially exhibiting) cognition, ranging from molecular networks to organisms to engineered systems.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
    *   **Note:** Parameters represent the core concepts discussed in this theoretical/review paper, rather than physical implementation metrics. Values are sets of examples or qualitative descriptions provided in the text.

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*    Units: Qualitative Descriptor: "different time constants" mentioned generally.

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The emergent global orders mentioned include "cognitive response patterns" (in hybrots), potentially specific learned behaviors or decision strategies, and the outcomes of morphogenetic processes (e.g., anatomical structures resulting from navigation in morphospace). Cognitive functions like learning, decision-making, and potentially sentience itself are presented as emergent properties.
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Explicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog/Digital/Hybrid/Neuromorphic/Biological/Chemical/Other (Colloid-based, Material-based e.g., hysteresis)

### **5.3 Computational Primitive:**

    *   Content: Logic gates, Memristors (natural and artificial), Information processing within nervous tissue, GRNs, or material systems (e.g., related to hysteresis).
    *   **Sub-Type (if applicable):** Logic gate: (Generic); Memristor: (Generic); Information processing: (Generic)

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
    *   **Note:** The paper discusses timescales qualitatively, emphasizing differences across systems and scales, rather than providing specific quantitative values.

### **6.2 Active Inference:**

    *   Content: Partial
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** Anticipation accuracy (correlation between predicted and actual stimuli/outcomes), Behavioral changes minimizing deviation from inferred goals (e.g., path efficiency in problem space navigation), Complexity/dimensionality of inferred internal models (e.g., via state-space reconstruction from behavior), Rate of adaptation/learning in response to environmental changes (model updating speed). CT-GIN could model the internal state (`BeliefNode`), sensory input (`ObservationNode`), action output (`ActionNode`), and the predictive relationships (`PredictionEdge`) and update rules (`UpdateFunctor`).

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The paper mentions several types/examples of adaptation mechanisms: associative learning, non-associative learning (habituation), immune conditioning/memory, conditioned hysteresis (in materials), learning in GRNs, learning in AI/software, learning processes underlying navigation in various problem spaces (transcriptional, physiological, morphospace). However, it does not delve into the specific underlying molecular or algorithmic details for most examples, focusing more on the functional equivalence across substrates.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behaviors discussed are cognitive functions: intelligence, decision-making, learning (associative, non-associative), sentience (subjective experience), anticipation, risk assessment, stimulus discrimination, cooperative/competitive strategies, mimicry, problem-solving, attention control, goal-directed responses. These are presented as potentially emerging from diverse physical substrates.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper extensively discusses the challenges of validating cognitive behaviors, especially sentience ("Black Box" problem). It notes that validation currently relies on *inference* from observable correlates: behavioral responses (verbal/written, body language, task performance like in hybrots) and neuroimaging data (for brains). It critiques the inconsistency in applying validation standards (double standards, anthropocentric bias) and advocates for strict operationalization based on observable, quantifiable response patterns, treating system origin/composition as irrelevant a priori. It acknowledges the limitations of current methods, particularly reliance on behavioral analogies and inductive reasoning by similarity.

---

#Key: [aguilera_exploring_2018]

# Exploring Criticality as a Generic Adaptive Mechanism

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a computational model of an embodied agent implemented in the Mountain Car benchmark environment. The agent's controller is a fully-connected Boltzmann machine (a type of stochastic recurrent neural network) with 6 sensor units and 6 neuron units (including 2 motor output units). The system's purpose is to explore criticality as an adaptive mechanism. It learns by adjusting the Boltzmann machine's weights (biases `h` and couplings `J`) using a gradient ascent rule designed to maximize the controller's heat capacity (a proxy for criticality), without an explicit task reward. The agent interacts with the simulated environment, receiving sensory input (based on car acceleration) and producing motor actions (-1, 0, 1) determined by the state of its motor neurons.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters listed are key to the learning and operation of the Boltzmann machine controller as described in the implementation section. Values are explicitly provided in the text. β=1 is the reference point used during training and analysis.

## M2: Energy Flow
*   **Note:** 'Energy' in this context refers primarily to the statistical mechanics energy function of the Boltzmann machine (Eq 1), governing its state transitions, and the computational effort, not physical energy flows in a material.

### **2.1 Energy Input**

    *   Content: The primary "energy" input driving the system's operations is computational energy required to run the simulation steps (Glauber dynamics for state updates, environment physics updates, learning rule calculations). From the statistical mechanics perspective, the external input comes via sensor neurons reflecting the state of the Mountain Car environment.

### **2.2 Energy Transduction**

    *   Content: 1. **Information to Network State:** Sensor input (car state) influences the 'local field' (Hi) of neurons (Eq 2). 2. **Network State Dynamics:** The Boltzmann machine transitions between states (σ) probabilistically based on its energy function E(σ) and temperature β (Eq 1, 2 - Glauber dynamics). 3. **Network State to Action:** The state of motor neurons determines the agent's action in the environment. 4. **Experience to Parameter Change:** System states and interactions are processed via the learning rule (Eq 5-9) to update network parameters (h, J), transducing system dynamics information into structural changes (memory).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss computational efficiency or thermodynamic efficiency. The focus is on the functional aspects of criticality and learning, not resource consumption. Efficiency is not a measured or discussed parameter.

### **2.4 Energy Dissipation**

    *   Content: From a computational perspective, energy is dissipated as heat by the computing hardware running the simulation, but this is not discussed. From the statistical mechanics model perspective, the concept analogous to dissipation is the stochasticity introduced by the temperature parameter (β), allowing exploration of the state space rather than settling deterministically into the lowest energy state. The learning process itself (gradient ascent) consumes computational resources. Quantified values are not provided. Assessment: Medium (inherent stochasticity and computational cost).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term (fixed post-training)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: ~84 parameters (6 biases + 12*11/2 = 66 symmetric couplings if fully connected internal + 6*6 sensor-neuron connections if all-to-all, exact topology isn't fully specified but implies full connectivity). The capacity lies in the continuous range these parameters can take.
*   Units: Parameters (real numbers)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: 0 (post-training)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial

### **4.2 Local Interaction Rules:**

    *   Content: 1. **Neuron Dynamics:** Each neuron `i` updates its state (σ_i) stochastically based on its local field `H_i = h_i + Σ_j J_ij σ_j`. The probability of transitioning to state σ'_i is given by Glauber dynamics: `P(σ'_i | σ) = 1 / (1 + exp(-2βσ'_i H_i))` (Eq 2). 2. **Learning Rule (Parameter Dynamics):** The biases `h_i` and couplings `J_ij` are updated based on a gradient ascent rule aiming to maximize the simplified heat capacity C'_i (Eq 6). The updates are `Δh_i ∝ ∂C'_i/∂h_i` and `ΔJ_ij ∝ ∂C'_i/∂J_ij` (Eq 7, 9), computed using averages of functions involving local states and fields. These rules govern how individual components (neurons, weights) interact and change based on local information.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Neuron Dynamics | Glauber Dynamics Update | β (Inverse Temperature) | Varied [10^-1, 10^1] for analysis, 1 for training | dimensionless | Section 4 | Explicit | β controls stochasticity. |
    | Neuron Dynamics | Local Field Calculation | h_i (Bias) | Learned, initially [-0.01, 0.01] | dimensionless | Section 3 | Explicit | Learned parameter. |
    | Neuron Dynamics | Local Field Calculation | J_ij (Coupling) | Learned, initially [-0.01, 0.01] | dimensionless | Section 3 | Explicit | Learned parameter. |
    | Learning Rule | Gradient Ascent | µ (Learning Rate) | 0.02 | dimensionless | Section 3 | Explicit | Controls update step size. |
    | Learning Rule | Regularization | λ (Regularization Strength) | 0.002 | dimensionless | Section 3 | Explicit | Penalizes large weights. |

### **4.3 Global Order:**

    *   Content: The emergent global order targeted and partially achieved is a state of criticality in the neural controller. This is characterized by: 1. Statistical properties of the network states (activity patterns σ) approximating a Zipf's law distribution (Fig 3A). 2. A peak in the system's heat capacity C(σ'_i|σ) near the operating temperature (β=1) (Fig 3B). 3. Emergence of distinct behavioral regimes (e.g., oscillating vs. resting, ability to climb the mountain) corresponding to the transition point near β=1 (Fig 4). 4. Maximized synergistic information between sensor, hidden (internal), and motor neurons near the operating point (Fig 5C).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Neuron Dynamics | Glauber Dynamics (Eq 2) | β | [10^-1, 10^1] / 1 | dimensionless | Explicit | Controls stochasticity | Section 4 |
| Learning | Gradient Ascent on C' (Eq 9) | µ | 0.02 | dimensionless | Explicit | Learning step size | Section 3 |
| Learning | Gradient Ascent on C' (Eq 9) | λ | 0.002 | dimensionless | Explicit | Weight regularization | Section 3 |
| Neuron Dynamics | Local Field H_i (Eq 2) | h_i | Learned | dimensionless | Explicit | Neuron bias | Section 3 |
| Neuron Dynamics | Local Field H_i (Eq 2) | J_ij | Learned | dimensionless | Explicit | Neuron coupling | Section 3 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Criticality Signature | State Distribution | Rank Exponent (Zipf) | Approx -1 (over ~3 decades) | dimensionless | Explicit | Measures scale-free distribution | Plot log(Prob) vs log(Rank) | Fig 3A |
| Criticality Signature | Thermodynamic Response | Heat Capacity C(σ'_i|σ) | Peak value | Dimensionless (or J/K if physical) | Explicit | Measures sensitivity near transition | Calculated from Eq 4 via simulation | Fig 3B |
| Behavior | Agent Trajectory | Median Vertical Position y | Varies with β, transition near β=1 | dimensionless | Explicit | Characterizes behavioral regime | Simulation analysis | Fig 4D |
| Information Flow | Sensor/Hidden/Motor Interaction | Synergy ϑ(Y; X1, X2) | Peak value | bits | Explicit | Measures emergent information | Calculated from Eq 12 via simulation | Fig 5C |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    *   **Metrics:** Heat Capacity (Eq 4, 6), Zipf's Law (Fig 3A), Behavioral Metrics (Median y position, Fig 4D), Information Synergy (Eq 12, Fig 5C).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Neuromorphic/Stochastic Neural Network

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operation performed by the material (the Boltzmann machine) is the stochastic thresholding based on weighted sum of inputs. Each neuron `i` computes its local field `H_i = h_i + Σ_j J_ij σ_j` and then stochastically updates its state `σ_i` based on this field and the temperature `β` using the sigmoid-like probability function inherent in Glauber dynamics (`P(σ'_i = +1| σ) = 1 / (1 + exp(-2βH_i))`). This involves weighted summation followed by stochastic activation.
    *   **Sub-Type (if applicable):** Stochastic Thresholding / Weighted Summation + Sigmoid Activation (probabilistic)

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Simulation Step | 1 | step | Section 2, 3 | Explicit | Basic unit of time progression. |
        | Training Trial Duration | 5000 | steps | Section 3 | Explicit | Duration over which learning gradients are accumulated. |
        | Total Training Duration | 5,000,000 (1000 trials * 5000 steps) | steps | Section 3 | Explicit | Total learning period. |
        | Analysis Duration | 1,000,000 | steps | Section 4 | Explicit | Period for observing behavior post-training. |
        | Behavioral Oscillation Period (Example) | ~50-100 (visual est.) | steps | Fig 4B,C (bottom) | Implicit | Characteristic time of emergent behavior (varies). |

    *   **Note:** Timescales are given in simulation steps as defined in the paper. Physical time is not specified.

### **6.2 Active Inference:**

    *   Content: Partial/Unclear
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** Predictive information between sensor and motor states; KL divergence between predicted and actual state transitions under the learned model; time required for the model to adapt to environmental changes by monitoring C' gradient. Simulate environments with predictable vs. unpredictable dynamics and measure how well C' maximization distinguishes them or adapts.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is a gradient ascent learning rule designed to maximize a simplified measure of the neural controller's heat capacity (C', defined in Eq 6). The change in parameters (biases `h_i` and couplings `J_ij`) is proportional to the partial derivative of C' with respect to that parameter (Eq 7). Specifically, `Δh_i = µ * ∂C'/∂h_i - λ*h_i` and `ΔJ_ij = µ * ∂C'/∂J_ij - λ*J_ij` (Eq 9), where µ is the learning rate and λ is an L2 regularization term. The gradients ∂C'/∂h_i and ∂C'/∂J_ij (Eq 7, 8) are computed based on statistical averages of local neuron states and fields over a trial period. This is a form of unsupervised learning or intrinsic motivation, where the objective is to reach a dynamical state (criticality) rather than optimize an external reward signal. It can be seen as a form of reinforcement learning where the "reward" is the instantaneous contribution to heat capacity.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors observed are related to the agent's movement patterns in the Mountain Car phase space (position `x` vs. velocity `v`). Depending on the inverse temperature parameter `β` (which scales the learned weights), the system exhibits distinct behavioral regimes: 1. At low β (e.g., 0.25), the behavior is more random/less structured, often failing to gain momentum (Fig 4A). 2. Near the critical point (β ≈ 1), agents often exhibit structured, oscillatory behavior, leveraging potential energy to climb the hills (Fig 4B). For 12 out of 20 agents, this regime enables successful climbing to the goal. 3. At high β (e.g., 4), behavior can become more deterministic or restricted, potentially getting stuck in specific patterns (Fig 4C). The key emergent behavior is the transition between these regimes near the critical point identified by the learning process.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claims of emergent behaviors (distinct regimes and transition) are validated through: 1. **Simulation and Visualization:** Plotting agent trajectories in phase space (x vs v) for different β values (Fig 4A-C). 2. **Quantitative Analysis:** Calculating behavioral metrics like median vertical position (y) and its quartiles as a function of β, showing a clear transition near β=1 for many agents (Fig 4D). 3. **Information Theory:** Analyzing entropy, mutual information, and synergy between sensor, hidden, and motor units across different β values, showing changes correlating with the behavioral transition (Fig 5). 4. **Comparison with Control:** Comparing criticality signatures (Zipf, heat capacity) of the learning agents with agents optimized directly for the task using a genetic algorithm, showing the latter lack these signatures (Fig 3). Reproducibility is addressed by running simulations for 20 different agents and showing average results with error bars (Figs 3, 5) or reporting counts (12/20 agents climb). Limitations include reliance on a single benchmark task and the variability in outcomes (8/20 agents don't exhibit the 'successful' climbing behavior).

---

#Key: [angioletti-uberti_re-entrant_2012]

# Re-entrant melting as a design principle for DNA-coated colloids

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of DNA-coated colloids (DNACCs) designed to exhibit re-entrant melting. Two types of colloids, X and X', are coated with specific DNA strands (α, β for X; α', β' for X'). Strands α-α' form strong interparticle bonds, while α-β and α'-β' form weaker intraparticle bonds. An inert DNA type is also grafted. The system uses the competition between temperature-dependent strong interparticle bonds (favored at intermediate T) and weak intraparticle bonds (favored at low T) to create a non-monotonic interaction potential. The purpose is to design DNACCs that crystallize on cooling but melt upon further cooling (re-entrant melting), aiming to suppress kinetic trapping and widen the window for self-assembly into target structures (e.g., binary crystals). The study uses Self-Consistent Mean-Field Theory (SCMFT) and Monte Carlo (MC) simulations to model and predict the phase behavior.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name        | Value               | Units         | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :-------------------- | :------------------: | :-----------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
    *   **Note:** Parameters listed are key physical dimensions and densities defining the simulated system, as explicitly stated in the text and figure captions. Reliability is "High" within the context of the model definition.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input is thermal energy, manipulated via temperature changes. Temperature dictates the thermodynamic favorability (via free energy ΔG = ΔH - TΔS) of DNA hybridization/dehybridization events, which control the inter-colloidal interactions.
    *   Units: K or °C

### **2.2 Energy Transduction**

    *   Content: Thermal energy is transduced into chemical potential energy changes associated with DNA hybridization and dehybridization. Differences in hybridization free energies (ΔG<sup>0</sup><sub>α</sub> for strong bonds, ΔG<sup>0</sup><sub>β</sub> for weak bonds) drive the formation or breaking of specific inter- or intra-particle DNA duplexes. These hybridization events, in turn, modulate the effective interaction potential (free energy of interaction) between colloids, driving attraction (leading to crystallization) or repulsion/reduced attraction (leading to melting). The energy flow is: Thermal Energy -> DNA Hybridization State -> Inter-Colloidal Potential Energy -> System Phase (Configuration).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The concept of energy efficiency in the traditional sense (work output / energy input) is not directly applicable or discussed for this self-assembly process driven by thermal fluctuations and equilibrium thermodynamics. The goal is not to perform work but to control structure via temperature. The paper focuses on thermodynamic stability (free energy) rather than efficiency.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation occurs implicitly through heat exchange with the thermal bath during the formation (exothermic ΔH) and breaking (endothermic ΔH) of DNA bonds as the system equilibrates at different temperatures. The simulations (MC) implicitly model this via acceptance/rejection rules based on energy changes (Boltzmann weights). However, the magnitude of dissipated heat is not quantified or discussed in the excerpt. Qualitatively, dissipation occurs whenever the system transitions between states with different numbers/types of bonds due to temperature changes.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are governed by the temperature-dependent hybridization of complementary ssDNA ends grafted onto the colloids.
        1.  **X-X' Interaction:** At T > T<sub>strong</sub>, weak repulsion (no bonds). At T<sub>weak</sub> < T < T<sub>strong</sub>, strong α-α' interparticle bonds form, leading to strong attraction. At T < T<sub>weak</sub>, weak α-β and α'-β' intraparticle bonds compete with and replace strong α-α' interparticle bonds (breaking 1 strong allows 2 weak), reducing the X-X' attraction significantly. ΔG<sup>0</sup><sub>α</sub> < ΔG<sup>0</sup><sub>β</sub> (δ<sub>weak-strong</sub> > 0).
        2.  **X-X (or X'-X') Interaction:** Weak α-β (or α'-β') bonds can form either interparticle or intraparticle. ΔG<sup>0</sup><sub>β</sub> governs these. The interaction decreases monotonically with temperature, becoming nearly constant at low T due to combinatorial entropy effects when most bonds are saturated.
        3.  **Inert Strands:** Provide additional repulsion (steric/entropic) between all pairs, tunable by length. Affects overall potential offset.
        These rules are implemented mathematically via SCMFT and simulated using MC methods based on hybridization free energies and configurational costs.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description                      | Parameter Name     | Parameter Value Range | Units | Data Source             | Implicit/Explicit | Justification                     |
    | :------ | :------------------------------- | :----------------- | :-------------------- | :---- | :---------------------- | :---------------- | :-------------------------------- |
    | 1       | Strong Interparticle Bond (α-α') | ΔG<sup>0</sup><sub>α</sub> | Varied (Fig 2a, 3a x-axis) | k<sub>B</sub>T | Text, Fig 2a, Fig 3a | Explicit          | Parameter defining bond strength. |
    | 2       | Weak Bond (α-β, α'-β')         | ΔG<sup>0</sup><sub>β</sub> | ΔG<sup>0</sup><sub>α</sub> + δ<sub>weak-strong</sub> | k<sub>B</sub>T | Text, Fig 2a, Fig 3  | Explicit          | Parameter defining bond strength. |
    | 1 vs 2  | Relative Strength              | δ<sub>weak-strong</sub> | 1, 3, 5, 7            | k<sub>B</sub>T | Fig 3 caption         | Explicit          | Key parameter for re-entrance.  |
    | 3       | Inert Strand Repulsion           | Length             | 38                    | nm    | Fig 4 caption         | Explicit          | Tunable repulsive component.      |

### **4.3 Global Order:**

    *   Content: The emergent global order includes a disordered fluid phase and ordered crystalline phases. For a 1:1 mixture, an Au-Cu type binary crystal structure emerges over a range of temperatures and packing fractions. At lower temperatures and higher packing fractions (η > 0.375, T < 35°C), phase separation into pure X and X' face-centred cubic (FCC) crystals is observed.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Explicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description                      | Parameter          | Value Range             | Units      | Implicit/Explicit | Justification                                     | Source          |
| :------ | :------------------------------- | :----------------- | :---------------------- | :--------- | :---------------- | :------------------------------------------------ | :-------------- |
| 1       | Strong Interparticle Bond (α-α') | ΔG<sup>0</sup><sub>α</sub>     | e.g., -40 to 0 (Fig 2a) | k<sub>B</sub>T | Explicit          | Free energy governs bond formation probability.     | Text, Fig 2a, 3 |
| 2       | Weak Bond (α-β, α'-β')         | ΔG<sup>0</sup><sub>β</sub>     | ΔG<sup>0</sup><sub>α</sub>+δ<sub>weak-strong</sub>  | k<sub>B</sub>T | Explicit          | Free energy governs bond formation probability.     | Text, Fig 2a, 3 |
| 1 vs 2  | Competition Trigger              | δ<sub>weak-strong</sub>  | > 0 (e.g., 1 to 7)    | k<sub>B</sub>T | Explicit          | Energy difference drives switch from inter to intra. | Text, Fig 3     |
| 3       | Inert Strand Contribution        | Length             | e.g., 38                | nm         | Explicit          | Modulates overall repulsion (entropic/steric).    | Fig 4 caption   |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description      | Parameter                    | Value Range | Units | Implicit/Explicit | Justification                                                    | Protocol                             | Source      |
| :---------- | :--------------- | :--------------------------- | :---------- | :---- | :---------------- | :--------------------------------------------------------------- | :----------------------------------- | :---------- |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description          | Value        | Units            | Source                             | Implicit/Explicit | Justification                                                              |
        | :----------------------------- | :----------: | :--------------: | :--------------------------------- | :---------------- | :------------------------------------------------------------------------- |
        | MC Simulation Step             | 1            | MC move/particle | Text (Methods)                     | Explicit          | Unit of simulation time progression.                                         |
        | MC Equilibration/Sampling Time | 2 x 10<sup>5</sup>, 2 x 10<sup>4</sup> | MC moves/particle | Text (Methods)                     | Explicit          | Duration of simulation runs for free energy/crystallinity calculations.      |
    *   **Note:** The excerpt provides timescales related to the simulation methodology but lacks explicit quantification of the physical process timescales (like hybridization rates or absolute crystallization times).

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behavior is temperature-dependent phase transition, specifically self-assembly into ordered crystalline structures (Au-Cu binary or FCC pure phases) upon cooling from a fluid phase, followed by melting back into a fluid phase upon further cooling (re-entrant melting). This non-monotonic dependence of the aggregated state on temperature is the key emergent behavior resulting from the designed competing DNA interactions. A secondary behavior mentioned is gel formation.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claims of emergent behavior (re-entrant melting, phase transitions, enhanced crystallization) are validated primarily through computational modeling:
        1.  **SCMFT:** A mean-field theory was developed to predict the interaction potentials arising from DNA hybridization (results shown dashed in Fig. 2).
        2.  **MC Simulations:** Extensive Monte Carlo simulations were performed to:
            *   Calculate the effective interaction potential between surfaces ( validating SCMFT, Fig. 2).
            *   Compute the full phase diagram for the bulk system using thermodynamic integration and NPT simulations (Fig. 4).
            *   Directly simulate crystallization kinetics and measure the degree of crystallinity (q̄<sub>6</sub> analysis) under various conditions (Fig. 5).
        The quantitative agreement between SCMFT and MC simulations (Fig. 2) validates the theoretical understanding. The phase diagram (Fig. 4) explicitly shows the predicted re-entrant fluid phase. The crystallinity simulations (Fig. 5) demonstrate the kinetic advantage over conventional DNACCs. Reproducibility is implied by standard simulation practices, but not explicitly detailed. Limitations are those inherent to the model (e.g., pairwise potentials, specific DNA model).

---

#Key: [maroudas-sacks_topological_2021]

# Topological defects in the nematic order of actin fibres as organization centres of Hydra morphogenesis:

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is the regenerating freshwater polyp *Hydra*, specifically focusing on the dynamics of the supra-cellular actin fiber network within its ectodermal tissue layer during whole-body regeneration from excised tissue fragments. The actin fibers, along with myosin motors, form contractile bundles (myonemes) organized in a nematic liquid crystal-like order. This nematic field, characterized by its director field and topological defects (singularities in orientation), provides a coarse-grained description of the morphogenesis process. The study investigates how the dynamics, interactions (merging, annihilation), and spatial location of these topological defects (+1, +1/2, -1/2 charges) act as organization centers that correlate with and potentially guide the formation of key morphological features (head, foot, body axis) during regeneration. The purpose is to understand the interplay between cytoskeletal mechanics (specifically nematic order and defects) and biochemical processes in coordinating robust morphogenesis. Key components include *Hydra* tissue (ectoderm, endoderm, mesoglea), supra-cellular actin fibers, myosin motors, cell-cell junctions, and the associated biochemical signaling pathways involved in regeneration.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Defect Core Size (Implied, for +1) | <= 40 | µm | Methods (Definition of +1 defect) | Explicit | Medium | Based on analysis definition |

    *   **Note:** Parameters focus on characterizing the system's physical scale and key descriptive elements (defects, timescale) relevant to the study's focus. Reliability is generally high for explicitly stated observations but medium for sizes where ranges are given or parameters derived from analysis definitions.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is biochemical, derived from the hydrolysis of ATP (Adenosine triphosphate) consumed by myosin motors to generate contractile forces within the actin myonemes. This makes the system an "active" nematic system, consuming energy to generate mechanical stresses far from thermal equilibrium. Additionally, osmotic pressure differences contribute to forces driving tissue swelling/rupture cycles.

### **2.2 Energy Transduction**

    *   Content: Chemical energy from ATP hydrolysis is transduced into mechanical energy by myosin motors interacting with actin filaments. This generates contractile stresses within the supra-cellular actin fiber network. These stresses drive tissue deformation, movement, folding, and contribute to the dynamics of the nematic field, including the movement and interaction of topological defects. Mechanical stresses also feedback into cell signaling and potentially actin organization. Osmotic pressure is transduced into mechanical forces causing tissue swelling and rupture.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any quantitative information or qualitative assessment regarding the energy efficiency of the actomyosin contractility or the overall morphogenetic process. Efficiency is not a focus of the study.

### **2.4 Energy Dissipation**

    *   Content: Energy is dissipated primarily as heat due to viscous processes within the tissue (cytoplasm, cell movement, viscoelastic mesoglea) and the surrounding medium during tissue deformation and movement driven by actomyosin contractility and osmotic forces. Friction between tissue layers or potentially during defect motion could also contribute. Rupture events release stored elastic and osmotic potential energy. Quantification is not provided. Qualitative assessment: likely High, given the active, dynamic nature of morphogenesis involving continuous remodeling and movement in a viscous environment.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Hours to Days
*    Units: Time

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The paper describes local interactions phenomenologically rather than providing explicit mathematical rules:
        1.  **Nematic Alignment:** Actin fibers tend to align locally parallel to each other (characteristic of nematic systems). Regions with aligned fibers induce order in neighboring disordered regions (Fig S5).
        2.  **Activity/Contraction:** Actin fibers are contractile (due to myosin), generating active stresses that influence tissue mechanics and potentially fiber alignment/defect dynamics.
        3.  **Defect Dynamics:**
            *   +1/2 defects are mobile, moving relative to the tissue (Fig 3A).
            *   Two +1/2 defects can merge to form a +1 defect (Fig 3C).
            *   A +1/2 defect and a -1/2 defect can annihilate (Fig 3D).
            *   +1 defects are stable and stationary relative to the tissue once formed (Fig 4A).
        4.  **Geometry Coupling:** Defect formation/presence is coupled to surface curvature (+1 defects in high positive curvature, -1/2 in negative/saddle curvature; tentacle formation involves simultaneous protrusion and defect appearance - Fig S7).
        5.  **Topological Constraint:** The total defect charge on the closed surface must sum to +2.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**

    *   Content: The primary global order that emerges is the regenerated *Hydra* body plan: a unipolar axis with a head (+1 defect) at one end and a foot (+1 defect) at the other. This involves the establishment of a specific, stable configuration of topological defects (typically two +1 defects defining the axis) and the associated large-scale alignment of the actin nematic field along this axis throughout the tissue. Secondary structures like tentacles also represent emergent ordered patterns involving specific defect configurations (+1 at tip, two -1/2 at base).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| 3 | +1/2, +1/2 Merge | Interaction Range/Barrier | < 40 µm (Pair definition) | µm | Mixed | Distance criteria given, energetics implicit | Fig 3B,C |
| 4 | +1/2, -1/2 Annihilation | Interaction Range/Barrier | < 40 µm (Pair definition) | µm | Mixed | Distance criteria given, energetics implicit | Fig 3B,D, Movie 3 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| 1 | Nematic Order | Local Order Parameter (Q) | ~0 to 1 | Dimensionless | Explicit | Defined and calculated from orientation field | Methods, Fig 1, S3, S4 | Image Analysis |
| 2 | Nematic Order | Coherence Field | ~0 to 1 | Dimensionless | Explicit | Defined and calculated from orientation field | Methods | Image Analysis |
| 3 | Global Structure | Body Axis Polarity | Head/Foot | Categorical | Explicit | Main outcome studied | Fig 4 | Observation |
| 4 | Global Structure | Number of Axes | Integer (>=1) | Count | Explicit | Mentioned (Fig S3, S6) | Fig S3, S6 | Observation |
| 5 | Defect Config. | Total Charge | +2 | Dimensionless | Explicit | Topological constraint mentioned | Intro, Fig 1, Fig 2D | Topology |
| 6 | Defect Config. | Number of Defects (+1, +1/2, -1/2) | Integer (>=0) | Count | Explicit | Counted at different times | Fig 2D, Fig 3B, Fig S6B | Image Analysis/Observation |
| 7 | Area Fraction | Fraction of Ordered Area | 0 to ~1 | Dimensionless | Explicit | Calculated over time | Fig 2C | Image Analysis |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes (interpreted)

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog / Physical Computation

### **5.3 Computational Primitive:**

    *   Content: Pattern Formation / Selection based on Topological Constraints and Defect Dynamics. The system effectively "computes" the locations for head and foot formation based on the dynamics and interactions (merging, stabilization) of topological defects within the constraints imposed by the spheroid topology (total charge +2) and the initial conditions (inherited fiber alignment). The interaction rules of defects (merging, annihilation, stability based on type and context) act as computational primitives guiding the system towards a final stable configuration (body plan). Another primitive could be the coupling between defect type/location and local mechanics/biochemistry (e.g., +1 defects correlating with rupture sites and head organizer formation).

### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Tissue Folding | ~2 | hours | Fig 2A, Methods | Explicit | Stated timeframe for folding |
        | Nematic Ordering | ~24 | hours | Fig 2C, Fig 2D (inset) | Explicit | Time for ordered area fraction saturation / net charge stabilization |
        | Defect Emergence (+1) | 3-24 | hours | Fig 4D inset | Explicit | Distribution shown for +1 defect appearance |
        | Defect Lifetime (+1/2) | Hours (up to ~48-72) | hours | Fig 2D, Fig 3B | Explicit | +1/2 defects diminish significantly by 48-72h |
        | Defect Dynamics (Movement/Merging) | Minutes to Hours | time | Fig 3A, C, D; Movies 2-5 | Explicit | Visual observation in time-lapses |
        | Morphogenesis (Head/Foot Appearance) | ~48-72 | hours | Fig 2B | Explicit | Mature features visible by this time |
        | Regeneration (Full) | ~48-72 (2-3 days) | hours (days) | Fig 2, Abstract | Explicit | Stated overall duration |
        | Cellular Processes (Implied) | Shorter (Minutes?) | time | Fig S2 | Implicit | Cell shape changes faster than nematic field |

### **6.2 Active Inference:**

    *   Content: Unclear / Partial

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism involves a complex interplay of biophysical and biochemical processes:
        1.  **Actin Network Dynamics:** Continuous remodeling, alignment, and potential growth/shrinkage of actin fibers influenced by local stresses, boundary conditions, and biochemical signals. This drives the evolution of the nematic field and defect configuration.
        2.  **Active Stresses:** Myosin-driven contractility generates forces that drive tissue folding, deformation, and potentially influence cell behavior and differentiation.
        3.  **Topological Defect Dynamics:** The movement, merging, annihilation, and stabilization of defects act as organizing principles, influencing local mechanics and potentially signaling pathways.
        4.  **Biochemical Signaling:** Underlying gene expression patterns, morphogen gradients (e.g., from head/foot organizers), and cell signaling pathways interpret mechanical cues and direct cell differentiation and behavior. The paper explicitly mentions the head organizer emitting signals (Ref 7) and suggests mechanical environment at +1 defects feeds into the associated signaling network.
        5.  **Cellular Processes:** Cell shape changes, potential cell division, migration, or extrusion contribute to tissue remodeling.
        6.  **Feedback Loops:** Mutual feedback between mechanical states (stress, curvature, nematic order) and biochemical signaling is proposed as crucial for robust pattern formation (Discussion).
        The overall process resembles aspects of developmental pattern formation driven by reaction-diffusion systems (mentioned in Intro) but integrating mechanical fields and active matter physics.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main emergent behavior is **morphogenesis**, specifically the robust regeneration of a complete *Hydra* body plan (head, foot, body axis) from an initially disorganized or partially ordered tissue spheroid. Key sub-behaviors include:
        1.  **Directed Defect Dynamics:** Coordinated movement, merging (+1/2 -> +1), annihilation (+1/2 + -1/2 -> 0), and stabilization (+1) of topological defects.
        2.  **Body Axis Formation:** Establishment of a stable unipolar axis defined by the final positions of the head and foot primordia (associated with +1 defects).
        3.  **Localized Morphological Feature Formation:** Development of the head (mouth/hypostome, tentacles) and foot (basal disc) at specific locations correlated with defect dynamics (head at early +1, foot at late-merged +1).
        4.  **Localized Tissue Rupture:** Transient hole formation preferentially occurring at +1 defect sites, related to osmotic pressure release.
        5.  **Tentacle Budding:** De novo formation of protrusions coupled with the appearance of specific defect patterns (+1, -1/2, -1/2).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors are primarily validated through:
        1.  **Live Imaging & Observation:** Time-lapse microscopy (confocal, light-sheet) of lifeact-GFP labeled actin allows direct visualization of fiber organization, defect dynamics, and morphological changes over the regeneration period (Figs 2, 3, 4, S5, S7, Movies 1-5).
        2.  **Correlation Analysis:** Systematic tracking of defects (using photoactivated labels as fiducial markers) and correlating their type, location, and dynamics with the eventual fate of the tissue region (head/foot formation). Quantitative data is presented showing the frequency of specific outcomes (Fig 4D, E, F).
        3.  **Comparative Analysis:** Comparing defect statistics and outcomes for different initial conditions (e.g., square fragments vs. open rings, Fig 2D vs S6B) supports the link between initial state/dynamics and final morphology.
        4.  **Consistency with Theory:** Framing the observations within the theoretical context of active nematics and topological constraints provides a conceptual validation.
        Limitations include the lack of direct manipulation of defects to prove causality (correlation is shown), and the biochemical signaling pathways are inferred rather than directly measured simultaneously with mechanics in most experiments shown. Reproducibility is implied by the sample sizes reported (N values in figure legends).

---

#Key: [calvino_microcapsule-containing_2018]

# Microcapsule-containing Self-Reporting Polymers

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of polymer matrices (e.g., epoxy, polyurethane, silicone rubber, poly(acrylic acid)) containing microcapsules. These microcapsules encapsulate a cargo, typically a dye, pro-dye, or components of a chromic system dissolved in a solvent (e.g., hexylacetate, toluene, sunflower oil). The purpose is self-reporting of mechanical damage or excessive stress. When the polymer matrix experiences sufficient mechanical force (e.g., impact, incision, tension, compression), the embedded microcapsules rupture, releasing their cargo. The released cargo then undergoes a chemical or physical transformation, resulting in a detectable optical signal (color change or fluorescence change/turn-on) localized at the site of damage. Various operating principles exist, including simple dye release (with contrast enhancement via UV-blocking shells), cargo-matrix interactions (e.g., pH change, catalysis), interactions between components released from two different capsule types (e.g., charge-transfer complexes, click chemistry), and aggregation-induced optical changes (e.g., AIE, excimer formation).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                      | Value          | Units   | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :---------------------------------- | :------------- | :------ | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Mechanical energy applied to the composite material, causing stress or strain sufficient to rupture the embedded microcapsules. Sources mentioned include scratching, incision, compression, tensile deformation, and impact.

### **2.2 Energy Transduction**

    *   Content: Mechanical energy applied to the bulk material is transferred through the matrix to the embedded microcapsules. Upon reaching a critical stress/strain level, this mechanical energy causes fracture of the microcapsule shell. This rupture releases potential chemical energy (if reactions occur) or changes the physical state/environment of the encapsulated cargo (dye system). The change in the cargo leads to a change in optical properties (absorption or emission), converting the initial mechanical input into an optical signal (light energy absorption/emission). Specific mechanisms include: Mechanical Energy -> Capsule Fracture -> Cargo Release -> Chemical Reaction (e.g., acid-base, catalysis, click) / Physical Interaction (e.g., charge transfer, aggregation, excimer formation, solvent evaporation) -> Optical Signal (change in absorption/fluorescence).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Low. The primary goal is signaling, not efficient energy conversion. Most input mechanical energy likely dissipates as heat during matrix deformation and capsule fracture. The energy converted into an optical signal (emitted photons or change in absorbed photons) is expected to be a very small fraction of the input mechanical energy. The paper does not provide any quantitative efficiency metrics. The score reflects the system's purpose (sensing, not work output) and the likely high dissipation.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include: 1) Inelastic deformation of the polymer matrix under stress. 2) Energy required to fracture the microcapsule shells. 3) Viscous losses during fluid (cargo) release and flow into cracks/matrix. 4) Heat generated from any exothermic chemical reactions involved in the chromic response (though likely small). 5) Non-radiative decay pathways for fluorescent systems. Quantification is not provided in the text. Qualitative assessment: Likely High overall dissipation, dominated by matrix deformation and capsule fracture.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Skipping M3.2-M3.8 as M3.1 is "No")**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Skipping M4.2-M4.7 as M4.1 is "No")**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Skipping M5.2-M5.4 as M5.1 is "No")**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description                   | Value        | Units   | Source     | Implicit/Explicit | Justification                           |
        | :-------------------------------------- | :----------- | :------ | :--------- | :----------------: | :-------------------------------------- |
        | Cargo Release/Diffusion                 | Seconds-Minutes| s - min | Fig 4B, 5A | Explicit          | Observed timescale for color/fluor change |
        | Color/Fluorescence Change Development | Seconds-Minutes| s - min | Fig 3B, 4B, 5A| Explicit          | Observed timescale for effect        |
        | Solvent Evaporation (for Aggregation)   | Minutes      | min     | Section 2.4, Fig 5A | Explicit          | Mentioned as driver for AIE/excimer |
        | Shelf-Life (Capsule Stability)          | Months+      | months  | Section 4.3, 4.6 | Explicit          | Stated requirement/achievement      |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Skipping M7.2 as M7.1 is "No")**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is mechanochromism, specifically damage-activated optical reporting. The material exhibits a change in its optical properties (color change, fluorescence onset, or fluorescence color change) in response to mechanical stimuli (e.g., cracking, impact, high stress/strain) that cause the rupture of embedded microcapsules. This provides a visual or optically detectable indication of the location and potentially the extent of mechanical damage or overload.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation primarily relies on experimental observation. Claims are supported by photographic evidence (e.g., Figures 2B, 3B, 4C, 4D, 5A, 5B) showing visible color or fluorescence changes after intentional damage (scratching, impact, incision, deformation). Some studies quantify the optical change (e.g., fluorescence intensity ratios, Fig 5C) and relate it to the mechanical input (e.g., impact distance, Fig 5C). Control experiments are implicitly present (comparing damaged vs. intact areas). Reproducibility is implied by multiple reported examples using similar principles. Limitations include: often qualitative assessment, lack of standardized testing protocols, limited information on long-term signal stability/reliability, and detailed correlation between mechanical state (stress/strain fields) and optical signal intensity/distribution.

---

#Key: [seung_statistical_1992]

# Statistical mechanics of learning from examples

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is a theoretical framework using statistical mechanics (SM) to analyze supervised learning from examples in feedforward neural networks. The network consists of input nodes (S), an output node (σ), and N adjustable weights (W). The purpose is to study the network's ability to generalize, i.e., perform well on novel inputs after being trained on a limited set of P examples {(S<sup>l</sup>, σ<sub>0</sub>(S<sup>l</sup>))}. The training process is modeled as a stochastic dynamic (Langevin equation with noise parameter T) that leads to a Gibbs distribution of network weights P(W), minimizing a training energy function E(W) derived from an error function e(W;S) (e.g., quadratic error). The analysis focuses on the generalization error e<sub>g</sub>(T,P) (average error on the whole input space) and training error e<sub>t</sub>(T,P) as functions of the number of examples P (often scaled as αN, where α = P/N). The framework considers both realizable rules (target function can be perfectly learned) and unrealizable rules (target function cannot be perfectly realized by the given network architecture). Key theoretical tools employed include the high-temperature limit, annealed approximation (AA), and replica theory to handle quenched disorder from random example sampling. Specific examples studied involve single-layer perceptrons with varying weight constraints (continuous, binary) and output functions (linear, Boolean).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These are the key parameters defining the theoretical learning problem setup and analysis. R is specific to perceptron models but central to their analysis.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary "energy" concept is the training energy E(W) (Eq 2.1), a cost function representing the network's error on the training examples. It is not a physical energy input like Joules. The ultimate source driving the system's change (weight updates) is the information contained in the training examples, mediated by the learning algorithm which aims to minimize E(W). There's also a stochastic "energy" input related to the noise term Γ(t) in the Langevin dynamics (Eq 2.7), characterized by temperature T.
    *   Units: Dimensionless (Training Energy E(W) is typically defined as a dimensionless cost)

### **2.2 Energy Transduction**

    *   Content: The "energy" E(W) is transduced into changes in the network weights W via the learning dynamics (Eq 2.7). This involves calculating the gradient of E(W) with respect to W (-∇<sub>W</sub>E(W)). The stochastic dynamics (Langevin) uses this gradient, along with a noise term dependent on T, to update W over time. The goal is to find weight configurations W that minimize E(W) (and ideally, the generalization error e<sub>g</sub>). In the statistical mechanics view, the system explores the weight space according to the Gibbs distribution P(W) ∝ exp[-E(W)/T], where lower energy states are more probable.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper doesn't discuss energy efficiency in a physical thermodynamic sense (e.g., work done vs heat dissipated). The relevant concept is "learning efficiency," i.e., how quickly the generalization error decreases as the number of examples (P or α) increases. The paper analyzes the *shape* of learning curves (e<sub>g</sub> vs α), such as inverse power laws (e.g., Eq 3.19, 5.24) or exponential decay (e.g., Eq 5.32), which characterizes this learning efficiency. The efficiency depends heavily on the model (smooth vs non-smooth, realizable vs unrealizable) and parameters (T, α). For example, Eq 3.19 shows e<sub>g</sub> ~ T/(2α) for smooth, realizable networks, indicating decreasing error with more examples (higher α) but increasing error with noise (T).

### **2.4 Energy Dissipation**

    *   Content: Within the analogy of the stochastic Langevin dynamics (Eq 2.7), the noise term Γ(t), characterized by temperature T, acts analogously to thermal fluctuations and energy dissipation in a physical system. It prevents the system from getting stuck in local minima of E(W) and drives the system towards the equilibrium Gibbs distribution. Higher T implies stronger noise/dissipation, leading to wider exploration of the weight space but potentially hindering convergence to the optimal state (higher equilibrium error, shown in Eq 3.12, 3.14, 5.10). The paper analyzes the effect of T on generalization and training errors. There isn't a quantification of dissipation in physical units (e.g., Watts).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding with M3.2 and M3.3.)**

### **3.2 Memory Type:**

    *   *Retention:* Potentially long-term (weights are fixed after training, barring noise/further training). The replica theory implicitly assumes stable states.
    *   *Capacity:* Related to the number of weights N. The paper focuses on the large-N limit. Information capacity hasn't been quantified in bits directly, but Vapnik-Chervonenkis (VC) dimension is mentioned (related to capacity).
    *   *Read-out:* The network performs inference (calculates output σ for input S) based on W. Read-out "accuracy" relates to the generalization error e<sub>g</sub>.
    *   *Re-writability:* Weights are continuously updated during training. The state is adjustable.
    *   *Stability:* At T=0, learned states (minima of E) are stable. At T>0, weights fluctuate around minima according to the Gibbs distribution. Spin-glass phases indicate multiple stable/metastable states.
    The score reflects high capacity and re-writability, potentially long retention, but readout fidelity (1-e<sub>g</sub>) depends on training, and stability depends on T and the energy landscape (potential for multiple minima/spin-glass states). It's a distributed, analog/digital (depending on weight constraints) memory.

### **3.3 Memory Retention Time:**

*   Value: Long-term / Permanent (in absence of noise/retraining)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Scales with N
*   Units: Dimensionless (Number of independent parameters)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: 1 - e<sub>g</sub>(T, α)
*   Units: Dimensionless (Probability of correct prediction/classification)

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Related to T

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :--------------- | :--------------------------------------------- | :--------------------- | :------------ | :------------------------------------------- | :--------------- |:-----------------:| :---------------------------------------------------------------------------- |
    | Generalization   | Probability of error on novel inputs         | e<sub>g</sub>(T, α)    | Dimensionless | `MemoryNode` attribute `readout_fidelity`=1-e<sub>g</sub> | Eq 2.12, various sections | Explicit         | e<sub>g</sub> is the primary measure of performance/fidelity after learning.      |
    | Robustness to Noise | Sensitivity of performance to learning noise T | ∂e<sub>g</sub>/∂T        | Dimensionless | `MemoryNode` attribute `stability_factor`    | Eq 3.12, 5.10, 6.8 etc. | Explicit         | The dependence of e<sub>g</sub> on T is explicitly calculated/analyzed.         |
    | Spin-Glass Param | Measure of state degeneracy/overlap (q)      | q(T, α)                | Dimensionless | `MemoryNode` attribute `state_overlap`       | Eq 2.56, Sect V/VI | Explicit         | q indicates the structure of the solution space (memory states). q<1 implies disorder/degeneracy. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", proceeding with M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Neuromorphic / Analog (depending on specifics)

### **5.3 Computational Primitive:**

    *   Content: Weighted Summation followed by Non-linear Transfer Function.
    *   **Sub-Type (if applicable):** For the perceptron models (Sec IV-VI), the primitive is z = (1/√N) Σ W<sub>i</sub>S<sub>i</sub> followed by σ = g(z). The specific transfer function g(z) varies (linear, Boolean sgn(z), thresholded linear).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description          | Value                   | Units                  | Source          | Implicit/Explicit | Justification                                                                   |
        | :----------------------------- | :---------------------- | :--------------------- | :-------------- | :----------------: | :------------------------------------------------------------------------------ |
        | Learning Dynamics Convergence | Long (Implicitly)       | Arbitrary Time Units | Eq 2.7, Sec I   | Implicit          | Paper studies long-time properties / equilibrium state reached after learning dynamics. Specific convergence time not quantified. |
        | Noise Correlation Time         | Infinitesimally Short   | Arbitrary Time Units | Eq 2.8 (δ(t-t'))| Explicit          | Eq 2.8 defines the noise Γ(t) as white noise, meaning its correlation time is zero. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", proceeding with M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is the stochastic learning dynamics described by the Langevin equation (Eq 2.7): dW/dt = -∇<sub>W</sub>E(W) - ∇<sub>W</sub>U(W) + Γ(t). The weights W change based on the negative gradient of the training energy E(W) (error minimization), potentially modified by a weight constraint potential U(W), and perturbed by a stochastic noise term Γ(t) (with variance controlled by temperature T). This is essentially a form of stochastic gradient descent operating on the training error E(W), which is derived from the supervised learning signal (input-output examples). The goal is implicitly to find weights W that minimize the generalization error e<sub>g</sub>. It resembles error-correction learning.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is **Generalization**: the ability of the network, after training on P examples, to correctly process novel inputs not seen during training, as measured by the generalization error e<sub>g</sub>. Other key behaviors that emerge from the statistical mechanical analysis under specific conditions include:
        *   **Learning Curves**: The characteristic shape of e<sub>g</sub> (and e<sub>t</sub>) as a function of P (or α), e.g., inverse power law for smooth networks (Eq 3.12), exponential decay (Eq 5.32).
        *   **Discontinuous Learning Transitions**: A sharp, first-order phase transition from poor to perfect generalization at a critical number of examples (α<sub>c</sub>) in non-smooth networks learning realizable rules (e.g., Boolean perceptron with discrete weights, Sec V.D).
        *   **Spin-Glass Phases**: Existence of many degenerate, metastable states (local minima of the free energy) at low temperatures and/or low α, characterized by replica symmetry breaking (RSB) and non-zero Edwards-Anderson parameter q < 1. Found in both realizable (Sec V.C) and unrealizable models (Sec VI.B, VI.C).
        *   **Overtraining/Optimal Temperature**: In some unrealizable models, the generalization error e<sub>g</sub> may be non-monotonic with temperature T, exhibiting a minimum at T<sub>opt</sub> > 0 (Sec VI.B.2).

### **8.2 Behavior Robustness:**

        *   **Noise (T):** Generalization typically degrades with increasing T (lower robustness to noise), although optimal T>0 can occur for unrealizable rules. The system can still learn at finite T. (Eq 3.12, 3.14, 5.10, 6.8, 6.29). Robustness varies with the model.
        *   **Number of Examples (P or α):** Performance (generalization) generally improves with more examples (robustness to limited data). The rate of improvement depends on the model (power law, exponential). Finite α yields non-zero error (except for perfect learning transitions).
        *   **Realizability:** Unrealizable rules inherently limit performance (e<sub>g</sub> > 0 even for α → ∞). Learning curves and phase behavior differ significantly between realizable and unrealizable cases, indicating sensitivity to the task structure.
        *   **Network Smoothness:** Smooth vs non-smooth networks exhibit qualitatively different learning curves and transition behaviors (Sec III vs V/VI).
        *   **Spin-Glass States:** The presence of metastable spin-glass states suggests potential trapping and sensitivity to initial conditions or algorithmic details in the learning dynamics, reducing robustness of convergence to the optimal state (Sec V.C.4, V.D.3, VI.B.3, VI.C.2).
    The score reflects that the system *can* learn under noise and with limited data, but performance is sensitive to parameters, model details, and potential trapping in metastable states.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The emergent behaviors (learning curves, phase transitions, spin-glass phases) are validated theoretically using the tools of statistical mechanics (high-T limit, annealed approximation, replica theory - including RS and RSB calculations).
        *   **Learning Curves:** Asymptotic forms are derived analytically (e.g., Sec III, V.A, V.B, VI).
        *   **Phase Transitions:** Critical points (e.g., α<sub>c</sub>) and order of transitions are calculated using free energy analysis and order parameter behavior (e.g., Sec V.A, V.C, V.D). Stability analysis (e.g., Almeida-Thouless line, entropy calculation) determines phase boundaries (e.g., Fig 1, Fig 6).
        *   **Spin-Glass Phases:** Identified by replica symmetry breaking (RSB) analysis, non-zero Edwards-Anderson parameter (q<1), and/or negative entropy in RS solutions (e.g., Sec V.C.4, V.D.3, VI.B.3, VI.C.2, Appendix E).
        *   **Numerical Simulations:** Monte Carlo simulations are used to numerically verify theoretical predictions for specific models (e.g., linear-discrete, Boolean-discrete, mismatched models - Fig 3, 8, 9, 11, 14), providing quantitative checks on learning curves and transition points. Finite-size scaling analysis is sometimes employed (Fig 9).
    Limitations: The replica method involves non-rigorous steps (n→0 limit, RS/RSB ansatz). Annealed approximation and high-T limits are explicitly approximations. Numerical simulations are done for finite N and may have equilibration issues.

---

#Key: [lambert_quantum_2013]

# Quantum biology

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: This review paper summarizes experimental and theoretical evidence suggesting that non-trivial quantum mechanical effects (like coherence and entanglement) play functional roles in various biological systems. The primary examples discussed are: 1) Quantum coherent energy transport in photosynthetic light-harvesting complexes (e.g., FMO complex in green sulphur bacteria, algae) potentially enhancing energy transfer efficiency. 2) The radical-pair mechanism potentially underlying avian magnetoreception, where quantum spin dynamics (influenced by Earth's magnetic field) affect chemical reaction rates, providing a compass sense. 3) Other candidates briefly mentioned include electron tunnelling in proteins/enzymes, olfaction, and photoreceptor isomerization (vision). The purpose is to introduce these systems, outline the potential role of quantum effects, and present evidence for/against their functional biological significance. Components are biological molecules/complexes (proteins, pigments like BChl-a, cryptochromes, radical pairs) within their native cellular environments (temperature, solvent, scaffold proteins).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                    | Value       | Units      | Source (Fig/Table/Section)   | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :-------------------------------- | :---------: | :--------: | :-------------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Radical Pair Lifetime (Required)  | tens of (µs) | µs         | Sec "Avian magnetoreception" | Explicit          | Low                              | Inferred from req. for RF effects |

    *   **Note:** Parameters are specific examples mentioned for context within the reviewed systems. Reliability is often Medium/Low from a review perspective as it summarizes findings that may have varying original certainties or applicability across specific conditions not detailed in the review.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: For photosynthesis: photons (sunlight). For avian magnetoreception (radical-pair mechanism): photons (ambient light) initiate the radical pair formation. For other processes: thermal energy (enzyme catalysis), chemical potential (electron transfer), photons (vision).

### **2.2 Energy Transduction**

    *   Content: Photosynthesis: Photon energy -> Electronic excitation -> Coherent/Incoherent transfer through pigment network -> Chemical energy (charge separation at reaction centre). Magnetoreception: Photon energy -> Chemical energy (radical pair creation) -> Spin state energy (influenced by magnetic field + hyperfine interactions) -> Chemical energy (spin-dependent recombination yields). Electron Tunnelling: Chemical potential difference -> Kinetic energy of electron -> Chemical potential at new site. Vision: Photon energy -> Electronic excitation -> Structural energy (isomerization) -> Chemical signal cascade.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Photosynthesis is explicitly described as highly efficient ("Almost every photon (nearly 100%) that is absorbed is successfully transferred to the reaction centre"). While quantum coherence is proposed to offer only a few percent *additional* benefit over classical models like Förster, the baseline biological efficiency is extremely high. For other systems like magnetoreception, efficiency isn't the primary metric discussed; sensitivity is. Score reflects the high baseline biological efficiency mentioned.

### **2.4 Energy Dissipation**

    *   Content: Photosynthesis: Energy loss via fluorescence relaxation (timescale ~1 ns) if excitation doesn't reach the reaction centre fast enough. Coupling to the protein environment (source of decoherence/noise) involves energy exchange (thermal dissipation). Magnetoreception: Energy dissipation occurs during non-productive recombination pathways or thermal relaxation processes within the radical pair lifetime. Tunnelling/Enzymes: Thermal dissipation, relaxation processes. General: Decoherence mechanisms imply energy exchange/dissipation with the environment. Quantification is not provided in the review. Qualitative assessment: Loss mechanisms exist but are minimized in efficient systems like photosynthesis.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip to Module 4.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip to Module 6.)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description             | Value       | Units | Source                      | Implicit/Explicit | Justification                                              |
        | :-------------------------------- | :---------: | :---: | :-------------------------- | :----------------: | :--------------------------------------------------------- |
        | Photosynthesis Exciton Lifetime   | ~1          | ns    | Sec "Quantum coherent..." | Explicit          | Time before loss via fluorescence.                         |
        | FMO Coherence Lifetime (Room Temp) | up to 300   | fs    | Sec "Quantum coherent..." | Explicit          | Duration of observed quantum coherent dynamics.            |
        | FMO Transfer to RC                | ~1          | ps    | Sec "Quantum coherent..." | Explicit          | Time for excitation to leave FMO to reaction center.       |
        | Radical Pair Coherence Time (Req.) | tens of (µs) | µs    | Sec "Avian magnetoreception" | Explicit          | Required duration to explain sensitivity to weak RF fields. |
        | Rhodopsin Isomerization Time      | < 200       | fs    | Sec "Other quantum..."    | Explicit          | Timescale of the primary photochemical event in vision.   |
    *   **Note:** These are characteristic timescales of the quantum processes discussed within the biological systems.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip to Module 8.)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: 1. **Efficient Energy Transport:** In photosynthesis, near-unity transfer of absorbed photon energy (as electronic excitation) from antenna complexes to reaction centers, potentially aided by quantum coherence. 2. **Magnetoreception:** Behavioral orientation/navigation relying on sensing the direction (inclination) of the Earth's magnetic field, potentially mediated by the quantum spin dynamics of light-induced radical pairs. 3. **Electron Transfer:** Biologically crucial redox reactions involving long-range electron tunnelling through protein structures. 4. **Photochemical Reaction:** Rapid and specific photoisomerization in photoreceptors (e.g., rhodopsin) initiating the vision cascade. These are functional biological behaviors, potentially utilizing quantum mechanics.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

---

#Key: [stamps_active_2024]

# Active Inference Demonstrated with Artificial Spin Ice

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a numerical model of a three-dimensional Artificial Spin Ice (ASI) structure designed to demonstrate active inference based on the neurological Free Energy Principle (FEP). It consists of a bilayer of interacting nanomagnetic elements. The bottom layer is a square ASI geometry ("hidden layer") whose spins interact strongly. The top layer ("sensory layer") consists of superparamagnetic nanoelements that respond to an external environment (magnetic field) and mediate this information to the hidden layer via stray magnetic fields. The hidden layer's average magnetization is measured and fed back to control the environment's input (active inference loop). The system's purpose is to show that FEP and active inference, typically used to describe biological neural processes, can describe the dynamics of this non-biological magnetic system, potentially enabling experimental study and neuromorphic computing applications. The system aims to make the hidden layer's average magnetization converge towards a target value by adjusting the environmental input based on the inferred state.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Thermal energy from the environment, driving stochastic spin flips (modeled by temperatures T_s and T_h). Magnetic energy from the external environment field `h_e` acting on the sensory spins. Input also includes information from the generalized coordinates `x(t)` representing the environment state, which is mapped to the external field `h_e`.
    *   Value: Qualitative (T_s, T_h, h_e values given, but total energy input not calculated)
    *   Units: Energy (e.g., Joules, or relative units of dipole strength D)

### **2.2 Energy Transduction**

    *   Content: 1. Thermal energy facilitates stochastic spin reversals against energy barriers (dipole interactions, external fields). 2. External field `h_e` biases sensory spin alignment (magnetic potential energy to kinetic energy of spin flip). 3. Sensory spins' magnetic state generate stray fields acting on hidden spins (magnetic potential energy transfer). 4. Hidden spins interact via dipole fields, relaxing towards lower energy configurations modulated by sensory fields and thermal energy. 5. In the active inference loop, the calculated average magnetization `M` (an emergent property related to system energy state) influences the 'action' `a(t)`, which modifies the environment `dx/dt` (Eq 3) or `dx/dt` (Eq 14), subsequently changing the input field `h_e` (information to energy coupling).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss thermodynamic efficiency. The 'free energy' minimized (Eq 11) is a variational free energy functional from information theory/statistical mechanics, related to Bayesian inference, not thermodynamic free energy in the context of work extraction efficiency. The goal is information processing (inference) efficiency, not energy conversion efficiency.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation is inherent in the Monte Carlo simulation using Glauber dynamics, which models the coupling of the spins to a thermal bath. Each stochastic spin flip that lowers the system's energy effectively dissipates that energy difference into the bath. Magnetic hysteresis loops (Fig 1b, d, e) also represent energy loss per cycle. The paper does not quantify the dissipation rate or total dissipated energy. Qualitatively, dissipation occurs during spin relaxation processes. The 'friction' term `γ` in Eq 3 represents energy dissipation in the modeled environment.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Qualitative: Short-to-Medium term, dependent on T_h, N_h.
*    Units: Time steps (∆t) or physical time units if ν is specified.

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: 1. **Physical Interactions:** Nanomagnets interact via long-range dipole-dipole forces (approximated by dumbbell charges, Eq 19). Spins attempt to align with the local magnetic field (sum of fields from other spins and external/sensory sources). 2. **Dynamics:** Spin reversal occurs stochastically via Glauber dynamics. The probability of flipping spin `i` depends on the energy change `∆E(r_i)` (Eqs 20, 21) and the temperature (`T_s` or `T_h`). 3. **Active Inference Updates:** Generalized coordinates evolve according to differential equations (Eqs 8-10, 15-18). These involve local terms like `⟨χ(β_z(ϕ_α - µ_α))⟩_T` and `⟨χ(β_w(µ_1 + αµ_0 - v_d))⟩_T`, where `⟨χ(B)⟩_T` represents the thermally averaged magnetization response (Eq 4) to an effective local field `B`. Action `a(t)` also updates based on local state differences (Eq 10, 18).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Dipole | Interaction Strength | D (Prefactor in Eq 19) | Scaled to 1 (implicitly) | Energy | Methods text following Eq 19 | Implicit | Dipole strength unit sets energy scale |
    | Glauber | Sensory Temp | T_s | 0.1, 1.0 | D | Figs 2, 3; Methods | Explicit | Defines sensory spin flip probability |
    | Glauber | Hidden Temp | T_h | 0.4, 1.0 | D | Figs 1, 3; Methods | Explicit | Defines hidden spin flip probability |
    | Active Inf. | Precision Params | β_z, β_w | 1.0 - 10.0 | 1/D | Figs 3, 4 | Explicit | Weighting factors in free energy minimization |
    | Active Inf. | Environmental Friction | γ | Not specified | 1/time | Eq 3 | Explicit | Parameter in assumed environment model |
    | Active Inf. | Coupling | α | Not specified | Unitless | Eq 8, 9, 11 | Explicit | Parameter in assumed internal model |
    | Active Inf. | Rate Constant | κ | Not specified | Varies (affects time scale) | Eq 2, 7 | Explicit | Scales the rate of change in inference dynamics |

### **4.3 Global Order:**

    *   Content: 1. **ASI Order:** Low-energy configurations satisfying square ice rules (two-in, two-out at vertices), leading to zero average magnetization in the ground state. Metastable states with non-zero magnetization. 2. **Active Inference Order:** Convergence of the system's average hidden state `µ˜` and sensory state `ϕ˜` towards values consistent with a target parameter (`v_d` or `T_d`). This is reflected in the time evolution of average magnetization `M` (Figs 2, 3, 4). Oscillatory patterns (limit cycles) around the target can also emerge (Fig 4).
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Dipole | Dipole Interaction | D | Scaled to 1 | Energy | Implicit | Sets energy scale | Methods |
| Glauber | Thermal Flips | T_h | 0.4, 1.0 | D | Explicit | Governs hidden spin dynamics | Figs 1, 3, Methods |
| Glauber | Thermal Flips | T_s | 0.1, 1.0 | D | Explicit | Governs sensory spin dynamics | Figs 2, 3, Methods |
*(Note: This largely overlaps with M4.2.1, focusing on the rules directly leading to ASI self-organization)*

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Mag | Average Magnetization | M | -1 to 1 | Unitless (norm.) | Explicit | Primary observable tracking state | MC Sim, Active Inf. | Figs 1, 2, 3, 4 |
| VFE | Variational Free Energy | F | >= 0 | Energy (arb. units) | Explicit | Minimized during active inference | Active Inf. | Eq 11, Fig 3 |
| TargetDist | Distance to Target | | >=0 | Varies (Velocity, Temp Units) | Mixed | Difference between current state and target (e.g., |µ₁ + αµ₀ - v_d|) | Active Inf. | Eq 11, Fig 3 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Hybrid (Neuromorphic/Analog/Stochastic)

### **5.3 Computational Primitive:**

    *   Content: The most basic operations embodied are the evaluation of the terms within the active inference update equations (Eqs 8-10, 15-18). This primarily involves calculating the thermally averaged magnetization response `⟨χ(B)⟩_T` to effective fields `B`. These fields `B` are linear combinations of sensory states (`ϕ_α`), hidden states (`µ_α`), and target values (e.g., `ϕ_α - µ_α`, `µ_1 + αµ_0 - v_d`). The function `⟨χ(B)⟩_T` acts as a non-linear transfer function (likely resembling a tanh function for Ising spins). The overall computation approximates gradient descent steps on the variational free energy `F` (Eq 11).
    *   **Sub-Type (if applicable):** Weighted Summation, Non-linear Transfer Function (Ensemble Averaged Magnetization), Gradient Estimation.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|
*(Note: The system acts as an integrated whole; partitioning into discrete "units" with defined processing power is difficult and not done in the paper.)*

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Environment Step | ∆t | time steps | Implicit (used in MC sim & equations) | Implicit | Unit of simulation time progression |
        | Sensory Relaxation/Update | Related to N_s / ∆t | Steps⁻¹ or Frequency | Text (Fig 2 обсуждение N_s) | Explicit | Rate sensory spins sample environment/relax |
        | Hidden Relaxation/Update | Related to N_h / ∆t | Steps⁻¹ or Frequency | Text (Fig 1b обсуждение N_h) | Explicit | Rate hidden spins relax/update internal state |
        | Pulse Response Delay | ~6 | time steps | Fig 2a | Explicit | Delay between field pulse and max M response |
        | Active Inference Convergence | ~50-100+ | time steps | Figs 3, 4 | Explicit | Time for system to reach target/steady state |
        | Oscillation Period (Limit Cycle) | ~10-20 | time steps | Fig 4b | Explicit | Period of quasi-periodic oscillations shown |
    *   **Note:** Physical time depends on the attempt frequency ν and ∆t, which are not specified. Timescales are relative to the simulation time step ∆t.

### **6.2 Active Inference:**

    *   Content: Yes
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** Variational Free Energy `F` vs. time (Fig 3 shows minimization). Rate of convergence to target `v_d` or `T_d`. Accuracy of tracking time-varying targets. Robustness of convergence to noise or parameter variations. Amplitude and frequency of oscillations around the target (if any). Correlation between action `a(t)` and prediction error (e.g., `ϕ_1 - µ_1`). Information flow analysis between `x˜`, `ϕ˜`, `µ˜`, `a(t)`.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is the minimization of variational free energy `F` (Eq 11) via a gradient descent-like process implemented through the coupled dynamics of the hidden states `µ˜` (Eqs 8, 9, 15-17) and the action `a(t)` (Eqs 10, 18). The dynamics drive `µ˜` towards the most likely hidden states given the sensory input `ϕ˜` and the embedded target/prior (e.g., `v_d`, `T_d`). Simultaneously, `a(t)` is adjusted to change the environment (`x˜`) such that the resulting sensory input `ϕ˜` becomes more consistent with the internal prediction represented by `µ˜` and the target. The parameters `β_z`, `β_w` (related to inverse temperatures or precisions) modulate the strength of the updates and thus the adaptation process.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behaviors demonstrated are: 1. **Target Tracking:** The system adjusts its internal state (`µ˜`) and action (`a(t)`) such that the relevant environmental variable (`dx/dt` or `T(x)`) converges to a predefined target value (`v_d` or `T_d`). (Fig 3). 2. **Oscillation/Limit Cycles:** Under certain conditions (parameter values `β_z`, `β_w`), the system exhibits sustained quasi-periodic oscillations around the target value instead of stable convergence. (Fig 4). 3. **Enhanced State Sampling:** The sensory layer enables the hidden ASI layer to access a broader range of configurations compared to when an external field is applied directly, facilitating the active inference process. (Fig 1 main plots d, e vs b, c).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors (target tracking, oscillations, enhanced sampling) are validated primarily through numerical simulations (Monte Carlo). Figures 1, 2, 3, and 4 present simulation results demonstrating these behaviors. Consistency with the theoretical framework of Active Inference/FEP is shown by deriving the simulation equations (Eqs 8-10, 15-18) from the general principles (Eqs 2, 7). Control simulations (Fig 1 b,c vs d,e; Fig 2a vs 2b-d) are used to highlight the specific role of the sensory layer. Reproducibility is implied but not explicitly demonstrated (e.g., multiple runs with different random seeds). Limitations include reliance on simulation (not experiment) and lack of systematic parameter sweeps to map out behavioral regimes.

---

#Key: [cocconi_dissipation-accuracy_2025]

# Dissipation-accuracy tradeoffs in autonomous control of smart active matter

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a theoretical model of a single self-propelled agent (active Brownian particle - ABP) moving in two dimensions at a constant speed `w`. The agent's position `r(t)` and heading angle `θ(t)` are governed by overdamped Langevin equations, including translational (`Dt`) and rotational (`Dθ`) diffusion, and potentially a stationary external flow field `v(r)`. Crucially, the agent possesses feedback control over its heading: it exerts a self-generated torque proportional to `κ * Γ(θ,r)` attempting to align its heading `θ` with a predefined, position-dependent steering policy direction `θ*(r)`, where `Γ(θ,r) = -sin[θ - θ*(r)]`. The purpose is to study the stochastic thermodynamics of this controlled motility, specifically deriving the relationship between the energy dissipated for steering control (quantified by entropy production rate `S˙c`) and the accuracy of localisation (quantified by spatial variance `σ²`) near a target region (point or line). The model explores optimal steering policies that minimize energy expenditure for a given accuracy.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The energy input is implicitly modeled through the active forces driving self-propulsion (characterized by speed `w`) and the active torque enabling steering control (characterized by strength `κ`). The paper does not specify the underlying physical mechanism (e.g., chemical fuel) but treats `w` and `κ` as parameters defining the active processes that consume energy and perform work against dissipative forces (diffusion, viscous drag implied by overdamped dynamics).

### **2.2 Energy Transduction**

    *   Content: Energy is transduced into two primary forms of work:
        1.  Work done by the self-propulsion force (`wuˆ(θ)`) against dissipative forces (implicitly, viscous drag leading to overdamped motion, and diffusion). This contributes `S˙p = Pe` (dimensionless) to the entropy production rate.
        2.  Work done by the self-generated steering torque (`κΓ(θ,r)`) to control the heading angle `θ` against rotational diffusion and potentially flow gradients (though flow effects on torque are not modeled here). This contributes `S˙c = κ˜[κ˜⟨sin²(θ−θ*)⟩−⟨cos(θ−θ*)⟩]` (dimensionless) to the entropy production rate.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not define or calculate a measure of thermodynamic efficiency for the task (e.g., work output / energy input, or comparison to a thermodynamic bound for localisation). It focuses on quantifying the dissipation (`S˙c`) associated with achieving a certain accuracy (`σ²`), characterizing the *cost* rather than efficiency. The derived relationship `σ²S˙c = constant` (e.g., Eq. 15) is a tradeoff, not an efficiency metric.

### **2.4 Energy Dissipation**

    *   Content: Dissipation is explicitly quantified as the mean rate of entropy production (`S˙`) in the non-equilibrium steady state. It is additively decomposed into two components:
        1.  `S˙p = Pe`: Dissipation due to self-propulsion, independent of the steering policy. (Dimensionless rate).
        2.  `S˙c = κ˜[κ˜⟨sin²(θ−θ*)⟩−⟨cos(θ−θ*)⟩] ≈ -κ˜/2 ⟨∇·uˆ*⟩` (at low `κ˜`): Dissipation due to feedback control/steering. This term depends on the policy `uˆ*` and quantifies the cost of maintaining alignment and achieving localisation against noise. (Dimensionless rate).
        The physical mechanisms are the work done by the non-conservative active forces (propulsion and torque) required to maintain the non-equilibrium state against translational and rotational diffusion.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are defined by the coupled overdamped Langevin equations:
        1.  Position update: `r˙(t) = wuˆ(θ) + v(r) + sqrt(2Dt)ξt(t)`
        2.  Angle update: `θ˙(t) = -κ sin[θ - θ*(r)] + sqrt(2Dθ)ξθ(t)`
        These equations describe how the agent's state (`r`, `θ`) changes based on its current state, the external flow `v(r)`, the fixed steering policy `uˆ*(r) = (cos θ*(r), sin θ*(r))`, and stochastic noise (`ξt`, `ξθ`). Interactions are local in time (Markovian dynamics) and space (dependence on `r` and `θ`).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Eq. (1) | Position Dynamics | `w` | >0 (Constant) | Length/Time | Eq. (1) | Explicit | Model definition |
    | Eq. (1) | Position Dynamics | `Dt` | >0 (Constant) | Length²/Time | Eq. (1) | Explicit | Model definition |
    | Eq. (1) | Position Dynamics | `v(r)` | Varies (Function) | Length/Time | Eq. (1) | Explicit | Model definition (specific form varies by scenario) |
    | Eq. (2) | Angle Dynamics | `κ` | >0 (Constant) | 1/Time | Eq. (2) | Explicit | Model definition |
    | Eq. (2) | Angle Dynamics | `Dθ` | >0 (Constant) | 1/Time | Eq. (2) | Explicit | Model definition |
    | Eq. (3) | Steering Control | `θ*(r)` | Varies (Function) | Radians | Eq. (3) | Explicit | Policy definition (specific form varies by scenario) |

### **4.3 Global Order:**

    *   Content: The emergent global order is the non-equilibrium steady-state probability distribution `P(r,θ)` (or its marginal `ρ(r)`), which describes the statistical localisation patterns of the agent in phase space (position and orientation) or just configuration space. Examples include:
        *   Exponential localisation around a point target (`ρ(r) ∝ exp(-r/ℓ)`, Eq. 14).
        *   Modified exponential/potential-dependent localisation in radial flows (`ρ(r) ∝ exp[-V(r/ℓv) - r/ℓ]`, Eq. 16).
        *   Gaussian-like distribution with exponential tails along a target path (`ϱ(y)` given by Eq. 24, composed of Gaussian and exponential parts).
        *   Periodic density distributions along a path in periodic flows (`ϱ(x;λ)` given by Eq. D.4).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Eq. (1) | Position Dynamics (Propulsion) | `w` | >0 (Constant) | L/T | Explicit | Model definition | Sec 2.1 |
| Eq. (1) | Position Dynamics (Flow) | `v(r)` | Varies | L/T | Explicit | Model definition | Sec 2.1 |
| Eq. (1) | Position Dynamics (Noise) | `Dt` | >0 (Constant) | L²/T | Explicit | Model definition | Sec 2.1 |
| Eq. (2) | Angle Dynamics (Steering) | `κ`, `θ*(r)` | `κ`>0, `θ*(r)` varies | 1/T, rad | Explicit | Model definition | Sec 2.1 |
| Eq. (2) | Angle Dynamics (Noise) | `Dθ` | >0 (Constant) | 1/T | Explicit | Model definition | Sec 2.1 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Localisation Accuracy (Point Target) | Variance of Radial Displacement | `σr²` | >0 | L² | Explicit | Quantifies spread around target | Calculate from `ρs(r)` | Sec 3 |
| Localisation Accuracy (Path Target) | Variance of Orthogonal Displacement | `σy²` | >0 | L² | Explicit | Quantifies spread around target path | Calculate from `ϱs(y)` | Sec 4.1 |
| Localisation Length Scale (No Flow, Point) | Characteristic Decay Length | `ℓ = 2Deff/κ˜` | >0 | L | Explicit | Defines spatial extent of density | Formula from `ρs(r)` | Eq. 14 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", including M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog

### **5.3 Computational Primitive:**

    *   Content: Error Calculation & Proportional Control. The core operation is calculating the sine of the angular error `Δθ = θ - θ*(r)` and applying a restoring torque proportional to this value (`-κ sin(Δθ)`). This is a basic form of feedback control aimed at minimizing the angular error `Δθ`. The policy evaluation `r -> θ*(r)` is also a computational step (function evaluation).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Rotational Diffusion Time | `1/Dθ` | Time | Eq. 2 | Explicit | Inverse of diffusion coefficient. Fundamental timescale. |
        | Alignment Relaxation Time | `~1/κ` (for `κ˜ >> 1`) or `~1/Dθ` (for `κ˜ << 1`) | Time | Sec 2.1, 2.2 | Implicit | Characteristic time for alignment torque to overcome/compete with diffusion. Exact value depends on `κ˜`. Timescale of feedback control. |
        | Localisation/Traversal Time (Point Target, No Flow) | `~ℓ²/Dt` or `~ℓ/w` | Time | Sec 3 | Inferred | Timescale to diffuse (`ℓ²/Dt`) or actively move (`ℓ/w`) across the localisation length `ℓ`. |
        | Flow Variation Timescale (Sinusoidal Flow) | `~λ/w` | Time | Sec 4.2 | Implicit | Time for the agent moving at speed `w` to traverse one wavelength `λ` of the flow pattern. Relevant for CAAP performance. |
        | Policy Adaptation Delay (CAAP) | `~1/κ` | Time | Sec 4.2 | Implicit | The finite alignment strength `κ` introduces a delay in responding to changes in the required steering direction due to flow variations. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behaviors analyzed are:
        1.  **Localisation near a target:** The agent confines its motion statistically to the vicinity of a specific subspace (a point target in Section 3 or a line target path in Section 4). The degree of localisation (accuracy) depends on system parameters and the policy.
        2.  **Directed motion along a path:** When using policies like AAP or CAAP (Section 4), the agent exhibits net motion along the target path while simultaneously localising orthogonally to it.
        3.  **Dissipation-Accuracy Tradeoff:** A fundamental relationship emerges where increasing localisation accuracy (decreasing `σ²`) requires increased energy dissipation for control (`S˙c`) (Eqs. 15, 27).
        4.  **Optimal Navigation Strategy (Pareto Front):** For localisation along a path without flow, a set of optimal policies (coinciding with AAP) is identified that defines the Pareto front, representing the best possible tradeoff between localisation accuracy and dissipation cost (Section 4.1.3, Fig. 4).
        5.  **Escape Behavior:** Under certain conditions (e.g., strong divergent flow or uniform flow exceeding a threshold relative to alignment strength), the agent fails to localise and escapes to infinity (Sections 3, Appendix B).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Emergent behaviors are validated primarily through:
        1.  **Analytical Derivations:** Using stochastic thermodynamics and Fokker-Planck equations (often with approximations like gradient expansion) to predict steady-state distributions (`ρs`, `ϱs`), localisation accuracy (`σ²`), and dissipation (`S˙c`). Key results include explicit formulas for these quantities and the dissipation-accuracy tradeoff (Eqs. 14-18, 20-27, D.4-D.5, A.13, A.19).
        2.  **Numerical Simulations:** Direct simulation of the Langevin equations (1, 2) is used to generate trajectories and empirical distributions. These simulation results are compared quantitatively with theoretical predictions (e.g., density profiles in Fig. 2a, 3a, D1a; S˙c and σ² values in Fig. 2b, 3c, 5b, 6b, D1b). Agreement is generally good within the stated validity range of approximations (e.g., low `κ˜`, large `λ`).
        3.  **Parameter Space Exploration:** Simulations and theory explore how behaviors change across different parameters (`κ˜`, `Pe`, `ε`, flow parameters `vf`, `ℓv`, `A`, `λ`), validating predicted dependencies and identifying regimes like escape or optimality (Figs. 2, 3, 4, 5, 6, D1).
        Limitations: Validation relies heavily on the gradient expansion approximation (low `κ˜`, smooth spatial variations), which breaks down in some regimes (explicitly noted for high `κ˜` in Fig 2c discussion, and for small `λ` in Fig D1b). Some results are only derived for simplified scenarios (e.g., linear flow potential, purely sinusoidal flow).

---

#Key: [thorsen_microfluidic_2002]

# Microfluidic Large-Scale Integration

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of high-density microfluidic chips fabricated using multilayer soft lithography with polydimethylsiloxane (PDMS). These chips contain thousands of micromechanical valves and hundreds of individually addressable chambers, enabling complex fluid manipulations analogous to electronic large-scale integration (LSI). Key components include micromechanical valves, fluidic multiplexors (combinatorial arrays of valves acting as binary trees), flow channels, control channels, and reaction/storage chambers. The purpose is to demonstrate a scalable technology for integrating complex fluidic operations on a chip, enabling applications like high-density sample processing, microfluidic memory storage, and parallel assays (demonstrated with a comparator array). The system *does* complex fluid handling: loading reagents, compartmentalizing them into picoliter/nanoliter chambers, mixing, isolating, and selectively recovering contents. Two specific devices are described: a 1000-chamber memory storage device (25x40 array, 250 pL chambers) and a 256-chamber comparator array (4 columns x 64 chambers/column, ~750 pL chambers).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is external hydraulic or pneumatic pressure applied to the control channels to actuate the valves and potentially low external pressure (~20 kPa) applied to input ports to load fluids. Computer-controlled external solenoid valves regulate the pressure application.
    *   Value: ~40 kPa (valve actuation), ~20 kPa (fluid loading), Up to 300 kPa (accepted input)
    *   Units: kPa (kilopascals)

### **2.2 Energy Transduction**

    *   Content: The primary energy transduction mechanism is the conversion of pneumatic/hydraulic pressure energy into mechanical work to deflect the thin PDMS membrane at the valve junction. This deflection closes or opens the flow channel underneath. The multiplexor system transduces a small number of controlled pressure inputs (2 log2 n) into control over n flow channels by selectively applying pressure to combinations of control lines. In the comparator chip, chemical energy from the enzyme reaction (CCP converting Amplex Red) is transduced into optical energy (fluorescence).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper states that once a state is set (e.g., in the memory/display device), the device consumes "very little power" (p2). This suggests high efficiency for maintaining a static configuration (valves closed/open). Energy is primarily consumed during switching by the external solenoid valves and potentially during pressure maintenance if there are leaks (though PDMS seals well). Efficiency is not quantified in terms of Joules per operation or thermodynamic efficiency. The chemical amplification in the comparator (enzyme turnover) provides high signal gain, which could be considered a form of high "efficiency" in terms of converting enzyme presence into detectable signal, but not in standard energy terms.

### **2.4 Energy Dissipation**

    *   Content: Dissipation mechanisms are not explicitly quantified. Potential mechanisms include:
        1.  Viscous dissipation during fluid flow within channels (likely low due to low Re).
        2.  Mechanical damping/hysteresis in the PDMS valves during actuation.
        3.  Heat generated by external solenoid valves during switching.
        4.  Potential energy loss due to minor leaks in the pneumatic/hydraulic control system (though minimized by PDMS sealing).
        5.  Diffusion of molecules through PDMS (mentioned as a potential cross-contamination source, p3, implying energy dispersal).
    Qualitatively, dissipation seems low, especially for static states.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**

    *   Retention: Dependent on valve integrity and sustained pressure. Potentially long-term if actively maintained, but susceptible to leakage or pressure loss over time (not quantified). Short-term without active maintenance.
    *   Capacity: 1000 individually addressable states (bits). Explicitly stated.
    *   Read-out: Optical readout (visual/scanner) demonstrated. Destructive readout via purging is also possible. Accuracy is qualitatively high (clear visual distinction in Fig 2C).
    The score is moderate because while it demonstrates addressable storage, the retention mechanism relies on continuous external pressure (like DRAM refresh, though potentially slower decay), and it's not a solid-state or non-volatile memory type. Rewritability is achieved by purging and refilling.

### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 1000
*   Units: Chambers / bits

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Skipping M4.2-M4.7 as M4.1 is "No")**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid

### **5.3 Computational Primitive:**

    *   Content: Comparison/Thresholding (Comparator Chip); Addressable Storage/Retrieval (Memory Chip).
        *   **Comparator:** The basic operation is comparing the rate of an enzymatic reaction (input signal, amplified) within a chamber to an implicit threshold (determined by background fluorescence/diffusion/incubation time), resulting in a high/low fluorescence output. Mathematical description: Output = f( [Enzyme] * k_cat * t ), where f is a threshold function.
        *   **Memory:** The basic operations are Write (setting chamber state via filling/purging based on multiplexer address) and Read (observing chamber state optically or via purging based on multiplexer address).
    *   **Sub-Type:** Comparison: Biochemical Thresholding; Memory: Binary Storage.

### **5.4 Embodied Computational Units**
| Unit ID        | Description                                 | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------------- | :------------------------------------------ | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description          | Value        | Units    | Source             | Implicit/Explicit    | Justification                                      |
        | :----------------------------- | :----------- | :------- | :----------------- | :-----------------: | :------------------------------------------------- |
        | Diffusion Time (Comparator)    | "few minutes"| minutes  | Text (p2)          | Explicit           | Time barrier opened for substrate diffusion.       |
        | Reaction Incubation Time (Comp) | 1            | hour     | Text (p2)          | Explicit           | Time allowed for enzyme reaction before scanning.  |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Skipping M7.2 as M7.1 is "No")**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behaviors are:
        1.  **Multiplexed Addressing:** Controlling n flow channels using 2 log2 n control inputs (Fig 1).
        2.  **Fluidic Memory:** Storing and retrieving information encoded as the presence/absence of fluid in addressable chambers (Fig 2). Functions as a fluidic display.
        3.  **Parallel Compartmentalization & Assay:** Loading reagents, isolating them into numerous independent chambers, allowing reactions (e.g., enzymatic), and performing parallel comparisons (Comparator chip, Fig 3, Fig 4).
        4.  **Selective Recovery:** Individually addressing and purging the contents of specific chambers for collection/analysis (Fig 3B, described for comparator chip).
    These behaviors arise from the designed architecture and controlled actuation, not spontaneous emergence in the strict sense, but they represent complex system-level functions.

### **8.2 Behavior Robustness:**

        *   **Strengths:** PDMS allows tight seals around input pins, accepting high pressures (up to 300 kPa) without leakage (p2). The fabrication uses established techniques (p1, Ref 11). Successful operation of complex devices (1000s of valves) demonstrates a degree of robustness. Recovery of single cells is shown to be effective (p3).
        *   **Weaknesses/Perturbations Discussed:** PDMS incompatibility with nonpolar organic solvents (p3). Cross-contamination via diffusion through elastomer or surface adhesion (p3) - analogous to leakage currents. Fabrication tolerances could affect valve performance (implied). Potential for blockages or incomplete valve closure (not explicitly discussed but common in microfluidics).
    The score reflects successful complex demonstrations offset by acknowledged material limitations and potential failure modes common to microfluidics. Robustness is not quantified rigorously (e.g., operational lifetime, failure rates under stress, tolerance ranges for pressure/temperature).

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper validates the *designed* behaviors through experimental demonstration:
        *   **Multiplexing:** Implicitly validated by the successful addressing in memory and comparator chips requiring only 22 and 18 external connections respectively. Fig 1 illustrates the principle.
        *   **Memory/Display:** Validated by loading dye/water and selectively purging to spell "CIT", visualized optically (Fig 2C). Operational mechanics described (Fig 2B).
        *   **Comparator/Parallel Assay:** Validated by loading bacteria/substrate, compartmentalizing, reacting, and scanning fluorescence (Fig 4B). Control experiments (eGFP vs CCP bacteria, Fig 4C) confirm specificity.
        *   **Selective Recovery:** Validated by identifying single GFP-expressing E. coli cells, purging their specific chambers, and confirming recovery via microscopy and colony growth (p3).
     *   **Limitations:** While behaviors are demonstrated, validation focuses on proof-of-concept rather than rigorous quantification of reliability, repeatability across many devices, or performance under varying conditions. Claims are not strictly about "emergence" but designed functionality.

---

#Key: [wadhams_making_2004]

# Making sense of it all: bacterial chemotaxis:

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews bacterial chemotaxis, focusing on *E. coli* and *S. enterica* as model systems, with comparisons to other bacteria like *B. subtilis* and *R. sphaeroides*. Chemotaxis is the process by which bacteria bias their movement towards favorable chemical environments (attractants) and away from unfavorable ones (repellents). This is achieved through a signal transduction pathway involving transmembrane chemoreceptors (Methyl-Accepting Chemotaxis Proteins, MCPs), adaptor proteins (CheW), a histidine kinase (CheA), response regulators (CheY, CheB), a methyltransferase (CheR), and a phosphatase (CheZ in *E. coli*). Signal detection by MCPs modulates CheA autophosphorylation. CheA-P transfers phosphate to CheY and CheB. CheY-P binds to the flagellar motor switch complex (FliM), causing changes in rotational direction (tumbling/reorientation). CheB-P (a methylesterase) and CheR (methyltransferase) regulate the methylation state of MCPs, allowing the system to adapt to persistent stimuli. The flagellar motor, powered by an electrochemical gradient, drives helical flagella to propel the cell. The system integrates signals over time to sense temporal gradients. Variations exist, including different receptor types (e.g., cytoplasmic Tlps), different kinase/regulator combinations, alternative adaptation mechanisms (CheC/CheD/CheV), and different signal termination mechanisms (alternatives to CheZ). The system is presented as a paradigm for sensory signaling.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters listed are key characteristics of the system's physical implementation and dynamics as described in the text. Reliability is considered High for explicitly stated, well-established values from cited sources, Medium for values stated as approximate or derived from specific experimental contexts (like cluster size).

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The system utilizes two primary energy sources: 1) Chemical energy from ATP hydrolysis for the phosphorylation cascade (CheA autophosphorylation and subsequent phosphotransfer to CheY and CheB). 2) Electrochemical potential energy from the proton (H+) or sodium (Na+) gradient across the cytoplasmic membrane to power the rotation of the flagellar motor.

### **2.2 Energy Transduction**

    *   Content: 1) Chemical energy (ATP hydrolysis) is transduced into the potential energy of phosphorylated proteins (specifically the high-energy phosphoryl group transferred from ATP to CheA His48, then to CheY Asp57 and CheB Asp56). This stored potential energy drives conformational changes associated with protein activity (e.g., CheY-P binding to FliM, increased CheB-P methylesterase activity). 2) Electrochemical potential energy (ion gradient) is transduced into mechanical rotational energy by the flagellar motor (MotA/MotB stator interacting with the FliG rotor).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The review does not provide quantitative data or qualitative assessment regarding the energy efficiency of either the phosphorylation cascade or the flagellar motor. While the motor is known to be highly efficient elsewhere in the literature, this review does not state it.

### **2.4 Energy Dissipation**

    *   Content: 1) Heat is dissipated during ATP hydrolysis and protein conformational changes associated with phosphorylation/dephosphorylation. 2) Spontaneous and catalyzed (CheZ, CheC, potentially others) dephosphorylation of response regulators (CheY-P, CheB-P) releases the stored phosphoryl group energy, likely as heat. 3) Viscous drag dissipates mechanical energy as the flagella rotate and propel the bacterium through the liquid medium (mentioned indirectly via Reynolds number discussion in Box 2). 4) Friction within the flagellar motor components likely occurs, though not explicitly mentioned as a dissipation mechanism. Quantification is not provided. Qualitative assessment: Viscous drag is significant at the microbial scale (low Reynolds number); dephosphorylation rates imply continuous energy turnover in the signaling cascade.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding to M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Seconds to minutes
*    Units: Time (Qualitative Descriptor)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Multiple states per receptor, multiple receptor types.

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", proceeding to M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: 1) Protein-protein binding: Specific binding occurs between the cytoplasmic domains of MCPs and CheW, between CheW and CheA (P5 domain), and potentially directly between MCPs and CheA (P5 domain). CheR binds to a C-terminal motif (NWETF) on specific MCPs (Tsr, Tar). CheY and CheB bind competitively to the CheA P2 domain. CheY-P binds to FliM on the motor. (Sec: The chemosensory system, Receptor adaptation, CheW, The histidine protein kinase CheA, The response regulator CheY). 2) Inter-receptor interactions: Within clusters, MCP dimers are proposed to pack as trimers of dimers or other higher-order arrays, potentially involving interactions between cytoplasmic and/or periplasmic domains. Ligand binding to one receptor is proposed to influence neighbors (allosteric interactions within the cluster). Methylation state might affect these interactions. (Sec: Localization of methyl-accepting chemotaxis proteins, Refs 33, 57, 58, 59, 60, 62, 63, 108). 3) Protein-membrane interactions: Transmembrane domains anchor MCPs; localization might involve interactions with specific membrane regions or cytoskeletal elements (though the latter is not detailed). (Sec: Escherichia coli methyl-accepting chemotaxis proteins). 4) Flagellar interactions: Hydrodynamic forces cause anti-clockwise rotating flagella to form a bundle (Box 3 figure legend description, Ref 15 implicitly).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**

    *   Content: 1) Polar Chemoreceptor Clusters: MCPs, CheW, and CheA co-localize into distinct patches (~200 nm diameter), often preferentially at the cell pole(s). In *R. sphaeroides*, distinct polar (MCP-based) and cytoplasmic (Tlp-based) clusters are observed. (Sec: Localization..., Fig 4). 2) Flagellar Bundle: During smooth swimming, multiple helical flagella rotating CCW coalesce into a single bundle propelling the cell. (Box 2, description associated with Fig Box 3).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Cluster Size | Diameter of polar chemoreceptor clusters | Diameter | ~200 | nm | Explicit | Directly stated value. | Immunofluorescence/Electron Microscopy (implied by Refs 52, 53, 55) | Sec: Localization... |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", proceeding to M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid

### **5.3 Computational Primitive:**

    *   Content: The system performs several integrated operations:
        *   **Sensing:** Detection of specific chemicals by MCPs/Tlps.
        *   **Temporal Differentiation/Comparison:** Implicitly comparing current receptor occupancy/signaling with the recent past (encoded in the adaptation/methylation state) to detect changes over time.
        *   **Signal Integration:** Combining inputs from multiple receptor types and individual receptors (potentially via cluster interactions).
        *   **Amplification:** Kinase (CheA) activity modulation leads to amplified changes in response regulator (CheY) phosphorylation levels. High sensitivity/gain mentioned (Sec: The chemosensory system).
        *   **Adaptation (Integral Feedback):** Methylation system adjusts the baseline sensitivity, effectively integrating the stimulus history.
        *   **Thresholding:** The flagellar motor switch responds to CheY-P concentration, likely in a cooperative or threshold-like manner (though non-cooperativity mentioned for isolated switch complex in Ref 88).

### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Motor Switching Frequency (baseline) | ~1 | s⁻¹ (implies ~1s interval) | Sec: Bacterial chemotaxis as a model system | Explicit | Stated frequency of direction change. |
        | Adaptation Timescale | Seconds to minutes | time | Sec: Receptor adaptation, Fig 2 legend (Implied) | Mixed | Explicitly described as resetting over time; timescale inferred qualitatively. |
        | CheY-P Lifetime (with CheZ) | ~200 | ms | Sec: The phosphatase CheZ and signal termination | Explicit | Explicitly stated half-life with CheZ action. |
        | CheY-P Lifetime (spontaneous) | ~20 | s | Sec: The phosphatase CheZ and signal termination | Explicit | Explicitly stated half-life without CheZ. |
        | Signal Transduction (periplasm to CheA) | Slow kinetics mentioned | time | Sec: Escherichia coli methyl-accepting chemotaxis proteins | Explicit (qualitative) | Explicitly contrasted with rapid ligand binding. |
        | Motor Rotation Speed | Up to ~10³ | Hz | Box 2 | Explicit | Stated rotational frequencies. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", proceeding to M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: Adaptation in *E. coli* relies on changes in the methylation level of specific glutamate residues on the cytoplasmic domain of MCPs. This level is controlled by two enzymes: 1) CheR, a constitutively active methyltransferase that adds methyl groups (from S-adenosyl methionine). 2) CheB, a methylesterase that removes methyl groups. CheB's activity is regulated by phosphorylation; CheA-P phosphorylates CheB, increasing its methylesterase activity ~100-fold. Attractant binding inhibits CheA autophosphorylation, reducing CheB-P levels and leading to net methylation by CheR. Repellent binding (or attractant removal) stimulates CheA, increasing CheB-P levels and causing net demethylation. Methylation state affects the receptor's ability to stimulate CheA: higher methylation increases CheA stimulation, counteracting the effect of attractant binding and resetting the pathway. Some MCPs require interaction with others (Tsr/Tar) for CheR-mediated methylation due to lacking the CheR-binding motif. Variations exist (e.g., *B. subtilis* system involving CheC/CheD/CheV).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is chemotaxis: a biased random walk involving alternating periods of relatively straight swimming ("runs") and reorienting movements ("tumbles"). Runs are typically achieved by coordinated counter-clockwise (CCW) rotation of multiple flagella forming a bundle. Tumbles result from a switch in rotation direction of one or more motors to clockwise (CW), disrupting the bundle. The frequency of tumbling is modulated by the sensory system in response to temporal gradients of chemoeffectors, biasing movement towards favorable conditions. Other described behaviors include flagellar bundling/unbundling, motor switching (CW/CCW), stopping or slowing, and surface motility (gliding, twitching - Box 2, though chemotaxis typically refers to swimming).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The review describes chemotactic behavior primarily through functional observation (biased movement in gradients) and the underlying mechanisms (runs and tumbles linked to motor switching). Validation methods mentioned or implied include: 1) Behavioral Assays: Observing bacterial movement in chemical gradients (e.g., capillary assays, soft agar plates - Ref 17 implicitly). 2) Genetic Analysis: Studying mutants (deletion or point mutations) and their behavioral phenotypes (smooth-swimming, tumbling - mentioned for cheW/A, cheZ deletions). 3) Biochemical Assays: Studying protein interactions, phosphorylation kinetics, methylation rates in vitro. 4) Structural Biology: Determining protein structures (Fig 3). 5) In vivo Imaging: Real-time imaging of flagella (Ref 15), FRET studies of protein interactions (Refs 21, 22, 29, 55). 6) Computational Modeling: Simulating pathway dynamics and comparing to experimental data (Refs 18, 19, 20, 60, 65). These methods collectively validate the link between molecular mechanisms and the emergent chemotactic behavior. Limitations might include differences between in vitro and in vivo conditions, or simplifying assumptions in models.

---

#Key: [luo_machine-learning-assisted_2022]

# Machine-Learning-Assisted Recognition on Bioinspired Soft Sensor Arrays

__Paper Type:__ Hybrid * (Experimental fabrication and testing combined with computational/ML analysis)

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a Bioinspired Soft Sensor Array (BOSSA) designed for user and object recognition. It consists of a 5x5 array of sensing units. Each unit incorporates low-modulus porous silicone rubber as the top layer, cascaded row + column electrodes embedded below it, and a silicone substrate base. The sensing mechanism relies on the triboelectric effect generated during contact, making the array sensitive to both pressure and material properties. The sensor output signals (from 5 row + 5 column channels) are conditioned, digitized, wirelessly transmitted, and processed using machine learning algorithms (Support Vector Machine - SVM for user identification via keystroke dynamics; Multilayer Perceptron - MLP for object recognition via placement/extraction patterns) to perform classification tasks. Its purpose is environment awareness, specifically individual and object recognition for smart home/industry applications.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input is mechanical energy from physical contact (pressure, deformation) applied to the sensor array surface by a user's finger or an object. This mechanical interaction drives the triboelectric effect.

### **2.2 Energy Transduction**

    *   Content: The main energy transduction mechanism is the triboelectric effect occurring at the interface between the contacting material (e.g., human skin, object surface) and the porous silicone rubber. Mechanical energy from contact/separation causes charge separation due to different electron affinities, creating an electrical potential difference. This potential drives electron flow in the external circuit connected to the row and column electrodes, transducing mechanical energy into electrical energy (voltage/current signals).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: As a *power generator*, the efficiency is very low. The peak power output measured is 442.4 nW under optimal load (0.5 GΩ) and specific contact conditions (Fig 2h). This represents a minuscule fraction of the input mechanical energy. However, as a *sensor*, the energy transduction is efficient enough to generate easily detectable voltage/current signals (up to several volts, Fig 2c-g) from typical contact forces (e.g., 0.5 N), enabling high-accuracy recognition tasks. The score reflects its poor performance for power harvesting but good performance for self-powered sensing signal generation.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation occurs through several mechanisms:
        1.  **Mechanical Damping:** Viscoelastic nature of the porous silicone rubber dissipates mechanical energy as heat during deformation and relaxation (Implicit, inherent material property). Qualitatively Medium/High due to low modulus/porous nature.
        2.  **Electrical Resistance:** Resistance in the electrodes, wiring, and measurement circuitry (including the high load resistance mentioned in Fig 2h) dissipates electrical energy as heat (Implicit). Value depends on circuitry, but significant given the high impedance nature of triboelectric outputs.
        3.  **Charge Recombination/Leakage:** Imperfect charge separation/retention and leakage currents dissipate stored electrical potential energy (Implicit).
    Quantification is not provided in the paper.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceed.)**

### **3.2 Memory Type:**

    *   **Retention:** High/Long-term (Trained model parameters are persistent unless retrained).
    *   **Capacity:** Medium/High (Depends on model complexity - number of support vectors in SVM, weights/neurons in MLP, capable of distinguishing 10 users or 20 object states).
    *   **Read-out Accuracy:** High (Reflected in the high classification accuracies reported: 98.9% for SVM, 98.6% for MLP).
    *   **Re-writability:** Yes (The models can be retrained with new data).
The memory is computational, not based on inherent material states. The score reflects good performance on key memory aspects within its computational context, but lacks the physical embodiment aspect of higher-level material memory.

### **3.3 Memory Retention Time:**

*   Value: Long-term (Effectively permanent until retraining)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Sufficient to distinguish 10 users or 20 object states (10 placement + 10 extraction).
*   Units: Classes / Distinguishable States

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: 98.9 (SVM); 98.6 (MLP)
*   Units: %

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: ~0 (digital); Sensor drift: 11.4% over 20k cycles
    *   Units: % loss / time or cycles

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Accuracy_SVM | Classification accuracy on test set (User ID) | 98.9 | % | `MemoryNode` attribute | Fig 3h | Explicit | Explicitly reported classification result. |
    | Accuracy_MLP | Classification accuracy on test set (Object Rec.) | 98.6 | % | `MemoryNode` attribute | Fig 4e | Explicit | Explicitly reported classification result. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**

### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip to Module 6.)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Sensor Contact Frequency (Characterization) | 1 - 4 | Hz | Fig 2g | Explicit | Explicitly varied in experiments. |
        | Sensor Durability Test Duration | >11 (at 0.5N, 20k cycles, assuming 2Hz) | hours | Fig 2i | Implicit | Calculated from 20k cycles / (assumed 2 Hz for cycle time). |
        | Keystroke Signal Valley Width (W) | Variable (e.g., ~0.1-0.2 approx) | s | Fig 3g | Explicit | Defined and visualized, value estimated from figure. |
        | Keystroke Interval Valley-Peak (T1) | Variable (e.g., ~0.05 approx) | s | Fig 3g | Explicit | Defined and visualized, value estimated from figure. |
        | Keystroke Interval Valley-Valley (T2) | Variable (e.g., ~0.2-0.3 approx) | s | Fig 3g | Explicit | Defined and visualized, value estimated from figure. |
        | MLP Input Data Length per Sample | 70 | time points | Section 2.4 | Explicit | Stated as data length for valid signal per channel. |
    *   **Note:** Sensor response time (rise/fall time) not explicitly quantified, though signal shapes in Fig 2f, 3e suggest it's in the millisecond range. ML training/inference times not specified.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes (Algorithmic Adaptation)

**(Conditional: M7.1 is "Yes", proceed.)**

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is algorithmic learning within the SVM and MLP models during training.
        *   **SVM:** Likely involves finding an optimal separating hyperplane in a high-dimensional feature space, potentially using techniques like Sequential Minimal Optimization (SMO). The adaptation involves selecting support vectors and determining their weights/bias. (Mechanism details not explicitly stated, inferred based on standard SVM).
        *   **MLP:** Involves adjusting the weights and biases of the neural network connections via backpropagation of errors, typically using gradient descent-based optimization algorithms (e.g., Adam, SGD) to minimize a loss function (e.g., cross-entropy) on the training data. (Mechanism details not explicitly stated beyond MLP structure in Methods 4.4/Fig S9, inferred based on standard MLP training).
    The adaptation is driven by the labeled training data (supervised learning) and the optimization algorithm seeking to minimize classification errors.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behaviors are:
        1.  **User Identification:** Classifying which of 10 users is interacting with the sensor based on single-pixel keystroke dynamics patterns.
        2.  **Object Recognition:** Classifying the placement or extraction event for 10 different objects based on multipixel spatiotemporal tactile patterns.
        3.  **Tactile Mapping:** Visualizing contact location and relative force/pressure distribution across the 5x5 array (demonstrated qualitatively, e.g., writing letters Fig 3d, object placement Fig 4a-iii).

### **8.2 Behavior Robustness:**

        *   **Classification Accuracy:** High accuracy reported on test datasets (98.9% user ID, 98.6% object rec.) suggests robustness to variations within the learned classes under tested conditions.
        *   **Sensor Durability:** The sensor itself shows reasonable physical robustness, maintaining function with acceptable signal drift (~11.4% voltage drop) over 20,000 contact cycles (Fig 2i).
        *   **Crosstalk:** Low crosstalk demonstrated (NECT = 0.14-0.19 for specific case, Fig 3g, S6), indicating robustness to signal interference between channels.
        *   **Generalization:** MLP model shows good generalization, avoiding overfitting (Fig S8). SVM performance also high.
        *   **Limitations:** Robustness to untrained users/objects, different environmental conditions (temp, humidity), significant sensor aging beyond 20k cycles, or physical damage is not evaluated. The score reflects good performance under lab conditions but acknowledges untested real-world robustness factors.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The key behaviors (user/object recognition) are validated through standard machine learning practices:
        1.  **Dataset Collection:** Large datasets were collected (300 samples for user ID, 4000 for object rec.).
        2.  **Training/Validation/Testing Split:** Data was split for training, validation (for MLP optimization), and testing (6:2:2 ratio for MLP). 10-fold cross-validation used for SVM.
        3.  **Quantitative Metrics:** Classification accuracy is reported using confusion matrices (Fig 3h, 4e). MLP training/validation accuracy and loss curves are provided (Fig S8).
        4.  **Control/Comparison:** Performance is benchmarked against chance level (implicitly high given the number of classes). Crosstalk analysis (NECT) validates signal integrity (Fig S6). Feature extraction process is detailed (Fig 3g).
        5.  **Reproducibility:** Methods for fabrication, signal processing, and ML are described, aiding reproducibility.
        **Limitations:** Validation is primarily statistical on collected datasets. Real-world dynamic testing scenarios are limited (shown qualitatively in videos). Robustness to variations beyond the training data distribution is not quantified. The behaviors are primarily programmed/learned classification, not strictly "emergent" in the sense of arising unexpectedly from simple local rules.

---

#Key: [osat_non-reciprocal_2023]

# Non-reciprocal multifarious self-organization

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a theoretical/computational model investigating self-organization dynamics in a collection of interacting 'tiles' on a lattice. It extends the concept of 'multifarious self-assembly' (where specific reciprocal interactions guide assembly into multiple predefined structures) by incorporating programmable 'non-reciprocal' interactions. The components are distinct tile types (up to M types) arranged on a 2D lattice, potentially with empty sites. Interactions are defined between adjacent tiles: reciprocal interactions (U) favor specific neighbor pairs found in target structures, providing the basis for equilibrium assembly (analogous to Hopfield network memory retrieval); non-reciprocal interactions (R, strength λ) add an asymmetry depending on which tile joins an existing structure, driving non-equilibrium transitions between assembled structures. The purpose is to demonstrate and characterize how programmable non-reciprocal interactions can achieve automated dynamical control and sequential transitions ('shape-shifting') between multiple self-assembled structures, mimicking complex biological processes like the cell cycle, using a common pool of building blocks. The system is simulated using a generalized Monte Carlo method incorporating both interaction types.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                 | Value                  | Units        | Source (Fig/Table/Section)   | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :----------------------------- | :---------------------: | :-----------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The system operates under implicit thermal energy (k<sub>B</sub>T) which drives stochastic tile movements/attempts in the Monte Carlo simulation. Specific interactions contribute effective potential energy changes: reciprocal bonds (U) lower energy by -ε, while non-reciprocal interactions modify transition rates based on an energy scale λ, effectively injecting energy into specific state transitions to drive the system out of equilibrium. The chemical potential μ acts as an energy cost/gain for adding/removing tiles. The non-reciprocal term λ is the key driver of non-equilibrium dynamics.
    *   Value: ε, λ, μ, k<sub>B</sub>T
    *   Units: Energy (often implicitly k<sub>B</sub>T=1)

### **2.2 Energy Transduction**

    *   Content: Thermal energy (implicit) drives attempts to change tile states (addition, removal, swap). Favorable reciprocal interactions (-ε) convert potential energy into system stability, promoting assembly. Non-reciprocal interactions (λ) bias specific state transitions (tile additions depending on approach direction), converting the 'programmed' potential energy difference λ into directed dynamics (shape-shifting). Energy is transduced during tile binding/unbinding events and configuration changes. ΔH in Eq. 6 represents the change in energy (Hamiltonian H) including reciprocal terms and chemical potential. Λ in Eq. 7 represents the non-reciprocal contribution biasing the transition.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not define or quantify energy efficiency in a thermodynamic sense (e.g., work done / energy input). The focus is on the *effectiveness* of using the non-reciprocal energy scale λ to achieve the desired function (shape-shifting) versus undesired outcomes (chimeras, dispersion). One could *infer* a qualitative efficiency based on the parameter ranges where desired behavior occurs, but no explicit metric is provided. For instance, successful shifting occurs within specific λ ranges (Fig 3, Fig 4), suggesting optimal 'efficiency' in utilizing λ for function in those ranges. Low λ fails to induce shifts (inefficient use of λ for dynamics), high λ leads to instability (dispersion, inefficient).

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation is intrinsically linked to the non-equilibrium nature driven by non-reciprocal interactions. The paper explicitly measures entropy production (ΔΣ) as "the sum of the logarithm of the ratio of probabilities for direct and reverse moves along the path of the dynamics" (Section: Shape-shifting structure). This quantifies the degree of irreversibility and non-equilibrium activity, representing dissipation into the implicit thermal bath. Higher ΔΣ correlates with shape-shifting events (Fig 2g), indicating dissipation during these transitions. Other dissipation mechanisms (e.g., related to failed assembly attempts) are implicitly present in the stochastic dynamics but not separately quantified. Assessment: Quantified via ΔΣ (entropy production), unitless (in k<sub>B</sub>).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable/Long-term (in assembly regime)
*    Units: Simulation time steps (τ<sub>s</sub>) or potentially seconds in a physical realization.

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: m
*   Units: Number of distinct structures

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: 1 - Error (e.g., > 0.95 for successful assembly)
*   Units: Dimensionless (Overlap Fraction) or %

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Related to 1/τ<sub>shift</sub> or rate of transition to undesired states (chimera, liquid, dispersion).
    *   Units: 1 / time steps (τ<sub>s</sub><sup>-1</sup>)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Error | Fraction of incorrect/missing tiles vs target | <0.05 (Good) | Dimensionless | `MemoryNode` attribute | Methods, Fig 2g, 3, 4 | Explicit | Defines retrieval fidelity. |
    | Frequency of Shifts (f) | Fraction of runs ending in a valid sequence state | Variable (Fig 4b-j) | Dimensionless | `TransitionEdge` attribute | Fig 4 | Explicit | Measures robustness/reliability of sequential recall under non-reciprocity. |
    | Shift Yield (Correct) | Fraction of shifts successfully completed | High (e.g., >0.8 for M=1600) | Dimensionless | `TransitionEdge` attribute | Fig 4m | Explicit | Measures robustness of sequential transitions against premature/waiting shifts. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content:
        1.  **Reciprocal (Equilibrium Assembly):** Defined by the interaction matrix/potential U (Eq. 1). If a pair of adjacent tiles (A, B) with a specific orientation (□ ∈ {\, /}) exists in *any* of the target structures S(1)...S(m), their interaction energy is -ε. Otherwise, it's 0. This rule favors bonds present in the stored patterns. Mathematically: `Ur(A □ B) = -ε if A □ B ∈ ∪ᵢ Ir(S(i)), else 0`. (Methods, Eq. 1, 2). Governs assembly towards nearest energy minimum matching a stored pattern.
        2.  **Non-Reciprocal (Non-Equilibrium Shifting):** Defined by the interaction rate modifier R (Eq. 4) based on the transition sequence (e.g., S(ℓ) → S(ℓ+1)). Specific directional approaches (■ ∈ {↘, ↖, ↗, ↙}) between tiles (e.g., A approaching B which is part of the structure) are assigned a non-reciprocal bias λ if that specific approach is part of the programmed transition sequence. Otherwise, the bias is 0. Mathematically: `Rn(A ■ B) = λ if A ■ B ∈ Inr(S(ℓ)→S(ℓ+1)), else 0`. (Methods, Eq. 4, 5). This bias modifies the Monte Carlo acceptance probability (Λ term in Eq. 6, 7), driving transitions along the programmed sequence.
        3.  **Monte Carlo Moves:** Randomly select a lattice site, attempt to change tile type (add, remove, swap) with probability `p = min{1, exp(Λ - ΔH)}`, where ΔH includes reciprocal (U) and chemical potential (μ) contributions, and Λ includes non-reciprocal (R) contributions (Eq. 6, 7). This implements the kinetic pathway based on the energetic rules.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :-------------------- | :---: | :----------: | :----------------: | :------------: |
    | 1       | Reciprocal Interaction | ε              | e.g., 3.0 - 25.0      | k<sub>B</sub>T | Fig 2, 3    | Explicit          | Energy scale for bonds in target structures. |
    | 2       | Non-Reciprocal Interaction | λ              | e.g., 0 - 25        | k<sub>B</sub>T | Fig 2, 3, 4 | Explicit          | Energy scale/bias for programmed transitions. |
    | 3       | Tile Density Control | μ              | e.g., 0 - -40       | k<sub>B</sub>T | Fig 2, 3, 4 | Explicit          | Controls overall density via energy cost/gain of tiles. |

### **4.3 Global Order:**

    *   Content: The primary emergent global orders are the specific, predefined target structures S(1)...S(m), which are typically random permutations of M tiles on a √M × √M lattice (e.g., 40x40). Other possible global states also emerge depending on parameters: 'chimera' states (incorrect assemblies mixing features of multiple S(i)), 'liquid' states (disordered, dynamic configurations with few specific bonds), and 'dispersion' states (mostly empty lattice, disassembled tiles). The shape-shifting behavior is a dynamic sequence of these globally ordered structures: S(1) → S(2) → ...
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| 1       | Reciprocal Bond Formation | ε         | e.g., 3.0 - 25.0 | k<sub>B</sub>T | Explicit | Defines stability of target structures. | Methods (Eq. 1), Fig 2, 3 |
| 2       | Non-Reciprocal Transition Bias | λ         | e.g., 0 - 25    | k<sub>B</sub>T | Explicit | Drives shifts between structures. | Methods (Eq. 4), Fig 2, 3, 4 |
| 3       | Tile Addition/Removal | μ         | e.g., 0 - -40   | k<sub>B</sub>T | Explicit | Controls tile density/binding affinity. | Methods (Eq. 3), Fig 2, 3, 4 |
| 4       | Stochastic Dynamics | T (implied) | 1           | k<sub>B</sub>T | Implicit | Sets thermal energy scale for MC. | Methods (Eq. 6) |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| 1 | Structure Fidelity | Overlap (O) | 0 - 1 | Dimensionless | Explicit | Measures similarity to target structure S(i). | 1 - Error calculation | Methods, Fig 2g |
| 2 | Tile Density | ρ | ~0 - 1 | Tiles/Site | Explicit | Measures fraction of occupied sites. | Direct calculation | Fig 3e, Extended Data Figs |
| 3 | System Energy | E | Variable | k<sub>B</sub>T | Explicit | Total energy based on Hamiltonian H. | Sum over bonds/tiles (Eq. 3) | Fig 3f, Extended Data Figs |
| 4 | Non-Equilibrium Activity | Entropy Production (ΔΣ) | ≥ 0 | k<sub>B</sub> (unitless) | Explicit | Measures irreversibility/dissipation. | Sum log(P<sub>fwd</sub>/P<sub>rev</sub>) | Fig 2g |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | U → S(i) Assembly | Local reciprocal rules determining global structure S(i) | High (in assembly regime) | 8 | Overlap O (>0.95), Low Error (<0.05) | Explicit (Metrics) | Local interactions reliably produce the globally defined target structures under correct conditions. | Fig 3a, Methods |
    | R → Sequence | Local non-reciprocal rules determining global transition sequence | Medium-High (in shifter regime) | 7 | Shift Frequency f (>0.8), Shift Yield | Explicit (Metrics) | Local biases generally produce the programmed sequence, but stochasticity and parameter sensitivity introduce deviations (premature/waiting shifts). | Fig 4b-m |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** (Score reflects how well local rules faithfully determine the global outcome) 0: No relation; 3: weak correlation; 5: Significant correlation, some predictability; 7: High predictability, some deviations; 9: Near-perfect mapping; 10: Perfect mapping.
    *   **Metrics:** Overlap (O), Error (1-O), Shift Frequency (f), Shift Yield.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Neuromorphic / Analog / Other (State Machine)

### **5.3 Computational Primitive:**

    *   Content:
        1.  **Associative Memory Retrieval:** Given a partial or noisy input (seed structure), the system converges to the closest complete stored pattern S(i). This is achieved via energy minimization dynamics governed by reciprocal interactions U.
        2.  **Sequential State Transition:** Given the current state S(i), the non-reciprocal interactions R compute and drive the transition to the next predefined state S(i+1) in the sequence.
    *   **Sub-Type (if applicable):** Associative Memory Retrieval, Sequential Logic/State Transition.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Monte Carlo Step Time | τ<sub>s</sub> (1 lattice sweep) | Arbitrary / Steps | Fig 4k (caption) | Explicit | Basic unit of time in simulations. |
        | Structure Retrieval Time | τ<sub>retrieval</sub> | τ<sub>s</sub> | Fig 4k, l | Explicit | Time for seed to grow to 95% overlap. Scales approx. linearly with M. |
        | Structure Shift Time | τ<sub>shift</sub> | τ<sub>s</sub> | Fig 4k, l | Explicit | Time to transition between structures (O<sub>i</sub>=0.95 to O<sub>i+1</sub>=0.95). Scaling depends on M. |
        | Simulation Duration | up to 4 x 10<sup>6</sup>, 20 x 10<sup>6</sup> | τ<sub>s</sub> | Fig 4 (caption), Ext Data Fig 5 | Explicit | Total time simulation runs. |

### **6.2 Active Inference:**

    *   Content: Unclear/Partial
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** Information theory metrics could quantify the mismatch between the current configuration and the target structure (related to 'surprise' or 'free energy'). The rate at which this mismatch decreases during assembly (τ<sub>retrieval</sub>) could relate to inference speed. For shifting, the predictability metric (f) relates to how well the system follows the 'goal' sequence. One could potentially reformulate the dynamics in terms of minimizing a free energy functional incorporating the non-reciprocal drive, but this is not done in the paper. CT-GIN: Track information flow from `EnvironmentNode` (parameter settings, seed) to `ConfigurationNode`, quantify mismatch with `TargetStructureNode`, relate configuration changes (`ActionEdge`) to mismatch reduction rate.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content:
        1.  **Multifarious Self-Assembly:** Spontaneous formation of one specific target structure S(i) from a common pool of tiles, selected by initial conditions (e.g., seed) and governed by reciprocal interactions U.
        2.  **Shape-Shifting (Sequential Self-Organization):** Autonomous, programmed transitions through a sequence of different target structures (S(1) → S(2) → S(3)...) driven by non-reciprocal interactions R.
        3.  **Chimera Formation:** Emergence of incorrect, disordered structures containing features of multiple target patterns, resulting from competing interactions or non-optimal parameters.
        4.  **Liquid Phase:** Dynamic, disordered state with high tile mobility and few specific bonds, typically at high T (low ε) or low density (high μ).
        5.  **Dispersion Phase:** State where tiles are largely unbound and dispersed, occurring if interactions are too weak to stabilize assembly or if non-reciprocal drive destabilizes structures.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors are primarily validated through extensive Monte Carlo simulations across a wide parameter space (ε, μ, λ). Behaviors are operationally defined and distinguished using quantitative metrics:
        *   **Overlap/Error (O, 1-O):** Used to identify successful assembly vs. chimeras or other states (Methods, Fig 2g, 3a). Thresholds (e.g., O>0.95) define successful retrieval.
        *   **Density (ρ):** Helps distinguish between dense phases (assembly, chimera, liquid) and sparse phases (dispersion) (Fig 3e).
        *   **Energy (E):** Characterizes the stability and nature of the final state (Fig 3f).
        *   **Entropy Production (ΔΣ):** Quantifies non-equilibrium activity, correlating strongly with shape-shifting events (Fig 2g).
        *   **Shift Frequency (f) & Timescales (τ):** Quantify the dynamics and success rate of the shape-shifting sequence (Fig 4).
     Control experiments are implicitly performed by varying parameters (e.g., comparing λ=0 vs λ>0). Phase diagrams (Fig 3) map out the parameter regions for each behavior. Reproducibility is implied through averaging over multiple runs (e.g., 5 or 100 realizations in Fig 3, 4 captions). Limitations mainly involve finite simulation time and system size, though scaling is explored (Fig 4l,m, Ext Data Fig 5). Brownian dynamics simulations (Ext Data Fig 6) provide further validation towards physical realizability.

---

#Key: [holler_autoselective_2020]

# Autoselective transport of mammalian cells with a chemotactic droplet

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system uses a 1-decanol droplet capable of chemotaxis (directed motion) in response to a NaCl chemical gradient when surfactants are present in the aqueous phase. Mammalian cells are encapsulated within alginate hydrogel capsules. Certain living lung cancer cell lines (H460, H1299, A549), when encapsulated, produce biosurfactants. These surfactants mediate the adhesion of the hydrophilic alginate capsule to the hydrophobic decanol droplet and also enable the droplet's chemotactic movement in the salt gradient. The system's purpose is to achieve autoselective transport of living mammalian cells that produce these specific biosurfactants under the given conditions, distinguishing them from dead cells or cells that do not produce surfactants. The system integrates an artificial component (chemotactic droplet) with a biological component (cells producing surfactants). A double encapsulation method is described for cells sensitive to the process.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Values listed are representative primary parameters mentioned. Decanol volume varies between chemotaxis tests (20µl) and transport (250µl). Reliability is High as these are explicitly stated experimental parameters.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input driving the droplet motion (chemotaxis) is the chemical potential gradient established by the difference in NaCl concentration across the aqueous phase. This gradient leads to differences in interfacial tension around the droplet when surfactants are present. Secondary energy input comes from the metabolic activity of living cells producing the necessary biosurfactants, although this enables rather than directly drives the motion.

### **2.2 Energy Transduction**

    *   Content: 1. Chemical potential energy (NaCl gradient) is transduced into interfacial energy differences across the droplet surface via the interaction of NaCl with the surfactant monolayer. 2. Interfacial energy differences generate fluid motion (Marangoni flow) within and around the droplet. 3. Fluid motion results in the net movement (kinetic energy) of the droplet, directed along the gradient (chemotaxis). 4. The moving droplet exerts a mechanical force on the adhered capsule, transducing droplet kinetic energy into capsule kinetic energy for transport.

### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**

    *   Content: The primary dissipation mechanism is viscous drag as the droplet and capsule move through the aqueous medium. Heat is also generated due to fluid motion (viscous heating) and potentially minor chemical reactions at the interface, although these are likely negligible. Energy is also dissipated in the continuous rearrangement of the surfactant monolayer. Quantification is not provided. Qualitative Assessment: High (dominated by viscous drag).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: 1. **Surfactant Adsorption:** Surfactant molecules (biosurfactants from cells or decanoate) adsorb at the decanol-water interface, forming a monolayer and lowering interfacial tension. 2. **Gradient Sensing:** The local concentration of NaCl affects the behavior/arrangement of the surfactant molecules at the interface, leading to local variations in interfacial tension (γ). Higher NaCl concentration generally leads to higher interfacial tension in this system (inferred from droplet moving towards higher salt). 3. **Marangoni Flow:** Gradients in interfacial tension (∇γ) along the droplet surface drive fluid flow from regions of low γ to high γ. ∇γ ≠ 0 → Fluid Flow. 4. **Adhesion Rule:** Sufficient concentration of biosurfactant modifies the alginate capsule surface (making it less hydrophilic/more compatible with decanol) enabling adhesion between the capsule and the droplet surface, likely via hydrophobic interactions.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :------------------: | :---: | :----------: | :----------------: | :------------: |
    | 2, 3    | Gradient Sensing / Marangoni Flow | Surface Tension Change (Δγ) upon NaCl addition | ~2.8 (Live Cells Supernatant), ~1.1 (Dead Cells Supernatant) | mN/m | Fig 5 | Explicit | Values explicitly measured and reported for different conditions. |
    | 4       | Adhesion Rule | Capsule-Droplet Association Time | ~1.5 (Live Cells Sup.), < few min (Controls) | hours | Fig 2 | Explicit | Values explicitly measured and reported for different conditions. |
    | 1, 2, 3 | Surfactant Conc. Effect | Surface Tension (γ) | ~4.5 (Live Cells Sup.), ~7.3 (DMEM only) | mN/m | Fig 4 | Explicit | Values explicitly measured and reported for different conditions. |

### **4.3 Global Order:**

    *   Content: The emergent global order is the directed, persistent motion (chemotaxis) of the decanol droplet, potentially carrying the adhered alginate capsule, along the macroscopic NaCl concentration gradient towards the source.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Explicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| 1 | Surfactant Adsorption | Surface Tension (γ) | 4 - 8 | mN/m | Explicit | γ depends on surfactant presence/type. | Fig 4 |
| 2 | Gradient Sensing (Salt effect) | Surface Tension Change (Δγ) | 1 - 4 | mN/m | Explicit | Δγ depends on surfactant type and viability. | Fig 5 |
| 3 | Marangoni Flow | Droplet Velocity | ~0.01 - 0.06 | cm/s | Explicit/Implicit | Velocity depends on Δγ (Suppl Fig S4); 0.0131 cm/s explicitly stated for DMEM only (low Δγ), higher velocity implied for high Δγ (~5x faster inferred from text comparing to decanoate). | Results, Suppl Fig S4 |
| 4 | Adhesion | Association Time | minutes - ~1.5 hours | time | Explicit | Time depends on surfactant source/concentration. | Fig 2 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| GO1         | Chemotactic Motion | Transport Success Rate | 0 or 1 (0% or 100%) | ratio | Explicit | Depends on cell viability/surfactant prod. | Transport Assay | Results |
| GO2         | Chemotactic Motion | Mean Path Length (% of total dist.) | ~56 (DMEM only), ~100 (Live Cells Sup.) | % | Explicit/Implicit | 56% stated for DMEM, 100% implied for successful transport (Suppl Fig S4 correlates Δγ to "better chemotactic performance"). | Transport Assay | Results, Suppl Fig S4 |
| GO3         | Chemotactic Motion | Mean Velocity | ~0.013 (DMEM only), Higher (Live Cells Sup.) | cm/s | Explicit/Implicit | 0.0131 cm/s stated for DMEM. Higher implied by Suppl Fig S4 and comparison to decanoate system. | Droplet Motion Tracking | Results, Suppl Fig S4 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :-------------: | :-----------: | :------: | :----------------: | :------------: | :-----: |
    | Local-Global | Mapping local surfactant-salt interactions (Δγ) to global chemotactic velocity/success | High | 8 | Transport Success Rate (%), Velocity (cm/s), Δγ (mN/m) | Implicit | Predictability is high (10/10 trials work/fail appropriately). Yoneda score inferred high because global behavior strongly and predictably depends on local interface physics (Δγ). Lack of explicit Yoneda analysis. | Results, Fig 5, Suppl Fig S4 |
    | Local-Global | Mapping local surfactant-capsule adhesion (Assoc. Time) to global transport capability | High | 8 | Association Time (hrs), Transport Success Rate (%) | Implicit | Predictability is high (long assoc. time correlates with transport). Yoneda score inferred high because global behavior (transport) strongly depends on local adhesion. Lack of explicit Yoneda analysis. | Results, Fig 2 |

        * Rubric: 0 = No clear link between local interactions and global behavior. 5 = Qualitative correlation exists. 8 = Quantitative correlation demonstrated, predictable outcome. 10 = Formal proof or rigorous demonstration of local rules uniquely determining global emergent behavior across various conditions.
    *   **Metrics:** Transport Success Rate, Chemotactic Velocity, Surface Tension Change (Δγ), Capsule-Droplet Association Time.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes (primitive)

**(Conditional: M5.1 is "Yes", including M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog

### **5.3 Computational Primitive:**

    *   Content: Conditional Selection / Filtering / Thresholding. The system selects capsules for transport based on whether the encapsulated cells produce sufficient biosurfactants (above a certain threshold) under the assay conditions. This acts as a filter, passing "live/producing" cells and rejecting "dead/non-producing" cells or empty capsules. Mathematically: Output = Transport *if* [Surfactant] > Threshold, else Output = No Transport.
    *   **Sub-Type (if applicable):** Thresholding / Chemical Gate

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :---------------: | :---------------: | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Cell Incubation (Surfactant Prod.) | 2 | days | Methods | Explicit | Duration required for cells in capsules to produce sufficient surfactant impacting supernatant. |
        | Capsule-Droplet Association Time | ~1.5 (max measured) | hours | Fig 2 | Explicit | Time capsule remains adhered in bi-phase test, reflects stability needed for transport. |
        | Chemotactic Transport | Minutes to Hours? | time | Suppl. Movies S1, S2 | Implicit | Exact duration not stated, depends on distance/velocity. Movies show movement over minutes. |
        | Cell Viability Assay (MTT Incubation) | 3 | hours | Methods | Explicit | Incubation time for the metabolic assay. |
        | Cell Proliferation Check | 2-3 | weeks | Results | Explicit | Timeframe observed for cells to reach confluence after transport. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behavior is the **autoselective chemotactic transport** of alginate capsules containing living mammalian cells. This selection is based on the ability of specific living cell lines (e.g., certain lung cancers), when encapsulated, to produce biosurfactants that enable both adhesion to the decanol droplet and the droplet's subsequent chemotactic movement in a salt gradient. The system effectively distinguishes and transports viable, surfactant-producing cells while leaving behind dead cells or empty capsules.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation relies on: 1. **Direct Observation:** Transport/non-transport visually observed and recorded (Suppl. Movies S1, S2). 2. **Reproducibility:** Consistent outcomes reported over multiple trials (10/10 success/failure rates for specific conditions, Results). 3. **Control Experiments:** Systematic comparison with negative controls (dead bleach-treated cells, empty capsules, non-producing cell lines, DMEM only) confirmed selectivity (Results, Fig 2, Fig 5). 4. **Mechanism Investigation:** Surface tension measurements (Figs 4, 5, Suppl Fig S3) and bi-phase association tests (Fig 2) link the behavior to surfactant presence and interface properties. 5. **Viability Analysis:** TB, MTT assays, and proliferation checks confirm cargo viability post-transport (Fig 3, Table 1). Limitations include testing only a few cell lines and specific environmental conditions.

---

#Key: [friston_free-energy_2010]

# The free-energy principle: a unified brain theory?

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is the Free-Energy Principle (FEP), a theoretical framework proposed to explain brain function (action, perception, learning) in adaptive systems. It posits that any self-organizing system at equilibrium with its environment must minimize its variational free energy, an information-theoretic quantity that serves as an upper bound on surprise (negative log-evidence). Components include: the agent (brain), the environment, sensory states, internal states (representing a recognition density, q(ϑ|μ)), action, and an implicit generative model (p(˜s,ϑ|m)) representing the agent's model of how sensory data are caused. The purpose is to maintain the system's states within physiological bounds (homeostasis) by minimizing the long-term average of surprise through minimizing free energy via changes in action (sampling expected sensations) and perception (updating the recognition density/internal states to better predict sensations). It aims to unify various brain theories (e.g., Bayesian brain, predictive coding, optimal control) under a single optimization principle.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
    *   **Note:** The paper describes a theoretical principle. Key "parameters" are abstract mathematical constructs (functions, densities) rather than specific numerical values. Their specific form depends on the particular application/model being considered (e.g., the bird song model in Fig 1 has specific parameters).

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary "input" in the context of the FEP as discussed is sensory information/data (˜s(t)) from the environment. While biological systems require metabolic energy, the FEP focuses on information-theoretic quantities. The paper links FEP to resisting disorder (thermodynamics), but doesn't quantify metabolic energy input required for the computations.

### **2.2 Energy Transduction**

    *   Content: The paper focuses on *information* processing, not physical energy transduction in the thermodynamic sense. Sensory information (input) is "transduced" into updates of internal states (μ) representing the recognition density (q(ϑ|μ)). This occurs through minimizing free energy, typically via gradient descent dynamics (Box 2 equations). This minimization process involves comparing predictions (derived from the generative model and current internal states) with sensory input to calculate prediction errors (ξ), which then drive changes in internal states (perception) and action (a). This information flow aims to make the internal representation a better model of the environmental causes (ϑ) of sensation.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper discusses minimizing "complexity" (a component of free energy related to the divergence between recognition and prior densities) which relates to efficient coding and parsimonious representations (Infomax section). This implies an optimization towards *informational* efficiency. However, it doesn't quantify thermodynamic/metabolic energy efficiency. The principle itself aims to find the most efficient representation/action strategy given the generative model. Qualitatively, efficiency is high in an information-theoretic sense (minimizing surprise/prediction error using the least complex representation), but metabolic cost is not addressed.

### **2.4 Energy Dissipation**

    *   Content: In the FEP context, "dissipation" isn't primarily thermodynamic heat loss. The analogous concept is the minimization/reduction of "surprise" or "prediction error". Free energy itself represents the upper bound on surprise, and the system acts and perceives to reduce this quantity. The gradient descent dynamics (Box 2) effectively "dissipate" prediction error until expectations align with sensations. The paper mentions resisting the tendency to disorder (linking to the 2nd law of thermodynamics and fluctuation theorem) but frames the core mechanism as information-theoretic optimization, not physical energy dissipation pathways.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable (Short-term to Long-term)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | KL Divergence | Difference between recognition density and true posterior | Minimized | Dimensionless (nats/bits) | `KL_Divergence(q||p_posterior)` | Box 1b | Explicit | Represents inference fidelity; minimizing F minimizes KL divergence. |
    | Precision (γ) | Inverse variance of beliefs/prediction errors | Optimized | Inverse Variance Units | `PrecisionNode (μγ)` | Box 1a, Box 2 | Explicit | Represents confidence/reliability of encoded information/memory. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rules described are those of the neuronal message-passing scheme proposed for implementing FEP, specifically predictive coding (Box 2). These involve:
        1.  **Prediction Error Calculation:** Error units (ξ) compute the difference between the current expectation/state (μ) (or its predicted consequence via the generative model functions g, f) and incoming signals (sensory input at the lowest level, or prediction errors from the level below, or top-down predictions). This difference is weighted by precision (Π). E.g., ξ(i)v = Π(i)v(μ(i-1) - g(μ(i))).
        2.  **Expectation Update (Recognition Dynamics):** State units (μ) update their activity based on a gradient descent on free energy. This integrates prediction errors from the same level (ξ(i)) and the level below (ξ(i+1)), effectively adjusting expectations to reduce errors. E.g., dμ(i)v/dt = Dμ(i)v - (∂vε(i))^T ξ(i) - ξ(i+1)v. (Where ε represents prediction components based on μ, and Dμ is a temporal derivative operator).
        3.  **Plasticity (Parameter Update):** Synaptic efficacies (parameters θ of the generative model) are updated to minimize free energy, which is shown to be formally equivalent to Hebbian/associative learning. E.g., Δμθij = -∂θij ε^T ξ (change in efficacy depends on correlation between prediction error and prediction influence).
        4.  **Precision Update:** Synaptic gain/precision (parameters γ controlling Π) is updated based on the statistics of prediction errors, related to attention/neuromodulation. E.g., Δμγi = ½tr(∂γi Π(ξξ^T - Π(μγ))).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | 1 | Prediction Error | Precision (Π or γ) | > 0 | Inverse Variance | Box 1, Box 2 | Explicit | Precision weights prediction errors. |

### **4.3 Global Order:**

    *   Content: The global order that emerges is the maintenance of the agent's characteristic physiological and sensory states within a limited repertoire (phenotype), ensuring low entropy and resisting a tendency to disorder. This corresponds to homeostasis and the system consistently occupying a small set of "unsurprising" or "valuable" states defined by its generative model and priors. At the neural level, this could manifest as stable patterns of activity, coherent representations, and adaptive behavioral sequences.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Pred Err Calc | Weighting error signal | Precision (Π or γ) | > 0 | Inverse Variance | Explicit | Weights influence of error signals | Box 1, Box 2 |
| Gen Model | Structure of predictions | Model Parameters (θ) | Model Specific | Model Specific | Explicit | Define the learned causal structure | Box 2 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Homeostasis | Maintenance of phenotype | Sensory State Entropy (H) | Low | Dimensionless (nats/bits) | Explicit | FEP minimizes surprise, implying low entropy | Minimize F | Main Text |
| Inference | Accuracy of internal representation | Free Energy (F) | Minimized | Dimensionless (nats/bits) | Explicit | F serves as bound on surprise/model evidence | Minimize F | Box 1, Main Text |
| Behavior | Consistency with predictions | Prediction Error (ξ) | Minimized | Depends on variable | Explicit | Action/perception minimize prediction error | Minimize F | Box 1, Box 2 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    *   **Metrics:** Free Energy, Entropy, Prediction Error Magnitude, KL Divergence.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog / Neuromorphic / Probabilistic Inference (Approximation)

### **5.3 Computational Primitive:**

    *   Content: Prediction Error Calculation and Weighted Summation/Integration (Gradient Calculation). The most basic operations described in the predictive coding implementation (Box 2) are:
        1.  Calculating the difference between expectations/predictions (derived from μ, g, f) and incoming signals (inputs or errors from other levels), weighted by precision (Π). (Prediction Error Calculation: ξ = Π * Error).
        2.  Integrating these weighted prediction errors over time to update the internal states (μ) according to gradient descent dynamics. (Weighted Summation/Integration: dμ/dt ∝ Σ ξ_contributions).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|
* **Note:** The paper describes theoretical units; specific quantitative metrics like processing power or energy cost are not provided. Representation is typically assumed to be analog/continuous.

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Perception/Inference | Milliseconds to Seconds | ms - s | Fig 1c | Explicit | Time to infer causes from sensory stream (e.g., ~600ms in Fig 1c). |
        | Action/Motor Control | Milliseconds to Seconds | ms - s | Fig 2, Fig 3 | Explicit | Timescale of reaching movements or car dynamics. |
        | Learning/Plasticity | Seconds to Lifelong | s - years | Main Text (Learning section) | Explicit | Parameter updates (synaptic efficacy) occur over various timescales based on experience. |
        | Neuronal Dynamics | Milliseconds | ms | Box 2 (Implicit) | Implicit | Gradient descent equations imply continuous dynamics, typical neuronal timescales are ms. |
    *   **Note:** Specific values are often dependent on the example system (e.g., birdsong, motor control).

### **6.2 Active Inference:**

    *   Content: Yes
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** Prediction error reduction rate over time due to action; Correlation between intended (prior expectation based) state trajectory and actual state trajectory achieved through action; KL divergence between policies based on priors and executed actions; Time constant of state convergence to attractor states specified by priors (e.g., Fig 2, Fig 3); Bayesian surprise associated with action outcomes.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The primary mechanism described is gradient descent on free energy with respect to the parameters (θ) of the generative model. This is presented as formally equivalent to Hebbian or associative plasticity ("cells that fire together wire together"). Specifically (from Box 2 and Cell Assembly section), the change in synaptic efficacy (Δμθij) is proportional to the correlation between the presynaptic prediction (influence of state μj on prediction ε) and the postsynaptic prediction error (ξ). When correlated, the connection strength increases, enabling predictions to suppress errors more efficiently. Hierarchical models allow for the optimization of empirical priors based on sensory data. This mechanism allows the system to learn the causal structure of its environment and adapt its internal model to better predict and respond to sensory inputs. Optimization of precision (μγ) is also a form of adaptation, related to attentional mechanisms and learning the reliability of signals.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors emerging from the FEP are:
        1.  **Perception:** Inferring the probable causes (ϑ) of sensory signals (˜s) by optimizing internal states (μ) to minimize prediction error (e.g., perceptual categorization in Fig 1).
        2.  **Action:** Selecting motor commands (a) to sample sensory inputs that are consistent with predictions or prior expectations, thereby minimizing prediction error or fulfilling goals encoded as priors (e.g., active inference, motor control in Fig 2, exploration/exploitation in Fig 3).
        3.  **Learning/Adaptation:** Modifying the internal generative model (parameters θ, precision γ) based on experience to improve future predictions and behavior.
        4.  **Homeostasis/Self-Preservation:** Maintaining physiological states within viable bounds by avoiding surprising (improbable, high free-energy) sensory encounters.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper validates claims of emergent behavior primarily through:
        1.  **Conceptual Unification:** Demonstrating how FEP subsumes or relates to other established theories (Bayesian Brain, Predictive Coding, Optimal Control, etc.), suggesting it captures similar phenomena.
        2.  **Simulations:** Presenting results from computational simulations based on FEP, showing that minimizing free energy can reproduce specific behaviors like perceptual categorization (Fig 1), motor control/reaching (Fig 2), and solving control problems like the mountain car (Fig 3). These simulations serve as existence proofs for the behaviors emerging from the principle under specific generative models and priors.
        Control experiments or quantitative analysis of emergence (beyond showing the behavior occurs) are not detailed in this review. Limitations include the reliance on specific model choices for simulations.

---

#Key: [goettems_physics_2024]

# On the physics of dissipative systems: classical dynamics and quantum dissipative adaptation

**Paper Type:** Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The work investigates dissipative systems in classical and quantum regimes. Classically, it analyzes two Brownian particles in a common bath using system-reservoir approaches (bilinear and nonlinear coupling), proposing a modified spectral function dependent on inter-particle distance to reconcile the models and address issues like anomalous diffusion and hydrodynamic interactions. Quantum mechanically, it explores the dissipative adaptation hypothesis using a time-dependent driven spin-boson model (a particle in a metastable double-well potential coupled to a bosonic bath) via path integrals, aiming to relate thermodynamic quantities (work absorption, heat dissipation) to non-equilibrium self-organization phenomena like particle localization in an unstable well. The components are the system (Brownian particles or TLS/spin) and the environment (harmonic oscillator bath). The purpose is to understand the physics of dissipation, reconcile different theoretical models, and explore the thermodynamic underpinnings of self-organization in driven quantum systems.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters are central to defining the models and their dynamics as described in the provided text. Values are symbolic or taken from figure captions representing specific simulations discussed.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: For the classical Brownian motion, the energy input is implicitly thermal energy from the bath via random fluctuations (f(t) in Eq. 1.1, F(t) in Eq. 2.17). For the quantum dissipative adaptation part, the energy input is explicitly work absorbed from an external drive (e.g., time-dependent potential/bias ϵ(t) in Eq. 6.2, single-photon pulse in Fig 8).
    *   Units: Energy (e.g., Joules) or Frequency (if related to drive)

### **2.2 Energy Transduction**

    *   Content: Energy from the environment (thermal bath or external drive) is transferred to the system (Brownian particles or spin). In the dissipative process, this energy is then transferred from the system back to the environment (bath), manifesting as heat dissipation. Mechanisms include collisions (classical), system-bath coupling (e.g., bilinear Ckx or exponential e^ikx in Lagrangians, spin-position coupling q₀σzΣCk xk in Hamiltonian Eq. 5.44), and interaction with the drive field (e.g., coupling to ϵ(t)σz in Eq. 6.2). Energy is stored internally in the system's potential V(x) or Hamiltonian HS(t).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper focuses on the fundamental physics of dissipation and thermodynamics (work, heat, entropy production, adaptation), not on the efficiency of a specific energy conversion task. Efficiency metrics are not defined or evaluated. The concept of "dissipative adaptation" relates work absorption and dissipation to self-organization, but not in terms of a task efficiency.

### **2.4 Energy Dissipation**

    *   Content: Dissipation is central to the thesis. It occurs via the coupling between the system (particles/spin) and the environment (bath oscillators). Mechanisms include frictional/damping forces proportional to velocity (ηq̇ in Eq. 1.1, ηeff q̇ in Eq. 2.43) in the classical case, arising from integrating out bath degrees of freedom. In the quantum case, dissipation arises from the system-bath interaction term in the Hamiltonian (e.g., Eq 1.4, 5.44), described via the influence functional (Eq. 3.4, F.1) containing dissipation kernel αR (Eq. 3.13) or Q''(t) (Eq. 6.66) related to the spectral density J(ω) (Eq. 2.6, 6.13) and coupling strength α (Eq. 6.14). Dissipation manifests as heat released into the bath (∆Q in Eq. 4.2, Q_diss in Eq. 4.14, ∂Q in Eq. 4.10). Quantification depends on parameters like η or α, temperature T, and the spectral function J(ω). Qualitatively, it's the primary mechanism for energy loss from the system to the environment.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*    Units: time (e.g., seconds)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Units: 1/time (e.g., s⁻¹)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes (context-dependent)

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local interactions governing the potential self-organization are: 1) System-Bath coupling: Defined by terms like Ck[x + x ]Rk (bilinear, Eq. 2.1) or κke^ikxi Rk (nonlinear, Eq. 2.14) or (q₀σz/2)ΣCk xk (spin-boson, Eq. 5.44). These mediate energy/momentum exchange leading to dissipation and fluctuations. 2) System-Drive coupling: Defined by terms like ℏϵ(t)σz/2 (Eq. 6.2), representing the interaction of the system with the time-dependent external field providing energy. 3) (Potentially) Inter-particle interactions mediated by the bath: Arising from the shared environment, leading to effective forces or correlated noise (e.g., ηe[u] terms in Eq. 2.21, V'(u) in Eq. 2.21).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Sys-Bath | Dimensionless Dissipation Strength | α | 0.21, 0.8 (examples) | dimensionless | Eq 6.14, Fig 16 | Explicit | Parameter controlling overall dissipation. |
    | Sys-Drive | Drive Amplitude | ϵd | (ϵd/∆)² ≈ 12 (example) | frequency or energy | Eq 6.63, Fig 16 | Explicit | Amplitude of external driving field. |
    | Sys-Drive | Drive Frequency | ωd | 9 GHz, 3 GHz (examples) | frequency | Eq 6.63, Fig 16 | Explicit | Frequency of external driving field. |

### **4.3 Global Order:**

    *   Content: The potential emergent global order discussed is primarily the localization of the quantum system (spin/particle) in a specific state, potentially a non-equilibrium one (e.g., the unstable well in the driven spin-boson model, Sec 6.5, or state |b> in Fig 8). This represents an organized state preferred due to dissipative adaptation principles, contrasting with thermal equilibrium distributions. For the classical particles, bath-mediated interactions (V_e(u)) could potentially lead to structured spatial arrangements, though this is less emphasized than the dynamic effects.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Localization | Population difference or occupation probability | P(t) = <σz(t)>, P(∞) | [-1, 1] | dimensionless | Explicit | Measures the degree of localization in one state (|L> vs |R>). Steady state value P₀ is key order parameter. | Sec 5.1, 6.5, Fig 12, Fig 16 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Bath Correlation Time | ~Ω⁻¹, ~ωc⁻¹ | time | Eq 2.6, 6.13 | Implicit | Physical interpretation of cutoff frequency; defines memory timescale. |
        | System Oscillation Period (TLS) | ~1/∆, ~1/E | time | Eq 5.3, 5.5, 5.9 | Explicit | Intrinsic timescale of coherent oscillations (Rabi). |
        | Drive Period | 1/ωd | time | Eq 6.63 | Explicit | Timescale of the external driving force. |
        | Observation Time | t, T | time | General | Explicit | Duration over which the system dynamics are observed or calculated. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes (in the specific context of Dissipative Adaptation)

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism is "Dissipative Adaptation". The system evolves towards configurations (microstates or distributions over microstates) that have a history of effectively absorbing work from the external drive and dissipating it as heat into the environment. This is described by Eq. 4.5 (classical) and conceptually extended to quantum systems (Sec 4.4, Eq. 4.12). The change is driven by the interplay between the external drive (performing work W) and the system-bath coupling (leading to heat dissipation Q and internal energy change ∆E). States associated with higher, reliable work absorption and dissipation become statistically favored over time. It's not described via standard learning rules but as a thermodynamic selection principle based on interaction history.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: Key behaviors studied are: 1) Classical: Modified Brownian motion dynamics, including potentially anomalous diffusion, environment-induced interactions (effective potentials V_e, distance-dependent dissipation η_e[u]), and hydrodynamic effects. 2) Quantum: Tunneling dynamics in a double-well potential (coherent oscillations, incoherent relaxation, localization), response to external driving fields, and potential non-equilibrium self-organization (localization in unstable states) driven by dissipative adaptation.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The "emergent" behaviors (like non-equilibrium localization via dissipative adaptation) are validated theoretically through derivations (e.g., path integral calculations leading to Eq. 6.21, 6.46) and potentially confirmed/explored via numerical simulations (implied by references to plots like Fig 16 from external papers like Ref 121). The validation is within the theoretical/computational model framework. The text mentions reconciling models with physical expectations (e.g., avoiding unphysical results in bilinear coupling, Sec 2.1), suggesting a form of conceptual validation against known physics. Experimental validation is sometimes referenced via citations (e.g., Ref 121) but not performed within this thesis itself.

---

#Key: [li_computational_2024]

# Computational morphogenesis for liquid crystal elastomer metamaterial

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a computational inverse design framework using topology optimization integrated with a nonlinear Liquid Crystal Elastomer (LCE) model. Its purpose is to discover and design LCE metamaterials with complex, programmable, temperature-activated and interactive nonlinear behaviors. The components are LCE materials with four different director orientations (0°, 45°, 90°, 135°), a passive neo-Hookean elastomer, a finite element method (FEM) solver for the nonlinear LCE model under periodic boundary conditions, and a topology optimization algorithm (Method of Moving Asymptotes - MMA). The system designs periodic unit cells of metamaterials to achieve specific functionalities upon temperature change: maximized area expansion/contraction, programmable opening size change while retaining overall size, temperature-switchable stress-strain relations, and temperature-switchable deformation modes.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters listed are key material properties for the LCE model and a fundamental design parameter (unit cell size). Order parameters Q0 and Q represent the initial (low temp) and final (high temp) states used in several examples, defining the thermal stimulus. Values for passive material modulus vary between examples and are mentioned in the text.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input driving the spontaneous deformation is thermal energy, causing a change in temperature. This change is modeled via the change in the LCE order parameter Q (from Q0 to Q). Mechanical energy is also input in some examples (Sections: Thermally switchable nonlinear stress–strain relations, Thermally switchable deformation mode) via applied strain/stress for testing mechanical properties at different temperatures.

### **2.2 Energy Transduction**

    *   Content: Thermal energy input leads to a change in the LCE molecular order (nematic-isotropic transition, change from Q0 to Q). This change in molecular order alters the material's stored energy function (Eq. 1) and induces a spontaneous mechanical deformation (anisotropic contraction locally). Within the designed metamaterial structure, these local LCE contractions are transduced via the geometry and passive material components into complex macroscopic deformations (e.g., area expansion, opening size change, shape change). In cases with mechanical loading, stored elastic energy is involved, and its relation to applied work is modified by the temperature-induced pre-strain/pre-stress from the LCE components (leading to switchable stress-strain curves and deformation modes like buckling).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify the efficiency of converting thermal energy input into mechanical work output or programmed deformation. The focus is on achieving specific kinematic or mechanical response targets, not energy efficiency. Assessing efficiency would require quantifying heat input, work output, and losses, which are not discussed.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation mechanisms are not explicitly discussed or quantified. Potential mechanisms in a real LCE system would include viscoelastic losses during deformation and heat dissipation to the environment during temperature changes. The computational model uses a hyperelastic stored energy function (Eq 1) which implies reversible, non-dissipative mechanical behavior, although the underlying phase transition involves entropy changes (related to heat). Buckling, mentioned in the switchable behavior sections, is path-dependent and can involve rapid energy release, but dissipation associated with it isn't quantified.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: 0 (within the model)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog

### **5.3 Computational Primitive:**

    *   Content: Complex Nonlinear Function Mapping. The material structure embodies a complex, highly nonlinear function `f` that maps input state variables (Order Parameter Q, Average Deformation Gradient F_ave) to output state variables (e.g., Average Stress P_ave, Opening Area A_H, Deformation Mode). The specific function `f` is determined by the optimized topology (geometry and material distribution) and the constitutive laws (Eq. 1, passive material model). For example: `(A_H / A_H0) = f_opening(Q)` or `P_ave = f_stress(F_ave, Q)`. Buckling involves thresholding/bifurcation behavior.
    *   **Sub-Type (if applicable):** Nonlinear transduction, potentially involving thresholding (buckling).

### **5.4 Embodied Computational Units**
    | Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
    *   **Note:** The analysis presented is quasi-static, focusing on equilibrium states under different temperatures or loads. Dynamic timescales are not considered.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors are complex, programmed thermomechanical responses at the macroscale, emerging from the optimized microstructural design of the unit cell. Specific behaviors include:
        1.  Maximized temperature-induced spontaneous area expansion (e.g., factors of 3.38, 4.39).
        2.  Precisely programmed area change of an internal opening upon temperature rise while maintaining overall unit cell area (e.g., target ratios from 0.33 to 2.5 achieved with <6% error).
        3.  Temperature-switchable nonlinear stress-strain relations (e.g., linear response at low T, bi-linear plateau or different stiffness plateau at high T).
        4.  Temperature-switchable deformation modes under mechanical load (e.g., lateral expansion at low T, lateral contraction at high T via buckling mechanism).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation is performed computationally via nonlinear Finite Element Analysis (FEA) simulating the behavior of the optimized unit cells under periodic boundary conditions. The predicted behaviors (deformed shapes, area changes, stress-strain curves, lateral strains) are quantitatively compared against the target objectives (e.g., Figs 2c,d, 3b,c,d, 4a,d,e, 5a,b). Control examples (heuristic design Fig 2b, design without area constraint Fig 3b right) are used for comparison. Reproducibility is implied by the deterministic nature of the computational model. Limitations: Validation is purely computational; experimental validation is acknowledged as challenging and necessary future work, especially for buckling-sensitive behaviors.

---

#Key: [pfeifer_self-organization_2007]

# Self-Organization, Embodiment, and Biologically Inspired Robotics

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews the concepts of self-organization and embodiment in the context of biologically inspired robotics. It argues that these principles, derived from biology, are crucial for designing autonomous robots capable of operating in complex, uncertain environments. The "system" described is not a single robot or material but rather a collection of design principles and examples demonstrating how the dynamic coupling between control (brain), morphology (body), and environment, along with material properties, can lead to adaptive, robust, versatile, and agile behaviors like locomotion, navigation, manipulation, and learning. Examples include robots inspired by insects, fish, salamanders, geckos, and humans. The purpose is to highlight principles for designing robots that partially outsource control to physical dynamics and material properties, achieving complex behaviors through emergence and self-organization rather than solely relying on complex hierarchical control.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** The listed "parameters" are key principles central to the review. Specific quantitative values for individual robot examples are generally not provided in this review excerpt.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Varies depending on the specific robot example. Common sources include electrical energy (for motors, controllers) and pneumatic pressure (for air muscles). Some examples (passive dynamic walkers) utilize gravitational potential energy. The ultralight ornithopter (Fig. 2E) requires external power.

### **2.2 Energy Transduction**

    *   Content: Energy is transduced via actuators (e.g., electrical motors converting electrical energy to kinetic energy via torques; pneumatic actuators converting pressure potential energy into linear/compliant force and motion), interaction with the environment (e.g., gravitational potential energy converted to kinetic energy in passive walkers), or biological processes (in the context of inspiration, e.g., neural signals to muscle contraction). Material properties mediate transduction (e.g., compliant materials storing and releasing elastic potential energy).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper explicitly mentions that many humanoid robots are still "energetically inefficient" (Introduction). It suggests that exploiting passive dynamics and material properties (embodiment) can lead to improved energy efficiency compared to purely control-heavy approaches (Introduction, Section "Implications of Embodiment"). However, no general quantitative efficiency metrics are provided for the reviewed systems. The low score reflects the stated inefficiency challenge in the field, despite the potential benefits of embodiment. Qualitative Assessment: Generally Low to Medium (with potential for High in optimized passive/compliant systems).

### **2.4 Energy Dissipation**

    *   Content: Dissipation mechanisms are implied through physical interactions inherent in robotics but not explicitly quantified or detailed. Examples include friction (in joints, between robot and environment like ground/water/air), heat loss (in motors, electronics), material damping (in compliant elements or spring-damper systems), and inelastic collisions. Passive dynamic walkers inherently rely on controlled energy dissipation through collisions/friction to achieve stable gaits. Damped oscillations are mentioned regarding material properties (Fig. 1 caption). Assessment: Medium to High (inherent in physical systems).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content:
        1.  **Physical Constraints/Dynamics (Passive Walkers):** Interactions governed by gravity, friction, inertia, and morphology (limb lengths, mass distribution, foot shape). E.g., The pendulum-like swing of a leg, impact dynamics at foot contact (Refs 2, Fig 2D).
        2.  **Mechanical Coupling (Insect Locomotion):** Physical forces transmitted through the body. Pushing back with one leg mechanically influences the joints/proprioceptors of other legs on the ground (Ref 11, Section "Implications of Embodiment").
        3.  **Material Compliance (General):** Elastic properties of materials (e.g., pneumatic actuators, spring-damper systems, flexible fins/wings) lead to passive reactions/adaptations to forces/perturbations (Refs 3, 10, 14, 17, Fig 1 caption, Fig 2C, 2E).
        4.  **Hydro/Aerodynamics (Swimming/Flying):** Interaction between body/fins/wings and the fluid medium, governed by fluid dynamics principles (Refs 3, 17, 37).
        5.  **Neural Oscillators (Locomotion Control):** Coupled oscillators in spinal cord models generate rhythmic patterns transmitted to actuators (Refs 18, 19, 36).
        6.  **Local Module Interactions (Modular/Collective):** Physical connections (hooks, Velcro) and potentially local communication/sensing leading to global structure or movement (Refs 7, 9, 48, Fig 2G, 2H).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**

    *   Content: The emergent global order is primarily functional behavior, such as:
        1.  **Stable Locomotion:** Rhythmic gaits (walking, running, swimming, flying) emerging from dynamic interactions (Refs 2, 10, 13, 17, 18, 19, 22, Fig 2A, C, D, E).
        2.  **Self-Stabilization:** Robustness to perturbations, maintaining stable locomotion despite disturbances (Refs 3, 10, 14, 22, 35, Fig 3).
        3.  **Coordinated Movement:** Synchronization of limbs or body segments (Refs 11, 18).
        4.  **Morphological Structures:** Specific shapes formed by modular robots (Ref 7, 48, Fig 2G).
        5.  **Collective Motion:** Coordinated movement of multiple units (Ref 9, Fig 2H).
        6.  **Information Structuring:** Spatiotemporal patterns in sensory input induced by sensory-motor loops (Ref 4, Fig 4).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Speed  | Hexapod locomotion | Speed | >4 | body lengths/s | Explicit | Value explicitly stated | Experiment (Ref 10) | Fig 2C caption |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Neuromorphic/Analog/Hybrid

### **5.3 Computational Primitive:**

    *   Content: Based on the examples:
        *   **Neural Signal Processing:** Emulation of neural functions like place fields, head-direction cells, sensory processing (e.g., phonotaxis), motor command generation (CPGs). (Refs 18, 32, 33, 36)
        *   **Control Algorithms:** Implicitly, feedback control, potentially optimization/learning algorithms (e.g., reinforcement learning for walking). (Refs 2, 22)
        *   **Physical Dynamics:** Exploitation of pendulum dynamics, spring-mass dynamics, material compliance for stabilization or movement generation ("intelligence by mechanics"). This is computation embodied in physics. (Refs 2, 10, 14, 17, 35)
        *   **Information Filtering/Structuring:** Sensory-motor coordination actively structures sensory input, simplifying processing. (Ref 4, Fig 4)
    *   **Sub-Type (if applicable):** Neural Processing: Pattern Generation (CPG); Control: Reinforcement Learning; Physical Dynamics: Passive Stabilization; Info Structuring: Entropy Reduction.

### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Adaptation/Learning (e.g., learning to walk) | Variable ("relatively short period of time") | Iterations/Trials/Time | Explicit (Qualitative) / Implicit (Quantitative) | Qualitatively mentioned for walker learning. | Sec: Implications of Embodiment |
        | Response to Perturbation (Self-stabilization) | Variable (e.g., few steps/cycles) | s / steps / cycles | Implicit / Fig 3 | Implied by the concept, visually suggested in Fig 3. | Fig 3 |
        | Morphological Change (Modular Robots) | Variable | s / min | Implicit | Dependent on mechanism, not specified. | Fig 2G |

### **6.2 Active Inference:**

    *   Content: Unclear

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes
        1.  **Passive Adaptation:** Material compliance and morphology allowing robots to passively adapt to small bumps or uneven terrain without explicit control changes (Refs 3, 10, 14, 22, Fig 2C, Fig 3).
        2.  **Learning Adaptation:** Robots learning control policies (e.g., reinforcement learning to walk on level ground, Ref 2, Fig 2D) or adapting gaits to different terrains (Ref 22).
        3.  **Morphological Adaptation:** Modular robots changing their shape (morphology) to suit different tasks or environments (Ref 7, 47, 48, Fig 2G).
        These represent persistent changes (material properties, learned controllers, body shape) influencing future behavior.

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content:
        1.  **Passive Mechanical Adaptation:** Resulting from material properties (compliance, damping) and morphological design (passive dynamics). The change is inherent in the physics, not a learned or controlled modification of parameters (Refs 3, 10, 14, 35).
        2.  **Reinforcement Learning (RL):** Adjusting control parameters (e.g., actuator commands, neural network weights) based on trial-and-error and reward signals to improve performance (Ref 2, Implicitly Ref 22).
        3.  **Neural Adaptation:** Changes in neural models, potentially mimicking biological plasticity mechanisms, although specific mechanisms (e.g., Hebbian) are not detailed in the excerpt. Mentioned in the context of navigation and potentially imitation learning (Refs 23, 24, 33, 43).
        4.  **Morphological Reconfiguration:** Physical change in the connection of modules based on external commands or potentially local rules (Refs 7, 48).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors discussed are:
        *   **Locomotion:** Crawling, walking, running, climbing, swimming, flying (Explicitly listed goals, Introduction; specific examples Fig 2A, C, D, E, F).
        *   **Navigation/Orientation:** Finding paths, homing, spatial awareness (Refs 12, 18, 32, 33).
        *   **Manipulation:** Grasping, interacting with objects (Mentioned as goal, Introduction; Fig 1 caption).
        *   **Adaptation/Self-Stabilization:** Maintaining function despite perturbations or environmental changes (Refs 3, 10, 14, 22, 35, Fig 3).
        *   **Imitation:** Learning by observing others (Refs 23, 24, 40).
        *   **Cooperation/Collective Behavior:** Multiple agents working together or forming structures (Refs 9, 50, 51, Fig 2H).
        *   **Morphing/Reconfiguration:** Changing shape (Refs 7, 47, 48, Fig 2G).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation is primarily demonstrated through the construction and testing of physical robots (explicitly preferred over simulation due to fidelity issues, Introduction, Fig 1 caption). Behaviors like locomotion, climbing, morphing, and self-stabilization are observed and documented (often qualitatively or with basic metrics like speed) in experiments (Refs 2, 7, 9, 10, 13, 18, 20, etc.; Figs 2, 3, 4). Simulations are also used, particularly for exploring parameter spaces or theoretical models (e.g., Fig 3 simulation, learning). Control experiments involve comparing performance under different conditions (e.g., sensory-motor coordination vs. random head movement in Fig 4). Reproducibility is implied by publication but not explicitly discussed. Limitations include difficulty in perfectly replicating real-world complexity and potential variability in experiments.

---

#Key: [varela-rosales_granular_2023]

```markdown
# Granular binary mixtures improve energy dissipation efficiency of granular dampers

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a granular damper simulated using the Discrete Element Method (DEM). It consists of a cubical container (8x8x8 cm³) partially filled with N frictionless spherical particles. The particles form a binary mixture of the same material (steel properties assumed) but different sizes (characterized by size ratio σ). The purpose is to study how the composition of this binary mixture affects the energy dissipation efficiency when the damper is subjected to vertical sinusoidal vibrations (z(t)=A_damp cos(2πf_damp t)) under gravity. Energy is dissipated through inelastic particle-particle and particle-wall collisions modeled using Hertz contact law combined with a viscous damping force. Three classes of mixtures are compared to a monodisperse reference system: (a) equal total mass, (b) equal total number of particles, (c) equal total mass and number of particles.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
    *   **Note:** Parameters listed are key to defining the physical system and simulation setup. Values are explicitly stated. Particle Young's Modulus and Poisson's ratio are also given but derived parameters like the damping constant A depend on implementation details not fully specified (like reference particle size for the restitution calculation). Size ratio σ is varied, not a single value. Number of particles N is determined by the mixture class rules.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the external mechanical driver imposing sinusoidal oscillations on the container in the vertical direction (z(t)=A_damp cos(2πf_damp t)).

### **2.2 Energy Transduction**

    *   Content: Mechanical energy from the oscillating container walls is transferred to the granular particles, increasing their kinetic energy. This kinetic energy is then dissipated into heat during inelastic collisions. The main energy transformations are: Work done by container walls -> Particle kinetic energy -> Heat (via dissipative forces during collisions). Collisions occur both between particles (particle-particle) and between particles and the container walls (particle-wall). The force model (Eqs. 1-2) combines elastic energy storage (Hertzian) and viscous dissipation during contact.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper quantifies energy dissipation efficiency using the metric E_d (Eq. 3), which relates the energy dissipated per cycle (E_pp + E_pw) to a theoretical maximum dissipation E_max. Figures 2, 8, 12 plot this efficiency (normalized by mass) as a function of driving amplitude A_damp for different size ratios σ across the three classes. Efficiency values vary significantly, reaching optimal values near A_damp ≈ 2.8 cm. For instance, in Fig. 2 (Class a), normalized efficiency E_d/m peaks around 0.0012-0.0016 J/(kg*cycle) depending on σ. Comparing mixtures to monodisperse shows that efficiency can be enhanced or reduced depending on the mixture class and size ratio. It's not possible to assign a single efficiency score to the "system" itself.

### **2.4 Energy Dissipation**

    *   Content: Dissipation occurs exclusively through the viscous damping component of the normal contact force during particle-particle (pp) and particle-wall (pw) collisions. The force model (Eqs. 1-2) includes a term proportional to sqrt(overlap) * overlap_rate (A * sqrt(ξ_ij) * ξ_ij_dot). This represents energy loss due to material viscosity during deformation. Friction is explicitly neglected. The dissipated energy per cycle is quantified by E_pp (Eq. 4) and E_pw (Eq. 5), representing integrals of the dissipative force component multiplied by the relative velocity over one oscillation period. The magnitude depends heavily on collision frequency, impact velocities, and particle properties (via constant A, derived from coefficient of restitution ε=0.75). Qualitative assessment: Dissipation is the core function; its magnitude is significant and the central focus of the study.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Skipping M3.2-M3.8 as M3.1 is "No")**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are defined by the DEM force model for frictionless, viscoelastic spheres described in Section 2. When two particles i and j (or a particle and a wall) are in contact (overlap ξ_ij >= 0), a repulsive normal force F_ij is calculated based on the Hertz law (elastic component proportional to ξ_ij^(3/2)) and a viscous damping term (dissipative component proportional to A * sqrt(ξ_ij) * dξ_ij/dt). The force is given by F_ij = max(0, [2E*sqrt(Reff_ij*ξ_ij) / 3(1-ν²)] * (ξ_ij + A * dξ_ij/dt)). Particle motion is governed by Newton's second law, integrating these contact forces along with gravity. These rules, applied locally between contacting pairs, lead to the global dynamics and emergent segregation.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Eq. 1 | Hertz+Viscous Normal Force | Young's Modulus (E) | 2e9 | Pa | Section 2 | Explicit | Value directly given for steel. |
    | Eq. 1 | Hertz+Viscous Normal Force | Dissipative Constant (A) | Determined via ε=0.75 | s | Section 2 | Mixed | Method described (target ε at 1 m/s), but specific value of A not given, likely depends on particle sizes (Reff). Arithmetic mean used for mixtures. |

### **4.3 Global Order:**

    *   Content: The emergent global order described is spatial size segregation, specifically a reverse Brazil nut effect. In the bidisperse mixtures subjected to vertical vibrations, the smaller particles tend to accumulate above the larger particles, forming distinct layers or regions within the container, particularly noticeable after collisions with the top and bottom walls (as shown conceptually in Fig. 3 and in simulation snapshots like Fig. 4).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Eq. 1 | Hertz+Viscous Normal Force | E | 2e9 | Pa | Explicit | Value provided | Section 2 |
| Eq. 1 | Hertz+Viscous Normal Force | A | (Derived from ε=0.75 @ 1m/s) | s | Mixed | Method specified, value implicit | Section 2 |
| Newton | Particle Dynamics | Particle Mass (derived from ρ, R) | Depends on R, σ | kg | Implicit | Derived from density and radius | Section 2 |
| Newton | Particle Dynamics | Gravity (g) | 9.8 | m/s² | Explicit | Value provided | Section 2 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Segregation | Reverse Brazil Nut Effect | Vertical position difference (small vs large particles COM) | Qualitative (small above large) | cm | Mixed | Explicitly stated, qualitatively shown in Figs 3, 4, but not quantified with an order parameter. | Visual inspection, analysis of particle z-coordinates | Section 5.2, Fig 3, Fig 4 |
| Segregation | Particle probability density spread | Extent of particle distribution | See Fig 4 (II, VIII) | cm | Explicit | Plots show density distribution differences. | Simulation data analysis | Fig 4 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Skipping M5.2-5.4 as M5.1 is "No")**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Driving Period (T = 1/f_damp) | 1/70 ≈ 0.0143 | s | Section 2, 3 | Explicit | Calculated from f_damp=70 Hz. |
        | Simulation Time Step | 1e-7 | s | Section 2 | Explicit | Value explicitly provided. |
        | Collision Duration | << Driving Period | s | Implicit | Typical for DEM simulations with stiff materials; must be resolved by the time step. Not explicitly stated. | Inferred | Based on DEM practice and the small time step used relative to driving period. |
        | Segregation Timescale | >> Driving Period | s | Implicit | Segregation is described as occurring "after the granular material collides with the top and bottom walls", suggesting it develops over multiple cycles. Not quantified. | Inferred | Based on description in Section 5.2 and general knowledge of segregation dynamics. |
    *   **Note:** Only timescales explicitly mentioned or directly calculable/inferable from the text are included.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Skipping M7.2 as M7.1 is "No")**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior studied is **energy dissipation**: the attenuation of mechanical vibration energy into heat through inelastic collisions. The key observable is the energy dissipation efficiency (E_d), quantified as the energy dissipated per oscillation cycle relative to a theoretical maximum. A secondary emergent behavior is **size segregation** (reverse Brazil nut effect) under vibration. The paper analyzes how the primary behavior (dissipation efficiency) depends on system parameters, particularly the composition (size ratio σ) of the binary granular mixture, revealing complex dependencies and optimal conditions (e.g., behaviors in Fig 2, 8, 12). Different dynamic regimes (gaseous vs. collect-and-collide) are also observed depending on drive amplitude (Section 4).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors (dissipation efficiency dependence, segregation, dynamic regimes) are validated through systematic DEM simulations. Energy dissipation is quantitatively analyzed using metrics (E_d) derived from simulation data (Section 3, Figs 2, 8, 12), including averaging over 50 periods and showing standard deviations (error bars). Segregation is validated visually via simulation snapshots (Fig 3, Fig 4) and analysis of particle spatial distributions/probability densities (Fig 4). Dynamic regimes (gaseous vs. collect-and-collide) are identified based on the behavior of the center of mass and energy dissipation characteristics as a function of amplitude (Section 4, Section 5.2), referencing prior work [2-4, 34]. Phase plots (Fig 7, 11, 15) are used to further analyze particle dynamics in different regimes/mixtures. Limitations include the idealizations in the model (frictionless spheres) and the focus on a specific frequency and container geometry. Reproducibility relies on implementing the described DEM model.

---

#Key: [seifert_reinforcement_2024]

# From reinforcement learning to agency: Frameworks for understanding basal cognition

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper proposes a theoretical framework integrating the TAME (Technological Approach to Mind Everywhere) framework for biological competency and goal-directedness with Reinforcement Learning (RL) from AI. The purpose is to create a symbiotic framework for understanding agency and basal cognition in biological organisms (from single cells to complex animals) and potentially for building artificial agents. The system described is conceptual, focusing on unifying principles of goal-seeking behavior, multi-scale competency, and learning across different substrates (biological and artificial). Components include concepts like agents, goals, states, actions, rewards (RL), cognitive horizons, multi-scale competency architecture (TAME), and biological examples illustrating these concepts (e.g., morphogenesis, regeneration, adaptation in planaria, tadpoles, Xenobots). It aims to use RL as a toolkit to quantify TAME concepts and inspire new RL algorithms from biology.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters define the RL component and a key TAME concept proposed for integration. They are fundamental to the framework's implementation, although presented conceptually.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The paper discusses biological agents expending energy to reduce the delta between a current state and a goal state, but does not specify primary energy sources or quantify input for the theoretical framework itself. For biological examples, it would be metabolic energy. For RL, it's computational cost.

### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable (ranging from short-term RL updates to long-term morphogenetic/evolutionary memory)
*    Units: Time (seconds, minutes, hours, days, generations) (Qualitative Descriptor: "Short-term" to "Very Long-term")

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Variable / High (potentially)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The paper discusses local interactions conceptually but doesn't define specific mathematical rules. For TAME, local interactions couple homeostatic loops of individual agents (cells, subcellular components) into collectives (Sec 2.1). Mechanisms mentioned include cell-cell communication vs. cytoskeletal bending (Sec 1, kidney tubule example), bioelectric signaling via gap junctions enabling long-range coordination and cell integration into networks pursuing organ-level goals (Sec 1, 2.1, 3.1, 4.2), and potentially genetic regulatory networks (Sec 3, E.coli example). For multi-agent RL (Sec 3.1), local rules would be the individual agent's policy interacting with the (perceived) state, including other agents. The paper highlights the need to understand algorithms guiding computations within composite agents and credit assignment among parts (Sec 2.1). For the E.coli example (Sec 3), specific equations (Eq. 1) describe local molecular dynamics (mRNA transcription/degradation) based on environmental input (pixel intensity).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | E.coli GRN | mRNA Transcription Rate | α | Varying (Intensity) | 1/time | Sec 3, Eq 1 | Explicit | Defines model |
    | E.coli GRN | mRNA Degradation Rate | β | Constant | 1/time | Sec 3, Eq 1 | Explicit | Defines model |
    | RL | Discount Factor | γ | 0-1 | Dimensionless | Sec 2.2 | Explicit | Defines model |

### **4.3 Global Order:**

    *   Content: The global order described includes complex anatomical structures resulting from morphogenesis (trees, snakes, elephants, Xenobots, normal frogs from scrambled faces, regenerated limbs/heads, correctly sized kidney tubules - Sec 1, 3.4), coordinated collective behaviors (biofilm synchronization - Sec 3.1), stable physiological states (anatomical homeostasis - Sec 1, 2.1), and potentially emergent strategies in multi-agent RL systems (Sec 3.1). TAME focuses on how local agents form larger-scale agents pursuing larger goals in new problem spaces (Sec 2.1).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Morphology | Target anatomical structure (e.g., Planarian head number) | Head Count | 1 or 2 | Integer | Explicit | Quantifies outcome of regeneration experiment | Observation after modification/regeneration | Sec 1 (Durant 2017) |
| Physiology | Adaptation to toxin (e.g., Barium resistance) | Gene Expression Levels | Fold Change | Dimensionless | Explicit | Quantifies adaptive physiological state | Transcriptomics | Sec 1 (Emmons-Bell 2019) |
| Behavior | Biofilm synchronization | Eating patterns | Synchronized/Unsynchronized | Categorical | Explicit | Describes collective behavior outcome | Experimental Observation | Sec 3.1 (Martinez-Corral 2019) |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Hybrid/Other (Biological computation potentially involving analog dynamics, network processing, learning; RL involves various algorithms - model-free, model-based, successor representation)

### **5.3 Computational Primitive:**

    *   Content: The framework suggests biological computation involves primitives like: Goal comparison (error minimization in homeostasis/morphogenesis, Sec 1, 2.1), State estimation (recognizing missing structures, Sec 1), Adaptation/Learning (adjusting gene expression, RL policy updates, Sec 1, 2.1, 2.2), Prediction (implied in goal-seeking, explicit in E.coli example Sec 3, RL model-based approaches Sec 2.2, successor representation Sec 2.2). For RL, primitives include value estimation (state-action value function), policy improvement, temporal difference calculation (Sec 2.2). The E.coli GRN example demonstrates prediction based on mRNA dynamics (Sec 3, Eq 1).
    *   **Sub-Type (if applicable):** Goal comparison, State estimation, Learning/Adaptation, Prediction, Value Estimation, Policy Improvement, Temporal Difference Calculation.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Morphogenesis/Regeneration | Days to Weeks (Implicit) | Time | Sec 1 | Implicit | Based on examples like frog development, planarian regeneration. |
        | Physiological Adaptation (Planaria/Barium) | ~1-2 weeks | Time | Sec 1 | Explicit | Stated in text. |
        | RL Updates (e.g., TD Learning) | Milliseconds to Seconds (Implicit) | Time | Sec 2.2 | Implicit | Typical timescale for neural correlates (dopamine). |
        | Bioelectric Signaling | Milliseconds to Seconds (Implicit) | Time | Sec 3.1 | Implicit | Based on known ion channel dynamics and potassium waves. |
        | GRN Dynamics (E.coli example) | Minutes to Hours (Implicit) | Time | Sec 3 | Implicit | Typical timescale for transcription/translation. |
        | Memory Retention (Metamorphosis) | Weeks to Months (Implicit) | Time | Sec 3.4 | Implicit | Duration of metamorphosis. |
        | Evolutionary Adaptation | Generations | Time | Sec 1, 5 | Implicit | Standard timescale for evolution. |
    *   **Note:** The framework spans a vast range of timescales, from rapid signaling to developmental and evolutionary processes.

### **6.2 Active Inference:**

    *   Content: Unclear/Partial

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: Multiple mechanisms are proposed. For RL, mechanisms include updating value functions or policies via algorithms like temporal difference learning, Monte Carlo methods, policy gradients, potentially using model-based planning or successor representations (Sec 2.2). For biological systems within TAME, mechanisms involve: modifying bioelectric memory patterns (Sec 1, 2.1), altering gene expression programs (Sec 1, planaria/barium example), dynamic cell behaviors (movement, proliferation, remodeling, communication) during morphogenesis/regeneration to reach anatomical setpoints (Sec 1, 2.1), potentially involving learning rules within GRNs or other subcellular networks (Sec 3, 4.1). Credit assignment mechanisms within collectives are highlighted as key but needing further study (Sec 2.1, 3.1). Evolution acts as a meta-learning mechanism shaping adaptive capabilities (Sec 1, 5).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors discussed are goal-directed activities across different scales and spaces. These include: Morphogenesis (building specific anatomical structures, Sec 1), Regeneration (restoring target morphology after damage, Sec 1, 3.4), Anatomical Homeostasis (maintaining correct structure, Sec 1, 2.1), Physiological Adaptation (adjusting internal state to novel environments, e.g., barium resistance, Sec 1), Problem-Solving (achieving goals despite perturbations, "same ends through different means", Sec 1), Learning (improving performance via RL, Sec 2.2), Collective Action (biofilm synchronization, Sec 3.1; multi-agent RL, Sec 3.1), Exploration/Play/Mimicry (mentioned in abstract).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper cites experimental work validating the described biological behaviors (e.g., Vandenberg et al. 2012 for tadpole face normalization; Fankhauser 1945 for kidney tubules; Durant et al. 2017 for planarian 2-headed form; Emmons-Bell et al. 2019 for barium adaptation; Kriegman et al. 2020, 2021 for Xenobots; Schultz et al. 1997 for RL correlates in monkeys). Validation methods are those typical of experimental biology (observation, manipulation, molecular analysis, imaging). For the theoretical RL/TAME framework itself, validation would require computational modeling and prediction compared against experiments, which is proposed as future work (Sec 4). The E.coli prediction example (Sec 3) uses simulation based on a standard GRN model and compares correlation coefficients (Fig 3).

---

#Key: [mcmullen_self-assembly_2022]

# Self-assembly of emulsion droplets through programmable folding

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of two flavors (A - blue, B - yellow) of colloidal emulsion droplets functionalized with specific DNA strands. These droplets first self-assemble into linear, alternating chains (colloidomers) using strong backbone DNA interactions, accelerated by an intermittent magnetic field in a ferrofluid dispersion. The primary purpose is to demonstrate programmable folding of these chains into specific, predetermined 2D and 3D geometries ("foldamers"). This folding is achieved by introducing weaker, secondary DNA interactions (homophilic A-A, B-B, and heterophilic A-B) with distinct melting temperatures. By carefully controlling the temperature protocol (cooling steps), these secondary interactions are activated sequentially, guiding the colloidomer chain through a "downhill folding" pathway into a unique final structure. The system combines experiments (microscopy), simulations (DPD), and theory (folding pathway analysis, search algorithm) to design, realize, and analyze these colloidal foldamers and their potential for further hierarchical assembly.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters relate to the specific implementation enabling folding control. Simulation unit length is included as a key parameter for computational aspects. Melting temperatures are crucial for the hierarchical folding protocol.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input modulating the system's state (folding) is thermal energy, controlled via temperature changes. Temperature dictates the hybridization state (bound/unbound) of DNA strands based on their melting temperatures (Tm). An initial energy input via a magnetic field is used to accelerate chain formation but is removed before folding.
    *   Units: °C (for temperature control) or Joules (conceptually, thermal energy difference).

### **2.2 Energy Transduction**

    *   Content: Thermal energy is transduced into chemical potential energy changes associated with DNA bond formation/breaking. As temperature decreases below the Tm of specific DNA interactions, the favorable binding free energy drives hybridization, forming bonds. This bond formation restricts the conformational freedom of the colloidomer chain, releasing conformational entropy and driving the system towards lower potential energy folded states (downhill folding). The energy landscape is explicitly discussed (Fig 2a). Simulations model this via interaction potentials (Eq 1) dependent on binding strength (ε).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The concept of energy efficiency in the traditional sense (work output / energy input) is not applicable or discussed. The process is driven by thermodynamically favorable transitions (downhill folding) guided by temperature changes, aiming for structural specificity, not work extraction. If viewed as information encoding (sequence/protocol -> structure), efficiency isn't measured. The process likely has low thermodynamic efficiency as most thermal energy manages the bath temperature, not direct bond formation work. No metrics are provided.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation occurs primarily through viscous drag as droplets rearrange within the fluid medium during folding. In simulations (DPD), dissipation is explicitly modeled via dissipative forces inherent to the DPD thermostat, maintaining constant temperature (kT=1). Heat is inevitably lost to the surrounding thermal bath during experimental temperature control and equilibration. The breaking of transient non-specific interactions or incorrect secondary bonds before correct ones form would also dissipate energy. Quantification is not provided in the excerpt. Qualitatively, dissipation is inherent to the dynamic rearrangement process in a viscous fluid.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term (within experimental observation)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Up to 310 (for N<=13, ABC sequences)
*   Units: distinct foldamer geometries (states)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: Variable (e.g., 38% to 100%)
*   Units: % Yield (relative yield)

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Yield (Relative) | Fraction of rigid structures reaching target geometry | 38-100 | % | `MemoryNode`.`yield` | Fig 3b | Explicit | Measures success rate of encoding the intended structure. |
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: 1. Backbone Formation: Droplets A and B bind via strong, complementary 20bp DNA strands (effectively irreversible under folding conditions). 2. Secondary Interactions: Specific pairs of droplets (AA, BB, or AB, depending on the DNA functionalization and protocol step) bind via weaker DNA strands (P: 8bp palindrome, C+D: 6bp+spacer, D: 6bp palindrome) if they are spatially proximate *and* the temperature is below the respective Tm for that interaction type. Binding is assumed irreversible once formed ("downhill folding"). Droplets can rearrange after binding facilitated by surface diffusion of DNA. Simulations use a short-range isotropic potential (Eq 1) with specific interaction ranges (r_i=1.05σ) and strengths (ε) activated sequentially.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | 1 | Backbone Binding | DNA Length | 20 | bp | Methods, Fig 1a | Explicit | Specifies backbone bond. |
    | 2 | Secondary Binding P | Tm | ~40-45 | °C | Methods, Fig 1d | Explicit | Interaction strength hierarchy. |
    | 2 | Secondary Binding C+D | Tm | ~30-35 | °C | Methods, Fig 1d | Explicit | Interaction strength hierarchy. |
    | 2 | Secondary Binding D | Tm | ~27 | °C | Methods, Fig 1d | Explicit | Interaction strength hierarchy. |
    | 2 | Secondary Binding (Sim) | Interaction Range (r_i) | 1.05 | σ | Methods (Eq 1) | Explicit | Defines spatial extent of interaction in simulation. |

### **4.3 Global Order:**

    *   Content: The emergent global order consists of specific, often rigid, 2D and 3D geometric configurations of the droplet chain, termed "foldamers". Examples include Triangle, Chevron, Ladder (N=6); Rocket, Flower (N=7); Hourglass (N=8); Poodle (N=9); Crown, Bed (N=10); Star (N=11+); Polytetrahedron (N=6, 3D). Some protocols may yield floppy (non-rigid) intermediate or final states. Foldamers can exhibit complex topology, including stable holes for N>=13.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| DNA_Tm | Temperature-dependent DNA binding | Melting Temp (Tm) | ~27 to ~75 | °C | Explicit | Controls which bonds form at which step. | Methods, Fig 1d |
| DPD_Pot | Simulation interaction potential | Interaction Strength (ε) | Variable (e.g., up to 40 for backbone) | kT | Explicit | Models DNA binding energy in simulations. | Methods (Eq 1) |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Yield | Success rate of forming target geometry | Relative Yield | 38-100 | % | Explicit | Quantifies predictability/success of achieving the order. | Specific (e.g., I, II, III) | Fig 3b |
| Holes | Topological feature (presence of internal voids) | Hole Count | 0, 1+ | Count | Explicit | Characterizes topology, emerges for N>=13. | Specific | Fig 4a, Ext Data Fig 5 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid

### **5.3 Computational Primitive:**

    *   Content: Sequence-to-Structure Mapping via Hierarchical Constraint Satisfaction. The fundamental operation is the selective formation of bonds based on sequence proximity and temperature-dependent interaction rules. This acts as a physical algorithm that takes the linear sequence information and the temporal protocol information and maps it onto a specific 3D spatial arrangement by sequentially satisfying bonding constraints according to a programmed hierarchy. It's a complex mapping, not reducible to simple logic gates.

### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Backbone Assembly | Minutes (Implicit) | min | Methods (process described) | Implicit | Time needed to form initial chains before folding. Not specified but implied to be finite. |
        | Folding Step Duration (τ) | 5-30 | min | Methods, Fig 1d, Fig 2a | Explicit | Time allowed for equilibration/bond formation at each temperature step. |
        | Full Folding Process (Heptamer Example) | ~20 | min | Fig 2a | Explicit | Total time from unfolded chain to final foldamer for a specific case. |
        | Simulation Timestep (DNA) | 10^-2 | simulation time units (s_t) | Methods | Explicit | Timescale for resolving solvent dynamics in DPD. |
        | Simulation Timestep (Colloids) | 10^-4 | simulation time units (s_t) | Methods | Explicit | Timescale for resolving colloid dynamics in DPD. |
    *   **Note:** Simulation times are relative; experimental times provide real-world scale.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior is the formation of specific, complex, and often rigid 2D and 3D geometric structures (foldamers) from linear droplet chains through a programmed sequence of local interactions. A secondary behavior is the potential for these foldamers to further self-assemble into higher-order supracolloidal architectures (e.g., dimers, ribbons, mosaics) via remaining or newly activated interactions, demonstrated in simulation.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of specific foldamer formation (emergent structure) are validated through direct experimental observation using fluorescence microscopy (Figs 1c, 2, 3b). Structures are identified by comparing experimental images to theoretically predicted geometries. Yields are quantified by counting structures (Fig 3b caption). Theoretical predictions are validated by comparing predicted folding trees/final states with experimental observations (Fig 2a shows experimental images populating a theoretical tree) and simulation results (Ext Data Figs 3, 4). The foldamer search algorithm is validated by its success in predicting protocols that work experimentally. Supracolloidal assembly is validated only via simulation (Fig 4b). Reproducibility is implied by reported yields from multiple observations (n values in Fig 3b caption). Limitations include potential misidentification of structures, finite observation time potentially missing slower transitions out of kinetic traps, and the 2D projection potentially obscuring 3D aspects.

---

#Key: [hauser_towards_2011-1]

# Towards a theoretical foundation for morphological computation with compliant bodies

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### 1.1 System Description
    *   Content: The paper presents a theoretical framework and computational models for Morphological Computation (MC) using compliant bodies, specifically mass-spring systems. It proposes that the complex, nonlinear dynamics of physical bodies can serve as computational resources. Two main models are introduced:
        1.  **Filter Bank Model (Fig 1a, b):** An array of linear mass-spring systems (filter bank) processes an input stream u(t). Each system acts as a time-invariant, fading memory filter (B_i). A static, nonlinear readout function 'f' (e.g., an Artificial Neural Network - ANN) combines the outputs x(t) of the mass-spring systems to approximate a target filter F. The physical body (mass-spring array) primarily provides temporal integration.
        2.  **Recurrent Network Model (Fig 1c, d):** A randomly connected network of *nonlinear* springs and masses processes the input u(t). This network itself performs both temporal integration and nonlinear projection (acting like a kernel). A simple *linear*, static readout adapts weights (w_out) to combine the internal states (e.g., spring lengths l_i(t)) to approximate the target filter F.
        The purpose is to demonstrate that complex physical bodies can approximate arbitrary time-invariant filters with fading memory (representable by Volterra series), thereby simplifying the learning task required by an external controller (readout) to only adjusting static, often linear, weights. This outsources computation to the body's physics.

### 1.2 Implementation Clarity

### 1.3 Key Parameters
        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### 2.1 Energy Input
    *   Content: The paper models external influence as forces applied to mass points (u in Eq 1, Eq 4; τ in Eq 3; scaled input signal u(t) mapped to horizontal force Fx in Eq 5). The ultimate *source* of this energy (e.g., electrical motor driving input, environmental interaction) is not the focus and is abstracted as a time-varying force or torque input signal. For the computational process itself, energy is needed to simulate the differential equations.

### 2.2 Energy Transduction
    *   Content: Input energy (via applied forces) is transduced into kinetic energy (mass movement, p¨x, p¨y in Eq 5, 6; x_2 in Eq 1, Eq 4) and potential energy stored in the deformed springs (related to p(x1) in Eq 4; k/m * x1 term in Eq 1). Energy is transferred between kinetic and potential forms during oscillations. Damping terms (d in Eq 1, q(x2) in Eq 4) represent energy dissipation.

### 2.3 Energy Efficiency
    *   Justification/Metrics: The paper does not discuss or quantify the energy efficiency of the morphological computation process itself (i.e., computation performed per unit of energy input or dissipated). It mentions high energy consumption as a characteristic of classical robots (Sec 1) but doesn't evaluate its own models on this axis.

### 2.4 Energy Dissipation
    *   Content: Energy dissipation is explicitly included via damping terms in the models: linear damping 'd' in Eq 1 (`d/m * x_2`) and nonlinear damping function 'q(x_2)' (specifically `d1*x2 + d3*x2^3`) in Eq 4. These terms represent energy loss from the mechanical system, typically as heat due to friction or material hysteresis. The magnitude depends on the velocity (x_2) and the damping constants (d, d1, d3). Quantification requires specific parameter values and system states, but is qualitatively present and necessary for stability (fading memory). The pendulum model (Eq 3) also includes a linear friction term (`μ/ml^2 * ω`).

## M3: Memory

### 3.1 Memory Presence:
    *   Content: Yes

### 3.2 Memory Type:

### 3.3 Memory Retention Time:

### 3.4 Memory Capacity (Optional - if applicable)

### 3.5 Readout Accuracy (Optional - if applicable)

### 3.6 Degradation Rate (Optional - if applicable)

### 3.7 Memory Operations Energy Cost (Optional - if applicable)

### 3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)

---

## M4: Self-Organization and Emergent Order

### 4.1 Self-Organization Presence:
    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### 5.1 Embodied Computation Presence:
    *   Content: Yes

### 5.2 Computation Type:
    *   Content: Reservoir Computing / Analog Computing / Filter Approximation

### 5.3 Computational Primitive:
    *   Content: The fundamental computation performed by the material body is the transformation of a continuous input time series u(t) into a higher-dimensional vector of time series x(t) (internal states, e.g., spring lengths or mass positions/velocities). This transformation embodies both temporal integration (due to dynamics/memory) and potentially nonlinear projection (due to nonlinear springs/interactions in the recurrent model). Mathematically, this transformation approximates the action of a bank of complex filters {B_i} or a kernel mapping. The overall system (body + readout) performs the approximation of a target nonlinear filter F (potentially represented by a Volterra series operator). The *body's* primitive is the state-space transformation/filtering.
    *   **Sub-Type (if applicable):** Temporal Filtering / Nonlinear State-Space Projection

### 5.4 Embodied Computational Units

## M6: Temporal Dynamics

### 6.1 Timescales:
        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Simulation Timestep | 1 | ms | Sec 3, 4.1.2 | Explicit | Set integration step for simulation. |
        | Input Signal Frequencies | 2.11, 3.73, 4.33 | Hz | Sec 3 | Explicit | Frequencies used for sinusoidal input generation. Imply timescales of ~230-470 ms. |
        | Volterra Kernel Timescale (μ) | 0.1 | s | Sec 3, Eq 2 | Explicit | Mean delay time in the Gaussian kernel. |
        | Volterra Kernel Timescale (σ) | 0.05 | s | Sec 3, Eq 2 | Explicit | Standard deviation of delay time in Gaussian kernel. |
        | Learning/Testing Duration | 30 (learn), 10 (val), 10 (test) / 45 (learn), 5 (test) | s | Sec 3, Sec 4.3 | Explicit | Duration of phases in simulation experiments. |
    *   **Note:** The intrinsic timescales of the mass-spring dynamics depend on the randomly chosen parameters (k, m, d) and are not explicitly calculated or reported in the paper, though they are implicitly present and crucial for the system's function.

### 6.2 Active Inference:
    *   Content: No

## M7: Adaptation

### 7.1 Adaptive Plasticity Presence:
    *   Content: Yes (in the readout component)

### 7.2 Adaptation Mechanism:
    *   Content: The adaptation mechanism is supervised learning applied to the static readout component.
        1.  **Model 1 (Filter Bank + ANN):** A supervised learning algorithm (specifically, BFGS quasi-Newton optimization, Sec 3) adjusts the weights of the ANN readout to minimize the Mean Squared Error (MSE) between the system's output y(t) and the target filter output (Fu)(t).
        2.  **Model 2 (Recurrent Net + Linear Readout):** Standard linear regression (specifically, calculating weights via Moore-Penrose pseudoinverse of the collected internal states L and target outputs T: `w_out = L† * T`, Sec 4.1.3) is used to find the optimal linear weights `w_out` that minimize the MSE between the weighted sum of spring lengths and the target output(s).
        In both cases, the learning optimizes the output mapping based on examples of input-target pairs, using the fixed dynamics of the physical body as a pre-processing step.

## M8: Emergent Behaviors

### 8.1 Behavior Description:
    *   Content: The primary functional behavior of the system is the approximation of complex, nonlinear, time-invariant filters with fading memory. Specific examples demonstrated include:
        *   Approximating a quadratic Volterra series operator with a Gaussian kernel (Sec 3).
        *   Emulating the dynamics of a damped pendulum (Sec 3).
        *   Representing the inverse dynamics of a two-link robot arm (mapping end-effector trajectory to joint torques) (Sec 4.2).
        *   Emulating second-order and tenth-order nonlinear difference equations (Sec 4.3).
        The behavior is the system's output time series y(t) attempting to match a target time series generated by the filter/system being emulated, given the same input u(t). Multitasking (approximating multiple filters simultaneously using the same body with different readouts) is also demonstrated (Sec 3, Sec 4.3).

### 8.2 Behavior Robustness:

### 8.3 CT-GIN Emergent Behavior Validation
     *  Content: The paper validates the primary behavior (filter approximation) through quantitative comparison of the system's output with the target output using Mean Squared Error (MSE) on separate testing datasets (Sec 3, Fig 3c, 3d; Sec 4.2, Fig 8c, 8f; Sec 4.3, Fig 9c, 9d, 9e). Control comparisons are made against systems without the morphological component (ANN/LR applied directly to raw input, Fig 3c, 3d; Fig 8d; Fig 9c, 9d, 9e), demonstrating the body's contribution. The effect of parameter diversity is tested by comparing heterogeneous vs. homogeneous bodies (Sec 3, Sec 4.2). Reproducibility is suggested by showing results from multiple random network initializations (Fig 8e, 8f). Limitations include the lack of testing against noisy inputs/readouts or varying environmental conditions. The term "emergent behavior" isn't strongly applicable here, as the behavior is explicitly trained approximation of a known target, rather than spontaneous complex patterns arising solely from local rules.

---

#Key: [hu_shaping_2019]

# Shaping the Assembly of Superparamagnetic Nanoparticles

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system uses evaporation-guided assembly of ferrofluid droplets (suspensions of superparamagnetic Fe3O4/polystyrene hybrid nanoparticles, mgPS NPs, stabilized with SDS) on a superamphiphobic substrate under an external magnetic field to create solid supraparticles. The purpose is to shape the assembly of these nanoparticles into controlled, anisotropic microstructures (supraparticles) like barrel-like, cone-like, and two-tower-like shapes by tuning nanoparticle concentration and magnetic field strength/orientation. These supraparticles retain superparamagnetism and can incorporate other colloids for binary structures or be used to create magnetically actuable microswimmers. Key components are the mgPS NPs, water, SDS surfactant, superamphiphobic substrate, and an external magnetic field source (permanent magnet). The process involves drying a droplet of the NP suspension, allowing solvent evaporation and magnetic forces to direct nanoparticle assembly into a fixed supraparticle structure.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name             | Value               | Units   | Source (Fig/Table/Section)   | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------------------- | :------------------ | :------ | :--------------------------- | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary external energy input directing the assembly (beyond ambient thermal energy for evaporation) is the static magnetic field provided by a permanent magnet. Thermal energy drives the evaporation process.
    *   Value: 16, 80, 160 (for magnetic field strength)
    *   Units: kA/m

### **2.2 Energy Transduction**

    *   Content: 1. Magnetic Field Energy -> Mechanical Work: The external magnetic field exerts forces on the superparamagnetic NPs, influencing their spatial distribution within the droplet (dragging towards substrate, alignment) and contributing to the deformation of the droplet (Rosensweig instability) as concentration increases during evaporation. This directs the assembly process against surface tension and viscous forces. 2. Thermal Energy -> Phase Change (Evaporation): Ambient thermal energy drives the evaporation of water, increasing NP concentration. 3. Potential Energy (Surface Tension) -> Mechanical Deformation: As evaporation proceeds and a shell potentially forms, surface tension forces contribute to stress buildup and eventual buckling, releasing mechanical energy. Magnetic forces modulate these surface effects.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss or quantify the energy efficiency of the assembly process (e.g., ratio of potential energy stored in the final structure vs. input magnetic/thermal energy). The focus is on morphology control, not energy conversion efficiency. Qualitative assessment: Likely very low efficiency in terms of converting input magnetic/thermal energy into the potential energy of the final structured supraparticle.

### **2.4 Energy Dissipation**

    *   Content: Energy is primarily dissipated as heat during: 1. Viscous drag as NPs move through the fluid during assembly/alignment. 2. Mechanical energy loss during inelastic buckling or structural rearrangements. 3. Latent heat of vaporization during water evaporation (transfer to environment). 4. Potential magnetic hysteresis loss (though superparamagnetic materials ideally have zero hysteresis, real materials might have minimal losses). Quantification is not provided. Qualitative assessment: Significant dissipation via evaporation (latent heat), moderate via viscous effects and buckling.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding to M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term / Permanent

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Low (discrete states)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", proceeding to M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content:
        1.  **NP-NP Interaction:** Superparamagnetic NPs experience magnetic dipole-dipole interactions (alignment/chaining) in the external field H, modulated by thermal energy (kT) and steric/electrostatic repulsion (influenced by SDS surfactant). Aggregation tendency increases with concentration.
        2.  **NP-Fluid Interaction:** Governed by viscous drag (Stokes) during movement and Brownian motion.
        3.  **NP-Interface Interaction:** NPs and surfactant accumulate at the air-water interface during evaporation, increasing surface concentration and potentially forming a shell. Surface tension (γ) acts on the droplet shape.
        4.  **NP-Substrate Interaction:** Minimized wetting on superamphiphobic surface, but residual pinning/attraction can occur, influenced by NP concentration and magnetic forces dragging NPs towards the substrate (affecting contact angle and line, Fig 2a, 2e).
        5.  **Magnetic Field Interaction:** NPs align with the external field H. Field gradients exert forces. Droplet shape responds to magnetization (M) exceeding critical value (Mc) based on surface tension (Rosensweig instability, Eq 1). Droplet splitting occurs when contact line exceeds critical wavelength (λc) dependent on field, magnetization, and surface tension (Eq 4). Magnetization M depends on H and NP volume fraction (Eq 2, 3).
        6.  **Evaporation Dynamics:** Water evaporation concentrates NPs and surfactant, changing M and γ over time, potentially leading to shell formation, buckling, and crossing critical thresholds (Mc, λc).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID                       | Description                                           | Parameter Name          | Parameter Value Range     | Units   | Data Source        | Implicit/Explicit | Justification                                         |
    | :---------------------------- | :---------------------------------------------------- | :---------------------- | :---------------------- | :------ | :----------------- | :----------------: | :---------------------------------------------------- |
    | Magnetic Field Interaction    | Rosensweig Instability Threshold                      | Critical Magnetization (Mc) | Depends on γ, ρ, μ (Eq 1) | A/m     | Eq 1               | Explicit          | Equation provided.                                  |
    | Magnetic Field Interaction    | Droplet Splitting Threshold                           | Critical Wavelength (λc)  | Depends on γ, μ, H, M (Eq 4)| m       | Eq 4               | Explicit          | Equation provided.                                  |
    | Magnetic Field Interaction    | NP Magnetization vs Field                             | Volume Susceptibility (χ) | Implicit in M(H)=χH       | unitless| Eq 2               | Explicit          | Equation provided, value not given but measurable.    |
    | NP-Interface / Fluid Interaction | Suspension Surface Tension                            | γ                       | 49 ± 1                  | mN/m    | Methods            | Explicit          | Value measured and stated.                          |
    | NP Concentration Dynamics     | Initial Concentration                                 | c_NP                    | 0.3 - 30                | wt %    | Methods, Results   | Explicit          | Varied parameter, range stated.                   |
    | External Field                | Applied Magnetic Field Strength                       | H                       | 0, 16, 80, 160          | kA/m    | Methods, Results   | Explicit          | Varied parameter, values stated.                  |

### **4.3 Global Order:**

    *   Content: The emergent global order is the final 3D morphology of the dried supraparticle. Specific distinct ordered structures observed are: deflated ball (no field), cone-like, barrel-like, and two-tower-like shapes, depending on the initial NP concentration and applied magnetic field strength.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Explicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID                       | Description                                        | Parameter        | Value Range                    | Units   | Implicit/Explicit | Justification                             | Source          |
| :---------------------------- | :------------------------------------------------- | :--------------- | :----------------------------- | :------ | :----------------: | :---------------------------------------- | :-------------- |
| Rosensweig Instability        | Governs initial droplet deformation under field    | Mc               | See Eq 1                       | A/m     | Explicit          | Equation 1 provided                     | Eq 1            |
| Droplet Splitting             | Governs splitting into multiple towers             | λc               | See Eq 4                       | m       | Explicit          | Equation 4 provided                     | Eq 4            |
| Magnetic Alignment            | Particle alignment and interaction strength        | H                | 0 - 160                        | kA/m    | Explicit          | Experimental parameter varied             | Results, Fig 3a |
| Concentration Effects         | Influences M, γ (minor), buckling, splitting timing | Initial c_NP     | 0.3 - 30                       | wt %    | Explicit          | Experimental parameter varied             | Results, Fig 3a |
| Surface Interaction           | Wetting/Pinning                                    | Contact Angle    | Varies (Fig 2a, 2e)            | degrees | Explicit          | Measured and shown to depend on c_NP | Fig 2a, 2e      |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description                | Parameter                  | Value Range                         | Units   | Implicit/Explicit | Justification                                           | Protocol                        | Source          |
| :---------- | :------------------------- | :------------------------- | :---------------------------------- | :------ | :----------------: | :------------------------------------------------------ | :------------------------------ | :-------------- |
| Anisotropy  | Shape aspect ratio         | Aspect Ratio               | >1 for Cone, Barrel, Two-Tower      | unitless| Explicit          | Used to characterize anisotropy (e.g., Fig 3d, SI S13) | Image Analysis (Side View)      | Fig 3d, SI S13  |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type                    | Description                                               | Predictability (Score 0-10) | Yoneda Score (Score 0-10) | Metrics                                           | Implicit/Explicit | Justification                                                                                                | Source          |
    | :--------------------------- | :-------------------------------------------------------- | :-----------------------: | :-----------------------: | :------------------------------------------------ | :----------------: | :----------------------------------------------------------------------------------------------------------- | :-------------- |
    | Parameters -> Global Shape | Mapping initial c_NP and H to final supraparticle shape | 8                         | 7                         | State Diagram Regions (Fig 3a), Shape Categories | Explicit          | Fig 3a shows clear mapping (predictability). Local rules (Eq 1-4) underpin this transformation (Yoneda). Fidelity high but not perfect. | Fig 3a, Eq 1-4 |

    *   **Yoneda Embedding Fulfillment Score [0-10]:** 7. Rubric: 0=No connection between local rules & global state; 5=Qualitative link demonstrated; 7=Quantitative rules partially explain global state map; 10=Complete quantitative model predicts global state from local rules. Here, key rules (Eq 1, 4) and mechanisms (buckling, pinning) are identified and linked to the observed shapes and the state map (Fig 3a), but a full predictive model isn't presented.
    *   **Metrics:** State diagram boundaries (c_NP, H values), classification accuracy within regions, identification of governing equations/mechanisms (Rosensweig instability, droplet splitting criteria, buckling).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description                     | Value       | Units     | Source                      | Implicit/Explicit | Justification                                           |
        | :---------------------------------------- | :---------- | :-------- | :-------------------------- | :----------------: | :------------------------------------------------------ |
        | Total Drying Time                         | ~45         | minutes   | Fig 1d                      | Explicit          | Stated in figure caption/text describing drying curves. |
        | Initial Symmetric Shrinking Phase         | ~30-40      | minutes   | Fig 1d, Results             | Explicit          | Time before significant shape change mentioned.         |
        | Buckling/Deformation Event (No Field)     | ~minutes    | minutes   | Fig 1b, Results             | Implicit          | Described as happening "after 35 min".                   |
        | Cone Formation / Elongation Phase (Field) | ~2-10       | minutes   | Fig 1d, Results (SI S13)    | Explicit          | Stated duration for rapid height increase.              |
        | Barrel Formation (Buckling post-cone)     | ~minutes    | minutes   | Fig 2c, Results (Video S3)  | Implicit          | Occurs rapidly after cone elongation reaches critical point. |
        | Two-Tower Splitting Event                 | ~minutes    | minutes   | Fig 2g, Results (Video S4)  | Implicit          | Occurs rapidly after a period of constant contact line. |
        | Time to reach Splitting Threshold (30wt%) | ~26         | minutes   | SI Fig S9                   | Explicit          | From analysis of contact line dynamics.                 |
        | Microswimmer Response Time                | << seconds? | seconds   | Fig 4c (Implicit from graph) | Implicit          | Velocity changes rapidly with field on/off in Fig 4c. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors described are:
        1.  **Morphogenesis/Shape Formation:** The system controllably forms distinct anisotropic 3D supraparticle shapes (cone, barrel, two-tower) from an initial ferrofluid droplet through evaporation-guided assembly under a magnetic field. This behavior depends critically on initial NP concentration and magnetic field strength.
        2.  **Co-Assembly:** The system allows incorporation of other non-magnetic nanoparticles (TiO2, PS) into the ferrofluid to form binary supraparticles, retaining anisotropy above a certain magnetic particle threshold (~50-75%).
        3.  **Magnetic Actuation (Microswimming):** The resulting anisotropic supraparticles (specifically cone-like) can be suspended in water and propelled/oriented by external magnetic field gradients, acting as microswimmers.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent shape formation are validated through systematic experimental variation of key control parameters (initial NP concentration c_NP, magnetic field strength H) and characterization of the resulting structures using optical microscopy and SEM (Figs 1-3, SI Figs). The relationship between parameters and structure is summarized in a state diagram (Fig 3a). Time-resolved imaging (Fig 1b-d, Fig 2c, 2g, Videos S1-S4) provides evidence for the dynamic processes (buckling, splitting) leading to the final shapes. Co-assembly is validated by fabricating binary particles and imaging them (Fig 3e, SI Fig S16). Microswimming is validated by tracking particle movement under controlled magnetic fields (Fig 4, Videos S5-S6). Limitations include lack of quantification of field gradients and limited exploration of robustness to other perturbations.

---

#Key: [govern_optimal_2014]

# Optimal resource allocation in cellular sensing systems

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is a theoretical model of a general class of cellular sensing systems where a receptor (binding external ligand L) drives a downstream push–pull network (e.g., phosphorylation/dephosphorylation cycle of a readout protein x). The components include receptors (RT), downstream readout molecules (XT), ligand (L), and fuel molecules (e.g., ATP) providing energy (Δμ). The purpose is to sense the external ligand concentration (c) with maximum precision (minimum error δc/c) given constraints on available cellular resources (receptors, time, readout molecules, energy). The theory aims to identify fundamental limits and optimal resource allocation strategies. Examples include E. coli chemotaxis, GTPase cycles, and MAPK cascades.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |


## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the turnover of fuel molecules, such as ATP, which drives the push-pull network out of thermodynamic equilibrium. This chemical energy is used to maintain the non-equilibrium state necessary for reliable sensing and information transfer (breaking time reversibility).
    *   Value: Δμ (Free energy drop per cycle)
    *   Units: k<sub>B</sub>T

### **2.2 Energy Transduction**

    *   Content: Chemical energy from fuel turnover (e.g., ATP hydrolysis) is transduced into the chemical modification of readout molecules (e.g., phosphorylation). This energy expenditure drives the reaction cycles (activation x -> xp and deactivation xp -> x) away from equilibrium, enabling reliable encoding (increasing 'q') and maintenance of the receptor state information within the readout molecule population. The free energy drop Δμ = Δμ₁ + Δμ₂ across the activation and deactivation steps quantifies this transduction.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide a single numerical efficiency value but defines efficiency in terms of optimal resource allocation. An optimally designed system (Eq. 5) minimizes wasted resources, meaning energy (w) is utilized just sufficiently to match the limits set by other resources (receptor/time RTτr/τc and readout molecules XT). Efficiency means not spending more energy than necessary to achieve the precision limit set by other factors. The minimum work required per reliable sample is discussed (approaching 4 kBT).

### **2.4 Energy Dissipation**

    *   Content: Energy is dissipated as heat due to the non-equilibrium nature of the push-pull cycle driven by fuel turnover. The total work done (w = n_Δμτr) represents the free energy consumed and ultimately dissipated during the relaxation time τr to maintain the sensing process. The dissipation is necessary to break time-reversibility and ensure reliable information encoding (q > 0). The paper quantifies the energy required per sample (Δμ/q) and the total energy (w). Dissipation mechanisms are inherent to the irreversible chemical reactions (phosphorylation/dephosphorylation cycles).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: τ<sub>r</sub> (Relaxation time)
*    Units: s

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Related to X<sub>T</sub> (Total number of readout molecules)
*   Units: count (or bits, if information content quantified)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: Related to q (sample quality) and N<sub>I</sub> (number of independent samples)
*   Units: Dimensionless (or related to error rate δc/c)

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Related to k<sub>r</sub> or τ<sub>r</sub><sup>-1</sup>
    *   Units: s<sup>-1</sup>

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Effective Sample | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Precision | Inverse of squared fractional sensing error | (δc/c)⁻² | Dimensionless | `MemoryNode` attribute `fidelity_metric` | Eq. 2, 4 | Explicit | Sensing precision directly measures how well the readout state reflects the true concentration. |
    | Reliability | Sample quality factor, relates energy to reliability | q = (e<sup>Δμ₁</sup>-1)(e<sup>Δμ₂</sup>-1)/(e<sup>Δμ</sup>-1) | Dimensionless | `MemoryNode` attribute `reliability_factor` | Eq. 3, Text | Explicit | Factor q explicitly measures the reliability/quality of each sample contributing to the memory state. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog

### **5.3 Computational Primitive:**

    *   Content: Time Integration / Temporal Filtering. The core computation performed by the downstream network is integrating the receptor occupancy signal over the relaxation time τr to average out noise. This acts as a low-pass filter. The linearization and inversion of the input-output relation xp(c) to estimate c (Eq. 1) is also part of the overall computation, happening implicitly through how the cell interprets xp.
    *   **Sub-Type (if applicable):** Temporal Filtering: Low-Pass

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Receptor Correlation Time | τ<sub>c</sub> | s | Eq. 3, Text | Explicit | Fundamental timescale of receptor state fluctuations. E. coli estimate: ~10 ms. |
        | Network Relaxation Time / Integration Time | τ<sub>r</sub> | s | Eq. 3, Text | Explicit | Effective time window for signal integration and memory. E. coli estimate: ~100 ms. |
        | Time Interval between samples (same receptor) | Δ = 2τ<sub>r</sub> / (N<sub>eff</sub>/R<sub>T</sub>) | s | Eq. 3, Text | Explicit | Microscopic timescale related to sampling frequency. |
        | Response time (E. coli chemo.) | τ<sub>r</sub><sup>-1</sup> rate varies | s | Text (Ref 24) | Explicit | Experimental values cited for attractant/repellent response (~0.5 s or ~0.05s). Value depends on specific system parameters |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip to Module 8.)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is **concentration sensing**: estimating the external ligand concentration (c) based on the internal state of the readout molecules (xp). This involves filtering noise through time integration. A secondary, higher-level emergent behavior/principle is **optimal resource allocation**: in an efficiently evolved system, the different resource classes (receptor/time, readout molecules, energy) are predicted to be equally limiting (Eq. 5), minimizing waste.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The behavior of concentration sensing is validated by deriving the sensing error (precision limits, Eq. 2, 4) from fundamental physical principles (molecular sampling, thermodynamics). The emergent principle of optimal resource allocation (Eq. 5) is validated by comparing its predictions (scaling and ratio of X<sub>T</sub>/R<sub>T</sub>, magnitude of τ<sub>r</sub>/τ<sub>c</sub>, relation of X<sub>T</sub> to work w) against experimental data from the E. coli chemotaxis system (Fig 1B, comparison text). The agreement found supports the validity of the principle. Quantitative analysis (fitting data in Fig 1B, comparing estimated vs predicted timescales and work) is used.

---

#Key: [hu_small-scale_2018]

# Small-scale soft-bodied robot with multimodal locomotion

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of an untethered, millimetre-scale, magneto-elastic soft robot and external electromagnets generating time-varying magnetic fields for actuation. The robot is a rectangular sheet made of silicone elastomer (Ecoflex 00-10) embedded with hard magnetic neodymium-iron-boron (NdFeB) microparticles. It possesses a programmed single-wavelength harmonic magnetization profile. The purpose is to demonstrate a small-scale soft robot capable of multimodal locomotion (swimming, surface swimming, meniscus climbing, rolling, walking, jumping, crawling) across different terrains (liquid, solid) and performing simple tasks like pick-and-place and cargo release, actuated wirelessly via external magnetic fields. The magnetic field controls both the robot's shape deformation and its rigid-body rotation for steering and locomotion.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Robot Scale | Millimetre (implied ~few mm length) | mm | Text (passim), Fig Scale Bars | Implicit | Medium | Estimated from figures/context |


## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the electrical energy supplied to the external electromagnets, which generate the time-varying magnetic fields (B). The magnetic field itself acts as the energy carrier to the robot.

### **2.2 Energy Transduction**

    *   Content: Electrical energy in electromagnets is converted to magnetic field energy. The external magnetic field interacts with the programmed magnetization (m) within the soft robot, generating spatially varying magnetic torques. These torques cause elastic deformation (shape change) of the robot body, storing elastic potential energy. The controlled sequence of deformations, interaction with the environment (fluid or solid surfaces), and rigid-body rotations (due to net magnetic moment M<sub>net</sub> aligning with the field B<sub>z</sub> component) transduce the stored elastic energy and magnetic potential energy into kinetic energy for locomotion (swimming, walking, rolling, jumping, crawling) or potential energy changes (climbing meniscus).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify efficiency. Magnetic actuation at this scale, especially using external electromagnets creating largely uniform fields for torque (not gradients for force, which is noted as less efficient but potentially useful supplementarily), is generally inefficient. Significant energy is likely lost in generating the magnetic fields over the required workspace volume, and further losses occur during transduction within the robot (material hysteresis) and interaction with the environment (viscous drag, friction). The score reflects the expected low efficiency characteristic of such systems, though the paper itself doesn't provide data. The paper mentions torque-based actuation is more efficient than gradient-based pulling (Supplementary Information S15), but provides no absolute efficiency value.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation mechanisms include:
        1.  Viscous drag during swimming in liquid (mentioned implicitly via Reynolds number discussion).
        2.  Friction during walking, rolling, and crawling on solid surfaces (implicit).
        3.  Internal material damping/hysteresis within the elastomer during cyclic deformation (implicit).
        4.  Heat loss (I²R) in the electromagnets generating the fields (external to the robot, implicit).
        5.  Energy lost during impact in jumping (implicit).
        The paper does not quantify these mechanisms. Dissipation due to fluid interaction is likely significant during swimming (Reynolds number 74-190 indicates inertial effects but also viscous forces). Friction is necessary for walking/rolling/crawling but also dissipative. Material damping is inherent to elastomers. Overall dissipation pathways are qualitatively identifiable but not quantified. Assessment: Medium to High, depending on the mode and environment.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Skipping M3.2-M3.8 as M3.1 is "No")**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Skipping M4.2-M4.7 as M4.1 is "No")**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Skipping M5.2-5.4 as M5.1 is "No")**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Jellyfish Swimming Cycle | ~40 | ms | Fig 2a | Explicit | Time difference between initial and final state shown. |
        | Meniscus Climbing | ~12 | s | Fig 2b | Explicit | Time difference between initial and final state shown. |
        | Landing (Peeling) | ~7.5 | s | Fig 2c | Explicit | Time difference between initial and final state shown. |
        | Immersion | ~3 | s | Fig 2d | Explicit | Time difference between initial and final state shown. |
        | Rolling Step (Partial Rotation) | ~8 | s | Fig 2e (for ~270 deg) | Explicit | Time shown for part of a full rotation. |
        | Walking Cycle | ~210 | ms | Fig 2f | Explicit | Time difference between initial and final state shown. |
        | Crawling Cycle | ~20 | ms | Fig 2g | Explicit | Time difference between initial and final state shown. |
        | Jumping Sequence | ~50 | ms | Fig 2h | Explicit | Time difference between initial and final state shown. |
        | Cargo Release | Implicitly Fast | ms/s | Fig 4d, Text (p.84) | Implicit | Described as "quickly reversed" field. |
    *   **Note:** Timescales are indicative based on figure sequences. Actual cycle/gait frequencies may vary and are detailed further in Supplementary Information.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Skipping M7.2 as M7.1 is "No")**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The system exhibits multiple distinct locomotion behaviors:
        1.  **Swimming (Submerged):** Jellyfish-like propulsion using alternating 'C' and 'V' shapes (Fig 2a).
        2.  **Surface Swimming:** Undulating gait on liquid surface (Fig 3a).
        3.  **Meniscus Climbing:** Deforming into 'C'-shape to utilize buoyancy and surface tension (Fig 2b).
        4.  **Landing:** Transitioning from liquid surface to solid via peeling rotation (Fig 2c).
        5.  **Immersion:** Transitioning from liquid surface to bulk via curling/rotation (Fig 2d).
        6.  **Rolling:** Moving on solid surface in a curled 'C'-shape via rotating field (Fig 2e).
        7.  **Walking:** Inchworm-like gait with anchoring and body extension/contraction (Fig 2f).
        8.  **Crawling:** Undulating gait within a tunnel (Fig 2g).
        9.  **Jumping:** Using deformation and rotation to generate impulsive force against a substrate (Fig 2h).
        Also demonstrates basic manipulation:
        10. **Pick-and-Place:** Gripping/releasing object via curling/uncurling (Fig 4c).
        11. **Cargo Release:** Ejecting strapped cargo via rapid shape change (Fig 4d).
        These behaviors arise from the interaction of the programmed magneto-elastic material with specific, externally applied magnetic field sequences. While complex, they are designed responses rather than spontaneously emergent in an unpredictable way from local rules alone.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper validates the behaviors primarily through experimental demonstration, captured via video and presented as image sequences (Figs 2, 3, 4). The link between specific, externally applied magnetic field sequences (described qualitatively and referencing Supplementary Information) and the resulting behavior is shown. Control is achieved by modulating the external field. Reproducibility is implied by showing multiple cycles or consistent execution (e.g., Fig 2f walking sequence). Theoretical models are presented (and validated against experiments in Supplementary Information S4-S8, S12) to explain the mechanisms underlying some modes (jumping, rolling, walking, meniscus-climbing, undulating swimming), providing further validation of the physics. Limitations are acknowledged (e.g., model mismatches for undulating swimming due to pre-stress). The paper doesn't frame these as computationally 'emergent' but as controllable physical phenomena.

---

#Key: [kwak_optimal_2021]

# Optimal design and experimental verification of piezoelectric energy harvester with fractal structure

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a piezoelectric energy harvester designed to improve energy conversion efficiency from multi-directional (specifically rotating) vibrations. It utilizes a novel fractal structure, composed of stacked arc circle shapes, intended to maximize energy transfer efficiency to piezoelectric elements even when vibration forces are not perfectly aligned vertically. Key components include the piezoelectric elements, the specifically designed fractal structure (optimized via curvature and L/H ratio analysis, fabricated from aluminum), and supporting structures (e.g., integrated into a modified bearing system). The purpose is to efficiently harvest mechanical vibration energy, particularly from rotating machinery like bearings, and convert it into electrical energy, potentially for powering wireless sensors. The study analyzes the electromechanical coupling, investigates nonlinearities (hysteresis due to slip friction), and verifies performance through single-mode and bearing-rotor system experiments.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters selected represent key design, material, operational, and performance aspects discussed. Reliability is generally high as values are explicitly stated or directly measured/used in simulations presented.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is mechanical vibration, specifically multi-directional vibrations arising from rotating machinery (e.g., bearings) or applied via an exciter in experiments.
    *   Value: Variable frequency (5-200 Hz), Variable amplitude (e.g., 150 mVpp input to exciter, Imbalance mass 280 g.cm for rotor tests)
    *   Units: Hz (frequency), Force (N), Acceleration (m/s²), Velocity (m/s)

### **2.2 Energy Transduction**

    *   Content: Mechanical energy from vibrations is transferred through the fractal structure. The fractal structure focuses and transmits the force onto the piezoelectric elements. The piezoelectric elements, under mechanical stress/strain induced by the transmitted force, convert the mechanical energy into electrical energy via the direct piezoelectric effect. This electrical energy is characterized by output voltage (V) and power (P).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper highlights improved power efficiency compared to single-directional systems, achieving up to 7 mW around 50 Hz (Abstract). The fractal structure is optimized for force transmission efficiency (Sec 2.1, Fig 5). Efficiency is discussed qualitatively regarding coupling coefficients and nonlinearities. Specific overall efficiency percentages (Electrical Power Out / Mechanical Power In) are not explicitly calculated across all conditions, but the focus is on *improving* efficiency via the fractal design and understanding limiting factors (coupling, nonlinearity). The score reflects the demonstrated power output and optimization effort, tempered by the lack of explicit efficiency metrics and the presence of loss mechanisms. Max Power: 7 mW.

### **2.4 Energy Dissipation**

    *   Content: Several dissipation mechanisms are identified:
        1.  **Mechanical Damping (ds):** Intrinsic damping within the structure and piezoelectric material (Eq 1, 3). Assessed qualitatively via damping ratio (ζ) (Eq 7).
        2.  **Electrical Damping (de):** Damping related to the electrical circuit and energy conversion process (Fig 3, Eq 6, implied in Eq 4 denominator). Discussed in Sec 3.4 (correlation with power).
        3.  **Hysteresis/Nonlinearity:** Observed in force-displacement/velocity curves (Figs 10-13, 18, 19). Attributed primarily to slip friction (ΔEloss) between the fractal structure and piezoelectric elements due to imperfect assembly/contact, especially near resonance (Sec 3.4, Fig 20, Eq 7-9). This slip friction leads to energy loss. Quantified qualitatively (~40% nonlinearity in vertical vibration, ~20% in tangential).
        4.  Structural deformation/fatigue (mentioned as limitation in prior art, Sec 1, potentially present though structure designed for stability).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No". Skipping M3.2-M3.8.)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No". Skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No". Skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Input Vibration Period | 5 - 200 | ms | Sec 2.2, 3.2 (Derived from 5-200 Hz) | Implicit | Derived from explicitly stated frequency range. |
        | System Resonance Period (Approx) | 10 - 20 | ms | Sec 3.4, Fig 19 (Derived from ~50-100 Hz) | Implicit | Inferred from discussion of resonance phenomena and Fig 19 labeling. |
        | Experimental Duration (Rotor Test) | 150 | s | Fig 24 Caption | Explicit | Stated in figure caption. |
    *   **Note:** Key timescales relate to the input frequencies and the system's resonant behavior.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No". Skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behavior is the transduction of multi-directional mechanical vibrations into electrical energy using piezoelectric elements integrated with an optimized fractal structure. A secondary observed behavior is nonlinear force-displacement/velocity response, including hysteresis, particularly near resonance, attributed mainly to slip friction. The system also influences the dynamics (stiffness, damping) of the larger rotor-bearing system it is integrated into (Sec 4.2).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary behavior (energy harvesting) is validated through direct experimental measurements of output voltage and calculated power under controlled single-mode excitation (varying frequency, force angle, contact area; Figs 8, 10-15, Appendix D) and within a bearing-rotor system (Fig 24). Nonlinear behavior (hysteresis) is validated through force vs. displacement/velocity measurements using load cells and LDV (Figs 10-13, 18, 19). The effect on system dynamics is validated via impact tests and rotordynamic analysis/simulation (Figs 22, 23). Control experiments implicitly compare performance under different conditions (e.g., angle 0° vs 45°). Reproducibility isn't explicitly discussed but standard experimental methods are used. Limitations include potential variability due to assembly conditions (Sec 3.4).

---

#Key: [kaspar_rise_2021]

# The rise of intelligent matter

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: This perspective paper reviews the progress towards "intelligent matter" – synthetic materials exhibiting basic features of intelligence, defined as the ability to perceive information, retain it as knowledge (memory), and apply it towards adaptive behaviour. It proposes a hierarchical classification: Structural -> Responsive -> Adaptive -> Intelligent Matter, based on the integration of four key functional elements: sensors, actuators, a communication network, and long-term memory. The paper discusses implementations using molecular systems (e.g., self-replicators, reaction networks), soft materials (e.g., hydrogels, elastomers, artificial muscles/skin), and solid-state materials (e.g., phase-change materials, 2D materials, nanoparticle networks, photonic systems). The purpose is to outline a development trajectory, provide examples, and identify challenges and future directions for creating matter capable of distributed information processing, learning, and self-regulation, with applications in soft robotics, adaptive skins, and neuromorphic computing.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** The paper is a review and does not present a single experimental system with its own specific parameters. It cites parameters from other works within the text, but does not consolidate key parameters for a representative system.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The paper discusses various energy inputs used to stimulate or power the reviewed material systems. These include light (photons), electrical current/voltage, force/pressure (mechanical energy), heat (thermal energy), magnetic fields, and chemical potential (e.g., reactants, fuel). Specific examples mention light (e.g., photoresponsive composites, phototactic colloids), electrical current/voltage (e.g., phase-change materials, artificial muscles), force (e.g., triboelectric skin), heat (e.g., shape memory composites, artificial muscles, associative learning hydrogels), magnetic fields (e.g., microswarms, Janus colloids), and chemical reactions (e.g., enzyme-powered motility, self-replicating molecules, homeostatic materials).

### **2.2 Energy Transduction**

    *   Content: The paper describes energy transduction as a core aspect of functionality. Examples include:
        *   Sensing: Conversion of input signal energy (heat, light, pressure) into another form processable by the material (electrical potential, molecular structure change, resistance change). (Box 1, Fig 3c, Fig 5a,d).
        *   Actuation: Conversion of input energy (heat, light, electrical) into mechanical work (shape change, motion), optical changes (color), or property changes (conductivity, phase). (Fig 3a,f).
        *   Power Generation: Conversion of mechanical energy (triboelectric effect, hydrogel deformation) or chemical gradients (ion gradients) into electrical power. (Fig 3b, Ref 44).
        *   Computation/Memory: Conversion of electrical or optical energy into material phase changes (phase-change materials) or resistance changes (memristors), modulating subsequent energy transmission (light absorption, electrical conductance). (Fig 5a,b).
        *   Chemical Reactions: Driving chemical reactions (e.g., catalysis) using input energy (light, heat) or releasing chemical energy (exothermic reactions). (Fig 4a,b).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper mentions energy efficiency as a motivation (e.g., brain vs. conventional computing) and highlights low power consumption for specific approaches (e.g., photonics). However, it does not provide quantitative efficiency metrics or a comparative analysis across the reviewed systems. Assigning a single score for the diverse systems reviewed is not feasible or meaningful based solely on this text. Qualitative mentions suggest efficiency is a target, particularly for neuromorphic/photonic approaches ('extremely low power consumption').

### **2.4 Energy Dissipation**

    *   Content: The paper implicitly addresses dissipation in several contexts. Joule heating is mentioned for phase-change material switching. Heat dissipation is mentioned in the context of the von Neumann bottleneck and homeostatic materials (Fig. 4a). Friction/viscous forces are implicit in descriptions of microswarms and soft robot motion. Out-of-equilibrium systems (active matter) inherently involve energy dissipation to maintain their state (Ref 18, 33). However, the paper does not systematically identify or quantify specific dissipation mechanisms across the reviewed examples. Qualitative assessment: Dissipation is present (and sometimes functional, e.g., Joule heating for switching, necessary for active matter), but often represents energy loss (e.g., heat in computation).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding to M3.2 and M3.3.)**

### **3.2 Memory Type:**

    *   **Chemical/Structural (Soft Matter):** Hydrogels using nanoparticle clustering (Ref 52) or liquid crystal network alignment changes (Ref 53) store learned associations. Motion memory devices use resistance switching on rigid islands (Ref 45). Polymer self-healing erases memory of damage (Ref 46-49). (Scores vary, potentially lower retention/fidelity than solid-state options, ~4-6). Fig 3c,d,e,f.
    The review covers memory ranging from non-volatile, potentially multi-level solid-state options to more transient or chemically encoded soft-matter implementations.

### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", proceeding to M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The paper describes local interaction rules qualitatively for several examples:
        *   **Robot Swarms (Ref 26):** Individual robots follow programmed algorithms and communicate only with nearest neighbours.
        *   **Paramagnetic Nanoparticle Swarms (Ref 27):** Repulsive fluidic and attractive magnetic interactions between chain-forming nanoparticles. Dependence on oscillating magnetic field parameters.
        *   **Colloidal "Living Crystals" (Ref 28):** Osmotic and phoretic effects steer interactions upon light illumination.
        *   **Magnetic Janus Colloids (Ref 29):** Interactions governed by precessing magnetic field.
        *   **Hierarchical Microswarms (Ref 30):** Interactions based on size/dielectric properties under AC electric fields or UV light.
        *   **Dissipative Self-Assembly (Ref 33):** Interactions via time-oscillatory potentials.
        *   **Molecular Replicators (Ref 34, 35):** Coupled chemical reactions, intermolecular interactions, competition for feedstock.
        *   **Mn12/DNA Network (Ref 86):** Nonlinear current-voltage characteristics in a self-organized redox network.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID          | Description                             | Parameter Name                  | Parameter Value Range | Units   | Data Source       | Implicit/Explicit | Justification                                     |
    | :--------------- | :-------------------------------------- | :------------------------------ | :-------------------- | :------ | :---------------- | :---------------- | :------------------------------------------------ |

### **4.3 Global Order:**

    *   Content: The paper describes various emergent global orders:
        *   **Swarm Patterns:** Complex 2D shapes (robots, Fig 2a), ribbon-like microswarms capable of splitting/merging (nanoparticles, Fig 2b), flocking and cargo transport (colloids, Fig 2c), pattern formation for protection (natural swarms).
        *   **Crystalline Structures:** 2D "living crystals" (colloids, Ref 28), 3D crystals/microtubes (Janus colloids, Ref 29).
        *   **Dynamic Patterns:** Oscillatory motion (enzyme protocells, Fig 4b), synchronized behavior (Janus colloids, Ref 29).
        *   **Molecular Organization:** Emergence of distinct replicator sets (Ref 35).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID          | Description                             | Parameter                      | Value Range | Units   | Implicit/Explicit | Justification                                     | Source            |
| :--------------- | :-------------------------------------- | :----------------------------- | :---------- | :------ | :---------------- | :------------------------------------------------ | :---------------- |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID     | Description           | Parameter                         | Value Range | Units   | Implicit/Explicit | Justification                                    | Protocol | Source         |
| :-------------- | :-------------------- | :-------------------------------- | :---------- | :------ | :---------------- | :----------------------------------------------- | :------- | :------------- |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", proceeding to M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Neuromorphic, Reservoir Computing, Analog (Optical/Physical), Digital (Boolean Logic), Hybrid. The paper covers multiple types:
        *   **Neuromorphic:** Explicitly discussed using phase-change materials, 2D materials, memristors to emulate neurons/synapses (Fig 5a-d).
        *   **Reservoir Computing:** Explicitly discussed using various physical systems (optoelectronic, memristor, atomic switch, carbon nanotube networks) leveraging their dynamics (Fig 5f).
        *   **Analog (Optical/Physical):** All-optical neural networks using diffraction (Ref 84), metastructures solving equations with microwaves (Ref 85), computation via physical processes/nonlinearities (Ref 80, 81, 86).
        *   **Digital (Boolean Logic):** Nanoparticle networks configured into Boolean logic gates (Ref 80, Fig 5e).

### **5.3 Computational Primitive:**

    *   Content: The paper mentions several computational primitives embodied in materials:
        *   **Synaptic Weight/Nonlinear Activation:** Emulated by phase-change materials or 2D devices in neuromorphic systems (Fig 5a-d). Sub-Type: Thresholding/Modulation.
        *   **Logic Gate:** Boolean gates (AND, OR, etc.) realized in nanoparticle networks (Fig 5e). Sub-Type: Logic Gate.
        *   **Nonlinear Projection/Filtering:** Intrinsic function of the 'reservoir' in reservoir computing (Fig 5f). Sub-Type: Nonlinear Transformation.
        *   **Diffraction/Interference:** Performs computations (e.g., matrix multiplication equivalent) in optical neural networks (Ref 84). Sub-Type: Wave Propagation/Interaction.
        *   **Integral Operator:** Implemented by microwave interaction with metastructures (Ref 85). Sub-Type: Mathematical Operator.
        *   **Classification/Feature Extraction:** Performed by disordered dopant networks (Ref 81) or image sensors (Ref 79). Sub-Type: Pattern Recognition.
        *   **Stochastic Resonance:** Observed in Mn12/DNA network (Ref 86). Sub-Type: Signal Processing.

### **5.4 Embodied Computational Units**
| Unit ID                | Description                                    | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification                                          |
| :--------------------- | :--------------------------------------------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :----------------------------------------------------- |

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description        | Value                            | Units     | Source          | Implicit/Explicit | Justification                                                  |
        | :--------------------------- | :------------------------------- | :-------- | :-------------- | :---------------- | :------------------------------------------------------------- |
        | Phase-Change Switching       | Fast (implicit)                  | ns-us?    | Ref 61, 64      | Implicit          | Mentioned as fast, specific values not given in review         |
        | Optical Computing Speed      | Speed of light (in medium)       | m/s       | Para before Ref 84| Explicit          | Explicitly stated                                              |
        | Reservoir Computing Dynamics | Comparable to input signals      | Varies    | Para before Ref 92| Explicit          | Explicitly stated requirement for short-term memory            |
        | Self-Healing Time            | ~14                              | hours     | Fig 3e, Ref 47  | Explicit          | Explicitly shown in figure/cited work                          |
        | Memory Retention             | Non-volatile to transient        | seconds-years | Section M3.3    | Mixed             | Explicitly mentioned non-volatile, transient inferred        |
    *   **Note:** The review mentions various processes occurring on different timescales, but rarely quantifies them directly. Values often need to be inferred or are qualitative.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", proceeding to M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: Mechanisms described include:
        *   **Feedback Loops:** Explicitly mentioned for adaptive matter. Examples: Chemo-mechano-chemical feedback loops in homeostatic materials (Ref 21, Fig 4a); coupled enzyme reactions regulating buoyancy (Ref 54, Fig 4b); coordination in robot/colloid swarms via nearest-neighbour communication/interaction (Ref 26, 30, 31).
        *   **Network Interaction:** Adaptive matter requires a network for feedback (Fig 1, Box 1). This can be physical connections (robot communication), chemical reaction networks (molecular systems, Ref 34-36), coupled physical interactions (colloids, swarms), or signal pathways in solid-state/photonic systems.
        *   **Learning Rules (Implicit in Neuromorphic/Memory Systems):** For systems classified as potentially "learning" or "intelligent," adaptation involves changes based on stored information (memory). Examples: Associative learning in hydrogels modifies material response (Ref 52, 53); neuromorphic systems adapt synaptic weights based on training data/algorithms (although the learning rule itself isn't detailed in this review, Refs 61, 64, 79); reservoir computing adapts output weights (Ref 118, 119); material learning in nanoparticle networks uses artificial evolution or gradient descent (Ref 80, 82).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The paper describes a wide range of functional behaviors arising from the reviewed material systems:
        *   **Shape Change/Actuation:** Lifting weights, mimicking muscle/hand gestures, gripping objects, self-folding (artificial muscles, hydrogels, shape memory composites; Fig 3a,f).
        *   **Locomotion:** Swarm movement, obstacle avoidance, navigated locomotion, phototaxis, oscillatory vertical movement (swarms, colloids, protocells; Fig 2b,c, Fig 4b).
        *   **Sensing:** Detecting proximity, contact, pressure, dampness, temperature, light, chemicals, stress (artificial skin, sensors embedded in materials; Fig 3b,c).
        *   **Self-Regulation/Homeostasis:** Maintaining temperature within a narrow range (chemo-mechano-chemical system; Fig 4a).
        *   **Self-Assembly/Pattern Formation:** Forming complex shapes, crystals, dynamic patterns (swarms, colloids; Fig 2a,b, Ref 28, 29).
        *   **Information Processing/Computation:** Performing logic operations, classification, signal processing, solving equations (neuromorphic systems, optical networks, material networks; Fig 5).
        *   **Memory/Learning:** Storing information, associative learning (phase-change materials, hydrogels, neuromorphic systems; Fig 3c,f, Fig 5).
        *   **Self-Healing:** Restoring material integrity after damage (polymers; Fig 3d,e).
        *   **Power Generation:** Generating electricity from contact or ion gradients (triboelectric skin, hydrogel stacks; Fig 3b, Ref 44).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: As a review paper, it primarily relies on the validation methods presented in the cited studies. The review itself does not perform new validations. It presents figures (e.g., Fig 2, 3, 4, 5) and cites references (e.g., Ref 21, 26, 27, 40, 43, 52, 53, 54, 64, 79, 80, 84) that presumably contain experimental data, simulations, and analyses validating the described behaviors. However, the review does not critically assess the rigor or limitations of the validation in the cited works. Claims of emergence are based on descriptions of global order arising from local rules (e.g., swarms, self-assembly). Reproducibility/reliability are generally implied by publication but not explicitly discussed comparatively.

---

#Key: [yang_physical_2021]

# Physical bioenergetics: Energy fluxes, budgets, and constraints in cells

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews the field of physical bioenergetics, focusing on biological cells as systems that function out of thermodynamic equilibrium. It discusses how cells harness energy flow (energy fluxes) derived from the environment (e.g., nutrients) via metabolic pathways (e.g., respiration, glycolysis) to convert Gibbs energy into usable forms (primarily ATP). This energy powers various cellular processes like biosynthesis, signaling, maintaining gradients, error correction, motility, gene regulation, and building structures (e.g., cytoskeleton, spindle). The paper explores methods to measure energy fluxes (global: calorimetry, respirometry; specific pathways: metabolic flux analysis, isotope labeling, fluorescence microscopy), estimates the energetic costs of key processes (e.g., protein synthesis, chromosome segregation, sensory adaptation), and examines the constraints imposed by energy fluxes on cellular functions (e.g., growth limits, motor efficiency, speed-accuracy trade-offs). The purpose is to highlight recent advances, open questions, and challenges at the interface of non-equilibrium physics, energy metabolism, and cell biology.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Kinesin Efficiency (Estimated) | ~20 | % | Section: Open Question: To What Extent Do Energy Fluxes Constrain Cellular Processes? (Ref 5) | Explicit | Medium | Cited experimental estimate |
        | F1-ATPase Efficiency (Suggested) | Near 100 | % | Section: Open Question: To What Extent Do Energy Fluxes Constrain Cellular Processes? (Ref 6, 7) | Explicit | Medium | Cited experimental suggestion |
        | KaiABC ATP Consumption (Measured) | 16 | ATP per monomer per cycle | Section: Open Question: What Are the Energetic Costs of Key Cellular Processes? | Explicit | Medium | Cited experimental measurement |
        | KaiABC ATP Consumption (Theoretical Min) | 2 | ATP per monomer per cycle | Section: Open Question: What Are the Energetic Costs of Key Cellular Processes? | Explicit | High | Based on known biochemistry |

    *   **Note:** The paper reviews various concepts and examples, often citing specific values from other studies. These parameters represent key quantitative points discussed. Reliability is generally 'Medium' as they are cited values, not primary data *from this paper*.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is chemical energy derived from the environment, typically in the form of nutrients (e.g., glucose). Gibbs energy is extracted from these nutrients through metabolic processes.
    *   Units: J/mol (Gibbs Energy)

### **2.2 Energy Transduction**

    *   Content: Energy from nutrients (Gibbs energy) is converted into intermediate forms usable by the cell, primarily through metabolic pathways like glycolysis and respiration. A key intermediate energy currency is ATP (adenosine 5'-triphosphate), generated via substrate-level phosphorylation or oxidative phosphorylation. The Gibbs energy stored in ATP's phosphate bonds is then transduced to power various cellular processes, including mechanical work (molecular motors), chemical synthesis (biosynthesis), maintaining concentration gradients (ion pumps), information processing, and heat generation. Energy flows from nutrients -> metabolic intermediates -> ATP -> cellular work/heat/waste.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper discusses thermodynamic efficiency specifically for molecular motors (kinesin ~20%, F1-ATPase ~100%) and mentions efficiency in the context of growth rate constraints and information processing trade-offs. It cites studies defining efficiency differently (e.g., useful energy dissipation / total energy dissipation for motors). However, it doesn't provide a single overall efficiency score for 'the cell' as a system, acknowledging complexity. It highlights efficiency as a key question and constraint. Qualitative assessment: Efficiency varies (Low for some processes like kinesin, potentially High for others like F1-ATPase) and is subject to trade-offs.

### **2.4 Energy Dissipation**

    *   Content: Energy is dissipated primarily as heat during metabolic reactions and ATP-consuming processes. The paper explicitly states that heat flux represents the enthalpic part of the global energy flux and can be measured using calorimetry. It also discusses energy dissipation in the context of non-equilibrium physics, mentioning stochastic thermodynamics providing lower bounds on entropy production and energy dissipation. Specific dissipation mechanisms include friction/viscous drag for molecular motors, heat loss from biochemical reactions (related to enthalpy changes, ΔH), and potential energy loss through processes like proton leak in mitochondria. Quantification is discussed via heat production rate (Watts or J/s) measured by calorimetry, or inferred from OCR. Applying thermodynamic uncertainty relations implies dissipation constrains fluctuations. Violation of fluctuation-dissipation relation used to quantify dissipation (Ref 5).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The paper does not explicitly detail the *specific* local interaction rules for the examples mentioned (spindle organization, active matter, KaiABC). It references active matter physics (Ref 12-16), which generally involves rules like self-propulsion, alignment/repulsion interactions between components (e.g., cytoskeletal filaments and motors), and interactions mediated by the surrounding fluid, all fueled by ATP hydrolysis. For KaiABC synchronization (Ref 78), the proposed local rules likely involve interactions between hexamers mediated by shared resources or signaling molecules, influenced by phosphorylation state and ATP consumption, but these rules are not detailed in this excerpt. For spindle self-organization, local rules involve motor protein interactions with microtubules, ATP-dependent force generation, and potentially reaction-diffusion dynamics of regulatory factors, but are not specified.
    * **Implicit/Explicit**: Implicit

### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**

    *   Content: For spindle self-organization, the emergent global order is the bipolar spindle structure itself. For active matter systems, it can be collective motion, pattern formation (vortices, bands), or dynamic steady states. For KaiABC, the emergent order is the synchronization of oscillations across the population of complexes.
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", including M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid (potentially involving threshold-like elements within analog dynamics). Sensory adaptation involves processing continuous signals, suggesting analog components. Kinetic proofreading involves discrete steps but operates within a stochastic chemical regime.

### **5.3 Computational Primitive:**

    *   Content: Examples include:
        *   **Filtering/Adaptation:** (e.g., in E. coli chemotaxis) Adjusting sensitivity to maintain response over a range of background signal levels, often involving feedback loops (Fig 3C, Ref 10).
        *   **Error Correction/Discrimination:** (e.g., Kinetic Proofreading) Enhancing the specificity of biochemical reactions (like DNA replication or translation) by introducing energy-consuming intermediate steps that allow preferential dissociation of incorrect substrates (Ref 8, 9, 97).
        *   **Signal Integration/Thresholding:** Implicit in many signaling pathways where multiple inputs are combined to produce an output, possibly involving cooperative binding or enzymatic thresholds.
    *   **Sub-Type (if applicable):** Filtering/Adaptation: Integral Feedback; Error Correction: Kinetic Proofreading.

### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
    *   **Note:** The paper mentions various processes with characteristic timescales but often doesn't provide specific values in this excerpt. Timescales are inferred from general biological knowledge of these processes.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: For sensory adaptation in E. coli chemotaxis (Ref 10), the mechanism involves a biochemical feedback network. Chemoreceptors detect ligands; their activity controls the phosphorylation state of CheY, which modulates flagellar motor bias. Adaptation occurs through enzymes CheR (methyltransferase) and CheB (methylesterase), which modify the methylation state of the receptors. Increased receptor activity (due to attractant binding) leads to increased CheB activity (demethylation), while less active receptors are preferentially methylated by CheR. This feedback loop adjusts the receptor's signaling output back towards a baseline level over time, even in the continued presence of the stimulus, effectively adapting the system's sensitivity. The paper notes energy (likely ATP or SAM for methylation) is required for this adaptation, and links adaptation speed and accuracy to energy dissipation (Fig 4C).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The paper discusses several emergent behaviors arising from energetic constraints and non-equilibrium dynamics:
        *   **Metabolic Switching:** Cells switching from respiration to fermentation at high growth rates (e.g., Crabtree/Warburg effect), proposed to be constrained by a maximal Gibbs energy dissipation rate (Ref 4, Fig 4A).
        *   **Limits on Motor Performance:** Constraints on the thermodynamic efficiency of molecular motors based on thermodynamic uncertainty relations, potentially explaining observed differences between kinesin and F1-ATPase (Ref 5-7, 95, Fig 4B).
        *   **Speed-Accuracy Trade-offs:** Constraints in information processing (e.g., DNA replication, sensory adaptation) where increased speed or accuracy requires higher energy dissipation (Ref 8-10, Fig 4C).
        *   **Growth Rate Limits:** The maximum rate at which cells can grow, potentially limited by energy supply or dissipation capacity (Ref 4, Fig 4A).
        *   **Spindle Self-Organization:** Formation of the mitotic spindle structure through the collective action of motors and microtubules powered by ATP (Ref 12, Fig 3A).
        *   **Synchronized Oscillations:** Emergence of global synchronization in systems like the KaiABC clock, potentially driven by energy dissipation (Ref 78).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper primarily reviews existing findings and proposes perspectives. Validation of the emergent behaviors described relies on the cited literature. For example:
         *   Metabolic switching constraint (Ref 4) is validated by fitting experimental data (growth rates, uptake rates) to a model incorporating a dissipation limit.
         *   Motor efficiency limits (Ref 5-7, 95) are validated through single-molecule experiments measuring work and ATP consumption, and comparison with theoretical bounds from stochastic thermodynamics.
         *   Speed-accuracy trade-offs (Ref 10) are validated by comparing theoretical predictions from models with experimental measurements of adaptation speed/accuracy under varying conditions (e.g., starvation).
         *   Spindle self-organization (Ref 12) is validated through microscopy observations combined with theoretical modeling (e.g., active matter physics).
         *   KaiABC synchronization (Ref 78) is validated by comparing theoretical models predicting synchronization based on energy cost with experimental measurements of ATP consumption.
     The review itself doesn't present new primary validation data but summarizes these external validations. Limitations often involve model assumptions or experimental precision.

---

#Key: [parr_active_2022]

# Active Inference: The Free Energy Principle in Mind, Brain, and Behavior

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is Active Inference, a theoretical framework presented as a normative approach to understand sentient behavior in living organisms (e.g., brains) and potentially artificial agents. It posits that these systems operate to minimize a quantity called variational free energy (an upper bound on surprise) through a continuous cycle of perception (updating internal beliefs/models based on sensory data) and action (sampling the environment to make data conform to beliefs/predictions). Key components include: a generative model (internal probabilistic representation of how sensations are caused), variational free energy (the quantity minimized), expected free energy (used for planning/policy selection), perception-as-inference, and action-as-inference. The purpose is to provide a unified theory explaining how organisms persist adaptively by minimizing the surprise of their sensory observations, encompassing perception, action, learning, planning, attention, and homeostasis.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name          | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :---------------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |


## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local rules are the message-passing algorithms derived from the free energy minimization principle applied to a specific generative model. These rules dictate how beliefs about a variable (or parameter) are updated based on beliefs about variables within its Markov blanket. Examples include variational message passing (Eq A.42) or gradient descent updates (Eq 4.13, B.5, B.6 for POMDPs; Eq 4.20, 4.21, B.46 for continuous models/predictive coding). These updates involve passing 'messages' (prediction errors, expectations) between 'units' (representing variables/beliefs) based typically on differences between current beliefs and predictions derived from connected units.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID            | Description         | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification                         |
    | :----------------- | :------------------ | :------------- | :-------------------- | :---: | :----------: | :----------------: | :------------------------------------ |
    | Prediction Error   | Difference signal   | Precision (Π)  | > 0                   | 1/Var | Ch 4, 5      | Explicit          | Weights the influence of errors.      |

### **4.3 Global Order:**

    *   Content: The emergent global order is the maintenance of the system's characteristic states within physiological and statistical bounds (homeostasis, allostasis), effectively resisting dissipation and surprise. This manifests as persistent, adaptive behavior where the agent actively samples and influences its environment to remain in preferred/predictable states defined by its generative model. It also includes coherent perceptual inference and goal-directed action selection emerging from local computations.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID         | Description                    | Parameter          | Value Range | Units | Implicit/Explicit | Justification                                    | Source    |
| :-------------- | :----------------------------- | :----------------- | :---------- | :---: | :----------------: | :----------------------------------------------- | :-------- |
| Belief Update   | Free Energy Gradient Descent   | Precision (Π, γ)   | > 0         | Varies| Explicit          | Determines influence of prediction errors/priors | Ch 4, 5   |
| Action          | Prediction Error Minimization  | Sensory Precision  | > 0         | 1/Var | Explicit          | Drives action to fulfill sensory predictions     | Eq B.48   |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID       | Description                             | Parameter                     | Value Range | Units | Implicit/Explicit | Justification                                       | Protocol       | Source  |
| :---------------- | :-------------------------------------- | :---------------------------- | :---------- | :---: | :----------------: | :-------------------------------------------------- | :------------- | :------ |
| System Integrity  | Maintenance within viable states        | Variational Free Energy (F)   | Minimized   | nats  | Explicit          | F is minimized to maintain organism's integrity   | Simulation     | Ch 3    |
| Homeostasis       | Regulation of physiological variables | Interoceptive Prediction Error| Minimized   | Varies| Explicit          | Autonomic reflexes minimize interoceptive errors  | Theory         | Sec 10.10 |
| Adaptive Behavior | Goal-directed action/perception       | Expected Free Energy (G)    | Minimized   | nats  | Explicit          | Policies minimizing G are selected                | Simulation     | Ch 2, 7 |
| Perception        | Coherent Inference of hidden states   | Posterior Belief Uncertainty  | Minimized   | nats  | Explicit          | Inference minimizes divergence term in F          | Simulation     | Ch 2, 4 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type       | Description                               | Predictability | Yoneda Score | Metrics                     | Implicit/Explicit | Justification                                       | Source  |
    | :-------------- | :---------------------------------------- | :------------- | :----------- | :-------------------------- | :----------------: | :-------------------------------------------------- | :------ |
    | Local-Global    | Local message passing -> Global behavior| High (via Sim) | 7            | FFE, Behavior Trajectory    | Implicit          | Global behavior is outcome of local rules, fidelity depends on model accuracy | Ch 4, 5 |
    | State-Behavior  | Inferred Beliefs -> Selected Actions    | High (via G)   | 8            | Policy Probability, Action  | Explicit          | Actions selected based on G, derived from beliefs | Ch 7, 8 |
    | Model-Emergence | Generative Model -> Self-Organization | Medium         | 6            | F, Attractor Dynamics       | Mixed             | FEP links model to self-organization, but specific emergent patterns hard to predict analytically | Ch 3    |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 7. Rubric: Score reflects how well the global behavior can be understood purely by composing the local interaction rules defined by the theory. Active Inference provides a strong framework (local message passing determines global inference/action), but complex emergent behaviors in intricate models might not be perfectly predictable solely from local rules without simulation (hence not 10). Score 0 = No relation; Score 5 = Global behavior qualitatively related; Score 10 = Global behavior perfectly derivable by composing local rules.
    *   **Metrics:** Variational Free Energy (as measure of global state), policy probabilities, action trajectories, posterior beliefs. Simulation outputs compared to theoretical predictions based on local rules (message passing equations).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Hybrid (Primarily Analog Probabilistic Inference, Neuromorphic analogies, can incorporate discrete elements)

### **5.3 Computational Primitive:**

    *   Content: Bayesian Belief Updating via Free Energy Minimization. This involves calculating prediction errors (difference between prediction and evidence/sensation) and updating expectations (beliefs about hidden states/parameters) based on these errors, weighted by precision. Mathematically, this is often implemented as gradient descent on free energy or specific message-passing rules (e.g., Eqs B.5, B.6, B.46).
    *   **Sub-Type (if applicable):** Prediction Error Calculation, Expectation Update, Precision Weighting.

### **5.4 Embodied Computational Units**
| Unit ID | Description                 | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification             |
| :------ | :-------------------------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :------------------------|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description        | Value             | Units   | Source            | Implicit/Explicit | Justification                                                    |
        | :--------------------------- | :---------------- | :------ | :---------------- | :----------------: | :--------------------------------------------------------------- |
        | Discrete Time Steps        | Model-dependent   | Time Step | Ch 4, 7           | Explicit          | Models like POMDPs evolve in discrete steps (τ).                  |
        | Continuous Time Dynamics   | Continuous        | Time    | Ch 4, 8           | Explicit          | Models like Predictive Coding evolve continuously (t).           |
    *   **Note:** The excerpt distinguishes between fast inference and slow learning, continuous and discrete time representations, and multiple timescales in hierarchical models. Specific numerical values are not provided as they depend on the system being modeled.

### **6.2 Active Inference:**

    *   Content: Yes
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** Prediction error magnitude/reduction rate (nats/s), posterior belief precision/entropy (nats), expected free energy components (epistemic vs pragmatic value, nats), policy entropy/precision (nats), time constants of belief convergence (s), correlation between simulated neural activity (e.g., prediction error units) and empirical data (e.g., fMRI BOLD, LFP power). Experiments could involve manipulating uncertainty or preferences and measuring resulting behavior and neural correlates, then fitting models (Ch 9).

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: Bayesian parameter estimation via variational free energy minimization. Beliefs about parameters (often represented by sufficient statistics like Dirichlet counts for categorical models or Gaussian means/precisions for continuous models) are updated based on prediction errors accumulated over time. For instance, Dirichlet parameters are updated by adding evidence (co-occurrence counts of states and outcomes, see Eq B.12). This can be viewed as a form of Hebbian learning ("cells that fire together wire together" analogy, Sec 5.5) but derived from Bayesian principles. It updates the mappings (e.g., A matrix) or dynamics (e.g., B matrix) of the internal model.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main emergent behaviors described are adaptive perception (inferring causes of sensations), goal-directed action selection (including exploration/exploitation balance), planning (selecting sequences of actions based on expected outcomes), learning (updating model parameters), attention (precision optimization), and homeostatic/allostatic regulation (maintaining physiological variables within preferred bounds). These arise from the single principle of minimizing variational/expected free energy. Examples range from simple foraging (Ch 7) and motor control (Ch 8) to complex sequences like reading (Sec 7.6) and communication (Sec 8.4).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary methods discussed for validating emergent behaviors are computational simulation (generating behavior from a specified model, e.g., Fig 7.7, 7.9, 7.11, 7.13, 8.3, 8.5, 8.7) and model-based data analysis (fitting models to empirical behavioral data to infer parameters and compare models, Ch 9). Reproducibility is demonstrated through providing code/equations (Appendix C). Robustness is implicitly tested by showing functional behavior under uncertainty (inherent in POMDPs/stochastic models). Limitations include the reliance on the chosen generative model's validity and the challenge of analytically proving emergence in complex systems.

---

#Key: [sabelhaus_-situ_2022]

# In-Situ Sensing and Dynamics Predictions for Electrothermally-Actuated Soft Robot Limbs

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### 1.1 System Description

    *   Content: The system is a planar soft robot limb actuated by an antagonistic pair of Shape Memory Alloy (SMA) wire coils embedded within a bulk silicone body. The limb includes embedded sensors: thermocouples attached to each SMA coil for temperature sensing and a soft capacitive bend sensor for angular deflection (θ). Actuation is achieved via electrical current controlled by Pulse-Width Modulation (PWM) signals applied to the SMAs, causing Joule heating and contraction. The purpose is to develop and validate a framework combining in-situ sensing (especially temperature) with a machine learning model (LSTM neural network) to accurately predict the dynamic motion (bending angle) of the electrothermally-actuated limb, particularly addressing the challenges of hysteresis and complex interactions in SMA actuators. The model predicts limb deflection based on PWM inputs and/or temperature sensor readings.

### 1.2 Implementation Clarity


### 1.3 Key Parameters

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Other parameters like LSTM training specifics (epochs, batch size, learning rate) are also mentioned but these 5 cover key hardware and model aspects.

## M2: Energy Flow

### 2.1 Energy Input

    *   Content: Electrical energy supplied by a benchtop power supply, controlled via Pulse-Width Modulation (PWM) signals to N-channel power MOSFETs which regulate current flow to the SMA wires.
    *   Value: 7 (Nominal Voltage)
    *   Units: V

### 2.2 Energy Transduction

    *   Content: Electrical energy applied to the SMA wires is converted into thermal energy via Joule heating. This thermal energy induces a phase transformation in the SMA material, causing it to contract and generate mechanical stress/strain. This mechanical energy performs work, causing the soft silicone limb structure to bend (deflect).

#### 2.3 Energy Efficiency

    *   Justification/Metrics: The paper does not quantify energy efficiency. However, electrothermal actuation, particularly using SMAs heated via Joule heating, is known to be generally energy inefficient due to significant heat loss to the surroundings and the energy required for the phase transformation hysteresis loop. The primary focus is on modeling and prediction, not efficiency optimization. Score assigned based on general knowledge of SMA actuators. Qualitative Assessment: Low.

### 2.4 Energy Dissipation

    *   Content: Energy is dissipated primarily as heat loss from the SMA wires and the silicone body to the surrounding environment via convection and radiation. There is also internal energy dissipation within the SMA material due to the hysteresis inherent in its phase transformation cycle. Viscoelastic damping within the silicone body also dissipates mechanical energy during motion. Resistance in wiring and MOSFETs causes minor electrical energy dissipation as heat. Quantification is not provided. Qualitative Assessment: High (primarily heat loss).

## M3: Memory

### 3.1 Memory Presence:

    *   Content: Yes

### 3.2 Memory Type:


### 3.3 Memory Retention Time:


### 3.4 Memory Capacity (Optional - if applicable)


### 3.5 Readout Accuracy (Optional - if applicable)


### 3.6 Degradation Rate (Optional - if applicable)

### 3.7 Memory Operations Energy Cost (Optional - if applicable)
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### 3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### 4.1 Self-Organization Presence:

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### 5.1 Embodied Computation Presence:

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### 6.1 Timescales:

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | LSTM Input Time Window | 2 (t, t-1 steps) | timesteps | Eq (11), Sec 2.4 | Explicit | Eq (11) shows inputs from t and t-1. |
        | Controller Hold Times | 1-30 | s | Sec 2.2 | Explicit | Explicitly stated range used for data collection. |
        | Open-Loop Prediction Horizon | ~10 (or more) | min | Abstract, Sec 1.1, Sec 4.1, Fig 7 | Explicit | Explicitly stated duration of successful prediction rollouts. |
        | LSTM Training Epochs | ~10-20 | epochs | Sec 2.4 | Explicit | Stated number of epochs used for training. |
        | Dynamics Observation/Operation | Seconds to Minutes | s, min | Figs 4, 7, Sec 2.2 | Explicit | Deduced from data plots (Fig 4 spans ~500s) and prediction horizon (Fig 7 shows 600s = 10min). |

### 6.2 Active Inference:

    *   Content: No

## M7: Adaptation

### 7.1 Adaptive Plasticity Presence:

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### 8.1 Behavior Description:

    *   Content: The primary functional behavior is planar bending/deflection of the soft limb structure. This motion is driven by the controlled contraction of one or both antagonistic SMA coil actuators in response to electrical input (PWM) and resulting temperature changes. The system can achieve unidirectional (single SMA active) or bidirectional (both SMAs active) bending.

### 8.2 Behavior Robustness:


### 8.3 CT-GIN Emergent Behavior Validation

     *  Content: The behavior (limb bending) is not claimed to be emergent in the sense of arising unexpectedly from complex local interactions. It's the designed function of the actuator system. Validation focuses on the *accuracy of the dynamics model* in predicting this behavior. This is done through:
         1.  One-step-ahead prediction tests (Sec 3, Fig 6).
         2.  Multi-step open-loop rollout simulations compared against ground truth sensor data (Sec 3, Fig 7).
         3.  Comparison with a simpler least-squares model (Sec 3, Table 1, Fig 8).
         Quantitative analysis uses RMSE (Root Mean Squared Error) between predicted and measured angles (Sec 2.4, Eq 13; Sec 3, Table 1). The validation demonstrates the model's ability to predict the designed behavior accurately, including hysteretic effects, but not the emergence of unexpected behaviors. Reproducibility is supported by open-sourcing the dataset and code (Data Availability Statement).

---

#Key: [louvet_reprogrammable_2024]

# Reprogrammable, in-materia matrix-vector multiplication with floppy modes

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a soft elastic metamaterial, experimentally realized using water-jet cut rubber, designed to perform matrix-vector multiplication (MVM) in-materia. It operates by utilizing floppy modes (near-zero energy deformation modes) arising from its specific geometric structure. The overall MVM is decomposed into a network of elementary operations, each implemented by a 2x2 metamaterial unit cell. Input vectors are applied as mechanical displacements to input rods, and the resulting output vector is measured as displacements of output rods. The system is designed to be reprogrammable, meaning the matrix elements (A_ij) it computes can be altered after fabrication using integrated bistable compliant mechanisms that selectively activate or deactivate constraints within the unit cells. Components include the patterned rubber sheet, stepper motors for input actuation, a camera with optical flow algorithm for output measurement, and bistable compliant mechanisms for reprogrammability. The purpose is to demonstrate a physical substrate for MVM for applications in embodied intelligence, smart MEMS, and in-sensor edge computing, leveraging the low energy requirements of floppy modes.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters selected characterize the physical implementation and performance from the experimental sections.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Mechanical energy introduced via prescribed displacements applied by stepper motors to the input degrees of freedom (input rods). The ultimate source is electrical energy powering the motors.

### **2.2 Energy Transduction**

    *   Content: Electrical energy in stepper motors is transduced into mechanical work to displace input rods. This mechanical work deforms the elastic metamaterial. Ideally, for floppy modes, minimal elastic potential energy is stored during this deformation. The deformation propagates through the structure according to the designed constraints, resulting in mechanical displacement of the output rods.

### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**

    *   Content: The primary identified dissipation mechanism is material hysteresis, attributed to the viscoelastic response of the polymer material (rubber) used for fabrication. Non-idealities in actuator-sample couplings are also mentioned, likely contributing frictional losses. These are qualitatively assessed (observed hysteresis in Fig 4a, 5a, 6b) but not quantified. Qualitative assessment: Present/Medium (Hysteresis clearly observed).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding with M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Short-term (Hysteresis); Long-term/Non-volatile (Bistable State)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 3 (for reprogrammable unit cell)
*   Units: distinct states (matrix coefficients)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", including M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog

### **5.3 Computational Primitive:**

    *   Content: Matrix-vector multiplication. Mathematically, ⃗y = A⃗x, where ⃗x is the input displacement vector, ⃗y is the output displacement vector, and A is the matrix encoded by the metamaterial's structure (specifically, the angles θ in the unit cells, tunable via bistable mechanisms). For the unit cell described by Eq. 1, the primitive is a specific 2x2 transformation.
    *   **Sub-Type (if applicable):** Linear Transformation

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
    *   **Note:** Relevant timescales are identified but not quantified in the paper.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is the computation of matrix-vector multiplication (⃗y = A⃗x) through mechanical deformation. Input displacements (⃗x) applied to specific points are transformed into output displacements (⃗y) at other points, governed by the programmed matrix A embedded in the material's structure via floppy modes. Secondary behaviors include nonlinear saturation of the output displacement at larger inputs (resembling a sigmoid function) and hysteresis in the input-output relationship. Reprogrammable units allow switching between different MVM behaviors (different matrices A).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary behavior (MVM) is validated through direct experimental measurement. Input displacements were applied using stepper motors, and output displacements were measured using a camera and optical flow algorithms (Section IV). The measured input-output relationship was fitted to a linear model, and the resulting matrix was compared to the target design matrix, quantifying the error (Fig 4a, 5a). FEM simulations were used to validate the design principles, predict behavior with non-ideal beam properties (finite stiffness), and explore limitations (aspect ratio vs. matrix size, Fig 3). Nonlinear saturation and hysteresis were also experimentally observed and characterized (Fig 4a). Validation appears robust for the demonstrated scales and conditions. Limitations (scalability, hysteresis effects) are acknowledged.

---

#Key: [khalil_precise_2015]

# Precise Localization and Control of Catalytic Janus Micromotors Using Weak Magnetic Fields:

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of spherical Janus micromotors (5 µm diameter) made of silica (SiO2) colloids, half-coated with multiple layers including Cobalt/Platinum (Co/Pt) for magnetic alignment and a final Platinum (Pt) layer for catalysis. These micromotors are suspended in a hydrogen peroxide (H2O2) solution. Their purpose is to achieve autonomous motion via catalytic decomposition of H2O2 and precise localization/control in 2D and 3D spaces using external, controlled weak magnetic fields generated by electromagnetic coil systems. The motion is tracked using microscopy, and a closed-loop control system adjusts the magnetic fields to guide the micromotors towards reference positions. Potential applications mentioned include targeted drug delivery.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name             | Value            | Units        | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------------------- | :--------------: | :----------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source for self-propulsion is the chemical potential energy released during the catalytic decomposition of hydrogen peroxide (H2O2) into oxygen (O2) and water (H2O) on the platinum surface of the Janus micromotor. Electrical energy is also supplied to the electromagnetic coils to generate the controlling magnetic fields.

### **2.2 Energy Transduction**

    *   Content: 1. Chemical energy (H2O2) is converted into kinetic energy (micromotor self-propulsion) via asymmetric catalytic reaction on the Pt surface, creating chemical gradients and fluid flow. 2. Electrical energy supplied to the coils is converted into magnetic field energy. 3. This magnetic field energy exerts a torque on the micromotor's magnetic dipole, converting magnetic potential energy into rotational kinetic energy for alignment, and a translational force (via field gradients), converting magnetic potential energy into translational kinetic energy (though the paper argues propulsion dominates over magnetic force for translation).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify the efficiency of converting chemical energy (H2O2) to kinetic energy. Based on typical values for catalytic micromotors, this efficiency is known to be extremely low (<<1%). The magnetic control efficiency is also not quantified but likely low, considering energy dissipated in coils vs. energy imparted to the particle. Score reflects the propulsion efficiency. Qualitative Assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: The primary dissipation mechanism is viscous drag from the fluid (hydrogen peroxide solution). The drag force is explicitly calculated (Eq. 2, Table 1) as 1.32 x 10^-12 N at 14 µm/s. Rotational drag is mentioned (Eq. 1, term αω) but not quantified. Heat generation occurs during the catalytic reaction (exothermic) and resistive heating in the electromagnetic coils (Joule heating), but these are not quantified.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description      | Value        | Units | Source     | Implicit/Explicit | Justification                                       |
        | :------------------------- | :----------: | :----: | :---------: | :----------------: | :-------------------------------------------------- |
        | Micromotor Speed (Avg 2D)  | 13 ± 7       | µm/s  | Sec 3.1     | Explicit          | Average speed derived from experiments.         |
        | Micromotor Speed (Avg 3D Z↓)| 19.1         | µm/s  | Sec 3.2     | Explicit          | Average speed derived from experiments.         |
        | Micromotor Speed (Avg 3D Z↑)| 9.8          | µm/s  | Sec 3.2     | Explicit          | Average speed derived from experiments.         |
        | Control Loop Update (2D)   | ~67 (1/15)   | ms    | Table 2     | Implicit          | Inferred from camera frame rate (15 fps).        |
        | Control Loop Update (3D)   | ~8.3 (1/120) | ms    | Table 2     | Implicit          | Inferred from camera frame rate (120 fps).       |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors are: 1. **Self-Propulsion:** Autonomous motion driven by catalytic decomposition of H2O2 (speeds ~10-20 µm/s). 2. **Magnetic Alignment:** Orientation of the micromotor's magnetic dipole along external magnetic field lines. 3. **Controlled Motion/Localization:** Directed movement towards specified reference positions in 2D and 3D space under closed-loop magnetic field control, achieving convergence within specific regions (ROC ~6-7 µm).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Behaviors are validated through direct experimental observation and measurement using microscopy and feature tracking. Self-propulsion speed is measured (Fig 4, Sec 3.1, Sec 3.2). Controlled localization is demonstrated via trajectories (Fig 4, Fig 5) and quantified using the Region-of-Convergence (ROC) metric (Sec 3.1, Sec 3.2). Control experiments isolating propulsion (zero field) vs. controlled motion (field applied) are shown (Fig 4). Reproducibility mentioned via averaging results over multiple experiments (Sec 3.1). Limitations: Validation is primarily within controlled lab conditions (capillary tube, specific H2O2 conc.). Emergence in the strict sense (arising from local rules without global control) is not claimed or validated for the controlled motion; rather, the behavior is directly imposed by the external controller.

---

#Key: [vihmar_how_2023]

# How to measure embodied intelligence?

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper proposes a conceptual framework for measuring Embodied Intelligence (EI) and introduces the idea of an "intel-unit" as the smallest entity displaying program execution beyond simple stimuli-responsiveness. It analyzes three case studies to explore EI emergence through situatedness: 1) A wild oat awn (non-living, hygroscopic structure) exhibiting humidity-driven movement for seed dispersal. 2) A 'fortune teller fish' (hygroscopic polymer film) showing humidity-dependent curling interpreted via a scale, potentially forming feedback loops with human users. 3) A knitted woollen sweater providing insulation via humidity-responsive fiber crimping, possibly adapting through wear. The purpose is to explore quantification metrics for EI, emphasizing the crucial role of the environment (situatedness) in defining functionality and executing the "program code" embodied in materials, structure, and their interaction.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source for the actuation described in the case studies is the environmental gradient driving water molecule absorption/desorption (change in chemical potential related to humidity). For the awn and fish (on a palm), this is environmental humidity. For the sweater, it's environmental humidity and potentially body heat/moisture from the wearer. The paper mentions the awn uses energy harvested from the environment for movement.

### **2.2 Energy Transduction**

    *   Content: The main energy transduction mechanism is chemo-mechanical. The change in chemical potential (humidity gradient) drives the absorption/desorption of water molecules into the hygroscopic materials (cellulose, sodium polyacrylate, wool keratin). This causes conformational changes at the molecular level (swelling/shrinking), which are amplified by the material's structure (helical awn, thin fish film, crimped wool fibers in knitted structure) into macroscopic mechanical motion (twisting, curling, crimping/change in loft).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss or provide any metrics for the energy efficiency of the transduction processes in the case studies. A qualitative assessment is not possible based on the provided text.

### **2.4 Energy Dissipation**

    *   Content: Dissipation mechanisms are not explicitly discussed or quantified. Potential mechanisms would include internal friction within the materials during deformation, friction with the environment (awn on soil, fish on palm, sweater fibers rubbing), and heat loss associated with the phase change of water during absorption/desorption. Qualitative assessment is Low/Medium depending on the specific interaction (e.g., awn movement through debris could involve significant friction).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes (Tentative/Implicit for Sweater)

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term (Qualitative)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes (Conceptual/Analogical)

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog / Other (Structural/Morphological Computation)

### **5.3 Computational Primitive:**

    *   Content: The paper does not define a single basic computational primitive. Based on the descriptions:
        *   Awn: Complex, environment-dependent spatio-temporal trajectory generation aiming at a goal (seed burial). Could be viewed as a complex transfer function mapping humidity input + environmental state -> movement output.
        *   Fish: Non-linear mapping (stimulus intensity -> shape change category) combined with a lookup table (shape category -> interpretation).
        *   Sweater: Adaptive regulation (humidity/temperature -> fiber crimp -> insulation level).
        *   Overall: The "computation" seems to be the complex, situated physical dynamics itself, rather than being decomposable into standard logic gates or clear mathematical operations. Perhaps best described as "Environment-Coupled State Transformation".

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Awn response time (example) | 10 - 700 | s | Fig 1 | Explicit | Data from figure caption showing twisting over time. |
    *   **Note:** The paper provides limited quantitative timescale information, mostly qualitative descriptions or illustrative examples.

### **6.2 Active Inference:**

    *   Content: Unclear/Partial

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes (for Sweater)

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism appears to be physical/structural adaptation. Prolonged wearing imposes stresses and strains, potentially leading to plastic deformation of fibers, rearrangement of the knitted structure, or felting (mentioned in Sec 2.3) that causes the sweater to conform to the wearer's body shape. It's driven by the physical interaction (environment = wearer's body) over time. It's not described in terms of known learning rules like Hebbian or reinforcement learning.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors described are environment-dependent actions enabled by the combination of material properties, structure, and situatedness:
        *   Awn: Humidity-driven twisting motion that, when interacting with a complex environment (soil surface), increases the statistical likelihood of seed burial (Seed Dispersal/Plantation).
        *   Fish: Humidity-dependent curling translated via a predefined scale into an indicator of the user's perceived emotional state or palm moisture level (Environmental/State Indication). Includes a potential feedback loop behavior when interacting with a human.
        *   Sweater: Humidity-responsive fiber crimping leading to changes in trapped air volume, resulting in adaptive thermal insulation (Thermoregulation). Includes structural adaptation to wearer shape over time (Conformation/Learning).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper validates claims of emergent behavior primarily through observation and conceptual argument.
         *   Awn: Describes observed "complex kinematic choreography" (Fig 1) and cites literature [7, 8, 9] supporting the seed dispersal function resulting from environmental interaction (Sec 2.1). Validation is inferential based on observed movement and cited biological function.
         *   Fish: Describes observed curling and references the accompanying scale/lookup table (Fig 2, Sec 2.2). Validation of the "mood telling" relies on the pre-existing product's premise; validation of the feedback loop is conceptual.
         *   Sweater: Describes the known mechanism of wool crimping for insulation based on fiber structure and humidity (Sec 2.3). Validation rests on established material science and textile properties, and the "learning" aspect is presented as an observation ("allows it to conform").
     *   Operational definitions are loose ("intel-unit," "program code"). No control experiments are presented within the paper. Quantitative analysis is minimal (awn timing example). Robustness/reliability/reproducibility are not systematically addressed. The main validation method is plausible mechanistic description coupled with functional interpretation based on context (biology, novelty item function, everyday object use).

---

#Key: [jiao_mechanical_2023]

```markdown
# Mechanical metamaterials and beyond

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper describes mechanical metamaterials as artificial materials engineered through the rational design of their microstructures to achieve unprecedented mechanical properties (e.g., ultra-lightweight, ultra-stiffness, negative Poisson's ratio, negative stiffness, programmable response). Components typically involve microstructural unit cells (e.g., chiral, lattice, origami/kirigami patterns) made from various base materials (metallic, polymeric). The purpose extends beyond passive mechanical properties to include multifunctionality like sensing, energy harvesting, actuation, adaptation, computation, and information processing, aiming towards intelligent mechanical metamaterials capable of interacting with their environment and adapting. The system is presented at multiple levels: material level (nm to µm, defines intrinsic properties), structural level (unit phase, µm to mm, microstructural units), and application level (overall phase, mm to m, functional devices).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                | Value        | Units     | Source (Fig/Table/Section)   | Implicit/Explicit   | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :---------------------------- | :----------: | :-------: | :---------------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
    *   **Note:** These parameters define the characteristic length scales and provide specific examples cited in the text for nanolattices. Reliability is 'Medium' for general scales (broad ranges) and 'High' for cited specific values.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy inputs discussed vary depending on the functionality. For basic mechanical properties, it's mechanical energy (stress/strain). For extended functionalities: mechanical (vibrations, waves, impact), thermal, electrical, magnetic fields, light (photons), acoustic energy. Specific examples include vibrations/waves for energy harvesting (piezoelectric/triboelectric), magnetic fields for actuation/memory, light for actuation (LCEs), thermal gradients for energy harvesting or actuation.

### **2.2 Energy Transduction**

    *   Content: Energy transduction mechanisms are central to the discussed functionalities:
        *   Mechanical -> Mechanical: Deformation, wave propagation, stiffness changes (intrinsic to metamaterials).
        *   Mechanical -> Electrical: Piezoelectricity, triboelectricity (for sensing/energy harvesting).
        *   Thermal -> Mechanical/Electrical: Shape memory effect (SMPs/SMMs), thermoelectric effects.
        *   Magnetic -> Mechanical: Magneto-mechanical coupling, magnetic actuation (for bistability, memory, actuation).
        *   Optical -> Mechanical: Photomechanical effect (e.g., LCEs for light-driven actuation).
        *   Electrical -> Mechanical: Electroactive polymers, dielectric elastomers.
        *   Various -> Thermal: Energy absorption/dissipation (e.g., impact resistance).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide quantitative data or a general assessment of energy efficiency across the diverse range of mechanical metamaterials and functionalities discussed. Efficiency is mentioned qualitatively for specific cited examples (e.g., "near unity efficiency" for a cited electromagnetic energy harvester, Ref 8), but no overall score can be assigned based on this review text alone. Assessment would require analysis of primary sources for specific systems.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation is mentioned implicitly as a function (energy absorption, vibration reduction, sound insulation - Table 1, Fig 2b) and potentially as a loss mechanism (e.g., material damping, friction within moving parts, heat loss during transductions). However, the paper does not quantify specific dissipation mechanisms or their magnitudes across different systems. Qualitative assessment: Likely varies significantly (Low to High) depending on the material, design (e.g., damping structures vs. efficient energy harvesters), and operating conditions.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**

    *   Structural Bistability/Multistability (e.g., origami, magnetic bits): Allows switching between discrete, stable states (Refs 71, 76, 128). High fidelity binary states demonstrated. Readout is possible (e.g., electrical signals correlated with mechanical state, Ref 23; reading magnetic state). Capacity depends on the number of elements (e.g., m-bits). Retention can be long-term (stable states). Score reflects the demonstration of re-writable, stable binary states.
    *   Shape Memory (SMPs/SMMs): Material remembers a programmed shape, recoverable via stimulus (e.g., heat). Retention can be long-term. Primarily single-shot or requires reprogramming. Lower versatility than bistable bits.
    *   Data Storage Metamaterials (Ref 160): Explicitly designed for digital information storage using self-recovering unit cells. Implies multiple re-writable states with potential for high capacity.
    The score of 6 acknowledges the existence of well-defined, re-writable, stable states (binary or shape) but recognizes that complex, high-capacity, easily accessible memory comparable to electronic systems is still emerging in this field, as suggested by the maturity levels (Fig 3b) and roadmap (Fig 5).

### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**

*   Units: bits (for binary systems), distinct states, information density

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Hybrid (Primarily Analog/Mechanical Logic, potentially interacting with Digital Interfaces)

### **5.3 Computational Primitive:**

    *   Content: Logic Gate (Boolean AND/OR/NOT implied by realizing 'all digital logic gates', Ref 23; Universal combinatorial logic and sequential logic (memory), Ref 129), Switching (e.g., acoustic switches, Ref 18), State transition (e.g., bistable elements for bits, Ref 128), potentially Wave-based operations (Ref 22, 213).
    *   **Sub-Type (if applicable):** Logic Gate: Various Boolean, Sequential; Switching; State Transition.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description         | Value   | Units                   | Source                    | Implicit/Explicit   | Justification                                  |
        | :---------------------------- | :-----: | :---------------------: | :------------------------: | :-----------------: | :--------------------------------------------- |
    *   **Note:** The paper operates at a high level and rarely specifies quantitative timescales. These would be found in the primary research papers cited.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: Mechanisms described include:
        *   **Stimulus-Responsive Materials:** Using functional materials (SMPs, SMMs, LCEs, electroactive polymers, magnetic materials, thermal materials) whose properties or shape change in response to specific stimuli (temperature, light, electric/magnetic fields), leading to adaptable overall behavior (Section 3, Table 2, Refs 13, 15, 22, 33, 34, 128, 129, 144-146). This is often pre-programmed responsiveness rather than learning from experience.
        *   **Reconfiguration:** Changing the geometry or connectivity, often through external actuation or stimuli (e.g., origami folding/unfolding, Ref 79, 81; magnetic switching of states, Ref 128; sequential excitation programming, Ref 129).
        *   **Tunability:** Adjusting properties like stiffness or Poisson's ratio actively or passively (Refs 47, 55, 56, 97-100, 128, 129).
        *   **AI-Driven Inverse Design:** While AI is used for *design optimization* (Section 3, AI-driven inverse design), the paper does not describe AI being *embodied* within the material itself to drive real-time adaptation or learning during operation.
        The adaptation described is primarily stimulus-driven reconfiguration or property tuning, rather than learning from experience in the sense of modifying internal models or rules based on performance feedback over time.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors are the manifestation of the designed mechanical properties (e.g., auxetic behavior, high stiffness-to-weight ratio, negative stiffness) and the intended functionalities: sensing physical quantities (strain, pressure), generating electrical energy from ambient motion (energy harvesting), changing shape or applying force (actuation), switching between stable states (memory), performing logical operations (computation), controlling wave propagation (filtering, guiding), adapting shape/properties to environmental conditions. These behaviors result from the designed microstructure and material composition.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

---

#Key: [cavagna_scale-free_2010]

```markdown
# Scale-free correlations in starling flocks

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system studied is a large flock of European starlings (Sturnus vulgaris). The primary purpose of the study is to understand collective response and information transfer within the group by analyzing velocity correlations between individual birds. The components are the individual birds. The study reconstructs the 3D positions and velocities of individual birds within large flocks (N=122 to 4,268) using stereometric imaging and computer vision techniques. It investigates how velocity fluctuations (deviations from the mean flock velocity) are spatially correlated, finding that the correlation length scales linearly with the flock size, indicating scale-free correlations. This suggests the flock behaves as a critical system, enhancing collective response to perturbations like predator attacks.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Correlation Function Decay Exponent (γ) | ~0.19 (for both orientation and speed) | dimensionless | Results, Insets Fig 3A & 3B | Explicit | Medium | Derived from fit of C'(x=1) vs ξ |

    *   **Note:** The value for γ is presented with caveats about the limited data range and alternative fits (logarithmic, constant).

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source for the system (the flock) is the metabolic energy derived from food consumed by the individual birds, which powers their flight muscles.

### **2.2 Energy Transduction**

    *   Content: The main energy transformation is the conversion of stored chemical energy (metabolic) within each bird into kinetic energy (manifested as velocity) and potential energy (changes in altitude, though likely minor compared to kinetic for level flight segments). This transduction occurs via muscle contractions powering wing beats. Energy is also transduced into heat and sound, though not discussed.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper provides no information to assess the efficiency of converting metabolic energy into the observed collective motion or information transfer. Quantifying this would require metabolic rate measurements and aerodynamic Kanalysis, which are outside the scope of the study.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation mechanisms include aerodynamic drag (friction with air), heat loss due to metabolic processes and muscle activity, and potentially sound production. The paper does not quantify these mechanisms. Qualitatively, aerodynamic drag is the primary dissipative force opposing flight.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The paper doesn't explicitly formulate the exact mathematical rules but refers to the underlying concept: "Interaction is local in space and its range is typically quite short." It cites a previous study (Ref 13) indicating interaction depends on topological distance (nearest neighbors) rather than metric distance, with an average interaction with approximately seven neighbors. The implied rule is likely related to alignment of velocity with neighbors, possibly with repulsion/attraction components not detailed here but common in flocking models (e.g., Refs 24, 25, 27 cited). The key aspects emphasized are the local nature and short range of *direct* interaction, contrasted with the long range of *correlation*.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | 1 | Topological Interaction Range | Number of Neighbors | ~7 | dimensionless | Results (citing Ref 13) | Explicit | Stated based on cited prior work. |

### **4.3 Global Order:**

    *   Content: The primary global order described is the high degree of alignment in the birds' velocities, quantified by the polarization parameter (Φ ≈ 0.96). Additionally, the paper identifies scale-free spatial correlations in velocity fluctuations as a key emergent global property. This means the correlation length (ξ) is not fixed but scales with the system size (L), and the asymptotic correlation function decays as a power law (C∞(r) ≈ 1/r^γ with small γ).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| 1 | Interaction Range | Topological Neighbors | ~7 | dimensionless | Explicit | Stated based on Ref 13. | Results (Ref 13) |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| GO1 | Velocity Alignment | Polarization Φ | 0.96 ± 0.03 | dimensionless | Explicit | Measured average value across flocks. | Eq. 1, Analysis | Results, Table S1 |
| GO2 | Correlation Structure | Correlation Length ξ | ξ ≈ 0.35 * L | m | Explicit | Derived from C(r)=0 condition, linear fit established. | Eq. 5, Analysis | Fig. 2C |
| GO3 | Correlation Decay | Asymptotic Exponent γ | ≈ 0.19 | dimensionless | Explicit | Derived from fit of C'(x=1) vs ξ. | Eq. 11, Analysis | Fig. 3 Insets |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    *   **Metrics:** Polarization (Φ), Correlation Length (ξ), Scaling coefficient (a in ξ=aL), Pearson correlation (r for ξ vs L fit), Power-law exponent (γ).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Data Acquisition Rate | 10 | frames/s (Hz) | Materials and Methods | Explicit | Rate at which flock configurations were recorded. |
    *   **Note:** The primary timescale explicitly mentioned relates to data capture. The analysis focuses on spatial correlations, sometimes averaged over time within an event, not explicitly on the temporal evolution *dynamics* of these correlations.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main emergent behaviors described are:
        1.  **Collective Motion/Flocking:** Individuals move in a highly coordinated, globally ordered state (high polarization Φ).
        2.  **Scale-Free Correlation:** Velocity fluctuations exhibit spatial correlations where the correlation length (ξ) scales linearly with the flock size (L), indicating the system's state influences behavior across the entire group, irrespective of size. This implies efficient information transfer across the flock.
        3.  **Collective Response (Implied):** The scale-free correlations are interpreted as the mechanism enabling rapid and coherent group-level responses to localized perturbations (like predator attacks), although the response itself is not directly analyzed in detail.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claim of emergent collective motion is validated by measuring the polarization parameter Φ across multiple flocks and finding it consistently high (Results, Table S1 cited). The main claim of emergent scale-free correlation is validated by: (1) Defining and calculating the correlation function C(r) for velocity fluctuations (Eq. 4, Figs 2A, S2). (2) Defining and calculating the correlation length ξ from C(r)=0 (Eq. 5). (3) Plotting ξ vs flock size L for 24 events and demonstrating a strong linear relationship (Fig 2C, r=0.98, P<10⁻¹⁶). (4) Analyzing the scaling function f(r/L) and the derivative C'(x=1) to estimate the power-law exponent γ (Eqs 9-11, Fig 3). (5) Using synthetic data to contrast scale-free vs non-scale-free scenarios (Fig 4). Reproducibility is demonstrated by consistency across multiple flocking events. Limitations might include potential biases in flock selection or measurement errors, although these are not discussed in detail in the provided excerpt.

---

#Key: [kriegman_scalable_2020]

# A scalable pipeline for designing reconfigurable organisms

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### 1.1 System Description
#### Content
The system is a scalable pipeline combining artificial intelligence (AI) design with biological realization to create novel, reconfigurable organisms ("biobots" or "xenobots"). It takes a desired behavior (e.g., locomotion) and biological building blocks (e.g., Xenopus laevis stem cells - passive epidermal and contractile cardiac progenitors) as input. An evolutionary algorithm running in a physics-based simulation (`in silico`) designs diverse candidate organisms composed of voxels representing these cell types. These designs are filtered for robustness to noise and manufacturability. Transferable designs are then physically constructed (`in vivo`) by harvesting, aggregating, and manually shaping/sculpting pluripotent stem cells and cardiac progenitor cells from Xenopus embryos. The pipeline includes a feedback loop where discrepancies between simulated and realized behavior inform constraints for subsequent evolutionary runs, aiming to improve transferability. The resulting organisms are millimeter-scale aggregates of cells capable of behaviors like locomotion, object manipulation/aggregation, and collective interaction, without nervous systems. The purpose is to create bespoke biological machines for functions like drug delivery, environmental remediation, or fundamental studies of morphogenesis and artificial life.
`SystemNode` attributes: `systemType`: Hybrid (AI Design + Biological Realization), `domain`: Bioengineering/Artificial Life/Robotics, `mechanism`: Evolutionary Algorithm + Physics Simulation + Cell-based Construction (Microsurgery/Sculpting), `components`: AI (Evolutionary Algorithm, Physics Simulator), Biological Materials (Xenopus laevis stem cells: epidermis, cardiac progenitors), Construction Tools (Forceps, Cautery Electrode), Environment (Petri dish, aqueous medium), `purpose`: Design and manufacture novel functional biological organisms. | Edges connect `SystemNode` to `ComponentNode`s (AI, Cells, Tools, Environment) and `FunctionNode` (Locomotion, Manipulation, etc.).
#### Implicit/Explicit
Mixed

### 1.2 Implementation Clarity
#### Score
8
#### Justification
The paper provides a clear overview of the pipeline (Fig. 1, SI Appendix Fig. S1) and details the evolutionary algorithm, simulation environment modifications (hydrodynamics, noise), filtering steps (robustness, build), and the biological construction process (cell harvesting, shaping, layering). Key steps like cell sourcing, tissue shaping, and behavior observation are well-documented (Fig. 3, Methods). However, some specifics of the evolutionary algorithm's genetic representation (SI Appendix, section S4) and the exact parameters of the physics simulation (SI Appendix, section S3) are primarily in the Supporting Information, slightly reducing clarity in the main text. The manual shaping step introduces variability not fully captured by the automated design process.
#### Implicit/Explicit
Mixed

### 1.3 Key Parameters
#### Table

| Parameter Name             | Value                     | Units          | Source (Fig/Table/Section)   | Implicit/Explicit   | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit)   |
| :------------------------- | :------------------------ | :------------- | :--------------------------- | :------------------ | :-----------------------------: | :--------------------------------- |

## M2: Energy Flow

### 2.1 Energy Input
#### Content
The primary energy source for the realized organisms is the intrinsic metabolic energy stored within the Xenopus laevis cells (initially yolk, then cellular metabolism). The paper states they function "without additional nutrients" for days or weeks, implying reliance on endogenous energy stores. For the computational part, the energy input is electrical energy for the computers running the simulations.
#### Value
#### Units
J (or W for computational power)
`EnergyInputNode`: attributes - `source`: Biological (Cellular Metabolism/Yolk); Electrical (Computation), `type`: Chemical; Electrical
#### Implicit/Explicit
Implicit (Biological); Inferred (Computational)

### 2.2 Energy Transduction
#### Content
The main energy transformation in the biological organisms is the conversion of stored chemical energy (ATP derived from metabolism) into mechanical work by the cardiomyocytes (heart muscle cells). This occurs through the process of muscle contraction, driven by actin-myosin interactions. This mechanical work generates forces that push against the environment (surface of the Petri dish), resulting in locomotion or object manipulation. Energy is also transduced into heat during metabolic processes and muscle activity. In the computational part, electrical energy is transduced into computational work and heat.
`EnergyTransductionEdge`: attributes - `mechanism`: Chemo-mechanical (Muscle Contraction via ATP hydrolysis); Electro-thermal (Computation), `from_node`: `EnergyInputNode` (Chemical/Electrical), `to_node`: `MechanicalWorkNode`, `HeatNode`, `ComputationNode`
#### Implicit/Explicit
Implicit (Biological mechanisms); Inferred (Computational)

### 2.3 Energy Efficiency
#### Score
1
#### Justification/Metrics
The paper does not provide any quantification of energy efficiency for the biological organisms. Muscle contraction efficiency (chemical to mechanical) is generally low (typically ~20-25% in biological systems, but likely much lower here given the artificial configuration and lack of optimized structures like tendons/levers). Locomotion in an aqueous environment at this scale using this method is likely very inefficient. The evolutionary algorithm selects for displacement, not energy efficiency, although constraints like limiting muscle percentage (due to metabolic cost) indirectly touch upon energy considerations. Score is low due to lack of optimization and inherent inefficiencies of biological motors at this scale/configuration.
Attribute `efficiency` of `EnergyTransductionEdge` (Chemo-mechanical)
#### Implicit/Explicit
Inferred

### 2.4 Energy Dissipation
#### Content
Major dissipation mechanisms include:
1.  **Heat:** Generated during cellular metabolism and muscle contraction (thermodynamic inefficiency). (Qualitative: High, inherent to biological processes).
2.  **Viscous Drag/Hydrodynamic Losses:** Energy lost to the aqueous medium as the organism moves through it. The paper explicitly models first-order hydrodynamics (drag force) in the simulation (Methods). (Qualitative: High, significant at this scale).
3.  **Internal Friction/Viscoelastic Losses:** Energy dissipated within the soft tissues during deformation and contraction cycles. (Qualitative: Medium/Low, likely less significant than drag).
4.  **Friction with Substrate:** Energy lost due to friction between the organism's ventral surface and the Petri dish. (Qualitative: Medium, depends on surface properties and contact area).
Quantification is not provided.
Creates `EnergyDissipationNode`s (Heat, Viscous Drag, Internal Friction, Substrate Friction) and `EnergyDissipationEdge`s connecting relevant nodes (e.g., `MechanicalWorkNode`, `SystemNode`) to these dissipation nodes.
#### Implicit/Explicit
Mixed

## M3: Memory

### 3.1 Memory Presence:
#### Content
Yes
#### Justification
The system exhibits memory in several forms:
1.  **Structural Memory:** The physical morphology of the fabricated organism represents a persistent state encoding the output of the design process. This structure dictates its potential behaviors.
2.  **Pipeline Memory:** The overall design pipeline incorporates memory by using constraints derived from previous design-build-test cycles to guide subsequent evolutionary searches (explicitly stated feedback loop G in Fig. 1).
3.  **Implicit Material Memory:** The biological tissue itself has inherent properties (e.g., viscoelasticity, potential for damage/repair) that might lead to history-dependent responses, though this is not the focus. The self-repair capability (SI Appendix, Fig. S9) implies a memory of the "correct" configuration.
However, the paper does *not* demonstrate classic learning or memory *within* the organism's behavior after it is constructed (e.g., habituation, associative learning). Memory is primarily structural or within the external design loop.
#### Implicit/Explicit
Mixed

**(Conditional: M3.1 is "Yes", include M3.2 and M3.3.)**

### 3.2 Memory Type:
#### Score
3
#### Justification
Defines `MemoryNode` types: `StructuralMemory`, `MorphologicalTargetMemory` (for self-repair), `PipelineParameterMemory` (external).
#### Implicit/Explicit
Mixed

### 3.3 Memory Retention Time:
#### Value
Days to Weeks (Structural/Morphological); Multi-generational (Pipeline)
#### Units
Time (Days/Weeks); Computational Cycles/Generations
#### Justification
The structural memory persists for the lifespan of the organism, stated as "days or weeks" (Abstract, Methods). Self-repair implies the target morphology memory also persists for this duration. The pipeline memory (constraints) persists across multiple generations/cycles of the design process.
#### Implicit/Explicit
Explicit (Organism lifespan); Mixed (Pipeline memory duration is implicit from description of iterative process)
Key attribute `retentionTime` of `MemoryNode` types.

### 3.4 Memory Capacity (Optional - if applicable)
#### Value
#### Units
#### Justification
For a single organism, the structural memory represents essentially one target state (the designed morphology). While self-repair implies robustness, it aims to restore this single state. Memory capacity isn't a primary focus or metric. Pipeline memory capacity relates to the complexity of constraints passed back, not quantified.
#### Implicit/Explicit
Inferred

### 3.5 Readout Accuracy (Optional - if applicable)
#### Value
#### Units
% (Error rate or Fidelity); Correlation/P-value
#### Justification
Memory readout accuracy isn't directly measured. However, the fidelity of sim-to-real transfer (Fig. 4, SI Appendix S9) assesses how well the "readout" of the structural memory (i.e., the organism's actual behavior) matches the intended behavior encoded by the design. The paper uses statistical tests (P<0.01 for direction match, P<0.001 for displacement difference upon inversion) to quantify this match/mismatch, indirectly reflecting the effectiveness of the structural memory readout through physical dynamics.
#### Implicit/Explicit
Mixed

### 3.6 Degradation Rate (Optional - if applicable)
#### Value
#### Units
#### Justification
The paper mentions a limited lifespan ("days or weeks") and self-limiting nature, implying eventual degradation, but the rate is not quantified. Self-repair mechanisms (SI Appendix, Fig. S9) counteract some forms of degradation/damage.
#### Implicit/Explicit
Implicit

### 3.7 Memory Operations Energy Cost (Optional - if applicable)
#### Table
#### Implicit/Explicit

### 3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)
#### Table
| :-------- | :------------------------------------------- | :----------- | :------- | :------------------------------------ | :------------ | :------------------ | :------------------------------------------------ |
| Sim2Real-Dir | Match btw Sim/Real movement direction      | P < 0.01     | P-value  | `MemoryReadoutEdge` attribute       | Fig 4, SI S9  | Explicit            | Statistical test result reported.               |
| Sim2Real-Inv | Displacement reduction upon inversion (Sim)  | P < 0.001    | P-value  | `StructuralMemoryNode` robustness | Fig 4, SI S9  | Explicit            | Statistical test result reported.               |
| Sim2Real-Inv | Displacement reduction upon inversion (Real) | P < 0.0001   | P-value  | `StructuralMemoryNode` robustness | Fig 4, SI S9  | Explicit            | Statistical test result reported.               |
| Self-Repair | Ability to close lacerations               | Qualitative  | Binary   | `MorphologicalTargetMemoryNode` fidelity | SI Fig S9     | Explicit (ref SI) | Described as automatically closing lacerations. |
#### Implicit/Explicit
Explicit

## M4: Self-Organization and Emergent Order

### 4.1 Self-Organization Presence:
#### Content
Yes
#### Justification
Self-organization occurs at multiple levels:
1.  **Tissue Formation:** Pluripotent stem cells, when dissociated and pooled, spontaneously aggregate and differentiate (into epidermis by default, guided towards cardiac tissue if progenitors included) to form a cohesive tissue mass (Fig 3A). This aggregation and initial fate specification rely on inherent cell-cell adhesion and signaling, not external templating at the cellular level during aggregation.
2.  **Self-Repair:** The ability of the organisms to autonomously close lacerations (SI Appendix, Fig. S9) indicates self-organization driven by local cellular behaviors (migration, adhesion) to restore structural integrity towards a target morphology.
3.  **Emergent Contractile Coordination:** Although cardiomyocyte signaling was *not* enforced in the design (actuation modeled as random noise), the paper notes that "emergent spontaneous coordination among the cardiac muscle cells produced coherent, phase-matched contractions which aided locomotion" in the realized organisms (Discussion). This is a functional emergent property from local cell interactions.
4.  **Collective Behaviors:** Behaviors like orbiting (temporary bonding and mutual rotation) and collective debris aggregation emerge from local interactions between multiple organisms (Fig 3F, SI S10, S11), not from a pre-programmed global plan.
The initial *shaping* by forceps/cautery imposes global form, but the underlying tissue cohesion and behavioral coordination involve self-organization.
#### Implicit/Explicit
Mixed

**(Conditional: M4.1 is "Yes", include M4.2-M4.7)**

### 4.2 Local Interaction Rules:
#### Content
The paper doesn't explicitly list all local interaction rules governing the *biological* self-organization, but they implicitly include:
1.  **Cell-Cell Adhesion:** Cells spontaneously aggregate and adhere (e.g., via cadherins) to form tissue. Strength/type depends on cell type.
2.  **Cell Signaling:** Implicit signaling pathways governing differentiation (e.g., default epidermal fate, cardiogenesis signals), coordination (e.g., gap junctions in cardiomyocytes enabling emergent electrical coupling), and repair (e.g., wound response signals). Notch signaling is explicitly manipulated to suppress cilia.
3.  **Mechanical Interactions:** Cells exert forces on each other (contraction, passive resistance). Tissue viscoelasticity.
4.  **Physical Laws:** Interactions with the environment (gravity, buoyancy, hydrodynamics/drag - explicitly modeled in simulation, present in vivo), surface friction.
For the *simulation*:
1.  **Voxel Connectivity:** Defines neighborhood relationships.
2.  **Physics Engine Rules:** Govern voxel movement, collisions, gravity, buoyancy, drag, material properties (passive vs. contractile actuation modeled as volume change), friction (implicit). Contractile voxels follow a sine wave actuation with random phase offsets (explicit).
Part of the `AdjunctionEdge` description. Defines edge types like `CellAdhesionEdge`, `CellSignalingEdge`, `MechanicalInteractionEdge`, `EnvironmentalInteractionEdge`, `SimulationPhysicsEdge`. Attributes include `interactionType`, `strength`, `range`.
#### Implicit/Explicit
Mixed

### 4.2.1 Local Interaction Parameters:
#### Table
| Rule ID                  | Description                          | Parameter Name         | Parameter Value Range   | Units   | Data Source          | Implicit/Explicit   | Justification                                                            |
| :----------------------- | :----------------------------------- | :--------------------- | :---------------------- | :------ | :------------------- | :------------------ | :----------------------------------------------------------------------- |
| Simulation Actuation     | Contractile voxel oscillation        | Frequency              | 2                       | Hz      | Methods              | Explicit            | Central pattern generator frequency.                                     |
| Simulation Actuation     | Contractile voxel phase              | Phase Offset StDev (Noise)| 0.4π                    | rad     | Methods (Robustness) | Explicit            | SD of normal distribution used for phase perturbation in robustness filter. |
| Simulation Environment   | Gravitational effect / Buoyancy    | g (effective)          | Decreased (Qualitative) | m/s²    | Methods              | Explicit            | Coefficient of gravitational acceleration decreased to approx. water.    |
| Biological Construction  | Cilia Suppression                    | Notch ICD mRNA         | 370                     | pg      | Methods              | Explicit            | Amount injected per cell.                                                |
| Biological Environment   | Rearing/Observation Medium Temperature | 14 / 20 / 22           | °C                      | Methods              | Explicit            | Different temperatures used for rearing/imaging/injection.               |

### 4.3 Global Order:
#### Content
The global order that emerges or is constructed includes:
1.  **Organism Morphology:** The final 3D shape of the organism, resulting from initial aggregation and subsequent subtractive sculpting (partially designed/imposed, partially self-maintained). Specific complex shapes like toroids were achieved (SI Appendix, Fig. S13).
2.  **Tissue Distribution:** Spatially organized regions of passive (epidermal) and active (cardiac) tissue within the organism (designed by evolution, realized by layering/shaping).
3.  **Coordinated Locomotion:** Directed movement resulting from the interplay of morphology, tissue distribution, and (partially emergent) coordinated contractions.
4.  **Collective Patterns:** Formation of temporary pairs orbiting each other; aggregation of environmental debris by individuals or groups.
Defines `ConfigurationalNode`s: `OrganismMorphology`, `TissueDistribution`, `LocomotionPattern`, `CollectiveBehaviorPattern`.
#### Implicit/Explicit
Mixed

### 4.4 Predictability of Global Order:
#### Score
6
#### Justification
Predictability varies depending on the aspect:
Overall score reflects moderate success in predicting behavior from design, acknowledging stochasticity and unmodeled biological factors.
#### Implicit/Explicit
Mixed

### 4.5. Local Interaction Rules (for Self-Organization)
#### Table
| Rule ID                  | Description                          | Parameter Name         | Value Range             | Units   | Implicit/Explicit   | Justification                                                  | Source              |
| :----------------------- | :----------------------------------- | :--------------------- | :---------------------- | :------ | :------------------ | :------------------------------------------------------------- | :------------------ |
| Actuation Phase          | Random offset in contraction timing  | Phase Offset StDev     | 0.4π                    | rad     | Explicit            | Used in robustness filter simulation.                          | Methods             |
| Notch Signaling          | Suppression of cilia formation       | Notch ICD mRNA Conc.   | 370 pg / cell           | mass    | Explicit            | Experimental manipulation.                                     | Methods             |

### 4.6. Globally Emergent Order and Order Parameters
#### Table
| Property ID             | Description                 | Parameter                 | Value Range                                  | Units          | Implicit/Explicit   | Justification                                                     | Protocol            | Source          |
| :---------------------- | :-------------------------- | :------------------------ | :------------------------------------------- | :------------- | :------------------ | :---------------------------------------------------------------- | :------------------ | :-------------- |
| Locomotion             | Directed movement           | Net Displacement          | ~0-2 body lengths (Sim, 1min); ~0-10 (Real, 10min) | body lengths   | Explicit            | Measured in simulations and experiments.                            | Tracking Software   | Fig 4           |
| Locomotion             | Directed movement           | Average Velocity          | Variable (e.g. red line Fig 2I > gray)     | m/s            | Explicit            | Calculated during evolution/simulation.                           | Simulation Output | Fig 2I          |
| Morphology             | Overall shape               | Minimal Concavity Size    | >= 100                                       | µm             | Explicit            | Threshold for persistence used in build filter.                   | Methods             | Methods         |
| Tissue Distribution    | Muscle content              | % Contractile Tissue      | <= 50                                        | %              | Explicit            | Constraint applied in build filter.                             | Methods             | Methods         |
| Collective Behavior    | Pair orbiting             | Occurrence (Qualitative)    | Observed                                     | Binary/Freq    | Explicit            | Phenomenon observed and described.                              | Observation         | Discussion, SI  |
| Collective Behavior    | Debris Aggregation        | Aggregated Area (Qualitative)| Observed (Fig 3F)                            | Area (mm²)     | Explicit            | Phenomenon observed and described.                              | Observation         | Fig 3F, SI      |

### 4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity
#### Table

## M5: Computation

### 5.1 Embodied Computation Presence:
#### Content
No
#### Justification
Computation, in the sense of information processing via logical or complex mathematical operations, is performed externally by the evolutionary algorithm and physics simulation to *design* the organism. The organism itself, once constructed, primarily exhibits physical dynamics and sensorimotor behavior based on its structure and the physics of its components (muscle contraction, passive material). While the interaction between morphology and physics *results* in behavior (which some might broadly term "morphological computation"), the paper does not claim or demonstrate that the organism performs intrinsic computation like logic operations, signal processing, or running algorithms. The Discussion mentions potential for future work equipping organisms with electrically active cells for cognitive/computational functions, implying the current organisms lack this.
#### Implicit/Explicit
Mixed

**(Conditional: If M5.1 is "No", skip to Module 6.)**

### 5.2 Computation Type:
#### Content
#### Implicit/Explicit
#### Justification

### 5.3 Computational Primitive:
#### Content
#### Implicit/Explicit
#### Justification

### 5.4 Embodied Computational Units
#### Table
#### Implicit/Explicit
#### Justification


## M6: Temporal Dynamics

### 6.1 Timescales:
#### Table

| Timescale Description          | Value             | Units    | Source                     | Implicit/Explicit   | Justification                                               |
| :----------------------------- | :---------------- | :------- | :------------------------- | :------------------ | :---------------------------------------------------------- |
| Muscle Contraction Cycle       | 0.5               | s        | Methods (2 Hz freq)        | Explicit            | Period = 1 / Frequency.                                     |
| Simulation Evaluation Period   | 10                | s        | Methods (Locomotion)       | Explicit            | Duration of simulated test for locomotion fitness.          |
| In Vivo Observation Period     | 10                | min      | Fig 4 Caption              | Explicit            | Duration of experimental observation runs for Fig 4.        |
| Development/Maturation (Post-Shape)| ~3               | hours    | Methods                    | Explicit            | Time allowed for reshaping tissue.                          |
| Aggregation/Healing Time       | 24 hr / 1 hr      | hours    | Methods                    | Explicit            | Time for initial aggregation / healing tissue layers.       |
| Organism Lifespan              | Days to Weeks     | Days     | Abstract, Methods        | Explicit            | Functional duration without external nutrients.             |
| Collective Aggregation Time    | 24                | hours    | Fig 3F Caption             | Explicit            | Duration shown for group debris aggregation.                |

### 6.2 Active Inference:
#### Content
No
#### Justification
The paper provides no evidence that the organisms utilize active inference. Their behavior (e.g., locomotion) is a result of their designed morphology and the physics of muscle contraction (with some emergent coordination), driven by internal energy stores. There is no indication of an internal predictive model, minimization of prediction error, or goal-directed action selection originating *within* the organism itself. The "goal" (e.g., maximize displacement) is imposed externally during the evolutionary design process, not pursued autonomously by the organism based on internal predictions or minimizing surprise.
#### Implicit/Explicit
Implicit
#### If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:

## M7: Adaptation

### 7.1 Adaptive Plasticity Presence:
#### Content
Yes (Pipeline); Partial (Organism)
#### Justification
*   **Pipeline:** The design pipeline itself clearly exhibits adaptation. It explicitly incorporates a feedback loop where sim-to-real discrepancies are used to generate constraints, modifying the evolutionary search to produce designs that are more likely to function as predicted when built (Fig 1G, Methods). This is adaptation of the design process over multiple cycles.
*   **Organism:** The individual organism exhibits limited adaptive plasticity.
    *   **Self-Repair:** The ability to repair damage (SI Appendix, Fig. S9) is a form of structural adaptation/robustness, restoring functionality after perturbation.
    *   **Behavioral Adaptation?:** The paper does *not* provide evidence that the organisms adapt their *behavior* based on experience (e.g., learning to navigate obstacles, changing locomotion gait). Their behavior seems largely fixed by their evolved morphology and basic cellular properties. Emergent coordination might potentially adapt slightly over time, but this is not shown or claimed.
Therefore, adaptation is clearly present in the design loop, and partially present in the organism as structural robustness/repair, but not clearly as behavioral learning.
#### Implicit/Explicit
Mixed

**(Conditional: If M7.1 is "Yes" or "Partial", include M7.2)**

### 7.2 Adaptation Mechanism:
#### Content
*   **Pipeline Adaptation:** The mechanism is feedback-driven constraint modification within an evolutionary algorithm. Performance differences between simulation and reality ("reality gap") for a set of designs are analyzed to identify common failure modes or successful design motifs. These insights are translated into new constraints (e.g., on morphology, material distribution) that are added to the fitness function or search space definition for subsequent evolutionary runs. This biases the search towards more transferable designs.
*   **Organism Adaptation (Self-Repair):** The mechanism is inherent biological wound healing. Cells at the site of injury likely undergo processes like migration, proliferation (potentially limited), and adhesion changes to close the gap and restore tissue continuity, driven by local biochemical signals and physical cues. This relies on the innate regenerative capacities of Xenopus embryonic tissue.
Defines `AdaptationNode` corresponding to the Pipeline Feedback Loop. Edges represent `InformationFlow` from `SimVsRealComparisonNode` to `ConstraintGenerationNode` to `EvolutionaryAlgorithmNode`. For the organism, `AdaptationNode` (SelfRepair) linked to `OrganismNode` via `Monad` edge representing internal state change for repair. Mechanism attribute: "Feedback Constraint Modification" (Pipeline), "Biological Wound Healing" (Organism).
#### Implicit/Explicit
Mixed

## M8: Emergent Behaviors

### 8.1 Behavior Description:
#### Content
The primary functional behaviors designed for and/or observed are:
1.  **Locomotion:** Self-propelled movement across a surface (Petri dish), driven by coordinated or stochastic contractions of embedded cardiac tissue pushing against the substrate (Fig 1, 2, 4).
2.  **Object Manipulation (Aggregation):** Spontaneous aggregation of particulate matter in the environment by motile organisms, occurring both individually and collectively (Fig 3E, 3F, SI S10, S11). More precise manipulation (end-effectors) was evolved in simulation only (SI S12).
3.  **Object Transport:** Evolved in simulation; designs featured a pouch (emergent hole topology exapted) to carry an object while locomoting (SI S13). Realized pouch morphology, but transport not shown in vivo.
4.  **Collective Behavior (Orbiting/Entanglement):** Multiple organisms interacting locally, sometimes forming temporary mechanical bonds leading to mutual orbiting before detaching, or entanglement (Fig 3F, SI S10, S11).
5.  **Self-Repair:** Autonomous closure of physical damage (lacerations) (SI S9).
Defines `BehaviorArchetypeNode`s: `Locomotion`, `ObjectAggregation`, `ObjectTransport (Sim)`, `CollectiveOrbiting`, `SelfRepair`. Attributes can include `environment`, `driver` (e.g., muscle contraction).
#### Implicit/Explicit
Explicit

### 8.2 Behavior Robustness:
#### Score
7
#### Justification
The system demonstrates robustness in several ways:
1.  **Design Robustness:** Designs are explicitly filtered for robustness to noise (random phase modulation of actuation) in simulation (Methods: Robustness Filter). The selected design (Fig 4A) maintained high performance under this noise.
2.  **Sim-to-Real Robustness:** Locomotion behavior (direction, effect of inversion) transferred successfully from simulation to reality despite the "reality gap" (differences in physics, noise, biological variability), suggesting the designed morphology itself confers some robustness (Fig 4).
3.  **Structural Robustness:** Organisms exhibit self-repair, recovering structure and likely function after physical damage (SI Fig S9).
4.  **Parameter Robustness:** Embryogenesis exhibits robustness to variations in cell size/number (mentioned in Discussion as biological context). The organisms leverage this innate cellular robustness.
However, robustness is not perfect; sim-to-real transfer isn't exact, and the operational limits (e.g., temperature range, environmental toxins) are not explored. The score reflects demonstrated robustness through filtering, sim-to-real success, and self-repair, balanced against unquantified aspects.
#### Implicit/Explicit
Mixed
Score contributes to reliability attributes (e.g., `noiseTolerance`, `damageRecovery`) of the `BehaviorArchetypeNode` or `SystemNode`.

### 8.3 CT-GIN Emergent Behavior Validation
#### Content
Validation methods for emergent behaviors include:
*   **Locomotion:** Quantified comparison between simulated and in vivo trajectories (Fig 4). Statistical tests used to assess the significance of directional agreement and the effect of inversion (SI S9). XY tracks extracted using software (Noldus Ethovision). Validated via operational definition (displacement) and control (inversion). Robustness tested via noise injection in simulation. Reproducibility suggested by using multiple organisms (n=6) and multiple trials per organism.
*   **Object Aggregation:** Validated qualitatively through time-lapse imaging showing particles being gathered by individual and groups of organisms (Fig 3E, 3F, SI S11). Observed both in silico (SI S10) and in vivo.
*   **Collective Orbiting:** Validated qualitatively through observation in both simulation and in vivo experiments (SI S10). Described as occurring "often".
*   **Self-Repair:** Validated qualitatively via imaging showing closure of lacerations over time (SI Fig S9).
*   **Emergent Coordination:** Validated qualitatively by observing locomotion despite random actuation model, suggesting coordination must be emerging in vivo (Discussion).
**Limitations:** Validation for collective behaviors and coordination is largely qualitative. Predictability and precise mechanisms often remain unclear. Reproducibility statistics (beyond locomotion direction) are limited in the main text.
#### Implicit/Explicit
Explicit

---

#Key: [barnaveli_pressure-gated_2024]

# Pressure-gated microfluidic memristor for pulsatile information processing

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a microfluidic memristor implemented as a conical channel connecting two aqueous electrolyte reservoirs. It utilizes ion transport (cations and anions) in water driven by simultaneously applied time-dependent voltage and pressure drops. The channel walls possess a negative surface charge. Its function is to exhibit memory-resistance (memristance), where its electrical conductance depends on the history of applied voltage and pressure. The system's purpose is demonstrated for pulsatile information processing, specifically enhancing the distinction between voltage time series and potentially doubling information bandwidth by using pressure as a second independent signal channel. Components include the conical channel, aqueous 1:1 electrolyte (ions + water), charged channel walls, reservoirs, and sources for time-dependent voltage and pressure. The behaviour is modelled using Poisson-Nernst-Planck-Stokes (PNPS) equations.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name          | Value        | Units                | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :---------------------- | :----------: | :-------------------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Polarization Time (τ)   | 8.33         | ms                   | Section III (Eq. 6)       | Explicit           | High (Calculated)             | L² / (12D)                        |

    *   **Note:** More than 5 parameters are listed as they are all critical for defining the system and its behavior as described. Reliability is "High" as these are input parameters or directly calculated values for the defined theoretical model.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The system has two primary energy inputs: 1) Electrical energy from the applied time-dependent voltage source V(t). 2) Mechanical energy (pressure-volume work) from the applied time-dependent pressure source P(t).
    *   Value: V(t) and P(t) are time-dependent (e.g., sinusoidal V0sin(ωt), P0sin(ωt) or block pulses ±V0, ±P0). Specific amplitudes are V0=1V, P0=-60mbar (optimal).
    *   Units: Voltage (V), Pressure (Pa or mbar).

### **2.2 Energy Transduction**

    *   Content: 1) Electrical energy drives ion migration (conduction) and electroosmotic fluid flow (voltage-induced flow). 2) Pressure energy drives fluid flow (Poiseuille-like flow) which advects ions. 3) Coupling occurs via the Stokes equation (Eq. 4), where the electric field term `-e(ρ+ - ρ-)∇φ` couples electrical potential to fluid flow (electroosmosis), and the Nernst-Planck equation (Eq. 3), where the fluid velocity `u` couples fluid flow to ion transport (advection). Energy is stored transiently in the non-equilibrium ion concentration profile (concentration polarization) and the kinetic energy of the fluid (though likely small effect).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify energy efficiency for the information processing tasks. It focuses on conductance modulation (I/V) and distinguishing signals based on the final conductance state. While energy is input, the efficiency of converting this energy into useful computation or information storage is not calculated or discussed. Qualitatively, the processes involve dissipative ionic currents and viscous fluid flow, suggesting efficiency is likely low, but no metric is provided.

### **2.4 Energy Dissipation**

    *   Content: Dissipation occurs primarily through: 1) Joule heating due to ionic current flowing through the electrolyte resistance (related to Ohmic conduction in Eq. 3). 2) Viscous dissipation in the fluid due to velocity gradients (related to the η∇²u term in the Stokes equation, Eq. 4). The paper does not quantify these mechanisms explicitly, but they are inherent to the PNPS model. Qualitative assessment: Both mechanisms are likely significant, particularly viscous dissipation given the microfluidic channel dimensions and pressure-driven flow, and Joule heating due to ionic current.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding with M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: ≈ 8.33 (τ)
*    Units: ms

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Effectively Analog (Continuum of states) / 16 distinguishable states demonstrated

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Governed by relaxation timescale τ ≈ 8.33 ms
    *   Units: 1/ms (characteristic rate is 1/τ)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :------------------------------ | :------------------------------------- | :---- | :-------------------- | :---------- | :---------------- | :---------------------------------- |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", proceeding with M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Neuromorphic Computing / Reservoir Computing (Analog)

### **5.3 Computational Primitive:**

    *   Content: History-dependent conductance modulation / Non-linear temporal filtering. The core operation is the modulation of the channel's electrical conductance `g(t) = I(t)/V(t)` based on the integral of past voltage `V(t)` and pressure `P(t)` inputs, weighted by the system's relaxation dynamics (timescale τ). This acts as a non-linear filter operating on the input time series, mapping different temporal patterns to different output conductance values. Mathematically, it approximates a dynamic transformation `g(t) = F[V(t'<t), P(t'<t)]`, where F incorporates the PNPS dynamics.
    *   **Sub-Type (if applicable):** Memristive state update.

### **5.4 Embodied Computational Units**
| Unit ID | Description                 | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification                                     |
| :------ | :-------------------------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :------------------------------------------------ |
* **Note:** Processing power and energy are not quantified. Frequency is estimated based on the memory relaxation time τ. The computation is analog.

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description             | Value        | Units | Source        | Implicit/Explicit | Justification                                    |
        | :-------------------------------- | :----------: | :---- | :------------ | :---------------- | :----------------------------------------------- |
        | Ionic Relaxation/Memory (τ)       | ≈ 8.33       | ms    | Sec III (Eq.6) | Explicit          | Calculated from L²/12D                           |
        | Fluid Flow Response (τ_f)         | ≈ 0.2        | µs    | Sec IV.A      | Explicit          | Calculated from R_b²/(η/ρ_m), stated in text     |
        | Driving Period (1/f)              | 40           | ms    | Sec IV.A      | Explicit          | Inverse of driving frequency ω/2π = 25 Hz      |
        | Pulse Duration (per pulse)        | τ/8 ≈ 1.04   | ms    | Sec IV.B      | Explicit          | Defined in pulse train protocol (Fig 4a)         |
        | Pulse Separation (between pulses) | τ/8 ≈ 1.04   | ms    | Sec IV.B      | Explicit          | Defined in pulse train protocol (Fig 4a)         |
        | Full 4-Pulse Train Duration       | τ ≈ 8.33     | ms    | Sec IV.B      | Explicit          | Total duration of sequence in Fig 5 analysis   |
        | Full 2-Pulse Train Dur. (V+P)   | 0.6τ ≈ 5     | ms    | Section IV.B  | Explicit          | Total duration of sequence in Fig 8 analysis  |
    *   **Note:** Multiple relevant timescales exist: the slow ionic relaxation governing memory, the very fast fluid response, and the timescales imposed by the external driving signals/pulses.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", proceeding with M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism is the dynamic change in the ion concentration profile (concentration polarization) within the conical channel, driven by the applied voltage and pressure gradients. Voltage gradients drive ions electrophoretically and induce electroosmotic flow; pressure gradients induce Poiseuille flow. These combined effects lead to ion accumulation or depletion in different regions of the channel, particularly near the tip, depending on the polarity and magnitude of the applied fields and flows (governed by PNPS equations). This altered concentration profile directly changes the local conductivity and thus the overall channel conductance. The 'adaptation' is the change in conductance state resulting from this physically driven concentration profile modulation. It's a passive adaptation driven by physics, not involving explicit learning rules like Hebbian or reinforcement learning.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: 1) Memristive Behavior: Exhibits history-dependent conductance, characterized by pinched hysteresis loops in I-V curves under periodic driving (Fig 3a). 2) Pressure Gating/Modulation: Applied pressure significantly enhances, reduces, or resets the memristive effect (hysteresis loop area) and modulates the instantaneous conductance (Fig 3a inset). Memory can be erased quasi-instantaneously by strong pressure pulses (Fig 3b). 3) Time Series Processing/Distinction: Maps different input time series (voltage pulses or combined voltage-pressure pulses) onto distinguishable final conductance states (Figs 5b, 8). 4) Increased Bandwidth (Potential): Using pressure as an independent signal channel potentially doubles the information bandwidth compared to voltage-only signaling (Section IV.B, Fig 7, Fig 8).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The behaviors (memristance, pressure gating, time series distinction) are validated through numerical simulations of the PNPS equations using the finite-element method (COMSOL). The results are presented as plots of current vs. voltage (Fig 3), conductance vs. time (Figs 4b, 5a), and final conductance states after pulse trains (Figs 5b, 6, 8). The validation relies on the physical realism of the PNPS model and the accuracy of the numerical solution. Control experiments (e.g., P=0 case in Figs 3a, 5a, 6) are used for comparison. Reproducibility is implicit in the deterministic nature of the simulations. Limitations include the idealizations inherent in the model (e.g., homogeneous surface charge, neglecting potentially complex electrode effects, no thermal noise).

---

#Key: [hadorn_specific_2016]

# Specific and Reversible DNA-Directed Self-Assembly of Modular Vesicle-Droplet Hybrid Materials

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of giant unilamellar vesicles (GUVs) and oil-in-water emulsion droplets (EDs) functionalized with complementary or non-complementary biotinylated single-stranded DNA (ssDNA) oligonucleotides via a streptavidin linker. The purpose is to demonstrate specific, DNA-directed self-assembly of these components into modular vesicle-droplet hybrid structures and to show the reversibility of this assembly process through thermal denaturation and competitive displacement with excess biotin. Key components include POPC, DSPE-PEG2000, DSPE-PEG2000-btn phospholipids for GUV/ED formation and stabilization, streptavidin conjugated with fluorescent dyes (Alexa Fluor 488 or 532), biotinylated ssDNA sequences (ssDNA1, ssDNA2, ssDNA3), mineral oil or DEP (diethyl phthalate) for GUV/ED core, and aqueous buffer solutions (HS, IS). The system uses specific DNA hybridization (ssDNA1 with complementary ssDNA3) to mediate aggregation between GUVs and EDs, while non-complementary pairs (ssDNA2 with ssDNA3) serve as specificity controls. Stability is modulated by the molar percentage of non-biotinylated PEGylated phospholipids (DSPE-PEG2000). Reversibility is triggered by heating above the DNA melting temperature (Tm) in the presence of excess biotin.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Key parameters chosen relate to the composition critical for stability and the conditions for assembly/disassembly.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input driving the self-assembly is the reduction in Gibbs free energy associated with the hybridization of complementary ssDNA strands tethered to the GUVs and EDs. Thermal energy is input to overcome the hybridization energy barrier and induce disassembly (reversibility), specifically heating to 60 °C. Chemical potential changes also drive disassembly via the addition of excess biotin competing for streptavidin binding sites. Mechanical energy (agitation, centrifugation) is used during preparation but not directly for assembly/disassembly processes shown.

### **2.2 Energy Transduction**

    *   Content: Chemical binding energy released upon DNA hybridization between complementary strands on GUVs and EDs is transduced into the mechanical work required to bring the vesicles and droplets together and hold them in an aggregated state, overcoming repulsive forces (like steric repulsion from PEG layers) and Brownian motion. Thermal energy input (heating) is transduced into increased kinetic energy of the DNA strands, leading to denaturation (melting) of the dsDNA linkers. Chemical potential energy change (excess biotin) drives the displacement of biotinylated DNA/phospholipids from streptavidin, breaking the links.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any quantitative measure or qualitative assessment of the energy efficiency of the assembly or disassembly process (e.g., ratio of binding energy converted to useful work vs. dissipated energy). The focus is on specificity and reversibility, not energetics.

### **2.4 Energy Dissipation**

    *   Content: Energy is likely dissipated primarily as heat to the surrounding aqueous environment during the exothermic DNA hybridization process. Viscous drag acting on the GUVs and EDs as they move to assemble represents another minor dissipation pathway (mechanical energy to heat). During thermal disassembly, the input heat is ultimately dissipated into the environment. The precise quantification is not provided. Qualitatively, dissipation via DNA hybridization is expected, while viscous effects are likely minor for these slow, diffusion/interaction-driven processes.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Dependent on conditions (Stable at RT for up to 2 days; Reversible upon heating >Tm or adding biotin)
*    Units: Hours / Days (Qualitative: Stimulus-dependent retention)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Low (Essentially binary per GUV/ED pair: Bound/Unbound)
*   Units: States

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: High (visual discrimination)

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Low (under assembly conditions)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rule is the specific binding (hybridization) between complementary ssDNA strands tethered to the surfaces of GUVs and EDs via streptavidin-biotin linkages. Specifically, ssDNA1 (on ED1) binds to ssDNA3 (on GUV1). Non-complementary strands (ssDNA2 on ED2 and ssDNA3 on GUV1) do not bind significantly under the experimental conditions. Biotin in solution competes with biotinylated DNA/phospholipids for binding sites on streptavidin, acting as a local inhibition rule. Steric repulsion from PEG layers (modulated by the 9 mol% DSPE-PEG2000) prevents non-specific fusion/aggregation.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | DNA Hyb | Specificity | Sequence | ssDNA1/ssDNA3 (complementary), ssDNA2/ssDNA3 (non-complementary) | sequence | Experimental Section | Explicit | Sequences provided. |
    | DNA Hyb | Strength (proxy) | Thermal Denaturation Temp | 60 (disassembly temp used > Tm_estimated) | °C | Experimental Section | Mixed | 60°C is explicit disruption temp. Estimated Tm (27.8°C) also mentioned but context suggests effective Tm in aggregate is higher. |
    | PEG Repulsion | Stability Modulation | DSPE-PEG2000 concentration | 9 | mol % | Experimental Section, Fig 1 | Explicit | Ratio explicitly stated as needed for stability. |
    | Biotin Comp | Inhibition | Biotin concentration | 0.2 (excess added) | mg/mL | Experimental Section | Explicit | Concentration used for disassembly provided. |

### **4.3 Global Order:**

    *   Content: The emergent global order consists of aggregates/clusters formed by multiple GUVs and EDs linked together via dsDNA bridges. The specific morphology or size distribution of these aggregates is not quantitatively characterized, but micrographs (Fig 2l, m) show structures involving several GUVs and EDs clustered together.
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| DNA Stab | Stability against thermal denaturation | Temperature | < ~ T_m_eff (stable), > ~ T_m_eff (unstable) | °C | Mixed | Disassembly at 60°C is explicit. Threshold (effective Tm) implicitly higher than solution Tm (27.8°C). | Experimental Section, Results & Discussion |
| PEG Rep | Steric repulsion preventing non-specific fusion | PEG density (mol %) | 1% (unstable), 10% (stable) | mol % | Explicit | Explicitly compared 1% btn-PEG vs 9% PEG + 1% btn-PEG. | Fig 1, Results & Discussion |
| Biotin Comp | Competitive binding to streptavidin | Biotin Concentration | ~0 vs 0.2 (added excess) | mg/mL | Explicit | Explicitly compared control vs excess biotin addition. | Experimental Section |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Aggregation | Formation of GUV-ED clusters | Aggregation State | Aggregated / Dispersed | Binary | Explicit | Visually determined by microscopy. | Microscopy | Fig 2 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|


## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Pre-incubation (Streptavidin-DNA) | 30 | min | Experimental Section | Explicit | Duration explicitly stated. |
        | Functionalization Incubation | 1 | h | Experimental Section | Explicit | Duration explicitly stated. |
        | Assembly Incubation | 4 | h | Experimental Section | Explicit | Duration explicitly stated. |
        | Disassembly Incubation (Heat+Biotin) | 16 | h | Experimental Section | Explicit | Duration explicitly stated. |
        | Aggregate Stability (at RT) | up to 2 | days | Results & Discussion | Explicit | Duration explicitly stated. |
    *   **Note:** These are macroscopic timescales reported for experimental steps or observed stability. Microscopic timescales (e.g., DNA hybridization kinetics) are not reported.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors are: 1) Specific Self-Assembly: GUVs and EDs selectively aggregate only when functionalized with complementary ssDNA strands. 2) Reversible Disassembly: The formed aggregates can be disassembled upon heating above the DNA melting temperature in the presence of excess biotin. 3) Stability Modulation: System stability (preventing unwanted fusion) is controlled by the concentration of non-binding PEGylated phospholipids.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation relies primarily on fluorescence microscopy imaging. Specific assembly is validated by comparing mixtures with complementary DNA (showing aggregates, Fig 2l,m) vs. non-complementary DNA (showing no aggregates, Fig 2n,o). Reversibility is validated by imaging aggregates before and after incubation at 60°C with excess biotin (showing disassembly, Fig 2p,q) and comparing to a control incubated with water instead of biotin (showing limited disassembly, Fig 2r,s). Stability modulation is validated by comparing results with 10% vs 1% total PEGylated lipid (Fig 1). The validation is qualitative/visual, relying on clear differences in observed structures. Reproducibility is implied but not explicitly quantified (e.g., number of trials, statistical analysis of aggregation degree).

---

#Key: [penketh_is_2022]

# Is DNA repair controlled by a biological logic circuit?

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is a hypothetical biological logic circuit proposed to control and orchestrate DNA repair processes within a cell. Its purpose is to integrate signals, make decisions, and regulate the timing and sequence of different DNA repair mechanisms to prevent interference and ensure efficient repair. The proposed components include: a biological clock (p53 protein pulses), logic gates (AND, NOR, NOT, etc.) implemented using protein kinases and phosphatases acting on specific amino acid residues of substrate proteins through phosphorylation (+P) and dephosphorylation (-P), input signals (e.g., presence of DNA damage, cellular status signals), output signals (activation/inactivation of specific DNA repair proteins/pathways like MGMT or MMR), and energy source (ATP for phosphorylation). The system is envisioned to function like a small CPU within the cell, enabling complex data processing and counting functions for DNA repair management.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name         | Value              | Units                  | Source (Fig/Table/Section)   | Implicit/Explicit   | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :--------------------- | :-----------------: | :---------------------: | :---------------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** The parameters listed characterize the proposed system's components and operation based on the text. Reliability is Med/High as they are either direct definitions or cited experimental values supporting the hypothesis.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is chemical energy stored in ATP (Adenosine triphosphate), which provides the phosphate group and the energy for the phosphorylation reactions catalyzed by protein kinases.

### **2.2 Energy Transduction**

    *   Content: Chemical energy from ATP hydrolysis is transduced into changes in protein conformation and activity. Protein kinases transfer a phosphate group from ATP to specific amino acid residues on substrate proteins. This phosphorylation (+P) event alters the protein's structure and function (activating or inactivating it), effectively changing the state of the biological logic gate or signaling component. Protein phosphatases reverse this by removing the phosphate group (-P), often releasing inorganic phosphate and heat, resetting the component's state. Energy is thus converted from chemical potential (ATP) to conformational potential energy and ultimately dissipated as heat during the cycle.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss or quantify the energy efficiency of the proposed phosphorylation-based logic circuits. It mentions the latching nature might make them energy efficient (p43), but provides no data or calculations. Qualitative assessment: Potentially Medium efficiency due to the 'latching' nature mentioned, avoiding continuous power draw like electronic gates, but ATP hydrolysis itself has inherent energy losses.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation occurs primarily through the hydrolysis of ATP during phosphorylation and the release of heat during both phosphorylation and dephosphorylation reactions as proteins change conformation and interact with the aqueous cellular environment. The paper does not quantify these dissipation mechanisms. Qualitative assessment: Dissipation is inherent to ATP hydrolysis and enzymatic processes.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**

*   Units: bits

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Digital (explicitly stated as analogous to digital logic)

### **5.3 Computational Primitive:**

    *   Content: Basic Boolean Logic Gates (AND, OR, NOR, NOT/Inverter) implemented via phosphorylation/dephosphorylation dynamics. These primitives are proposed to be combined to build more complex computational functions like data selection (Fig 2) and binary counting (Fig 3).
    *   **Sub-Type:** Logic Gates (AND, NOR, NOT). Also Sequential Logic elements (Flip-Flops proposed for counter).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description   | Value           | Units           | Source                     | Implicit/Explicit   | Justification                                                         |
        | :---------------------- | :-------------: | :--------------: | :-------------------------: | :----------------: | :-------------------------------------------------------------------- |
        | p53 Clock Cycle Time    | ~1 / 2.5 = 0.4 | hours per cycle | Introduction, Discussion (p43) | Explicit (cited)   | Frequency explicitly stated as ~2.5 cycles/hour.                      |
        | DNA Repair Process Time | Several        | hours           | Discussion (p43-44)        | Explicit            | Stated that processes take hours, motivating the clock cycle time.    |
    *   **Note:** The key timescale is the p53 clock, which orchestrates the proposed sequential logic. Other timescales are mentioned qualitatively or are implicit.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is the coordinated and regulated control of DNA repair processes. This includes: (1) Sequential execution of potentially interfering repair pathways, orchestrated by the p53 clock. (2) Decision-making based on cellular state inputs (implied, e.g., selecting repair pathways via circuits like Fig 2). (3) Counting events (e.g., repair attempts via circuits like Fig 3). (4) Temporary inactivation/activation of specific repair proteins (e.g., MGMT phosphorylation). The overall goal is to maintain genomic integrity efficiently.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper provides no direct experimental validation for the proposed integrated logic circuit behavior. Validation consists of: (1) Citing existing experimental evidence for individual components (p53 pulses, existence of kinases/phosphatases, MGMT phosphorylation, MMR futile cycles). (2) Presenting a theoretical model (logic gates, circuits) that *could* potentially explain or integrate these known biological phenomena (e.g., explaining the *purpose* of p53 pulses or MGMT phosphorylation). (3) Using analogies to established electronic logic circuits. Claims of the *overall coordinated behavior* arising from this specific logic circuit mechanism remain hypothetical and unvalidated experimentally within the paper.

---

#Key: [alapan_shape-encoded_2019]

# Shape-encoded dynamic assembly of mobile micromachines

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of mobile micromachines dynamically assembled from distinct structural (body) and motor (actuator) units using dielectrophoretic (DEP) interactions controlled by external electric fields. The 3D shape of the structural units is precisely designed (using two-photon lithography) to encode specific DEP force landscapes (local electric field gradients) when placed in a uniform external field. These shape-encoded forces direct the assembly of motor units (e.g., magnetic beads, self-propelled Janus particles) to specific locations on the structural body. The assembled micromachines exhibit locomotion driven by the motor units (e.g., rolling under rotating magnetic fields, self-propulsion). The assembly is reversible and reconfigurable by tuning the electric field parameters (amplitude, frequency). The purpose is to create programmable, reconfigurable mobile micromachines with versatile locomotion modes and functionalities derived from the modular assembly.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These are representative key parameters controlling the assembly and actuation. Other relevant parameters include medium properties (permittivity), particle permittivities, magnetic field strength/frequency, and specific geometric dimensions of the custom bodies (e.g., fillet/cavity radius).

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy sources are:
        1.  AC Electric Field: Applied across electrodes to generate DEP forces for assembly and mechanical coupling tuning. Also drives self-propulsion for Janus particles (sDEP/ICEP).
        2.  Rotating Magnetic Field: Applied via external coils to drive the locomotion of assemblies using magnetic actuators.
    *   Value: Electric Field: 6-10 Vpp; Magnetic Field: Not explicitly quantified in terms of power input, only control parameters (frequency, direction).

### **2.2 Energy Transduction**

    *   Content:
        1.  **Assembly:** Electrical energy (AC field) -> Non-uniform electric field energy landscape (modulated by body shape) -> Potential energy difference for polarizable particles -> Dielectrophoretic force -> Mechanical work (moving actuators to assembly sites) -> Potential energy stored in assembled configuration (electrostatic interaction).
        2.  **Magnetic Actuation:** Magnetic field energy (external rotating field) -> Magnetic torque on superparamagnetic actuators -> Mechanical rotation of actuators -> Mechanical work (rolling friction converts rotational to translational kinetic energy of the assembly).
        3.  **Self-Propulsion (Janus):** Electrical energy (AC field) -> Asymmetric ion flow around Janus particle (via sDEP/ICEP mechanisms) -> Fluid motion relative to particle -> Mechanical thrust -> Kinetic energy of the assembly.
        4.  **Coupling Control:** Electrical energy (AC field) -> DEP force magnitude -> Mechanical coupling strength (friction/locking) between actuator and body -> Modulates conversion of actuator rotation to body rotation/translation.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify energy efficiency. However, micro-scale systems driven by external fields, especially involving motion in viscous fluids, are generally known to have very low energy efficiency (large input power required for small mechanical output). DEP assembly itself might be relatively efficient in holding particles, but the actuation (magnetic rolling, self-propulsion) involves significant viscous drag losses. The score reflects an inferred low efficiency typical for such systems. Qualitative Assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include:
        1.  Viscous Drag: Energy loss due to motion of the micromachine and actuators through the fluid (dominant loss for locomotion). (Qualitative Assessment: High)
        2.  Joule Heating: Resistive losses in the medium due to the applied electric field. (Qualitative Assessment: Medium/Low, depends on medium conductivity and field strength/frequency).
        3.  Friction: Rolling friction between actuators and substrate; internal friction if parts slide relative to each other (e.g., free rotation joint). (Qualitative Assessment: Medium/Low, depends on coupling).
        4.  Magnetic Hysteresis: Potentially minor losses in superparamagnetic particles if field frequencies are very high (unlikely here). (Qualitative Assessment: Low)

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip to Module 4.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial

**(Conditional: If M4.1 is "Partial", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rule is the dielectrophoretic (DEP) force experienced by a polarizable microactuator in the non-uniform electric field created by the structural body. For the materials used (particles less polarizable than the medium, εp < εm), this force directs the actuators towards regions of lower electric field magnitude. The magnitude and direction of this force depend on:
        1.  The gradient of the squared electric field (∇|E|²) generated locally by the body's shape.
        2.  The polarizability difference between the particle and the medium (related to permittivities εp, εm).
        3.  The particle volume.
        4.  The applied electric field strength (|E|) and frequency (f) (especially for Janus particles where DEP response is frequency-dependent, Fig 4b).
        Mathematically (simplified time-averaged DEP force for a sphere): F_DEP ∝ Re[K(ω)] * ∇|E_rms|², where K(ω) is the frequency-dependent Clausius-Mossotti factor (related to complex permittivities) and E_rms is the root-mean-square electric field. The shape modulation creates specific low-field regions (e.g., under cavities, fillets - Fig 1) acting as attraction sites. Other implicit local rules include steric repulsion (particles cannot overlap) and hydrodynamic interactions (negligible compared to DEP at assembly).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | DEP Force | Magnitude Control | Applied Voltage (Vpp) | 6 - 10 | V | Figs 2, 3; Methods | Explicit | Controls |E| |
    | DEP Force | Direction/Magnitude (Janus) | Frequency (f) | 4 - 1000 | kHz | Fig 4; Methods | Explicit | Controls Re[K(ω)] and field penetration |
    | DEP Force | Shape Encoding | Fillet/Cavity Radius (r) | 0 - 20+ | µm | Fig 1c-h | Explicit (Simulations) | Determines local ∇|E|² |
    | Material Prop. | Particle-Medium Contrast | Relative Permittivity (ε_p, ε_m) | ε_p ≈ 2-4, ε_m = 80 | - | p. 1245; Methods | Explicit | Determines sign and magnitude of Re[K(ω)] |

### **4.3 Global Order:**

    *   Content: The emergent global order is the specific assembled configuration of motor units attached to the structural body at pre-defined locations determined by the body's shape. Examples include: actuators clustered at poles of a sphere (Fig 2b,c), actuators in 'wheel pockets' of a microcar (Fig 3a-c), actuators at docking sites of a microrotor (Fig 3e-g), Janus particles at specific hemicylindrical or filleted sites (Fig 4c-e), and hierarchical assemblies of units (Fig 5). In 3D, actuators follow helical tracks on columns (Fig 6a-c).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| DEP | Force determining assembly site based on field gradient minimization | Applied Voltage | 6-10 | V | Explicit | Controls interaction strength | Fig 1h, 2f |
| DEP | Site selectivity for Janus particles | Frequency | <25 vs >25 | kHz | Explicit | Controls sign of DEP force (attractive to low/high field regions) | Fig 4a,b,f |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| AssemblyConfig | Specific locations of actuators on body | Actuator Position | Defined by shape (e.g., poles, pockets, fillets) | µm | Explicit | Determines functionality (e.g., rolling, rotation) | Microscopy | Figs 2, 3, 4, 5, 6 |
| CouplingStrength | Mechanical lock between actuator and body | Coupling Regime | Free rotation vs Rigid body rotation | - | Explicit | Determines degrees of freedom | Varying Voltage | Fig 2f |
| LocomotionMode | Type of motion exhibited by assembly | Velocity / Angular Velocity | 0-20+ / 0-30+ | µm/s / deg/s | Explicit | Result of actuator type and configuration | Image Tracking | Fig 2e,f; Fig 4f |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog

### **5.3 Computational Primitive:**

    *   Content: The basic computational operation is the modulation of an external uniform field by a specific 3D geometry to generate a spatially varying force field (DEP force landscape). This can be viewed as a form of spatial filtering or transformation applied to the input field, where the geometry acts as the filter kernel. The output is the force vector experienced by an actuator at any given position. A secondary primitive for Janus particles is frequency-dependent thresholding, determining attraction to high-field (>25kHz) or low-field (<25kHz) regions (Fig 4b).
    *   **Sub-Type (if applicable):** Field Modulation/Spatial Transformation; Frequency Thresholding.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Assembly Time | Seconds (few to ~10s) | s | Fig 3c,g (visual estimate); Supp Video 4 | Explicit (visual) | Time taken for actuators to reach sites when field is turned on. |
        | Reconfiguration Time | Seconds (few) | s | Fig 4f,g (visual estimate); Supp Video 6 | Explicit (visual) | Time taken for actuators to move between sites when frequency is changed. |
        | Locomotion Velocity | 0 - 20+ | µm/s | Fig 2e, Supp Fig 11 | Explicit | Speed of assembled micromachine. |
        | Rotation Period (Magnetic) | ~ 1-10 (implied by ~Hz actuation) | s | Implicit (from typical magnetic actuation freqs) | Implicit | Time for one rotation driving locomotion/mixing. |
        | Rotation Period (Free Joint) | ~ 0.1 - 1 (inferred from Fig 2f rates) | s | Fig 2f | Explicit | Time for actuators to rotate around body. |
        | Electric Field Period | 1 / (4k - 1M) = 0.25 - 0.001 | ms | Methods (freq range) | Explicit | Period of the AC electric field. |

    *   **Note:** Assembly and reconfiguration times are visually estimated from figures/videos. Magnetic rotation period is inferred.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8.)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors are:
        1.  **Dynamic Assembly:** Formation of specific structures by attracting motor units to shape-encoded sites on a body using DEP forces.
        2.  **Reconfigurable Locomotion:** Controlled movement (translation, steering, rotation) of the assembled micromachine, driven by magnetic actuators (rolling) or self-propelled Janus particles. Locomotion mode (e.g., linear vs. rotational) can be switched by reconfiguring the assembly (Fig 4).
        3.  **Tunable Mechanical Coupling:** Control over the rotational degrees of freedom between body and actuators (free rotation vs. rigid body rotation) by adjusting DEP force strength (voltage) (Fig 2f).
        4.  **Micromanipulation:** Pick-and-place transport of non-magnetic objects using reversible assembly (Supp Fig 6).
        5.  **3D Transport:** Vertical transport of actuators along helical structures (Fig 6c,d,e).
        6.  **Microfluidic Pumping:** Generation of directed fluid flow using actuators assembled on elevated tracks (Fig 6f,g).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The behaviors are validated primarily through direct observation via optical microscopy, documented with images and video recordings (Figs 2-6, Supp Videos). Locomotion is quantified using particle tracking to measure velocity and trajectories (Fig 2e,f; Fig 4f; Methods). Fluid flow for pumping is visualized and quantified using particle image velocimetry (PIV) (Fig 6f,g; Methods). Finite element simulations are used to validate the shape-encoded DEP field modulation concept (Fig 1c-h, Fig 2b, Fig 3b,f, etc.). Control experiments implicitly include varying parameters like voltage, frequency, and number of actuators to demonstrate control over assembly and behavior (e.g., Fig 2e,f; Fig 4f,g). Reproducibility is suggested by error bars (s.d. of 3 replicates in Fig 2f). Limitations: Robustness testing across wider parameter ranges or environmental conditions is limited. Long-term stability is not assessed.

---

#Key: [chen_metamaterials_2023]

# Metamaterials: From fundamental physics to intelligent design

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews metamaterials, which are artificial structures composed of subwavelength meta-atoms arranged in arrays. Their properties (e.g., permittivity ε, permeability μ) are determined by the structure of these meta-atoms rather than their constituent materials. The review covers 3D metamaterials (e.g., SRRs, wire arrays for negative index) and 2D metasurfaces (resonant, geometric phase, propagation phase). The purpose is to control electromagnetic (EM) waves for applications like lensing, holography, cloaking, etc. A significant focus is on AI-driven (especially ANN) inverse design methods to optimize complex structures for desired functionalities, moving towards "intelligent" design processes. It does *not* describe a single, physically implemented system exhibiting autonomous cognizance.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters describe the general classes of systems reviewed, not a single specific implementation. Data reliability is 'High' as these are explicitly stated, well-established concepts in the field, cited throughout the review.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input is incident electromagnetic (EM) radiation (light, microwaves, etc.). For AI design processes, electrical energy is required for computation. For reconfigurable metamaterials (briefly mentioned), energy input (e.g., electrical bias) is needed for tuning.

### **2.2 Energy Transduction**

    *   Content: Incident EM energy is transduced via interaction with meta-atoms. Mechanisms include:
        1.  Scattering/Resonance: Energy is absorbed and re-radiated with modified phase, amplitude, or polarization (e.g., plasmonic resonance, Mie resonance).
        2.  Refraction/Reflection: EM energy path is altered based on effective medium properties (ε, μ) or phase gradients.
        3.  Absorption/Loss: EM energy converted to heat due to material properties (e.g., Ohmic loss in metals) or resonances.
        For reconfigurable elements, electrical energy might be transduced into changes in material properties (e.g., refractive index via phase-change materials or LCs, capacitance via varactors).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The review explicitly discusses efficiency as a key performance metric and challenge. Examples: Plasmonic metasurfaces suffer from loss (Sec 3.1), while dielectric metasurfaces aim for high efficiency (Sec 3.1, 3.3). Specific values are cited for examples, e.g., 86% for a TiO2 metalens (Sec 3.4), 80% for SiN metasurface (Sec 3.3). Losses are noted as a hindrance for 3D metamaterials (Sec 1, 3 intro). Efficiency is highly application- and design-dependent. Overall qualitative assessment: Varies from Low (plasmonic, resonant 3D) to High (optimized dielectric metasurfaces).

### **2.4 Energy Dissipation**

    *   Content: Primary dissipation mechanisms mentioned are intrinsic material losses (e.g., Ohmic losses in metals, absorption in dielectrics) and resonant losses. Fabrication imperfections can also contribute to scattering losses. For reconfigurable elements, energy dissipation occurs during switching (e.g., resistive heating). Quantification is not provided generally but acknowledged as significant, especially for plasmonic systems and 3D metamaterials (Sec 1, 3.1). Qualitative assessment: Can range from Low (well-designed low-loss dielectrics) to High (plasmonics, bulk 3D).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No (in the sense of intrinsic, self-modifying memory influencing future autonomous behavior as per the cognizant matter definition). Yes (in the limited sense of reconfigurable/programmable states or phase-change materials).

**(Conditional: Based on the 'Yes (limited sense)' answer, proceed with M3.2/M3.3 for reconfigurable aspects)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Potentially Long-term (Non-volatile) / Volatile

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Low (typically 1 bit per element for digital/binary)
*   Units: bits / states per meta-atom

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No (intrinsic). Yes (external/design). Yes (potential in optical ANNs).

**(Conditional: Based on 'Yes (potential in optical ANNs)', proceed for that specific case.)**

### **5.2 Computation Type:**

    *   Content: Neuromorphic / Analog (potentially Hybrid)

### **5.3 Computational Primitive:**

    *   Content: Linear Transformation / Matrix Multiplication (via diffraction). For optical ANNs, the diffractive layers physically implement linear transformations (analogous to weight matrix multiplication in electronic ANNs) through the process of light diffraction and interference. Non-linear activation functions are identified as a challenge/area for future work (Sec 6.6). Logic operations are also mentioned as realized in specific diffractive networks (Sec 6.6, ref [224]).
    *   **Sub-Type (if applicable):** Linear Transformation (Optical Diffraction)

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | EM Wave Period | Varies (fs to ns) | s | Implicit | Inferred | Based on operating frequencies (THz for visible, GHz for microwave). |
        | Resonance Lifetime | Varies | s | Implicit | Inferred | Characteristic of resonant phenomena described (Sec 3.1). |
        | Reconfiguration Speed (MEMS) | kHz to MHz | Hz | Sec 6.3 (ref [209]) | Explicit | Stated in the text for MEMS tuning. |
        | Reconfiguration Speed (Other e.g., Diodes, Phase Change) | ns to ms? | s | Sec 5.4, 6.3, Fig 7F | Implicit/Mixed | Millisecond response time mentioned for an intelligent cloak (Fig 7F, ref [164]). Diode/phase change speeds inferred general knowledge. |
        | Optical ANN Computation Time | ~Speed of light propagation | s | Sec 6.6 | Implicit | Inferred from nature of optical processing. |
    *   **Note:** Most timescales are characteristic of the physical processes or cited examples, not universally quantified for all metamaterials.

### **6.2 Active Inference:**

    *   Content: No (for standard metamaterials). Partial/Unclear (for AI-driven reconfigurable systems).
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** Prediction error reduction rate (difference between detected wave and target 'bare environment' wave over time), Timescale of adaptation (how quickly cloak adjusts to changes), Complexity of internal model (represented by ANN parameters), Control loop stability under varying environmental dynamics.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No (intrinsic). Yes (externally driven/programmed).

**(Conditional: M7.1 is "No" for intrinsic adaptation. If considering externally driven adaptation via AI control, proceed.)**

### **7.2 Adaptation Mechanism:**

    *   Content: For systems like the "intelligent cloak" (Sec 5.4), the adaptation mechanism resides in the *external AI controller* (ANN). The ANN processes sensor input and adjusts control signals (voltages) to the reconfigurable metasurface elements (varactor diodes) to modify the scattering properties. The ANN itself was presumably trained (adapted) offline using machine learning techniques (e.g., backpropagation, reinforcement learning methods mentioned in Sec 5.3 for color generation) to learn the mapping between incident waves, desired scattered waves, and required control signals. The material *responds* to these signals; it doesn't *learn* intrinsically.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors reviewed are diverse forms of EM wave manipulation: negative refraction, superlensing, cloaking (invisibility/carpet), transformation optics (illusion, mimicking gravity), wave front shaping (generalized Snell's law), focusing (metalens), beam engineering, polarization control (waveplates), holography, structural color generation, filtering, perfect absorption, tunable scattering (reconfigurable surfaces), imaging, object recognition (via integrated systems). These behaviors *result* from the collective interaction of EM waves with the designed meta-atom arrangement.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Behaviors are typically validated through:
        1.  Numerical Simulations (FDTD, FEM): Used extensively for design and prediction (Sec 4.1, 5.2).
        2.  Experimental Characterization: Measuring optical/microwave responses (transmission, reflection, phase, field patterns, focusing efficiency, imaging resolution) (e.g., Sec 2.2 Fig 2B, Sec 3.4 Fig 4).
        3.  Demonstrations: Showing the intended function (e.g., imaging with metalens, reconstructing holograms, cloaking effect) (Figs 2, 4, 7).
        Reproducibility is implied by multiple groups reporting similar phenomena, but quantitative robustness analysis is often specific to individual studies cited, not generalized in the review. Claims of "intelligence" often refer to AI design or complex functionality, not emergent cognition.

---

#Key: [harrison_mind_2022]

# Mind the matter: Active matter, soft robotics, and the making of bio-inspired artificial intelligence

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper presents a theoretical framework arguing that the materiality (physical substance and organization) of systems is crucial for cognition, challenging the traditional view of multiple realisability (MR 1.0) where cognition is seen as substrate-independent software. It draws on active matter physics, soft robotics, and basal cognition literature to propose an updated view (MR 2.0 or fine-grained functionalism), suggesting that cognitive functions, while potentially realisable in different media, depend significantly on fine-grained material properties and self-organizing dynamics characteristic of living systems (or systems approximating them). The purpose is to shift the theoretical foundations of AI and robotics towards emphasizing embodied, precarious, and materially-grounded approaches (like soft, active robots) that leverage principles of self-organization, homeostasis, and existential needs (survival, persistence) to achieve autonomous, goal-directed behaviour. Key components are the concepts of active matter, soft robotics, basal cognition, fine-grained functionalism, multiple realisability (1.0 vs 2.0), existential needs, and precarity. It advocates for building autonomous robots (AR) from competent, cognitive parts (scaling cognition down and up).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
    *   **Note:** This is a theoretical paper proposing a framework, not describing a specific implemented system with measurable parameters. Therefore, no key implementation parameters with values/units are applicable.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The paper conceptually discusses systems operating far-from-thermodynamic-equilibrium, which requires continuous energy input to maintain organization against entropy. It mentions active matter units converting "stored or ambient free energy into systematic movement" and maintaining homeostasis. Specific energy sources (chemical, thermal, electrical) are implied by the examples (e.g., molecular motors, oil droplets requiring chemical reactants) but not generalized or quantified for the proposed framework itself.

### **2.2 Energy Transduction**

    *   Content: Conceptually discussed in the context of active matter and biological systems. Examples include chemical energy converted to mechanical motion by molecular motors (Sec Active Matter, quoting Needleman and Dogic, 2017), transduction between chemical, kinetic, and electrostatic energy at the cellular level (Sec Traditional vs. fine-grained functionalism, quoting Godfrey-Smith 2016a). The framework emphasizes these physical processes as integral to the material basis of cognition, contrasting with abstract computational views. Specific transduction pathways for hypothetical AR are not detailed.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any quantitative or qualitative assessment of energy efficiency for the systems it discusses or proposes. The focus is on the *necessity* and *type* of energy flow for maintaining organization and driving activity in far-from-equilibrium systems relevant to cognition. Efficiency is not a central theme.

### **2.4 Energy Dissipation**

    *   Content: Mentioned implicitly through the discussion of far-from-equilibrium systems and dissipative structures (e.g., Rayleigh-Bénard cells, living organisms are explicitly dissipative). Dissipation is inherent in maintaining order against the second law of thermodynamics, requiring continuous energy input. Specific mechanisms (friction, heat loss) or quantification are not provided. The concept of "existential needs" is linked to the requirement to counteract dissipative tendencies (Egbert, 2021).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding to M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7 where applicable)**

### **4.2 Local Interaction Rules:**

    *   Content: The paper discusses local interactions conceptually rather than providing specific rules/equations. Examples mentioned include:
        *   **Active Matter:** Interactions of self-driven units converting energy, influenced by shape (anisotropy) and medium, leading to correlated motion (Marchetti et al., 2013). Interaction between internal dynamics and external medium via gradients (e.g., Marangoni flows in oil droplets, Hanczyc and Ikegami, 2010). Interactions of nanoscale molecular motors creating mesoscale structures (Needleman and Dogic, 2017).
        *   **Biological Systems:** Intercellular communication, bioelectrical signalling coordinating morphogenesis and development (Levin, 2019, 2021), cellular metabolism dynamics (Godfrey-Smith, 2016a), physical forces shaping tissues (Newman, 2019). Cells acting based on local morphogenetic goals.
        *   **Proposed AR:** Implied rules would involve local information processing, managing existential needs/precarity, maintaining homeostasis, based on the active materials constituting the robot.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    *   **Note:** While discussing conceptual rules, the paper doesn't define specific interaction parameters or their ranges for a general model.

### **4.3 Global Order:**

    *   Content: The paper explicitly discusses various forms of emergent global order resulting from local interactions:
        *   **Active Matter:** Correlated collective motion, mechanical stress (Marchetti et al., 2013), macroscopic patterns (e.g., swarming, nematic structures in microtubule networks cited via Sanchez et al., 2012), self-propelled droplet movement (Hanczyc & Ikegami, 2010).
        *   **Biological Systems:** Complex organismal morphologies achieved through development and morphogenesis (Levin, 2019; Newman, 2019), tissue structures, coordinated swarming behaviour (biofilms, multicellular bodies, animal groups).
        *   **Proposed AR:** Goal-directed behaviour, agency, autonomous self-maintenance, potentially novel morphologies (e.g., xenobots).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**
    *   **Note:** As in 4.2.1, specific parameters for the conceptual rules are not provided.

### **4.6. Globally Emergent Order and Order Parameters**
    *   **Note:** While global order is discussed (4.3), specific order parameters and their ranges are not defined or measured within the paper's framework.

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    *   **Note:** The concept of Yoneda embedding or formal assessment of local-to-global mapping fidelity using category theory is not mentioned in the paper.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", including M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog / Other (Morphological Computation / Physics-based Dynamics)

### **5.3 Computational Primitive:**

    *   Content: Not explicitly defined. The paper focuses on higher-level cognitive functions (memory, learning, decision-making, goal-directedness) emerging from underlying physical dynamics and self-organization, rather than identifying basic computational operations performed by the material itself. Primitives would likely relate to the fundamental physical interactions and self-organizing principles discussed (e.g., gradient following, threshold responses in cellular activity, physical constraint satisfaction) but are not formalized as computational primitives.

### **5.4 Embodied Computational Units**
    *   **Note:** The paper discusses conceptual units (cells, active matter particles) but doesn't quantify their computational properties.

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description                     | Value                                             | Units   | Source                                                 | Implicit/Explicit   | Justification                                                                        |
        | :---------------------------------------- | :------------------------------------------------ | :------ | :----------------------------------------------------- | :------------------ | :----------------------------------------------------------------------------------- |
        | Cellular Metabolism/Molecular Dynamics    | Nanoseconds to Seconds (Qualitative)              | s       | Sec Trad. vs Fine-grained Func. (quoting Godfrey-Smith) | Implicit          | Reference to water molecule collisions (trillions/sec) implies very fast timescales. |
        | Active Matter Dynamics (e.g., motors)   | Milliseconds to Seconds (Qualitative)             | s       | Sec Active Matter (general context)                    | Implicit          | Typical timescales for molecular motors and collective motion in cited examples.    |
        | Self-Organization / Pattern Formation     | Seconds to Hours/Days (Qualitative)               | s/h/d   | Sec Active Matter / Sec How Fine-grained... (Morphogenesis) | Implicit          | Depends on system; morphogenesis occurs over longer times than molecular dynamics.   |
        | Organismic Behavior / Cognitive Processes | Milliseconds to Lifespan (Qualitative)            | ms to Y | General context                                        | Implicit          | Cognitive processes span rapid responses to lifetime learning/adaptation.          |
        | Development / Evolution                   | Days to Years / Generations (Qualitative)         | d/Y/gen | Sec How Fine-grained... (Morphogenesis, Evolution)       | Explicit/Implicit | Development/evolutionary timescales explicitly relevant to the scaled approach.         |
    *   **Note:** Timescales are discussed implicitly through the phenomena mentioned (molecular dynamics, active matter, development, evolution). Values are qualitative estimates based on the domains discussed.

### **6.2 Active Inference:**

    *   Content: Unclear/Partial

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The paper discusses mechanisms conceptually rather than providing specific algorithms. Mechanisms are linked to:
        *   **Material Properties:** Exploiting the soft, active, and plastic nature of materials that allows for changes in shape, structure, or response based on history or environment (Sec Introduction, Sec Active matter and soft robotics). Fine-grained functionalism implies changes occur at the material level (Sec Traditional vs fine-grained...).
        *   **Self-Organization & Homeostasis:** Adaptation emerges from the dynamics of self-organizing systems striving to maintain homeostasis and meet existential needs in far-from-equilibrium conditions (Sec Introduction, Sec How fine-grained...). Changes are driven by feedback loops inherent in maintaining viability.
        *   **Basal Cognition:** Mechanisms associated with basal cognition like learning and decision-making at cellular/sub-organismal levels contribute to overall adaptation (Sec Introduction, Sec How fine-grained...). Cited examples include memory in bacteria, learning in protists.
        *   **Developmental Bioelectricity:** Bioelectrical networks guiding morphogenesis allow for plastic changes in body plan during development and regeneration (Levin, 2019, cited in Sec How fine-grained...).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors discussed as emerging from the proposed framework (grounded in materiality, self-organization, basal cognition) are:
        *   **Autonomous Goal-Directedness:** Systems capable of selecting and pursuing goals relative to their existential needs (survival, persistence) without continuous external control (Abstract, Intro).
        *   **Agency:** The ability to flexibly and actively engage with the environment (Intro, Sec How fine-grained...).
        *   **Valuing:** Attributing significance or valence to environmental states or actions based on existential needs/precarity (Intro, Sec How fine-grained...). Making things "matter" to the system.
        *   **Learning & Memory:** As fundamental cognitive capacities emerging from material dynamics (Abstract, Sec Trad vs fine-grained, Sec How fine-grained...).
        *   **Decision-Making:** Choosing actions to meet needs (Abstract, Sec How fine-grained...).
        *   **Self-Maintenance/Homeostasis:** Actively maintaining internal states within viable ranges (Intro, quoting Man & Damasio 2019).
        *   **Morphogenesis/Shape-Shifting:** Adaptive changes in physical form (discussed in context of biology and goal for soft robotics, Sec How fine-grained...).
        *   **Collective Behaviors:** Swarming, coordinated motion in active matter and biological systems (Sec Active Matter).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

---

#Key: [kim_nanoparticle-based_2020]

# Nanoparticle-based computing architecture for nanoparticle neural networks

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a nanoparticle-based computing architecture inspired by the von Neumann Architecture (VNA), termed Nanoparticle VNA (NVNA), implemented on a Lipid Nanotablet (LNT) platform. It performs molecular logic operations and forms nanoparticle neural networks (NNNs). The hardware consists of three types of DNA-modified nanoparticles on a lipid bilayer: immobile Nano-Memory (NM, gold NPs, green scattering) for data storage, mobile Nano-Floaters (NF) as processing units, and immobile Nano-Reporters (NR, gold-silver core-shell NPs, blue scattering) as output units. The software comprises DNA strands in solution (Instruction DNAs: Trap and Report types) that program the logic circuits by instructing NF binding kinetics to NM or NR via DNA hybridization based on the stored memory state (0 or 1) on NM. The purpose is to create a programmable, resettable, and potentially scalable molecular computing platform capable of executing arbitrary Boolean logic operations and forming NNNs like perceptrons for logical decision-making.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source driving the computation (nanoparticle assembly/disassembly) is the chemical potential energy released upon DNA hybridization and the thermal energy related to diffusion and dehybridization (reset step). Specific fuels (like ATP) are not mentioned; the energy comes from the formation of DNA duplexes. The system also requires thermal energy for operation and reset.

### **2.2 Energy Transduction**

    *   Content: Chemical potential energy from DNA hybridization is transduced into changes in the system's configuration, specifically the assembly of mobile NF nanoparticles onto immobile NM (trapping/memory interaction) or NR (reporting/output) nanoparticles. This binding changes the spatial distribution and association state of the nanoparticles, representing the computational result. Thermal energy drives the diffusion of NFs and is used to reverse the hybridization (reset step), transducing thermal energy into configurational changes (disassembly).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss or quantify energy efficiency. Molecular computing based on diffusion and binding is generally considered slow and potentially inefficient compared to electronic computing, although potentially low-power per operation. The need for solution exchange and temperature changes for reset also implies energy expenditure not directly related to computation. The score is low due to the complete lack of information and the inherently stochastic, diffusion-limited nature of the process. Qualitative Assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation mechanisms are not explicitly discussed or quantified. Potential mechanisms include: 1) Heat released during exothermic DNA hybridization. 2) Viscous drag/friction during NF diffusion in the lipid bilayer/solution. 3) Energy loss during nanoparticle collisions. 4) Heat required/lost during temperature changes for the reset step. Qualitative Assessment: Assumed to be Medium/High due to stochastic diffusion and chemical reactions, but not quantified.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: >12
*    Units: hours

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 1 (per NM particle)
*   Units: bit

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: Skipping M4.2-M4.7 as M4.1 is "No")**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Hybrid/Neuromorphic

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operation is **competitive, kinetically controlled nanoparticle binding**. Mobile NFs can bind to immobile NMs (memory/trap) or NRs (output/report). The relative rates of these binding events are modulated by the memory state stored on NMs and the specific mixture of Instruction DNAs (Trap and Report types) present. This kinetic competition effectively implements a decision process (NF is trapped vs. NF reports) which forms the basis for logic gates and perceptron activation functions. For example, an If-Then-Else statement is implemented: IF (fast trapping condition met based on NM state and Trap DNAs), THEN (NF is trapped, Output=0), ELSE (NF binds to NR via slower Report DNA, Output=1).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | NF Trapping to NM (Logic Allowed) | < 10 (goal) | minutes | Methods section ("achieve trapping faster than reporting") | Implicit | Goal stated for kinetic differentiation. Actual time depends on specific gate/conditions, likely minutes based on Fig 2A. |
        | NF Reporting to NR (Initial Lag) | ~5 | minutes | Results (Fig 2A) | Explicit | Explicitly mentioned lag time for Report DNA kinetics. |
        | NF Reporting to NR (Completion) | ~30 | minutes | Results (Fig 2A commentary) | Explicit | Stated that >85% assembly occurs within 30 min. |
        | Logic Operation Execution | ~30 | minutes | Materials and Methods | Explicit | Logic operation monitored for 30 min. |
        | Memory (NM) Storage Step | 30 | minutes | Materials and Methods | Explicit | Incubation time for Input DNA. |
        | Memory (NM) Retention | >12 | hours | Results (fig. S3 ref) | Explicit | Stability duration mentioned. |
        | System Reset | 30 | minutes | Results, Materials and Methods | Explicit | Duration of reset procedure at 50°C. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: Skipping M7.2 as M7.1 is "No")**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors are:
        1.  **Molecular Information Storage:** Storing binary data (0/1) on NM nanoparticles using DNA hybridization.
        2.  **Programmable Boolean Logic Execution:** Implementing a functionally complete set of Boolean logic gates (YES, NOT, AND, OR, NAND, NOR, XOR, XNOR, INH) by introducing specific sets of Instruction DNAs that control NF binding kinetics based on stored NM states.
        3.  **Nanoparticle Neural Network (Perceptron) Operation:** Forming single-layer and multi-layer perceptrons where NMs are inputs, NFs are hidden nodes (with activation determined by kinetic competition), and NRs are outputs. Instruction DNAs serve as programmable weights.
        4.  **Sequential Decision-Making:** Executing a decision tree process (e.g., a 2-bit comparator) by combining NNN operations with the system's reset function to perform sequential logical steps on a single chip.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation relies on experimental observation and quantification.
        1.  **Microscopic Imaging (DFM):** Time-lapse dark-field microscopy is used to observe nanoparticle positions, colors (distinguishing NM, NF, NR), and mobility, allowing tracking of individual binding events (NF-NM trapping, NF-NR reporting) (Fig. 1C).
        2.  **Quantitative Analysis (Reporting Ratio):** The ratio [(Reporting)/(Reporting + Trapping)] is calculated from DFM images to quantitatively determine the output state ('1' if > 0.2, '0' if < 0.2) (Fig. 2C, Fig. 3).
        3.  **Logic Gate Verification:** Experimental results for each gate are compared against the expected truth tables (Figs 2C, 3A, 3C, 3D; Table S1 referenced).
        4.  **NNN/Comparator Demonstration:** Successful execution of implemented NNN logic gates and the 2-bit comparator decision tree demonstrates the functionality (Fig. 3, Fig. 4).
        5.  **Control Experiments:** Implicit controls are present (e.g., comparing input '0' vs '1' cases). The kinetic measurements in Fig 2A characterize the differential responses underlying the logic.
        6.  **Reproducibility:** Reusability experiments (Fig 3B) show reproducibility over limited cycles.
        Limitations: Validation is performed under specific, optimized conditions. Robustness testing across a wider range of parameters is limited. Long-term stability beyond 4 cycles/12 hours is not shown. Statistical analysis of error rates across multiple trials is not detailed in the excerpt.

---

#Key: [dawson_differential_2023]

# Differential sensing with arrays of de novo designed peptide assemblies

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a differential sensor array composed of 46 unique, de novo designed self-assembling α-helical barrel (αHB) peptides, plus two controls. These αHBs form channels of varying sizes, shapes, and chemistries. The channels accommodate an environment-sensitive fluorescent dye (1,6-diphenyl-1,3,5-hexatriene, DPH). The purpose is to identify analytes (small biomolecules, complex mixtures like drinks or biological samples) by challenging the dye-loaded array with the analyte. Analyte binding causes differential displacement of the dye across the array members, resulting in a unique fluorimetric fingerprint. This fingerprint is then analyzed using machine learning (ML) models to classify the analyte or mixture. The system components are the de novo peptides, the DPH dye, buffer solutions, analytes, multi-well plates, liquid handling robots, a fluorescence plate reader, and ML algorithms for data analysis.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Table lists key operational parameters for the sensing assay.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Two primary energy inputs: 1) Chemical potential energy related to the binding affinities between the dye, analytes, and αHB channels. 2) Optical energy from the plate reader's excitation light source used to induce fluorescence.

### **2.2 Energy Transduction**

    *   Content: 1) Chemical potential energy drives the competitive binding process: higher affinity analytes displace lower affinity DPH dye from the αHB channels. 2) Absorbed optical energy (excitation light at 350 nm) by the bound DPH is transduced into emitted optical energy (fluorescence at 450 nm). Dye displacement reduces the amount of DPH in the hydrophobic channel environment, decreasing fluorescence intensity upon excitation.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide information to assess the energy efficiency of either the chemical binding displacement process or the fluorescence quantum yield of DPH within the barrels. Evaluating the overall system efficiency would also require considering the energy consumption of the plate reader and computational analysis, which is not discussed.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation mechanisms include: Non-radiative decay pathways for the excited DPH molecules (reducing fluorescence quantum yield), potential heat generation during binding/unbinding events (though likely negligible at the concentrations used), and energy loss within the optical components of the plate reader. These are not quantified in the paper. Assessment: Likely Low for binding events, Medium for fluorescence (typical quantum yields are <1).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping to Module 4.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The self-assembly is governed by local interactions defined by the heptad repeat (abcdefg) sequence: Hydrophobic residues typically at 'a' and 'd' positions drive core packing. Charged residues at 'b', 'c', 'e', 'g' positions influence oligomeric state and solubility through inter-helical electrostatic interactions (e.g., Glu-Lys/Arg pairs planned at 'b' and 'c'). Alanine at 'e' stabilizes interfaces. Residue identity at 'g' significantly impacts oligomer state (e=Ala, varied g=Ala, Asn, Gln, Glu, Ile, Ser). Specific mutations at 'a' and 'd' fine-tune channel chemistry and size (e.g., polar, charged, aromatic substitutions). These rules are based on established principles of coiled-coil design.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The emergent global order is the formation of discrete, stable α-helical barrel oligomers (mostly pentamers, hexamers, heptamers) with well-defined central channels. Specific examples confirmed by X-ray crystallography show highly ordered structures.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Helicity | Degree of alpha-helical structure | Mean Residue Ellipticity @ 222nm | Approx. -25000 to -35000 | deg cm² dmol⁻¹ | Explicit | CD spectra show high helicity. Values estimated from Supp. Fig. 4. | CD Spectroscopy | Supp. Fig. 4 |
| Thermal Stability | Melting Temperature | T<sub>m</sub> | Most > 95 | °C | Explicit | Measured via thermal denaturation CD. | CD Thermal Denaturation | Supp. Fig. 5 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6.)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
    *   **Note:** The paper does not report specific timescales for the dynamic processes involved in sensing, only for peptide characterization (thermal melts).

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip to Module 8.)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is *differential sensing* leading to *analyte classification/discrimination*. The array of αHBs, when loaded with DPH dye and exposed to an analyte, produces a specific pattern of fluorescence changes (a fingerprint) across its different elements due to varying degrees of dye displacement. This fingerprint serves as the input for ML models that classify the analyte (e.g., distinguishing between different amino acids, fatty acids, teas, or serum samples).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary emergent behavior claimed is the ability of the array's *collective* differential response to discriminate analytes, which is validated through ML classification performance. Validation methods include:
        1.  **Cross-Validation:** Nested stratified k-folds cross-validation is used to train and test ML models, providing an estimate of generalization performance (Methods, Table 1).
        2.  **Performance Metrics:** Accuracy, Precision, F1 Score are reported for classifications (Table 1).
        3.  **Comparison to Controls:** Performance is compared against dummy classifiers (random guessing) using statistical tests (5x2CV F-test) to ensure performance is significantly above chance (Methods, Supplementary Tables 3-5, 7, 10, 11).
        4.  **Feature Analysis:** Methods like KBest and permutation analysis identify which αHBs contribute most, validating that the discrimination arises from the differential response across the array (Results, Fig. 5).
        Limitations: Validation relies heavily on ML performance; direct validation of the *emergence* aspect (i.e., predicting classification success purely from local binding interactions) is not performed. Reproducibility across different labs/batches is not shown.

---

#Key: [kramar_encoding_2021]

# Encoding memory in tube diameter hierarchy of living flow network

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system studied is the slime mold *Physarum polycephalum*, a giant unicellular organism with a network-like body composed of interlaced tubes of varying diameters. The paper investigates how this organism encodes memory about the location of a nutrient source. The core mechanism involves the nutrient stimulus triggering the local release of a softening chemical agent. This agent is transported through the tubular network via cytoplasmic flows (driven by peristaltic contractions). Tubes receiving sufficient agent soften and dilate (grow in diameter), while other tubes shrink due to mass conservation. This differential change in tube diameters creates a new, persistent hierarchy that reflects the nutrient source's location. The purpose of this memory encoding is to guide future organism behavior, specifically migration and decision-making in foraging.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Tube Radius (resting, model) | 50 | µm | Methods | Explicit | Medium | Theoretical model parameter |
        | Cytoplasm Viscosity (model) | 6.4·10⁻³ | Ns/m² | Methods | Explicit | Low | Literature value used in model |

    *   **Note:** Model parameters are listed as stated in the Methods section. Experimental times and speeds are derived from observations described in Results and Figures.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The study focuses on the response to a nutrient *stimulus* (bacteria pellets or glucose/leucine solution), which implies chemical energy is available. However, the *trigger* for the observed mechanism is the *sensing* of the nutrient, leading to internal processes. The energy driving the cytoplasmic flow and tube contractions is internal metabolic energy derived from the organism's overall state, not directly quantified or linked to the specific stimulus event in this mechanism. The immediate energy input for the *process* studied is the chemical potential represented by the nutrient gradient/presence, triggering the release of the softening agent.

### **2.2 Energy Transduction**

    *   Content: 1. Nutrient Sensing: Chemical potential of the stimulus is transduced into a biological signal leading to the release of a softening agent (Chemical -> Biological Signal -> Chemical Release). 2. Agent Transport: Potential energy stored in the contracted actomyosin cortex drives cytoplasmic flow (Mechanical/Chemical -> Kinetic), advecting the softening agent. 3. Tube Softening: The chemical agent alters the material properties (elastic modulus) of the tube walls (Chemical -> Mechanical Property Change). 4. Tube Dilation/Shrinkage: Pressure differences resulting from flow and altered wall mechanics lead to changes in tube diameter, redistributing mass (Fluid Pressure/Mechanical -> Structural Change).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide information to quantify the energy efficiency of the memory encoding process (e.g., energy cost of agent release/transport vs. information stored or work done in restructuring). Biological processes are generally not perfectly efficient, but no specific assessment is possible from the text.

### **2.4 Energy Dissipation**

    *   Content: Energy is primarily dissipated through: 1. Viscous dissipation during cytoplasmic flow within the tubes (explicitly modeled via Stokes flow with viscosity µ). 2. Mechanical dissipation within the viscoelastic tube walls during contraction/relaxation and dilation/shrinkage (implied by the elastic modulus E and stress descriptions). 3. Chemical degradation of the softening agent (explicitly included in the model with decay rate k_deg). Quantification is not provided for the overall system, but model parameters (µ, k_deg, E) relate to these mechanisms. Qualitative assessment: Viscous dissipation is likely significant due to low Reynolds number flow (Re ~ 10⁻³).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding with M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: > 45 min, potentially hours
*    Units: minutes (min) / hours (hr)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: k_deg = 0.001 (in model)
    *   Units: s⁻¹

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", proceeding with M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content:
        1.  **Agent Release:** Nutrient stimulus locally triggers release of softening agent `c` (modeled as one-time release at stimulus site).
        2.  **Flow Generation:** Peristaltic contractions (σ_T = A cos(ωt - kz)) generate pressure gradients, driving cytoplasmic flow `u` governed by lubrication approximation of Stokes flow (Eq. 1). Flow depends on tube radius `a` and pressure gradient (related to stress σ_T + σ_E).
        3.  **Agent Transport:** Agent `c` is advected by flow `u` and disperses/diffuses (Taylor dispersion) within tubes, subject to decay (rate k_deg) (Eq. 3). ∂c/∂t = -ū∂c/∂z + ∂/∂z[(κ + ū²a²/48κ)∂c/∂z] - k_deg c.
        4.  **Tube Wall Softening:** Time-averaged agent concentration `<c>` locally reduces the elastic modulus `E` of the tube wall (Eq. 2). E = E₀ - δE * <c> / (<c>₀ + <c>).
        5.  **Diameter Change:** Tube radius `a` changes based on the balance between internal fluid pressure (related to σ_T + σ_E) and the (softened) elastic restoring stress σ_E = E/h(a - a₀), subject to global mass conservation (Eq. 4). ∂a²/∂t = -∂(a²ū)/∂z. Tubes receiving more agent soften and dilate, while others shrink to conserve volume.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | 2 | Flow Generation | Contraction Amplitude (A) | 10 | Pa | Methods | Explicit | Parameter in model. |
    | 2 | Flow Generation | Contraction Frequency (ω) | 2π/120 (model); 4-10 (exp) | rad/s; mHz | Methods; Fig 4D | Explicit | Parameter in model; Range observed experimentally. |
    | 2 | Flow Generation | Cytoplasm Viscosity (µ) | 6.4·10⁻³ | Ns/m² | Methods | Explicit | Parameter in model (literature value). |
    | 3 | Agent Transport | Agent Decay Rate (k_deg) | 0.001 | s⁻¹ | Methods | Explicit | Parameter in model. |
    | 3 | Agent Transport | Molecular Diffusivity (κ) | 10⁻¹⁰ | m²/s | Methods | Explicit | Parameter in model (estimated value). |
    | 4 | Tube Wall Softening | Baseline Elastic Modulus (E₀) | 10 (effective) | Pa | Methods | Explicit | Parameter in model (derived from literature). |
    | 4 | Tube Wall Softening | Softening Strength (δE) | 2.5 | Pa | Methods | Explicit | Parameter chosen for model. |
    | 4 | Tube Wall Softening | Agent Saturation Conc. (<c>₀) | 10.0 | (arbitrary units) | Methods | Explicit | Parameter chosen for model. |
    | 5 | Diameter Change | Resting Radius (a₀) | 50 | µm | Methods | Explicit | Parameter in model. |

### **4.3 Global Order:**

    *   Content: The emergent global order is the new spatial hierarchy of tube diameters across the network. Specifically, tubes closer (in terms of flow-based travel time) to the nutrient stimulus become significantly thicker, while tubes farther away shrink, creating a persistent, graded pattern reflecting the stimulus location. This leads to a directed structure favouring transport towards the memory location.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| 1-5     | See M4.2    | See M4.2.1 | See M4.2.1 | See M4.2.1 | Explicit         | Rules and parameters are explicitly defined in the model. | Methods, Eqs 1-4 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Hierarchy | Tube Diameter Distribution | Relative Tube Growth | Fig 2B shows range visually (approx -0.5 to +1.0) | unitless | Explicit | Fig 2B caption describes relative growth. | Image Analysis (Methods) | Fig 2B |
| Hierarchy | Tube Diameter Change over Time | Relative Diameter Change | Fig 3 shows dynamics visually | unitless | Explicit | Fig 3 caption describes relative diameter evolution. | Image Analysis (Methods) | Fig 3 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | Local Rule -> Global Hierarchy | Mapping from agent-flow-softening dynamics to the final tube diameter pattern. | Medium-High (Qualitative) | 2 | Visual comparison (Fig 4), Frequency dependence (Fig 4D) | Mixed | Predictability qualitatively supported by model-experiment match. Yoneda score low as formal mapping isn't discussed. | Results, Fig 4 |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 2 (Rubric: 0=No mapping discussed; 2=Qualitative link between local rules and global pattern described, but no formal category theory/Yoneda embedding analysis; 5=Quantitative correlation between local interactions and global order parameters; 8=Explicit attempt at functorial mapping; 10=Rigorous demonstration of Yoneda Lemma applicability).
    *   **Metrics:** Comparison of simulated vs. experimental time dynamics (Fig 4C), response time vs. frequency relationship (Fig 4D). Visual comparison of spatial patterns (implied by Fig 2B discussion).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6.)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Contraction Period | ~100-250 | s | Calculated from Fig 4D (4-10 mHz) | Explicit | Frequency range given. |
        | Stimulus Response Time (Initial Dilation) | < Few Minutes | min | Inferred from Fig 3/Fig 4 kinetics | Implicit | Figs show rapid onset. |
        | Memory Encoding Time (Hierarchy Established) | ~15 | min | Results, Fig 3 | Explicit | Stated in text, visible in Fig 3. |
        | Dilation Propagation | ~Seconds to Minutes | s/min | Depends on distance (speed 15 µm/s) | Mixed | Speed given, time depends on network size. |
        | Memory Retention | >45, potentially >310 | min | Fig 3, Fig 1 | Explicit | Visible in figures, discussed in text. |
        | Agent Decay (Model) | ~1000 (1/k_deg) | s | Methods (k_deg=0.001 s⁻¹) | Explicit | Parameter given in model. |
        | Migration Timescale | ~Hours | hr | Fig 1 (45-310 min) | Explicit | Visible in Fig 1 sequence. |

### **6.2 Active Inference:**

    *   Content: Unclear

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", proceeding with M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism involves the local release of a softening chemical agent triggered by a nutrient stimulus. This agent is transported by contraction-driven cytoplasmic flows. Tube segments receiving sufficient agent soften (reduction in elastic modulus, Eq. 2) and subsequently dilate due to internal pressure, assuming sufficient flow/mass supply (governed by flow dynamics Eq. 1 and mass conservation Eq. 4). Tubes not receiving the agent, or farther away, shrink to conserve the total volume/mass within the network. This differential growth/shrinkage adapts the network's transport properties, strengthening pathways towards the stimulus. The feedback is mediated by flow: the existing network structure influences flow, which transports the agent, which modifies the structure, which in turn modifies future flow. It resembles reinforcement (strengthening used pathways) and Hebbian principles ("tubes that flow together, grow together," paraphrased).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors described are: 1. **Network Reorganization:** Rapid change in the spatial hierarchy of tube diameters in response to a localized nutrient stimulus. 2. **Memory Encoding:** Imprinting the location of the nutrient stimulus into this persistent tube diameter hierarchy. 3. **Directed Migration:** Subsequent movement of the organism towards the location of the remembered stimulus, guided by the altered flow patterns within the reorganized network. 4. **Information Propagation:** Spreading the signal about the stimulus location through the network via flow-transported chemical agent.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behavior (network reorganization, memory encoding, directed migration) are primarily validated through: 1. **Direct Experimental Observation:** Time-lapse microscopy showing the change in tube diameters (Fig 2, Fig 3), migration (Fig 1). 2. **Quantitative Analysis:** Measurement of tube diameter dynamics, calculation of relative growth (Fig 2B), tracking dilation propagation speed (Fig 3), correlating response time with contraction frequency (Fig 4D). 3. **Theoretical Modeling:** Development of a physics-based model incorporating local rules (agent release, flow, softening, mass conservation) that successfully reproduces the key observed dynamics (Fig 4C) and frequency dependence (Fig 4D). The convergence of experimental observation, quantitative analysis, and theoretical prediction provides support for the claims. Reproducibility seems implied across different datasets (Fig 4D uses multiple datasets). Limitations include the model's simplification (single tube) and the unknown identity of the softening agent.

---

#Key: [yang_task_2019]

# Task representations in neural networks trained to perform many cognitive tasks

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a recurrent neural network (RNN) model trained using supervised learning to perform 20 different cognitive tasks (variants of memory-guided response, decision making, categorization, working memory, inhibitory control, etc.). Its components include input units (fixation, stimulus modalities 1 & 2, rule inputs), recurrent units (256 units in the reference model, with modifiable weights), and output units (fixation, motor response ring). The purpose is to investigate how neural representations of multiple cognitive tasks emerge and organize within a single network, specifically looking for functional specialization (clustering) and compositionality, providing a computational platform to explore mechanisms underlying cognitive flexibility in the brain.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name        | Value            | Units       | Source (Fig/Table/Section) | Implicit/Explicit   | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :-------------------- | :--------------: | :----------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These are key parameters defining the computational model's structure and dynamics used in the reference setting analyzed in detail.

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Up to 5
*    Units: seconds (s)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: High (e.g., >90% for most tasks)
*   Units: % correct performance

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Low (within the tested 5s delay)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial

### **4.2 Local Interaction Rules:**

    *   Content: The "local interaction rules" are the synaptic weight update rules dictated by the supervised learning algorithm (Adam optimizer minimizing mean squared error between network output and target output). Specifically, the change in a weight depends on the gradient of the global loss function with respect to that weight, which involves activity of pre- and post-synaptic units and backpropagated error signals. The continuous dynamics are governed by the RNN equation: `r_t = (1-α)r_{t-1} + α*f(Wrec*r_{t-1} + Win*u_t + b + noise)` where `f` is the activation function (e.g., Softplus). The learning rule modifies `Wrec`, `Win`, `Wout`, and `b`.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The emergent global order described is the formation of functional clusters of recurrent units. Units within a cluster exhibit similar patterns of task variance across the 20 tasks, indicating specialization for related cognitive processes or task components (e.g., Anti-tasks, DM tasks for modality 1, response generation). This order is visualized in the task variance matrix (Fig 2c) and t-SNE embedding (Fig 2d).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Clustering | Number of Clusters | k<sub>optimal</sub> | ~2-30 (range across networks) | clusters | Explicit | Distribution shown in Fig 3b, selection method in Methods. | k-means, Silhouette maximization | Fig 3b, Methods |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

### **5.2 Computation Type:**

    *   Content: Neuromorphic / Analog (within the simulation)

### **5.3 Computational Primitive:**

    *   Content: The basic computational operations are: (1) Weighted summation of inputs (from input layer and recurrent layer) and bias. (2) Application of a non-linear activation function (e.g., Softplus: `f(x)=log(1+exp(x))`) to the sum. (3) Time integration/update rule (`r_t = (1-α)r_{t-1} + α*f(...)`).
    *   **Sub-Type (if applicable):** Activation Function: Softplus, ReLU, Tanh, ReTanh.

### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description      | Value            | Units        | Source                     | Implicit/Explicit | Justification                       |
        | :------------------------- | :--------------: | :-----------: | :-------------------------: | :----------------: | :---------------------------------: |
        | Unit Time Constant (τ)     | 100              | ms           | Methods "Network structure" | Explicit          | Basic parameter of the unit dynamics. |
        | Integration Timestep (Δt)  | 20               | ms           | Methods "Network structure" | Explicit          | Discretization step for simulation. |
        | Task Epoch Durations       | Variable (e.g., ~200-1600+) | ms | Methods "Tasks..."          | Explicit          | Durations for fixation, stimulus, delay, go epochs vary by task/trial. |
        | Max Memory Retention (Tested) | 5000 (5)         | ms (s)       | Suppl Fig 2g, Results       | Explicit          | Longest delay tested for working memory tasks. |
        | Stimulus Integration Time (DM) | 400, 800, 1600   | ms           | Methods "DM family"         | Explicit          | Durations tested for perceptual DM tasks. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The primary adaptation mechanism is supervised learning using gradient descent (specifically, the Adam optimizer) to minimize the mean squared error between the network's output and a target output. This involves calculating gradients via backpropagation through time and updating all connection weights and biases. For sequential learning (Fig 8), a continual learning technique (Synaptic Intelligence) is used, which adds a penalty term to the loss function to protect weights deemed important for previously learned tasks, mitigating catastrophic forgetting. This involves tracking parameter importance (`Ω`) based on contributions to loss changes during past task training.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behavior is performing a repertoire of 20 different cognitive tasks accurately. These include tasks requiring working memory (e.g., Dly Go, maintaining information over a delay), decision-making (e.g., DM1/2, choosing based on stimulus strength; Ctx DM1/2, context-dependent choice), multi-sensory integration (MultSen DM), categorization (DMC/DNMC), and inhibitory control (e.g., Anti tasks, responding opposite to stimulus). The network exhibits psychophysically plausible behavior, such as improved performance with higher stimulus coherence or duration (Fig 1d) and integration of evidence from multiple modalities (Fig 1e). Key emergent *internal* behaviors include the formation of functional clusters (Fig 2) and compositional task representations (Fig 6).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content:
        *   **Task Performance:** Validated by measuring accuracy/performance metrics across all 20 tasks under various conditions (reported percentages, psychometric curves - Fig 1c,d,e; Fig 3b caption; Fig 8b,c; Suppl Fig 2).
        *   **Functional Clustering:** Validated by calculating task variance for each unit across tasks, normalizing, using k-means clustering based on these variance vectors, selecting optimal k via silhouette score maximization, and visualizing (Task variance matrix Fig 2c, t-SNE Fig 2d, Silhouette score Suppl Fig 4). Causal role validated by lesioning clusters and measuring performance impact (Fig 2e). Robustness checked across multiple network initializations (Methods).
        *   **Compositionality (Representation):** Validated by computing average population activity vectors ("task vectors") for related tasks, performing PCA, and showing parallel vector relationships in the low-dimensional state space (e.g., Go -> Dly Go vector ≈ Anti -> Dly Anti vector) across multiple networks (Fig 6).
        *   **Compositionality (Performance):** Validated by testing network performance when provided with algebraically combined rule inputs (e.g., performing Dly Anti task with rule input `Anti + (Dly Go - Go)`) and comparing to performance with standard rule inputs or other combinations (Fig 7b,c).
        *   **Continual Learning Effects:** Validated by comparing task performance after sequential training with and without the continual learning algorithm (Fig 8b,c) and analyzing resulting neural representations (FTV distributions, Fig 8d). Comparison to monkey PFC data is also presented (Fig 8e).

---

#Key: [mertan_no-brainer_2025]

# No-brainer: Morphological Computation Driven Adaptive Behavior in Soft Robots

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of virtual, 2D voxel-based soft robots simulated using EvoGym. These robots achieve adaptive locomotion without an explicit controller ("brain"). Their bodies are composed of different voxel types: passive (soft, rigid), active (sinusoidally actuating for locomotion energy), and sensory (4 types responding to 2 binary environmental stimuli by expanding/contracting). Robot morphologies are generated using Compositional Pattern Producing Networks (CPPNs) and optimized using the MAP-Elites evolutionary algorithm. The purpose is to demonstrate that complex, adaptive behaviors (changing direction based on stimuli) can emerge solely from the physical interactions and shape changes within the robot's body ("morphological computation") in response to environmental cues. A swarm configuration is also explored to show how combining these robots can implement more complex functions like logic gates (AND, NAND) and a D-latch.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters related to the EvoGym simulation physics (e.g., material properties beyond actuation range, friction) are not detailed in this paper but in the EvoGym reference [3]. CPPN parameters (node functions, mutation details) are partially described.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source for locomotion is the sinusoidal area change signal applied to the two types of active voxels (orange and teal). This signal drives their expansion and contraction.

### **2.2 Energy Transduction**

    *   Content: The input sinusoidal signal energy is transduced into mechanical work by the active voxels, causing them to change area. These local deformations propagate through the connected voxel structure (passive, sensory, other active voxels) via physical forces. This coordinated, morphology-dependent deformation results in the net displacement of the robot's center of mass, i.e., locomotion. The sensory voxels also transduce the presence/absence of stimuli into mechanical shape changes (expansion/contraction), which modifies the overall structure and thus alters how the energy from active voxels is converted into locomotion.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss or quantify the energy efficiency of locomotion (e.g., energy input vs. work done for displacement). Performance is measured purely by displacement.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation mechanisms are inherent to the physics simulation but not explicitly discussed or quantified in the paper. Likely mechanisms within the EvoGym simulator include internal damping/viscosity within the soft voxels during deformation and friction between the robot and the simulated ground surface. Assessed qualitatively as Medium, as dissipation is necessary for stable locomotion in physical simulations.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No (for individual robots); Yes (for the swarm system).
    For the swarm system (Section 4): The hand-designed interconnected structure of robots implementing logic gates (specifically the D-latch example in Fig 5) demonstrates state retention. The output of the starred robot depends not only on the current swarm stimuli but also on the *previous* state derived from the network's configuration and interactions, mimicking memory. The paper explicitly states this behavior "would typically require memory to achieve."

**(Conditional: M3.1 is "Yes" for the swarm, proceeding with M3.2 and M3.3 for the swarm context)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Short-term/Volatile

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 1
*   Units: bit

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: High (Qualitative)

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip rest of Module 4.)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", proceed to M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid

### **5.3 Computational Primitive:**

    *   Content: Stimulus-gated directional control / Logic gate implementation.
    *   **Sub-Type (if applicable):** For individual robots, the primitive is mapping the 4 possible stimulus patterns ({A,A}, {A,P}, {P,A}, {P,P}) to one of two locomotion directions (Left or Right). This acts as a 2-input, 1-output Boolean function where the output is expressed kinetically. For the swarm, the primitives are explicitly Boolean logic gates (AND, NAND) implemented through the behavior of individual robots.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Stimulus Change Interval | 10 | Actuation Cycles | Section 2 | Explicit | Duration for which stimuli remain constant in variable evaluation. |
        | Robot Response Time | <= 10 | Actuation Cycles | Section 2, Fig 2, 3 | Implicit | Robots change direction within the 10-cycle window after stimulus change. |
        | Evolutionary Timescale | 3000 | Generations | Section 2 | Explicit | Duration of the optimization process. |
        | Swarm Update Cycle | 1 | Actuation Cycle | Section 4 | Implicit | Assumed timeframe for one robot's behavior to influence another's input stimulus. |
    *   **Note:** The absolute time value of an "actuation cycle" depends on the frequency of the sinusoidal signal, which is not provided.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes (Behavioral adaptation); No (Structural adaptation within a run)

**(Conditional: M7.1 is "Yes" for behavioral adaptation, include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism is stimulus-induced morphological change leading to altered physical dynamics. Specific sensory voxels expand or contract based on the presence/absence of binary stimuli. This alters the robot's overall shape and potentially its mass distribution and stiffness profile. The interaction of this altered morphology with the actuation forces from the active voxels and the environment's physics results in a different gait or locomotion pattern, leading to a change in movement direction. This mapping from stimulus pattern to behavioral output is implicitly encoded in the evolved morphology by the evolutionary algorithm. There is no explicit learning rule operating during the robot's runtime.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors are:
        1.  **Adaptive Locomotion:** Movement (nominally Left or Right along the x-axis) whose direction is controlled by the pattern of two binary environmental stimuli. Different evolved robots exhibit different mapping functions (e.g., LLLL, LRRL, LRLR, etc.).
        2.  **Logic Gate Implementation (Swarm):** Specific robots evolved for certain stimulus-response mappings (e.g., RRRL, RRLL) are used as components in a larger, hand-designed swarm to implement Boolean logic functions (NAND, AND).
        3.  **Sequential Logic / Memory (Swarm):** The swarm configuration demonstrates a D-type latch behavior, exhibiting state retention dependent on the history of swarm stimuli.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation is primarily through simulation and observation.
        *   **Adaptive Locomotion:** Demonstrated via spacetime diagrams tracking the center of mass under changing stimuli (Figs 2, 3). Performance is quantified by displacement (Fig 4). Consistency checks between combined and single stimuli simulations are used to filter unstable gaits (Sec 2).
        *   **Logic Gates/D-Latch:** Demonstrated via spacetime diagrams of the swarm, showing individual robot behavior and the overall swarm output matching the expected logic function (Fig 5).
        *   **Limitations:** Validation is purely in simulation. Robustness testing is limited. Reproducibility is supported by providing code (Sec 2). The claim that this is the "first example of a closed-loop fully-morphological behavior" (Sec 4) might be strong, depending on definitions, but the specific implementation is novel.

---

#Key: [sharma_assembly_2023]

# Assembly theory explains and quantifies selection and evolution

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is Assembly Theory (AT), a mathematical and computational framework designed to quantify the complexity of objects based on their minimal assembly pathways from basic building blocks. It models how selection and evolution arise from the combinatorial possibilities of constructing objects (represented abstractly, e.g., as integers corresponding to linear polymers, or strings representing polymer sequences). Key components include: basic objects (e.g., monomer '1'), assembly operations (e.g., joining/addition), assembly paths (sequences of operations), assembly index 'a' (length of the shortest path), assembly pool (set of objects formed), Assembly Universe (space of all possible objects), selection parameters (𝛿, 𝛼), kinetic parameters (𝑘_T, 𝑘_P, 𝛽), and object copy numbers (𝑛_𝑖). The purpose is to provide a quantitative measure ('Assembly', A) that captures the effects of contingency, selection, and complexity in ensembles of objects, differentiating objects requiring selection/memory for formation from those likely to form randomly. It explores the dynamics of how unique objects are discovered (Assembly Possible vs. Assembly Contingent) and how their populations change over time considering production kinetics.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Assembly Index (a) | variable (e.g., 3, 4) | steps | Section 2, Eq. 1-3, Figs S1-S3, S5, S8-S9, S11-S18 | Explicit | High | Defined by shortest path algorithm |
        | Copy Number (n_i) | variable (e.g., 10^12) | count | Section 3.2, Eq. 1, 3, 16-21, Figs S8, S16-S18 | Explicit | High (in model) | Given as initial condition or solved from kinetics |
        | Selection Parameter (𝛼) | variable (0-1) | dimensionless | Section 5.2, Eq. 8, 14-15, 19-21, Figs S11, S17-S18 | Explicit | High (in model) | Phenomenological parameter in dynamic model |
        | Discovery Rate (k_T) | variable (e.g., 1e-5) | 1/time | Section 5.1, Eq. 5-7, 14-15, 18-21, Figs S11, S16-S18 | Explicit | High (in model) | Rate constant in dynamic model |
        | Production Rate Decay (𝛽) | variable (e.g., 0.5) | dimensionless | Section 8, Eq. 16, 19-21, Figs S16-S18 | Explicit | High (in model) | Parameter modifying kinetic rates |

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Variable/Potentially Infinite
*   Units: Number of distinct objects / bits (related to object description length)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: 100% (within the model)
*   Units: %

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: 0 (or governed by kinetics)
    *   Units: 1/time

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content:
        1.  **Combination Rule:** Objects combine pairwise. For linear chains (integers), this is addition: object `i` + object `j` -> object `i+j`. For polymers (strings), it's concatenation or potentially more complex joining operations (implicitly handled by the String Assembly Calculator referenced).
        2.  **Assembly Pool Update:** Newly formed unique objects are added to the assembly pool.
        3.  **Forward Process Selection (Undirected):** Two objects are chosen randomly from the assembly pool for combination (Section 6.2).
        4.  **Forward Process Selection (Directed):** The last object added (longest chain in the example) is combined with a randomly chosen object from the pool (Section 6.2).
        5.  **Length-Sorted Selection:** Objects in the pool are sorted by length, and a fraction (𝛿) is selected for combination (Section 7).
        6.  **Kinetic Rules:** The rate of formation/consumption of objects depends on copy numbers and rate constants (k_P, 𝛽) (Eq 16, 20).
        7.  **Discovery Dynamics:** The rate of discovering new unique objects at index a+1 depends on the number of objects at index a, modulated by the selection parameter 𝛼 (dN_(a+1)/dt = k_T * (N_a)^𝛼) (Eq 8, 18).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | 5 | Length-Sorted Selection | Selection Index (𝛿) | 10^-4 - 0.8 (example) | dimensionless | Section 7, Fig S15 | Explicit | Parameter controlling selection fraction. |
    | 6 | Kinetic Consumption | Production Rate Decay (𝛽) | 0.5 (example) | dimensionless | Section 8, Eq 16, 20, Figs S16-S18 | Explicit | Factor modifying rate constant with assembly index. |
    | 6 | Kinetic Consumption | Initial Production Rate (k_P) | 1e-6 (example) | 1/time | Section 8, Figs S16-S18 | Explicit | Base rate constant for production kinetics. |
    | 7 | Discovery Dynamics | Selection Parameter (𝛼) | 0.001 - 1.0 (example) | dimensionless | Section 5.2, 8, Eq 8, 14, 15, 18, Figs S11, S17, S18 | Explicit | Parameter controlling directedness of discovery. |
    | 7 | Discovery Dynamics | Discovery Rate (k_T) | 1e-5 (example) | 1/time | Section 5.1, 8, Eq 5, 18, Figs S11, S16-S18 | Explicit | Base rate constant for discovery dynamics. |

### **4.3 Global Order:**

    *   Content: The primary emergent global order described is the statistical distribution of unique objects as a function of their assembly index, N(a). Different rules and parameters lead to different distributions (e.g., Poisson-like for Assembly Possible, broader/shifted distributions for Assembly Contingent with varying 𝛼, Figs S11, S15, S18). Another global measure is the total Assembly (A) of the ensemble (Eq 1, 3), which reflects the integrated complexity and selection history. The structure of the Joint Assembly Space (Fig S12, S13) for multiple objects is also an emergent global property derived from individual paths.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| N(a) | Number of unique objects at assembly index 'a' | N(a) | Variable (e.g., 0 to >1e10) | count | Explicit | Key variable describing the emergent distribution. | Solved from Eq 5 or 8/15; Simulation (Figs) | Eq 5, 6, 8, 14, 15; Figs S5, S11, S14-S15, S18 |
| A | Total Assembly of ensemble | A | Variable (e.g., 0 to >1e15) | dimensionless | Explicit | Integrated measure of complexity and selection. | Calculated using Eq 1 or 3. | Eq 1, 3; Fig S8, S18h |
| a_max | Assembly index of max unique objects | a_max | Variable (e.g., t*k_T for 𝛼=1) | steps | Explicit | Peak of the N(a) distribution. | Calculated from derivative of N(a) or observed in simulations. | Section 5.1; Fig S11a, S11c, S15c, S15d |
| n(a) | Copy number of objects at index 'a' | n(a) | Variable (e.g., <1 to 1e12) | count | Explicit | Describes population size at each complexity level. | Solved from Eq 20; Simulation (Figs) | Eq 16, 19, 20; Figs S16-S18 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Discovery Timescale (𝜏_d) | ~1/k_T | time | Section 3.2, Section 8 (end) | Explicit | Characteristic time for discovering new unique objects. Defined relative to k_T. |
        | Production Timescale (𝜏_p) | ~1/k_P | time | Section 3.2, Section 8 (end) | Explicit | Characteristic time for producing copies of existing objects. Defined relative to k_P. |
        | Threshold Time (detection) | Variable (e.g., 1e4-1e7) | time | Section 8, Fig S17, S18g | Explicit | Time required for copy number n(a) to reach a detection threshold (e.g., 10). Depends on 𝛼, k_T, k_P, 𝛽. |
        | Simulation Time (t) | Variable (e.g., 10, 1e8) | time (dimensionless in figs) | Section 5.1, 5.2, 8, Figs S11, S16-S18 | Explicit | Duration over which dynamic models are evolved. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is selection acting on the assembly process.
        1.  **Discovery Selection (𝛼):** Parameter 𝛼 in Eq 8 (dN_(a+1)/dt = k_T * (N_a)^𝛼) biases the discovery rate of new unique objects. 𝛼 < 1 represents selection favoring specific paths (less exploration), while 𝛼 = 1 is undirected exploration (Assembly Possible). This models how the "search" adapts based on process directedness.
        2.  **Structural Selection (𝛿):** Parameter 𝛿 in the linear chain model (Section 7) represents selection based on object property (length). Only a fraction 𝛿 of sorted objects participates in further assembly, adapting the pool composition.
        3.  **Kinetic Selection (k_P, 𝛽):** The production kinetics (Eq 16, 20) incorporate preferential production/survival. The decay factor 𝛽 penalizes more complex objects (higher 'a'), adapting the copy number distribution n(a) based on complexity-dependent production efficiency.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors described are:
        1.  **Combinatorial Explosion:** The potential number of objects (Assembly Universe) grows super-exponentially (double-exponential for constant 𝛿 > 0) with assembly index (Section 4.1, Eq 4).
        2.  **Constrained Growth:** Introducing constraints (e.g., 𝛿 decreasing with 'a') tames the explosion, leading to sub-exponential or saturating growth of the Assembly Universe (Section 4.1).
        3.  **Dynamic Discovery & Production:** The system exhibits characteristic dynamics of discovering unique objects (N(a) vs. time, governed by k_T, 𝛼) and producing copies (n(a) vs. time, governed by k_P, 𝛽). Specific behaviors include the movement of the peak of N(a) over time (Section 5.1) and the potential collapse of copy numbers at high 'a' depending on parameters (Section 8, Fig S18).
        4.  **Selection-Driven Complexity:** Ensembles evolve towards higher complexity (increasing average 'a' or total Assembly A) when selection is present (low 𝛼, low 𝛿, favorable kinetics), compared to undirected exploration (Section 5.2, 7, 8, Figs S11, S15, S18).
        5.  **Shared Pathway Formation:** In systems with multiple objects, a shared assembly space emerges, reflecting common ancestry/components (Section 6, Fig S12).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation is primarily through:
        1.  **Mathematical Derivation:** Analytical solutions or approximations are derived for dynamic models (e.g., Eq 6, 7, 14, 15) and growth laws (Eq 4).
        2.  **Computational Simulation:** Numerical solutions (NDSolve) and simulations (e.g., forward process for linear chains) are used to generate results shown in figures (e.g., Figs S1, S2, S11, S13, S14, S15, S16-S18). Consistency between analytical predictions (where available) and simulation results provides validation.
        3.  **Illustrative Examples:** Simple systems like linear chains provide concrete examples demonstrating the concepts (e.g., shortest paths Fig S1, S2, S12).
        Limitations: Validation is internal to the theoretical/computational framework. Experimental validation in real physical/chemical systems is not presented in this excerpt. The mapping of abstract parameters (𝛼, 𝛿, 𝛽) to specific physical processes needs experimental verification.

---

#Key: [aya_reconfigurable_2020]

# Reconfigurable Large-Scale Pattern Formation Driven by Topological Defect Separation in Liquid Crystals

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of liquid crystalline materials (Nematic CCN47, E7+CB15; Smectic 10CB, 8OBE) confined in slabs. The primary purpose is to process topological defects (TDs) with opposite charges (|s|=1/2 disclinations) to shape large-scale, stable, periodic patterns (millimeters to centimeters). This is achieved by applying a "dragging field" during the isotropic-to-liquid crystal phase transition. The dragging field is implemented either via material flow (Poiseuille flow driven by capillary force during filling) or an electric-field-driven temperature gradient (using patterned ITO/Au electrodes). The patterns formed (e.g., zigzag stripes in Nematic, fingerprint in Chiral, modulated structures in Smectic) are stabilized by the balance between TD interactions (elastic forces) and surface anchoring forces, potentially aided by the dragging field preventing annihilation. Topological polymeric films based on this strategy are also fabricated by photopolymerizing a mixture containing a reactive monomer (RM257). The system allows for erasing (heating to isotropic) and rewriting patterns.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters are key to controlling the pattern formation process described. Reliability is high as they are directly reported experimental conditions or measured results.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Two primary energy inputs are used:
        1.  Mechanical energy via pressure difference driving capillary flow (manifesting as fluid kinetic energy and work done against viscous forces).
        2.  Electrical energy applied to ITO/Au electrodes, converted to thermal energy (Joule heating) to create temperature gradients. Thermal energy is also input/removed via controlled cooling/heating of the sample stage/heat sink.
    *   Value: Flow: Not quantified. Electrical: 1-5 V<sub>pp</sub> AC mentioned for heating. Cooling rates: 0.1-15 K min⁻¹.

### **2.2 Energy Transduction**

    *   Content:
        1.  **Flow Method:** Chemical potential energy (isotropic phase) -> Kinetic energy (flow) -> Viscous dissipation (heat) + Elastic potential energy (director field deformation around TDs) -> Surface interaction energy (anchoring). The flow provides the dragging force (work done on TDs) that competes with elastic forces.
        2.  **Temperature Gradient Method:** Electrical energy -> Thermal energy (Joule heating in ITO) -> Heat flow (creates temperature gradient) -> Chemical potential energy change drives phase transition boundary motion -> Work done on TDs (dragging force) + Elastic potential energy (director field deformation) -> Surface interaction energy (anchoring).
        In both cases, energy input drives the system out of equilibrium during the phase transition, enabling the formation of metastable patterned states. UV energy is used for photopolymerization, converting chemical energy (monomers) into polymer network structure (fixing the pattern).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss or quantify the energy efficiency of the pattern formation process (e.g., energy input required per unit area of patterned LC). The goal is pattern formation, not energy conversion efficiency. Therefore, scoring is not applicable.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include:
        1.  **Viscous dissipation:** Energy loss due to internal friction during LC flow (explicitly mentioned via viscosity η in force equation). Qualitative assessment: Significant during flow-induced patterning.
        2.  **Heat dissipation:** Energy loss to the surroundings from the heated electrodes (in the temperature gradient method) and during cooling stages. Qualitative assessment: Significant in the temperature gradient method.
        3.  **TD Annihilation:** Release of elastic energy when oppositely charged TDs annihilate (avoided/reduced by the dragging field). Qualitative assessment: Reduced compared to spontaneous relaxation, but likely some annihilation occurs.
        Quantification is not provided.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceed.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term (in NLC phase), Erasable (in Iso phase)
*    Units: Qualitative Descriptor

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Stability vs. Annealing Time | Time in Iso phase leading to pattern erasure | ≈10 | min (at 60°C) | Attribute `stabilityCondition` of `MemoryNode` | Fig 4 | Explicit | Experimentally determined time threshold for erasure. |
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", proceed.)**

### **4.2 Local Interaction Rules:**

    *   Content: The key local interactions governing the self-organization are:
        1.  **TD-TD Interaction:** Oppositely charged TDs (|s|=1/2) attract each other with an elastic force f<sub>inter</sub> = 2πK/D (explicitly given). Similarly charged TDs repel (implied by self-sorting and side-by-side arrangement).
        2.  **TD-Dragging Field Interaction:** A dragging force f<sub>drag</sub> acts on TDs, opposing annihilation for oppositely charged pairs under certain conditions (f<sub>drag</sub> ≈ 2πηvln(3.6/Er), explicit equation form cited). The dragging field direction influences the overall alignment and potential sorting (+1/2 vs -1/2 defects move differently relative to the phase boundary).
        3.  **TD-Surface Interaction:** Surface anchoring (planar, homeotropic, degenerate planar) influences the director field around TDs and provides stabilizing forces for the final pattern. The formation process can break initial surface degeneracy, creating local easy axes ("memory effect").
        4.  **LC Director Elasticity:** Minimization of elastic energy (splay, twist, bend deformations) influences TD core structure and long-range interactions. Flow alignment affects director orientation.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | TD-TD | Elastic interaction force between |s|=1/2 defects | Inter-defect Distance (D) | Variable (µm scale) | µm | Fig 1a, Fig 2g | Explicit | D is a variable dependency. |
    | TD-Drag | Viscous dragging force | Dragging Velocity (v) | Variable (µm/s scale inferred) | µm/s | Eq cited [39], Fig 2g | Mixed | Velocity dependence shown in Fig 2g, but absolute values not given explicitly. Eq form cited. |

### **4.3 Global Order:**

    *   Content: The emergent global order consists of large-scale (millimeter to centimeter), periodic patterns of topological defects and associated director field configurations. Specific examples described:
        *   **Nematic (planar):** Zigzag stripes of director orientation (periodic splay deformation) stabilized by aligned pairs of +1/2 and -1/2 defects (Fig 2a-e).
        *   **Nematic (temp gradient):** Array of π-walls parallel to the temperature gradient, with +1/2 defects gathered centrally (Fig 3c-e).
        *   **Chiral (homeotropic):** Fingerprint texture with stripes parallel to flow (helical axis perpendicular) (Fig 6a). (Described as "trivial" compared to others).
        *   **Smectic C (homeotropic):** 2D periodic modulation (precession) of the c-director (Fig 6b).
        *   **Smectic A (homeotropic):** Chains of distorted focal conic domains aligned perpendicular to flow, separated by homeotropic regions (Fig 6c).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Pattern Regularity | Anisotropy of FFT | Ellipsoidal Ratio | > 1 | Dimensionless | Explicit | Ratio of FFT peak widths quantifies anisotropy/order. | FFT Analysis | Fig 3m |
| Stripe Periodicity | Spacing of zigzag stripes | λ | ≈ 12 | µm | Explicit | Measured from POM/SEM images. | Microscopy | Fig 2f |
| Minimum Separation | Stable distance between TD pair under drag | D<sub>min</sub> | Variable (Function of v) | µm | Explicit | Measured using rheometer setup. | Rheo-microscopy | Fig 2g |
| Oscillation Angle | Max director deviation in zigzag | φ<sub>0</sub> | ≈ 40 | Degrees (°) | Explicit | Mentioned in text based on SEM/POM. Used in simulation. | SEM/POM/Simulation | Section discussing Fig 2b-e |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip to Module 6.)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|
|      |      |      |  |   |  |     |      |    |

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Cooling Rate (Flow) | 1 / (2-15) | min K⁻¹ | Text | Explicit | Inverse of rate given. |
        | Cooling Rate (Temp Gradient) | 1 / (0.1-10) | min K⁻¹ | Text | Explicit | Inverse of rate given. |
        | Iso Phase Annealing for Erasure | ≈ 10 | min | Fig 4 | Explicit | Time needed at 60°C to erase memory. |

    *   **Note:** Relevant timescales relate to the driving processes (cooling), memory effects (annealing), and underlying TD/LC dynamics (inferred).

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", proceed.)**

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is the selection of a specific, dynamically stabilized pattern based on the competition between local interactions (TD attraction/repulsion) and the strength/nature of the externally imposed dragging field (flow or temperature gradient) during the isotropic-to-LC phase transition. Faster dragging speeds or steeper gradients overcome TD attraction more effectively, preventing annihilation and promoting aligned or ordered structures (defect elongation regime, high FFT ratio patterns). Slower speeds/gradients allow annihilation or random nucleation to dominate. The surface anchoring also plays a crucial role in stabilizing the final adapted structure and contributes to the memory effect. The adaptation is driven by the non-equilibrium thermodynamics of the phase transition process under the influence of the dragging field, selecting a particular metastable state. It's a form of process-dependent structure selection rather than learning in a cognitive sense.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is the formation of specific, large-scale, periodic topological patterns (zigzag stripes, π-wall arrays, modulated Smectic structures, etc.) from an initial random state of topological defects in various liquid crystal phases. A related behavior is the memory effect, where these patterns are retained under certain conditions (NLC phase) and can be regenerated after specific phase transitions (ATr), or permanently fixed via polymerization. Another observed behavior is the self-sorting of topological charges (+1/2 vs -1/2) under the influence of the temperature-gradient-driven phase boundary movement.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent pattern formation are validated through:
        1.  **Direct Observation:** POM and SEM imaging directly visualize the resulting large-scale, ordered patterns (Figs 2, 3, 6). Dynamic processes are shown in movies (S1, S2).
        2.  **Quantitative Analysis:** FFT analysis of POM images is used to quantify the degree of order (regularity) and its dependence on the applied voltage/temperature gradient (Fig 3f-m). Periodicity is measured (Fig 2f). TD separation distance vs. shear speed is measured (Fig 2g).
        3.  **Control Experiments/Parameter Variation:** The dependence of pattern formation on initial TD density (Fig 2a), dragging speed (Fig 2g), and temperature gradient (Fig 3) systematically demonstrates the conditions under which ordered patterns emerge, contrasting with uniform or random states. Different LC phases are tested (Fig 6).
        4.  **Simulation:** Optical textures are simulated using the Jones Matrix method based on an assumed director field, showing agreement with experimental POM images (Fig 2c,d), supporting the interpretation of the observed structure.
        Limitations: Validation primarily focuses on the final pattern structure and its dependence on control parameters. The microscopic dynamics of TD interactions and sorting during formation are less directly validated, relying partially on inference and known LC physics. Reproducibility across different labs/setups is not addressed.

---

#Key: [madhusudanan_relaxation_2024]

# Relaxation and entropy generation in dewetting thin glassy polymer films trapped far from equilibrium

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews the behavior of thin glassy polymer films (e.g., Polystyrene (PS), Poly-(tert-butyl styrene) (PTBS)) prepared typically by spin-coating, which traps them in non-equilibrium conformational states far from thermodynamic equilibrium. This non-equilibrium state manifests as processing-induced molecular recoiling stress (σ_rec), also called residual stress (σ_res). The paper focuses on the relaxation of this stress and the associated entropy generation, primarily investigated using the dewetting technique on slippery substrates (like PDMS-coated Si). Dewetting involves heating the film above its glass transition temperature (Tg), causing holes to nucleate and grow, driven by capillary forces and the relaxation of σ_rec. The purpose is to understand the genesis, characteristics, relaxation mechanisms (via dewetting, thermal annealing, physical aging below Tg), and influencing factors (temperature, film thickness (h), molecular weight (Mw), preparation conditions like spin-coating speed (ω), plasticization) of σ_rec, including an entropy generation perspective (S_gen) on the relaxation process.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Molecular Recoiling Stress (σ_rec or σ_res) | ~0.1 - 10 | MPa (Implicitly, via strain & modulus) | Fig 7A, Text Sec 3.3, 5 | Mixed | Medium | Derived from strain (H-h₀)/h₀ and modulus G (Eq 5) |
        | Preparation Parameter (℘) | Dimensionless | Dimensionless | Eq 1, 3, 4, Fig 7 | Explicit | Medium | Calculated from preparation conditions (Eq 3, 4) |
        | Entropy Generation (S_gen) | ~10⁻⁶ - 10⁻² | J K⁻¹ m⁻² (Implicitly) | Eq 12, Fig 14, 15 | Mixed | Medium | Calculated from h_conv, Temps, τ_w (Eq 12) |

    *   **Note:** Values are indicative ranges based on figures and text. σ_rec is often inferred from strain and modulus. S_gen units depend on whether it's per unit area as calculated.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input for the relaxation and dewetting processes discussed is thermal energy, supplied by heating the polymer film to a specific temperature (T_dew or T_age) above or below Tg. During preparation via spin-coating, energy inputs include mechanical energy (rotation) and thermal/chemical potential energy driving solvent evaporation.

### **2.2 Energy Transduction**

    *   Content: Input thermal energy increases molecular mobility (kinetic energy) in the polymer film. This allows the relaxation of stored potential energy associated with the non-equilibrium chain conformations (manifesting as σ_rec, largely entropic potential energy). This released potential energy is primarily transduced into:
        1.  Kinetic energy of molecular motion/flow during dewetting (material displacement, rim formation).
        2.  Work done against viscous forces during flow.
        3.  Dissipated heat due to internal friction and viscous losses, ultimately transferred to the environment.
        Section 4 explicitly models the heat flow (dQ) from the heating stage (source) through the film to the ambient environment (sink) during dewetting.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify the efficiency of converting stored non-equilibrium energy (σ_rec) into useful work (e.g., driving dewetting flow specifically, excluding dissipation). The processes described (viscoelastic flow, molecular relaxation) are inherently dissipative. The entropy generation analysis (Section 4) focuses entirely on the heat transferred and entropy produced due to irreversibility, implying significant inefficiency in terms of directed work output versus energy dissipated. Efficiency is expected to be very low.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation occurs primarily through:
        1.  Viscous dissipation during polymer flow (dewetting).
        2.  Internal friction associated with molecular rearrangements during structural relaxation (aging, annealing, σ_rec decay).
        3.  Heat transfer (primarily convection, as modeled in Sec 4) from the heated film at T_dew to the cooler ambient environment at T_amb.
        Section 4 quantifies the rate of heat transfer per unit area (dQ_dot = h_conv * (T_dew - T_amb)) and the total entropy generated (S_gen) up to the relaxation time τ_w, which represents the accumulated effect of irreversible dissipation within the film due to relaxation and heat transfer across a temperature gradient. S_gen values are calculated and presented (Figs 14, 15), providing a quantitative measure related to dissipation. The magnitude is estimated as ~10⁻⁶ - 10⁻² J K⁻¹ m⁻² depending on conditions.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceed with M3.2 and M3.3.)**

### **3.2 Memory Type:**

    Overall score reflects a persistent physical state encoding history, tunable capacity, and observable influence, but it's a decaying memory read indirectly.

### **3.3 Memory Retention Time:**

*   Value: 10² - 10⁵ (τ_w or τ_res); Can be much longer during aging (Fig 9 shows decays over 10⁵ s). Varies strongly with T, Mw, h, ℘.
*    Units: s (seconds)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Tunable magnitude of σ_rec (~0.1 - 10 MPa) or related parameters like ℘.
*   Units: Pa or Dimensionless

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Related to 1/τ_w or 1/τ_res; follows Arrhenius or WLF dependence on Temperature (Fig 11). Aging follows exponential decay (Fig 9).
    *   Units: s⁻¹

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip to Module 6.)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | σ_rec Relaxation (τ_w / τ_res) | 10² - 10⁵ | s | Fig 5B, 6B, 6D, 7B, 10B, 11 | Explicit | Time for W to reach max or V scaling change |
        | Physical Aging Relaxation | > 10⁵ | s | Fig 9 (Characteristic decay time) | Explicit | Timescale over which properties change during aging |
        | Dewetting Process | 10² - 10⁴ | s | Fig 5, 6, 10 (Duration of experiments shown) | Explicit | Timescale for significant hole growth/rim evolution |
        | Bulk Reptation (τ_rep) | 10³ - 10⁷+ | s | Fig 11 (Calculated/Referenced) | Explicit | Characteristic time for bulk polymer chain disentanglement |
        | Spin-coating Evaporation (t_prep) | Likely < 1 - 10s | s | Implicit in Sec 3.2 (fast process) | Implicit | Time available for solvent evaporation during crucial stage |

    *   **Note:** Values are estimates based on figures and context. τ_rep depends heavily on Mw and T. Evaporation time depends on solvent and spin conditions.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip to Module 8.)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main behaviors reviewed are:
        1.  **Non-Equilibrium State Formation:** Creation of thin glassy polymer films with trapped conformations and molecular recoiling stress (σ_rec) via spin-coating.
        2.  **Stress Relaxation (Aging/Annealing):** Spontaneous or thermally activated evolution of the film structure towards equilibrium, reducing σ_rec over time. Observable via changes in dewetting dynamics, negative thermal expansivity (briefly mentioned).
        3.  **Dewetting Dynamics:** Nucleation and growth of holes, formation of asymmetric rims when heated above Tg. The dynamics (hole growth velocity V(t), rim width W(t)) are influenced by capillary forces and the relaxation of σ_rec, exhibiting characteristic scaling laws (e.g., V ~ t⁻¹, V ~ t⁻¹ᐟ²) and relaxation times (τ_w).
        4.  **Entropy Generation:** Production of entropy during the irreversible dewetting/relaxation process, calculated based on heat transfer and relaxation time, used to characterize the metastable states.
        5.  **Response to Plasticization:** Modification of σ_rec, modulus, viscosity, and relaxation dynamics upon addition of plasticizers (e.g., DOP).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Behaviors are validated primarily through experimental observation and quantitative analysis:
        *   **Dewetting Dynamics:** Optical microscopy and Atomic Force Microscopy (AFM) are used to measure hole radius (R), rim width (W), rim height (H) as functions of time (t). Dewetting velocity (V) is derived (Figs 4, 5, 6, 10, 12, 16, 17).
        *   **Stress Quantification (σ_rec):** Inferred from measured rim height and polymer plateau modulus (G) using Eq 5 (Sec 3.3, Fig 7A, Sec 5). Validation relies on modulus estimation/measurement.
        *   **Relaxation Timescales (τ_w):** Determined from the time W(t) reaches a maximum or V(t) scaling changes (Figs 5B, 6B, 6D, 7B, 10B, 11, 17A).
        *   **Scaling Laws:** Data (e.g., V vs t) plotted logarithmically to verify theoretical scaling exponents (Figs 5C, 10A).
        *   **Entropy Generation (S_gen):** Calculated using a thermodynamic model (Eq 12) requiring inputs like τ_w, T_dew, T_amb, and estimated h_conv (Sec 4, Figs 14, 15). Validation relies on the model assumptions and parameter estimations.
        *   **Reproducibility:** Implicitly validated by comparing results across different studies cited in the review (e.g., Fig 7, 11, 15).
        Limitations include reliance on models (for σ_rec, S_gen), estimation of parameters (G, h_conv), and potential variability in sample preparation and substrate properties.

---

#Key: [tzanov_multi-frequency_2020]

# Multi-Frequency Resonance Behaviour of a Si Fractal NEMS Resonator

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a top-down fabricated silicon-based nanoelectromechanical system (NEMS) resonator. The resonating body has a fractal structure composed of seven star polygons derived numerically using an Iterated Function System (IFS). The structure is suspended over a trench on a Silicon-on-Insulator (SOI) wafer, clamped at its 18 vertices by nanopillars connected to an insulating SiO2 layer. It is actuated electrostatically by applying a voltage between the resonator and the bottom Si-layer, inducing transverse mechanical vibrations. The purpose is to investigate the multi-frequency resonance behavior of this novel fractal geometry for potential applications like sensing, leveraging its potential for broadband frequency response and large functionalization area compared to simpler geometries like nanobeams. Characterization is performed using optical interferometry to measure out-of-plane displacement, and Finite Element Method (FEM) simulations (COMSOL) are used to model behavior, fit parameters (Young's modulus, stress), analyze symmetry effects, and explore electrical readout possibilities via piezoresistivity.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Young's Modulus (fitted) | 76 | GPa | Abstract, Section 4 | Explicit (fitted value) | Medium | Fitted via FEM simulation to match experimental nanobeam data |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Electrical energy is supplied via AC and DC voltages applied between the silicon resonator structure (via contact pads) and the grounded bottom Si-layer of the SOI wafer. This creates a time-varying electrostatic force.
    *   Value: Vpp up to 20V, Voff up to 5V (experimental); specific power/energy not quantified.
    *   Units: V (Voltage)

### **2.2 Energy Transduction**

    *   Content: Electrical energy (from applied voltages) is transduced into mechanical potential and kinetic energy (vibration of the fractal structure). The time-varying electrostatic force between the biased resonator and the ground plane drives the mechanical oscillation. Energy is also transduced back from mechanical strain to electrical resistance changes via the piezoresistive effect, as explored in simulations (Appendix B.2). For detection, mechanical displacement energy is transduced into optical phase/intensity modulation in the interferometric setup, which is then transduced into an electrical signal by the photodetector.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide quantitative data or analysis regarding the energy efficiency of the transduction from electrical input to mechanical output, or the overall system efficiency. It mentions "low excitation energy" qualitatively (Section 3) by comparing needed voltages to nanobeams, but provides no numerical efficiency value.

### **2.4 Energy Dissipation**

    *   Content: Energy is dissipated primarily through mechanical damping mechanisms. The paper quantifies this globally via the Quality (Q) factor of the resonance peaks (Q ≈ 800-960 for several peaks, up to ~1200 mentioned in Table A1). Sources of dissipation are implicitly anchors/clamping losses, material internal friction, potentially viscous damping from residual gas (though measured in vacuum), and thermoelastic damping. The paper mentions potential Q-enhancing effects like dissipation dilution and clamp-tapering (Introduction), and discusses simulating piezoresistivity which implies electrical resistance (Joule heating) as another minor dissipation pathway during electrical readout (Appendix B.2). Dissipation in air is mentioned as leading to lower Q factors (Appendix A.2).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8.)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Resonance Period (Inverse Frequency) | ~42 - ~500 | ns | Section 3, Fig 2 | Explicit | Calculated as 1/f from the experimental frequency range 2-24 MHz. |
        | Ring-down Time (Related to Q) | ~110 - ~160 | µs | Section 3, Appendix A.3 | Implicit | Estimated as Q / (π * f) using typical Q≈900 and f≈2 MHz and f≈5 MHz. Actual ring-down not measured. |
    *   **Note:** Resonance frequencies are explicitly measured. Ring-down times are related to the explicitly measured Q-factors but require calculation based on frequency.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behavior is multi-frequency mechanical resonance. When driven electrostatically across a range of frequencies, the fractal structure exhibits multiple distinct resonance peaks between 2 MHz and 24 MHz. These resonances correspond to different vibrational eigenmodes of the complex structure. Many peaks exhibit nonlinear behavior, including frequency shifts (hardening and softening) and hysteresis, depending on drive amplitude (Vpp) and DC offset (Voff). Some peaks split or merge under varying voltage conditions.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The multi-frequency resonance behavior is validated experimentally through systematic frequency sweeps using an interferometric measurement setup under varying electrostatic drive conditions (Vpp, Voff) in vacuum (Section 3, Fig 2-7, Appendix A). Repeatability is suggested by control peak monitoring (Section 3.2) and data from additional devices (Appendix A.2). The observed resonance frequencies and mode shapes (for the first few modes) are correlated with FEM simulations (COMSOL) where material properties and stress were adjusted to match experimental data (Section 4, Table 2). The observation of multiple peaks is directly linked to the complex, designed fractal geometry rather than claimed as computationally emergent from simple rules. Nonlinear characteristics (hysteresis, peak splitting) are directly measured (Section 3). Limitations include potential variability between devices, limited exploration of environmental parameter effects (e.g., temperature), and discrepancies between simulation and experiment for higher frequency modes.

---

#Key: [zhang_intelligence_2024]

# Intelligence at the Edge of Chaos

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system investigates the emergence of intelligent behavior in artificial systems (specifically Large Language Models - LLMs) by training them to predict the behavior of Elementary Cellular Automata (ECAs). The core components are: (1) Elementary Cellular Automata (ECAs), simple 1D rule-based systems generating diverse behaviors (uniform, periodic, chaotic, complex), used to generate training data. (2) GPT-2 models (LLMs), adapted for binary input/output, trained via next-token prediction on sequences generated by individual ECA rules. (3) Downstream evaluation tasks (ARC-inspired reasoning, chess move prediction) to assess the "intelligence" acquired by the LLMs during pretraining. The purpose is to explore the hypothesis that intelligence can emerge from modeling simple systems exhibiting complex behaviors, specifically probing the relationship between the complexity of the ECA rules and the downstream task performance of the LLMs trained on them. The system *does* train LLMs to predict ECA evolution and then evaluates their performance on unrelated reasoning and prediction tasks.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name          | Value          | Units        | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :---------------------- | :------------: | :----------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is electrical energy supplied to the computational hardware (NVIDIA H100 GPUs) used for training and simulation.

### **2.2 Energy Transduction**

    *   Content: Electrical energy is transduced into computational work performed by the GPUs. This involves executing instructions for ECA simulations, GPT-2 model training (forward/backward passes, gradient updates), and downstream task evaluations. A significant portion is inevitably transduced into thermal energy (heat) due to computational inefficiencies.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any metrics related to computational energy efficiency (e.g., FLOPs per Watt, training time per unit energy). Assessing efficiency would require external information about the H100 GPUs and potentially run-time measurements not included. Qualitatively, large model training is known to be energy-intensive (Low efficiency in terms of useful work vs. heat).

### **2.4 Energy Dissipation**

    *   Content: The primary dissipation mechanism is heat generated by the GPUs during computation. Other minor dissipation sources could include energy loss in power delivery and network components, but these are negligible compared to GPU heat output. Quantification is not provided. Qualitatively, dissipation is High due to the use of powerful GPUs for intensive training.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term (for weights); Short-term (for attention context)

### **3.4 Memory Capacity (Optional - if applicable)**

*   Units: Parameters (weights); Tokens/States (context window)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: Measured by downstream task performance (Efficiency/Accuracy). E.g., Chess Accuracy ~0.18-0.205.
*   Units: % Accuracy, Efficiency (1/epochs)

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are the specific ECA rules (256 possibilities for elementary CAs). Each rule is an 8-bit number defining the next state (0 or 1) of a cell based on the 2³ = 8 possible states of its neighborhood (the cell itself and its left and right neighbors in the previous time step). For example, Rule 110's definition specifies the output bit for each of the 8 input neighborhood patterns (111->0, 110->1, 101->1, 100->0, 011->1, 010->1, 001->1, 000->0). These rules are applied simultaneously to all cells in the 1D grid at each time step.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID         | Description             | Parameter Name          | Parameter Value Range   | Units | Data Source | Implicit/Explicit | Justification                                     |
    | :-------------- | :---------------------- | :---------------------- | :--------------------: | :---: | :----------: | :----------------: | :------------------------------------------------ |
    | Neighborhood    | Defines local influence | Neighborhood Size       | 3 (self + 2 neighbors) | Cells | Section 2.1  | Explicit          | Definition of Elementary Cellular Automata.       |

### **4.3 Global Order:**

    *   Content: The emergent global order varies depending on the ECA rule, falling into Wolfram's four classes: Class I (uniform states like all 0s or all 1s), Class II (simple periodic structures like repeating blocks or stripes), Class III (chaotic, random-like patterns), and Class IV (complex, localized structures, long transients, gliders, e.g., Rule 110). Examples include the Sierpinski triangle fractal generated by Rule 90.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes (within the ECA simulation context); No (in the sense of physical material computation).

**(Conditional: Considering the LLM's computation)**

### **5.2 Computation Type:**

    *   Content: Neuromorphic / Deep Learning (for the LLM)

### **5.3 Computational Primitive:**

    *   Content: For the LLM: The fundamental operation is sequence prediction (next-token prediction). This relies on underlying primitives like matrix multiplications, activation functions (within the Transformer architecture), and attention calculation. The *task* it learns is predicting future ECA states or downstream tasks (reasoning, chess moves).
    *   **Sub-Type (if applicable):** Sequence Prediction

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description         | Value        | Units                | Source              | Implicit/Explicit | Justification                                                                 |
        | :---------------------------- | :----------: | :------------------: | :-----------------: | :----------------: | :---------------------------------------------------------------------------- |
        | ECA Simulation Step           | 1            | discrete time step | Section 3.1         | Explicit          | ECAs evolve in discrete steps.                                                |
        | ECA Total Evolution           | 1000         | discrete time steps  | Section 3.1         | Explicit          | Duration of each ECA simulation run.                                          |
        | LLM Input Sequence Length    | 60           | discrete time steps  | Section 3.1, 3.3    | Explicit          | Length of the sequence fed to the LLM for training/prediction.             |
        | LLM Prediction Horizon (Short)| 1            | discrete time step | Section 3.1, Fig 5  | Explicit          | Models trained to predict the immediate next state.                           |
        | LLM Prediction Horizon (Long) | 5            | discrete time steps  | Section 3.1, Fig 5  | Explicit          | Models also trained to predict 5 steps ahead.                                 |
        | Training Epochs               | up to 10,000 | epochs               | Section 3.3, 4.1      | Explicit          | Duration measure for pretraining and downstream training.                     |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is supervised learning via backpropagation and stochastic gradient descent (specifically, the Adam optimizer). The LLM adjusts its weights to minimize the prediction error (loss function, likely cross-entropy or similar for binary prediction) between its predicted next ECA state(s) and the actual next state(s) in the training data. This process iteratively refines the model's internal representation of the ECA dynamics.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary *emergent* behavior investigated is the capability of the LLM, after being pretrained solely on predicting ECA dynamics, to perform well on *unrelated downstream tasks*: specifically, abstract reasoning (ARC-inspired tasks) and chess move prediction. The paper frames this downstream performance as emergent "intelligence." The direct behavior is sequence prediction (ECA, reasoning patterns, chess moves). The emergent aspect is the transferability of learned patterns/logic from the simple ECA prediction task to complex reasoning and strategy tasks.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claim of emergent behavior (intelligence transfer) is validated quantitatively by measuring the LLM's performance on the downstream tasks (reasoning efficiency, chess accuracy) after pretraining on different ECAs. Control is established by comparing performance across models trained on different ECA complexity classes (Section 5.1, Fig 2, Fig 3). Statistical significance of correlations between complexity and performance is reported (p < 0.05 indicated by asterisks in Fig 2, Fig 3). Reproducibility is implied by the detailed methodology. Limitations include the specific choice of downstream tasks and the interpretation of performance on these tasks as general "intelligence."

---

#Key: [hu_self-reporting_2025]

# Self-Reporting Multiple Microscopic Stresses Through Tunable Microcapsule Arrays

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of polymethylmethacrylate (PMMA) core-shell microcapsules filled with fluorescent dyes, embedded in a polydimethylsiloxane (PDMS) matrix. The microcapsules are synthesized using microfluidics and solvent evaporation to precisely control their diameter and shell thickness, thereby tuning their critical breaking force. Three types of microcapsules, each containing a different fluorescent dye (green, yellow, blue) and exhibiting a distinct breaking force (corresponding to stress thresholds of 3.2, 4.9, and 8.1 MPa, respectively), are assembled into linear chains within micro-traps on a PDMS template using sequential capillarity-assisted particle assembly (sCAPA). The purpose of the system is to create a self-reporting material capable of sensing and visualizing multiple levels of local microscopic stress through the selective rupture of microcapsules and the subsequent release of corresponding fluorescent dyes upon mechanical indentation. The spatial organization of the microcapsules into regular arrays improves the accuracy of stress mapping compared to randomly distributed capsules.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Radius is variable but an example value is given in Exp Section (4.53 um). Shell thickness depends on polymer amount (Fig 1e). Critical Force depends on h^2/r^2 (Fig 2c). Stress thresholds link breaking forces to macroscopic stress during indentation experiments. Spatial resolution is linked to the assembly pattern spacing.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is mechanical energy applied externally via an indenter (micro-indenter for single capsule tests, Vickers indenter for array tests).
    *   Value: Variable (e.g., indentation loads from 0.098 to 4.9 N)
    *   Units: N (Force) or MPa (Stress)

### **2.2 Energy Transduction**

    *   Content: Mechanical energy from the indenter is transduced into elastic strain energy within the microcapsule shell. When the local stress exceeds the material's fracture strength (tuned by h^2/r^2), this stored elastic energy is rapidly released through brittle fracture of the shell. The rupture allows the encapsulated dye molecules (stored chemical potential energy related to concentration gradient) to be released. Upon excitation (e.g., by UV light from the microscope), the released dye molecules convert absorbed light energy into emitted fluorescent light (optical energy).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The efficiency of converting input mechanical energy to the desired output (fluorescent signal indicating stress level) is likely very low. Most input mechanical energy is dissipated as heat during plastic/viscoelastic deformation of the PDMS matrix, frictional losses, and the fracture energy of the capsule shell (which is the intended mechanism but still a form of dissipation relative to the fluorescent output). The energy involved in dye fluorescence itself is also a small fraction of the excitation light energy. The paper does not provide any quantitative efficiency metrics. Qualitative Assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include:
        1.  Inelastic deformation (viscoelasticity) of the PDMS matrix during indentation (likely High).
        2.  Energy consumed during the brittle fracture of the PMMA microcapsule shell (Medium/Low per capsule, but cumulative).
        3.  Heat generated due to friction between the indenter and the PDMS, and potentially internal friction during deformation (Medium/Low).
        4.  Non-radiative decay processes during fluorescence, converting excitation energy to heat (Medium, typical for fluorescence).
        The paper does not quantify these mechanisms.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**

    *   Retention: Intended to be long-term (Fig S12 shows stability over time, Fig S13/S15 shows signal localization), limited by dye photobleaching or diffusion out of the trap. Qualitatively appears high within experimental timeframe.
    *   Capacity: Limited. Each "memory unit" (a chain of 3 capsules in a trap) can store one of four states (no rupture, green rupture, yellow rupture, blue rupture), corresponding to maximum stress intervals (<3.2 MPa, 3.2-4.9 MPa, 4.9-8.1 MPa, >8.1 MPa). This is very low capacity per unit compared to digital memory.
    *   Read-out accuracy: High, based on distinct fluorescence signals (Fig 3d, 4e, 4f). Quantification requires image analysis metrics (SNR, classification accuracy), not provided.
    The score is low because the memory is non-rewritable, has very limited states (low capacity), and primarily functions as a simple irreversible threshold indicator/recorder rather than a complex, adaptable memory system.

### **3.3 Memory Retention Time:**

*   Value: Long-term (within experimental observation)
*    Units: Qualitative Descriptor

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 4 states per chain/trap (~2 bits)
*   Units: states

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: High (Qualitative)

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Low (Qualitative)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: During sCAPA: Capillary forces exerted by the evaporating meniscus push microcapsules towards and into micro-traps patterned on the PDMS substrate. The interaction strength depends on particle/trap geometry, meniscus contact angle, evaporation rate, and suspension properties (surface tension, particle concentration). Specific parameters (Triton X-45, SDS concentrations, motor speed) are controlled to optimize deposition (Experimental Section, Table S1). During synthesis: Polymer (PMMA) dissolved with solvent (chloroform) and non-solvent (dye solution) phase separates upon solvent evaporation, with the polymer migrating to the oil-water interface due to insolubility and interfacial tension minimization, forming the shell.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | sCAPA | Capillary Assembly | Dragging Speed | 3-5 | μm s⁻¹ | Exp Section | Explicit | Speed controlled by linear motor. |
    | sCAPA | Capillary Assembly | Surfactant Conc. (Triton X-45) | Varies (e.g., 0.01 for h²/r²=0.038) | % w/v | Table S1 | Explicit | Controlled parameter for surface tension. |
    | sCAPA | Capillary Assembly | Surfactant Conc. (SDS) | Varies (e.g., 0.015 for h²/r²=0.038) | % w/v | Table S1 | Explicit | Controlled parameter for surface tension/wetting. |
    | Synthesis | Shell Formation | Polymer Amount (PMMA) | 0.1 - 0.6 | g (in 4mL CHCl3) | Fig 1e, Exp Section | Explicit | Controlled parameter determining shell thickness. |
    | Synthesis | Droplet Formation | Water Phase Flow Rate (related to Ca_w) | Variable | mL/hr (Implied) | Fig 1c/d | Explicit (Ca_w), Implicit (Flow rate) | Droplet size controlled via flow rate affecting Capillary number. |

### **4.3 Global Order:**

    *   Content: The emergent (templated) global order consists of regular, patterned arrays of microcapsule chains. Each chain typically comprises three distinct microcapsules arranged linearly within a micro-trap. The traps themselves are arranged in a pre-defined pattern (e.g., square lattice, letters E, T, H).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| sCAPA-1 | Capillary force guiding particle into trap | Surface Tension (Liquid/Air) | Tuned by surfactants | mN/m (Implied) | Mixed | Surfactants mentioned (Explicit), values inferred (Implicit) | Exp Section, Table S1 |
| Synth-1 | Polymer phase separation/shell formation | Polymer concentration | 0.1 - 0.6 / 4 | g / mL | Explicit | Directly controlled parameter. | Fig 1e, Exp Section |
| Synth-2 | Droplet formation in flow-focusing | Capillary Number (Ca_w) | Calculated (e.g., Fig 1d) | Dimensionless | Explicit | Calculated from flow rates, viscosity, interfacial tension. | Fig 1d, Eq 2 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| GO-1 | Regular Array Structure | Lattice Spacing (Trap spacing) | ~94 | μm | Explicit | Pixel size given in Fig 4f legend, related to trap spacing mentioned in Sec 3. | Microscopy | Fig 4f, Sec 3 |
| GO-2 | Occupancy/Yield | Deposition Yield (3 capsules) | ~80 | % | Explicit | Percentage of traps successfully filled with 3 capsules. | Microscopy | Fig 4d |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 0

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog (input) to Digital/Discrete (output) Thresholding

### **5.3 Computational Primitive:**

    *   Content: Thresholding / Comparison. The basic operation is: IF (Local Stress > Critical Rupture Stress) THEN Output Signal (rupture/fluorescence). The critical rupture stress value is pre-programmed by tuning h²/r² during synthesis.
    *   **Sub-Type (if applicable):** Thresholding

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

*Note: Processing power and energy/operation are not quantifiable from the text. Response time is inferred based on the nature of brittle fracture. Bit-depth is inferred from the number of distinct output states.*

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Indentation Loading Rate (Single Capsule) | 0.5 | μm s⁻¹ | Exp Section | Explicit | Rate applied by nanoindenter. |
        | Microcapsule Rupture Event | Very Short (<< 1s) | s | Inferred | Implicit | Brittle fracture is typically a very fast event (μs-ms). Video S1 might provide visual clues. |
        | Memory Retention (Signal Persistence) | Long-term | days/weeks+ | Fig S12, S13, S15 | Mixed | Explicitly stated as stable/localized; "Long-term" is qualitative interpretation. |
        | Dye Diffusion (Potential Signal Loss) | Slow (relative to rupture) | hours/days? | Inferred | Implicit | Diffusion would limit spatial resolution over time; inferred to be slow based on localized signal persistence (S13, S15). |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is multi-level, spatially resolved stress reporting/mapping. Upon mechanical indentation, the material locally indicates the maximum stress experienced by emitting specific fluorescent colors (or combinations) corresponding to the rupture thresholds of the embedded microcapsules (Green: 3.2-4.9 MPa, Yellow: 4.9-8.1 MPa, Blue: >8.1 MPa). This allows visualization of the stress distribution map across the indented area.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claim of spatially resolved, multi-level stress reporting is validated through controlled indentation experiments using a Vickers indenter at varying loads (0.098 to 4.9 N, corresponding to 0.16 to 8.1 MPa average stress). Fluorescence microscopy is used to image the samples post-indentation (Fig 3d, 4e, 4f). The observed fluorescence patterns (specific colors appearing at specific locations and loads) are directly correlated with the known rupture thresholds of the capsule types and the applied stress levels. Control experiments include non-indented samples (0 MPa stress) showing no fluorescence (Fig 3d, 4e) and comparison with randomly distributed capsules showing less informative results (Fig 4g). Quantitative analysis is primarily visual map generation; simulations (Fig S7, S8) provide theoretical comparison for stress distribution. Reproducibility is implied by consistency across figures and stated low variability (Sec 2.2). Limitations include reliance on average stress calculation and lack of quantification for signal analysis (e.g., SNR).

---

#Key: [martin_synaptic_2000]

# Synaptic Plasticity and Memory: An Evaluation of the Hypothesis

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews the Synaptic Plasticity and Memory (SPM) hypothesis, which posits that activity-dependent synaptic plasticity (primarily Long-Term Potentiation (LTP) and Long-Term Depression (LTD)) is the mechanism for encoding and storing memory traces in the central nervous system, specifically focusing on the hippocampus, amygdala, and cortex. It evaluates the evidence for this hypothesis based on criteria like detectability, mimicry, anterograde alteration, and retrograde alteration, considering experimental strategies such as correlation, induction, occlusion, intervention (pharmacological/genetic), and erasure. The system described is the biological neural network where memory formation is hypothesized to occur via modifications of synaptic strength (efficacy). Components include neurons, synapses, neurotransmitters (e.g., glutamate), receptors (e.g., NMDA, AMPA), intracellular signaling pathways (e.g., CaMKII, PKA, CREB), and associated proteins. The purpose is to evaluate the evidence linking these synaptic mechanisms to learning and memory processes at the behavioral level.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name           | Value                | Units          | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :----------------------- | :-------------------: | :------------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters characterize the synaptic plasticity phenomena (LTP/LTD) central to the reviewed hypothesis. Values represent typical ranges discussed or shown. Persistence varies significantly. Specificity is generally high but shown to be potentially local-volume dependent.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source for neural activity and the biochemical processes underlying synaptic plasticity is metabolic energy, primarily derived from ATP hydrolysis, which supports ion pumping (maintaining gradients), neurotransmitter synthesis/release, and intracellular signaling cascades. The immediate trigger for plasticity induction is electrochemical (ion flow, e.g., Ca2+ influx through NMDA receptors).

### **2.2 Energy Transduction**

    *   Content: Electrical energy (action potentials, synaptic potentials) is transduced into chemical energy (neurotransmitter release), which is then transduced back into electrical/ionic signals postsynaptically (receptor activation, ion flow like Ca2+ influx). This Ca2+ signal activates biochemical cascades (transduction into chemical modifications like phosphorylation via kinases like CaMKII, PKA) that lead to changes in synaptic efficacy (e.g., AMPA receptor trafficking/phosphorylation, protein synthesis for late LTP). Metabolic energy (ATP) fuels all these steps, especially ion pumping and synthesis/modification processes.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss or provide metrics for the energy efficiency of synaptic plasticity or memory formation. Assessing this would require detailed metabolic measurements not present in the review. Qualitatively, biological processes involve significant energy expenditure (e.g., maintaining ion gradients, protein synthesis), suggesting efficiency might not be maximal compared to theoretical limits, but this is inferred knowledge, not from the text.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include heat generated during ATP hydrolysis for ion pumping (e.g., Na+/K+ ATPase) to restore resting potentials after neural activity, heat from biochemical reactions (e.g., phosphorylation, protein synthesis), and potentially electrical dissipation (though minimal in biological systems compared to Joule heating in electronics). The paper does not quantify these. Qualitative assessment: High (biological systems inherently dissipate significant energy).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Hours to Weeks (potentially longer for late LTP)
*    Units: Time

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: High (Potentially very large)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Variable (depends on factors like protein synthesis, synaptic tagging, metaplasticity, novelty exposure)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes (Implicitly, at the network level)

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rules are those governing synaptic plasticity induction:
        1.  **NMDA Receptor-Dependent LTP/LTD:** Requires coincident presynaptic glutamate release and sufficient postsynaptic depolarization to relieve Mg2+ block, allowing Ca2+ influx. The *level* of Ca2+ influx determines direction (high -> LTP, moderate -> LTD - often linked to stimulation frequency/pattern, see BCM model Fig 2A). Key properties: associativity, input specificity (mostly).
        2.  **Metaplasticity:** Prior synaptic activity modifies the threshold (θM) for inducing subsequent LTP or LTD (BCM model, Fig 2A). High prior activity favors LTD, low prior activity favors LTP.
        3.  **Synaptic Tagging:** Induction of LTP/LTD sets a synapse-specific "tag" that can capture plasticity-related proteins synthesized elsewhere in the neuron, enabling input-specific consolidation of late-phase plasticity (Fig 2G).
        4.  **Timing-Dependent Plasticity:** (Mentioned for cortex, p. 659) Synaptic efficacy changes depend on the precise relative timing of presynaptic EPSPs and postsynaptic action potentials.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID          | Description                     | Parameter Name      | Parameter Value Range | Units | Data Source        | Implicit/Explicit | Justification                       |
    | :--------------- | :------------------------------ | :------------------ | :-------------------: | :---: | :----------------: | :----------------: | :---------------------------------- |
    | LTP Induction    | Postsynaptic Ca2+ threshold     | [Ca2+]_LTP          | High (Qualitative)    | Conc. | Properties Section | Implicit         | Stated high levels needed       |
    | LTD Induction    | Postsynaptic Ca2+ threshold     | [Ca2+]_LTD          | Low (Qualitative)     | Conc. | Properties Section | Implicit         | Stated low levels needed        |
    | Synaptic Tagging | Tag Duration / Capture Window | Tag Lifetime        | Minutes to Hours (?)  | Time  | p. 660-661         | Implicit         | Inferred from experiments described |
    | STDP             | Timing Window                   | Δt (Pre-Post Spike) | ~10 ms (example)      | Time  | p. 659             | Explicit         | Example value given for cortex    |

### **4.3 Global Order:**

    *   Content: The emergent global order is the stored memory trace itself – a specific spatial pattern of potentiated and depressed synaptic weights across a network of neurons (e.g., in the hippocampus, amygdala, or cortex). This pattern represents the learned information (e.g., a spatial map, a conditioned association, a skill). Other emergent patterns discussed contextually include network oscillations (theta rhythm, sharp waves) which interact with plasticity, and organized structures like place fields in the hippocampus.
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID          | Description                        | Parameter          | Value Range        | Units | Implicit/Explicit | Justification                       | Source      |
| :--------------- | :--------------------------------- | :----------------- | :----------------: | :----: | :----------------: | :---------------------------------- | :---------- |
| NMDA-Hebbian     | Coincidence Detection             | [Ca2+] Threshold   | Low/High          | Conc.  | Implicit         | Inferred from LTP/LTD mechanisms  | p. 656, 658 |
| STDP             | Spike Timing Dependence          | Δt Window          | ms                 | ms     | Explicit         | Discussed for cortical plasticity | p. 659      |
| Associativity    | Linking Weak/Strong Inputs        | Depolarization Level | Variable           | mV     | Explicit         | Classical property of LTP         | p. 655      |
| Input Specificity| Spatial Restriction of Plasticity | Distance           | ~70 µm (exception) | µm     | Explicit         | Classical property (and exception)  | p. 655, 659 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid/Neuromorphic

### **5.3 Computational Primitive:**

    *   Content: The fundamental computational primitive at the synapse level appears to be **Weighted Summation and Thresholding/Non-linear Transformation**. Synapses receive inputs, weight them by their efficacy, sum them postsynaptically (spatially and temporally), and this integrated signal (e.g., postsynaptic depolarization, Ca2+ level) is compared against thresholds to determine output (firing) and plasticity induction (LTP/LTD). Specific plasticity rules add complexity:
        *   **Coincidence Detection:** (NMDA receptor) - detecting simultaneous pre- and postsynaptic activity.
        *   **Temporal Filtering/Integration:** Synaptic/dendritic properties filter/integrate inputs over time.
        *   **Adaptive Weighting:** Synaptic efficacy (the weight) changes based on activity history (LTP/LTD).
        *   **(Potential) Multiplication/Gating:** Neuromodulators or specific receptor interactions could potentially gate or multiply signals, though not detailed as a core primitive here.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description        | Value                       | Units      | Source                       | Implicit/Explicit | Justification                                      |
        | :--------------------------- | :--------------------------: | :--------: | :--------------------------- | :----------------: | :------------------------------------------------- |
        | Synaptic Transmission        | Milliseconds                | ms         | General Neuroscience Knowledge | Inferred          | Basic timescale of synaptic potentials.            |
        | Action Potential             | ~1-2                        | ms         | General Neuroscience Knowledge | Inferred          | Duration of a neuronal spike.                      |
        | LTP/LTD Induction Protocol   | ms to minutes               | Time       | p. 653, Fig 1                | Explicit          | Duration of high/low frequency stimulation trains. |
        | Early LTP/LTD Persistence    | ~1-3 Hours (?)              | Hours      | p. 660                        | Explicit          | Protein-synthesis independent phase duration.      |
        | Late LTP/LTD Persistence     | Hours to Weeks (or longer)  | Time       | p. 655, 658, 660             | Explicit          | Protein-synthesis dependent phase duration.        |
        | Synaptic Tagging Window      | ~1-2 Hours                  | Hours      | p. 660-661                   | Explicit          | Time window for tag to capture proteins.           |
        | BCM Threshold Sliding        | Minutes to Hours (?)        | Time       | p. 656                        | Implicit          | Timescale over which activity history affects θM.   |
        | Memory Consolidation         | Hours to Days/Years         | Time       | p. 650, 689                  | Explicit          | Process of stabilizing memory traces.              |
        | Theta Rhythm                 | ~125-250 (4-8 Hz)           | ms (Period) | p. 658                        | Explicit          | Period of hippocampal theta oscillations.          |
        | Behavior/Learning Trials     | Seconds to Minutes/Hours    | Time       | General Methods              | Implicit          | Typical duration of behavioral tasks/trials.       |

    *   **Note:** Timescales range from milliseconds (synaptic events) to potentially years (long-term memory consolidation).

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The primary adaptation mechanism described is **Hebbian-like synaptic plasticity (LTP/LTD)**. Changes are driven by correlated pre- and postsynaptic activity. Strong correlation/coincidence leads to strengthening (LTP), while weaker or specific patterns of activity (or possibly uncorrelated activity in some LTD forms) lead to weakening (LTD). The change is driven by postsynaptic Ca2+ influx triggering downstream signaling cascades (kinases, phosphatases) that modify synaptic components (e.g., AMPA receptors). Metaplasticity provides a homeostatic element, adjusting the threshold for plasticity based on recent activity levels (BCM model). Synaptic tagging allows consolidation based on global neuronal events (like protein synthesis triggered by strong activation elsewhere) interacting with locally tagged synapses. This resembles unsupervised, correlation-based learning rules. Error-correction rules are mentioned in the context of psychological theories and network models but not as the core synaptic mechanism described.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior discussed at the system level (animal) is **Learning and Memory**. Specific examples reviewed include:
        *   **Spatial Learning:** (Hippocampus-dependent) e.g., water maze navigation, place field formation/stability.
        *   **Fear Conditioning:** (Amygdala-dependent) e.g., associating a cue (tone) with an aversive stimulus (shock), leading to freezing or potentiated startle.
        *   **Conditioned Taste Aversion (CTA):** (Insular cortex/amygdala) Avoiding a taste associated with malaise.
        *   **Odor Discrimination Learning:** (Piriform cortex/hippocampus) Learning to differentiate odors.
        *   **Motor Skill Learning:** (Motor cortex) Improving performance on a motor task.
        These behaviors emerge from the coordinated activity and plasticity within specific neural networks.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The review validates claims of links between synaptic plasticity and emergent learning/memory behaviors primarily through:
         1.  **Correlation Studies:** Comparing LTP properties (e.g., persistence) with learning performance across individuals or conditions (e.g., aging, transgenics, p. 666).
         2.  **Intervention Studies (Anterograde Alteration):** Blocking plasticity mechanisms (pharmacologically or genetically) during learning and observing impaired behavioral performance (e.g., NMDA antagonists impair spatial learning, p. 669-672; fear conditioning, p. 686). Control experiments are crucial to rule out non-specific effects (e.g., sensorimotor).
         3.  **Occlusion Studies:** Saturating plasticity before learning and observing impaired subsequent learning (p. 666-668).
         4.  **Induction Studies (Detectability):** Attempting to measure LTP-like changes (e.g., evoked potentials, transmitter release) after learning (p. 680-684).
         5.  **Erasure/Reversal Studies (Retrograde Alteration):** Reversing plasticity after learning to see if memory is impaired (less explored, p. 679-680).
     Limitations include difficulty definitively proving necessity and especially sufficiency, separating plasticity effects from other drug/genetic effects, and detecting small synaptic changes.

---

#Key: [adamatzky_computing_2011]

# Computing with liquid crystal fingers: Models of geometric and logical computation

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system uses cholesteric liquid crystals (LCs) confined in a thin layer between transparent electrodes. Applying an AC voltage above a threshold causes the formation and propagation of "cholesteric fingers" – localized domains of twisted LC alignment, visualized using dichroic dyes. These fingers nucleate at pre-defined defects/structures (e.g., SU8 protrusions). The system demonstrates computational capabilities: 1) Approximating planar Voronoi diagrams via branching fingers at higher voltages. 2) Performing convex subdivision of concave polygons using non-branching fingers originating from indentations. 3) Implementing a one-bit half-adder using collision-based computing principles, where fingers interact with obstacles and each other. A mobile automata model is also presented to simulate finger dynamics and computation. The purpose is to demonstrate geometric and logical computation using the physical dynamics of LC fingers. Components include cholesteric LC (MLC2037 doped with chiral additive zli811 and dichroic dyes), glass substrates with ITO electrodes, patterned SU8 structures for nucleation control, and an AC voltage source.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Cell Spacing / Pitch | ~5 (structures height) / Close to spacing | µm / µm | Sec 2 | Explicit/Mixed | Medium | Cell spacing inferred from structure height; Pitch relation explicitly stated as 'very close'. |
    *   **Note:** Other parameters like finger propagation speed or automata step size `d=0.5` exist but their direct physical correlation or units aren't consistently provided for the experiments.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is an external AC voltage applied across the ITO electrodes sandwiching the liquid crystal layer.
    *   Value: ~1.5 - >1.7 (Amplitude)
    *   Units: V

### **2.2 Energy Transduction**

    *   Content: Electrical energy input drives electrohydrodynamic effects in the cholesteric LC. The applied electric field interacts with the LC's negative dielectric anisotropy, tending to rotate the director parallel to the substrates (Fig 1). This destabilizes the initial homeotropic alignment, and combined with the LC's chirality and elastic properties, leads to the formation and propagation of localized twisted structures (cholesteric fingers). Electrical energy is transduced into potential energy stored in the elastic deformation of the LC director field and kinetic energy associated with finger propagation and fluid flow (electroconvection mentioned as related phenomenon in Sec 1).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any metrics for energy efficiency regarding the computation performed. Liquid crystal displays are generally low power, but the purpose here is computation via physical movement/interaction. Energy is primarily used to sustain the electric field and overcome viscous dissipation during finger propagation and LC reorientation. It's highly likely that only a tiny fraction of the input electrical energy is directly converted into the "work" of computation (changing finger trajectories). Efficiency is expected to be very Low.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation mechanisms include: 1) Dielectric losses within the LC material due to the AC field. 2) Viscous dissipation associated with the movement of the LC molecules during director reorientation and finger propagation (LCs are fluids). 3) Potential losses at the electrode interfaces or in the driving circuitry (not discussed). Quantification is not provided. Qualitatively, viscous dissipation is likely significant given the movement of fingers through the fluid medium. Dielectric losses are inherent to AC operation. Assessment: Medium to High dissipation relative to computational work.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content:
        1.  **Nucleation:** Fingers nucleate at specific points (defects, structure protrusions) when voltage exceeds a threshold (Sec 1, 2).
        2.  **Propagation:** Fingers extend/propagate while voltage is applied (Sec 2). The simulation model uses `xt+1 = xt+dsinα` and `yt+1 = yt+dcosα` with `d = 0.5` (Sec 3).
        3.  **Branching:** Above a critical voltage (~1.7V), fingers start to branch recursively (Sec 1, 4).
        4.  **Collision Avoidance/Repulsion:** Fingers repel each other (Sec 1) and deviate to avoid contact (Sec 3).
        5.  **Chirality Bias:** Fingers tend to turn in a preferred direction (e.g., left) upon collision or when avoiding obstacles, attributed to LC chirality (Sec 3, 5). In the model: If the target node is occupied, rotate left (`α = α + π/360`) and increment state `s` until a free node is found or `s` exceeds a threshold (Sec 3).
        6.  **Front Stoppage (Branching):** Wave-fronts formed by branching fingers stop propagating when they meet other fronts (Sec 4).
        7.  **Obstacle Interaction:** Fingers interact with predefined obstacles (regions of state '1' in the model), turning left upon collision (Sec 5, 6).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :-------------------: | :---: | :----------: | :----------------: | :------------: |
    | 3 | Branching | Voltage Threshold | > 1.7 | V | Sec 4 | Explicit | Explicitly stated voltage for branching onset. |
    | 5 | Collision Turning Bias (Model) | Rotation Increment | π/360 | rad | Sec 3 | Explicit | Explicitly defined rotation step in the model. |
    | 2 | Propagation Step (Model) | Step Size `d` | 0.5 | Lattice Units | Sec 3 | Explicit | Explicitly defined step size in the model. |

### **4.3 Global Order:**

    *   Content:
        1.  **Approximate Voronoi Diagram:** Edges formed by the non-occupied spaces where branching finger fronts meet (Sec 4, Figs 5, 6).
        2.  **Convex Subdivision:** Partitioning of a concave polygon into convex regions by the paths traced by non-branching fingers (Sec 5, Fig 7).
        3.  **Spiral Patterns:** Formation of spirals by interacting fingers when initiated sites are far apart (Sec 3, Fig 4).
        4.  **Logic Gate Function:** Specific output trajectories based on input finger presence/absence and collisions (Sec 6, Figs 8, 9, 10).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| 3 | Branching Onset | Voltage | > 1.7 | V | Explicit | Stated threshold voltage. | Sec 4 |
| 5 | Collision Turn Bias (Model) | Turn Angle Increment | π/360 | rad | Explicit | Model parameter definition. | Sec 3 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
*   **Note:** Order parameters are mostly qualitative descriptions or boolean states in this work, not continuously varying quantitative measures of order.

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | Local Rules -> Global Pattern | Mapping from finger interactions (collision, branching) to emergent Voronoi, subdivision, spirals, logic outputs. | Medium-High (See 4.4) | 4 | Qualitative comparison (simulation vs. target), Logic truth table | Mixed | Predictability assessed in 4.4. Fidelity assessed by visual comparison (Figs 5,6,7) and logic function verification (Fig 10, Sec 6). Yoneda score reflects qualitative match but lack of rigorous quantitative mapping fidelity measures. | Sec 4, 5, 6 |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 4. Rubric: 0=No relation; 2=Qualitative similarity; 4=Consistent qualitative mapping demonstrated via examples/simulations; 6=Quantitative mapping for some aspects; 8=Quantitative mapping rigorously validated; 10=Formal proof of equivalence. Example: Score 4 assigned as the paper shows via simulation and some experiments that local rules consistently produce the described global patterns, but without quantitative metrics of mapping accuracy or formal proofs.
    *   **Metrics:** Visual comparison of simulated/experimental patterns to ideal geometric constructs (Voronoi, convex shapes). Verification of logic gate truth tables via simulation.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", including M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Hybrid (Analog for geometric tasks, Collision-based/Digital-like logic for gates)

### **5.3 Computational Primitive:**

    *   Content: The most basic operations are:
        1.  **Collision/Interaction:** Two fingers colliding results in mutual deflection (typically left turn bias) or stopping. Finger colliding with an obstacle results in deflection (left turn bias). Finger colliding with a boundary/wall results in turning (left turn bias) and following the wall. (Sections 3, 5, 6).
        2.  **Branching:** A single finger tip splitting into two under higher voltage (Sec 1, 4).
        3.  **Propagation:** Movement of a finger tip through the medium (Sec 3).
        *   **Sub-Type (if applicable):** Collision/Interaction (Deflection/Annihilation/Stopping), Branching (Tip-splitting), Propagation (Directed Movement). These primitives combine to perform higher-level functions like logic gates (e.g., AND via selective collision/deflection in Fig 8) or geometric partitioning.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|
*   **Note:** Processing power, energy/operation, and bit-depth are not discussed or quantified in the paper. Timescales are suggested by experimental images (Fig 2) but not precisely measured or generalized.

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Finger Propagation (Experiment, Fig 2) | ~4 to 80 | s | Fig 2 caption | Explicit | Time points given for snapshots showing propagation distance (~100s µm). |
        | Finger Propagation Speed (Estimate) | ~1-10 | µm/s | Fig 2 | Implicit | Estimated crudely from distance/time in Fig 2 snapshots. |
        | Voltage Application Frequency | 1 | kHz | Sec 2 | Explicit | Stated operating parameter. |
        | Computation Time (Voronoi, Fig 5) | Not specified (likely minutes) | s / min | Fig 5 | Implicit | Observation of complex pattern formation; comparison to Fig 2 suggests longer times. |
        | Computation Time (Half-adder, Sim Fig 10) | Not specified (relative steps) | Simulation Steps | Fig 10 | Implicit | Simulation shows sequential steps, but real-time duration not provided. |
    *   **Note:** Timescales are primarily qualitative or estimated from figures.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content:
        1.  **Geometric Partitioning:** Approximating planar Voronoi diagrams using branching fingers competing for space (Sec 4). Subdividing concave polygons into convex shapes using non-branching fingers originating from indentations (Sec 5).
        2.  **Logical Computation:** Implementing Boolean logic gates (specifically a one-bit half-adder composed of AND-like gates) via controlled collisions between fingers and obstacles (Sec 6).
        3.  **Pattern Formation:** Generation of spiral patterns from interacting fingers under specific initial conditions (Sec 3, Fig 4).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content:
        *   **Voronoi/Subdivision:** Claims validated through experimental observation (optical microscopy images, Figs 2, 5) and corresponding mobile automata simulations (Figs 3, 6, 7). Visual comparison between experimental/simulated patterns and ideal geometric constructs is used (e.g., calculated Voronoi edges overlaid in Figs 5, 6). Reliability/reproducibility not quantified.
        *   **Logic Gates:** Primarily validated through mobile automata simulations demonstrating the correct truth table behavior for the designed half-adder (Sec 6, Fig 10) and the basic gate (Fig 8). Experimental validation is limited to showing basic finger collision/deflection (Fig 2), which provides plausibility but doesn't demonstrate functional gates. Robustness/yield not assessed.
        *   **Spiral Patterns:** Validated through simulation (Fig 4). Link to specific experimental conditions less clear.

---

#Key: [wang_robo-matter_2024]

# Robo-Matter towards reconfigurable multifunctional smart materials

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system, termed "Robo-Matter," consists of a large swarm (~1000) of specially designed, coin-sized, cylindrical micro-robots called "Magbots" operating on a 2D plane. Each Magbot has a vibration motor driving chiral rotation (CW or CCW), tunable magnetic rods for anisotropic binding, and a photoresistor responding to light intensity. The swarm interacts with a programmable spatial-temporal dynamic light field generated by an LED array platform. An overhead camera tracks Magbot positions, feeding information to a computer which controls the LED array in a closed feedback loop. The system's purpose is to demonstrate a proof-of-concept for reconfigurable multifunctional smart materials by leveraging the "Robot-Matter duality" of the swarm, achieving behaviors like self-assembly, phase transitions, adaptivity, morphing, healing, infiltration, and force generation through collective interactions modulated by local rules (magnetic binding, rotation) and global/local information exchange (light field).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Magbot Size | Coin-sized | mm (approx. ~24mm implied by US quarter comparison) | Fig 2c Caption | Implicit (Qualitative "coin-sized" explicit, specific dimension inferred) | Medium | Comparison to standard coin size |

    *   **Note:** Key parameters defining the physical system and its scale are listed. Magnetic strength is variable and depends on the magnets used; one specific value tested is given as an example. Magbot size is described qualitatively; a numerical estimate is inferred.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is electrical energy supplied to power the vibration motors within each Magbot (via rechargeable batteries) and the LED array platform.

### **2.2 Energy Transduction**

    *   Content: 1. Electrical energy (battery) -> Mechanical energy (vibration motor) -> Kinetic energy (chiral rotation of Magbot via interaction with brushes/surface). 2. Electrical energy -> Light energy (LED platform emits light field). 3. Light energy (from LED platform) -> Electrical signal (photoresistor in Magbot changes resistance). 4. Electrical signal (from photoresistor) -> Modulated Electrical energy (transistor circuit adjusts power to vibration motor). 5. Electrical energy -> Light energy (LED indicator on Magbot).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide quantitative efficiency metrics. Qualitatively, the efficiency is likely Low. Energy conversion from vibration to directed rotation via friction (brushes) is inefficient. Individual battery-powered motors for thousands of agents are generally less efficient than a global field for passive particles. Continuous operation of LEDs and camera/computer processing adds to energy consumption. Heat loss from motors and electronics is expected.

### **2.4 Energy Dissipation**

    *   Content: Significant dissipation occurs via: 1. Friction between Magbot brushes and the surface during rotation. 2. Inelastic collisions between Magbots. 3. Heat generated by the vibration motors. 4. Heat generated by the LED platform and control electronics. 5. Potential energy dissipation during magnetic binding/unbinding events (if hysteresis occurs). Quantification is not provided. Qualitatively, friction is likely the dominant dissipation mechanism related to Magbot motion.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip to Module 4.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: 1. **Chiral Rotation:** Each Magbot rotates (CW or CCW) driven by its internal motor, with speed (ω) dependent on local light intensity (IL). ω increases with IL (Fig 3f). 2. **Anisotropic Magnetic Binding:** Magbots interact via magnets placed in up to 6 pockets. Binding strength (Fm or E) and symmetry (2-fold, 3-fold, 6-fold tested) are tunable by magnet choice/placement (Fig 2b, 3a, 3b). Attraction occurs between magnets on different bots. 3. **Steric Repulsion:** Magbots exhibit volume exclusion. 4. **Light Intensity Response:** Photoresistor modulates motor speed based on local light intensity from the LED platform (Fig 3f, Methods).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | 1 | Chiral Rotation Speed | Angular Velocity (ω) | Increases with IL (Fig 3f shows ~1-4 rad/s approx.) | rad/s | Fig 3f, Supplementary Section 1.3.1 | Mixed (Functional form explicit, specific range implicit from graph) | Graph shows relationship, text describes dependence. |
    | 2 | Magnetic Binding | Binding Force (Fm) / Energy (E) | Variable (e.g., 0.66N mentioned) | N / J | Fig 3a, Fig 5a | Explicit (concept), Mixed (values) | Tunability explicit, specific values used in experiments represent samples. |
    | 4 | Light Intensity Response | Light Intensity (IL) | 0-255 (implied by 256 levels) | a.u. | Methods | Implicit (256 levels explicit, range inferred) | Methods mention 256 intensity levels. |

### **4.3 Global Order:**

    *   Content: The system exhibits several distinct emergent global collective states (phases): 1. **Active Glass-like:** Stable, disordered network structure, overall rotation. 2. **Active Crystal:** Stable, ordered (e.g., honeycomb for 3-fold bots), rotating structure. 3. **Liquid-like:** Dynamically evolving disordered network, overall rotation. 4. **Gas-like:** Highly dynamic collection of small clusters or individual rotating bots. These phases depend on activation strength (A, related to light intensity) and binding strength (E, related to magnetic force). (See Fig 4). Hierarchical clustering during self-assembly (Fig 5a) is another form of emergent order.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| M4.2-1 | Chiral Rotation Rate vs Light | Light Intensity (IL) | ~0-255 (tested up to 200 in Fig 5a) | a.u. | Mixed | Methods mention 256 levels; experimental range partially shown. | Methods, Fig 3f, Fig 5a |
| M4.2-2 | Magnetic Binding Strength | Binding Energy (E) / Force (Fm) | Variable (Categorical, e.g., "0.66N magnets") | J or N | Explicit (Concept), Mixed (Value) | Concept is explicit; specific values used are examples. | Results, Fig 3a, Fig 5a |
| M4.2-4 | Volume Exclusion | Magbot Diameter | ~Coin-sized | mm | Implicit | Size described qualitatively, important for packing. | Fig 2c |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| M4.6-3 | Neighbor Dynamics | Nearest Neighbor Change Rate (n_nc) | Low (Stable Phases) to High (Dynamic Phases) | s⁻¹ (inferred) | Explicit | Quantifies rate of change in neighbor identity. | Fig 4b, Supp. Sec. 2 | Image Analysis |
| M4.6-4 | Bond Dynamics | Bond Change Rate (n_bc) | Low (Stable Phases) to High (Dynamic Phases) | s⁻¹ (inferred) | Explicit | Quantifies rate of bond breaking/forming. | Fig 4b, Supp. Sec. 2 | Image Analysis |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Partial

**(Conditional: If M5.1 is "Partial", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Hybrid (Analog within Magbot, Digital in external controller)

### **5.3 Computational Primitive:**

    *   Content: **Embodied Primitive:** Thresholding/Signal Modulation (Analog). The Magbot's photoresistor and transistor circuit act as a light-sensitive controller, modulating motor speed (output) based on light intensity (input). This is analogous to a simple analog activation function or signal modulation. Mathematical Description: ω = f(IL), where f is a monotonically increasing function (Fig 3f). **External Computation:** Includes image processing (object tracking), rule-based control (light spot placement for migration, assembly, disassembly based on swarm state), potential path planning (implicit in infiltration).
    *   **Sub-Type (if applicable):** Embodied: Analog Signal Modulation. External: Digital Image Processing, Algorithmic Control.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Self-Assembly (Fig 5a, 0.66N) | ~100-200 (approx. to reach plateau) | s | Fig 5a | Implicit (from graph) | Estimated time for x(t) to approach 1 in Fig 5a. |
        | Structural Self-Correction (Fig 5b) | ~20 (approx.) | s | Fig 5b | Implicit (from graph) | Estimated duration of h6 increase in Fig 5b. |
        | Phase Transition (Compression, Fig 6a II) | 300 (5 min) | s | Fig 6a text, Methods | Explicit | Duration of compression stage explicitly stated. |
        | Phase Transition (Expansion, Fig 6a III) | 300 (5 min) | s | Fig 6a text, Methods | Explicit | Duration of decompression stage explicitly stated. |
        | Structural Self-Optimization (Fig 6b pulse) | ~20 (approx. duration of high h6) | s | Fig 6b | Implicit (from graph) | Estimated duration of the high-order state induced by the light pulse. |
        | Robotic Behaviors (healing, morphing, infiltration) | Minutes (implied qualitative) | s | Fig 7 captions, Supp Movies | Implicit | Timescales not quantified but processes shown in snapshots/movies appear to take minutes. |

    *   **Note:** Timescales are extracted from figures showing temporal evolution or stated durations in the text/methods. Self-assembly and self-correction times are approximate estimations from graphs. Robotic behavior timescales are qualitative.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes (Environmental Adaptation), No (Learned Adaptation)

**(Conditional: If M7.1 is "Yes (Environmental Adaptation)", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism is primarily **environmental stimulus response and externally guided reconfiguration**. Changes in global parameters like homogeneous light intensity (A) or boundary conditions (compression) drive the system between different stable/metastable collective phases (glass, crystal, liquid, gas) according to the phase diagram (Fig 4c). Complex reconfigurations like morphing, healing, and infiltration (Fig 7) are achieved via the closed-loop feedback system: the external computer senses the swarm state and applies specific, spatio-temporally varying light fields to induce desired disassembly, migration, and reassembly of Magbots or clusters according to predefined algorithms or manual control. The adaptation is a change in the collective configuration driven by physics (phase transitions) or external control signals manifested as light fields, not an internal modification of the Magbots' rules or parameters based on past performance.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The system exhibits a range of emergent collective behaviors arising from local interactions and/or external control:
        1.  **Phase Formation:** Spontaneous formation of distinct collective states (active glass, active crystal, liquid-like, gas-like) depending on activation and binding strength (Fig 4).
        2.  **Hierarchical Self-Assembly:** Ultra-fast formation of large structures through the assembly of smaller rotating clusters (Fig 5a).
        3.  **Structural Self-Correction/Healing (Basic):** Spontaneous reorganization of local disordered regions into the crystalline structure within the active crystal phase (Fig 5b).
        4.  **Environmental Adaptivity (Phase Transition):** Reversible transitions between phases (e.g., crystal to gas-like) in response to environmental changes like compression (Fig 6a) or light intensity pulses causing structural optimization (Fig 6b).
        5.  **Active Force Output:** Coordinated motion of a crystal assembly to push external objects (Fig 7b).
        6.  **Coordinated Internal Motion:** Cyclic motion of sub-blocks within a larger assembly driven by interactive light fields (Fig 7c).
        7.  **Smart Healing (Guided):** Directed migration and reassembly of disassembled units to repair damage using interactive light fields (Fig 7d).
        8.  **Smart Morphing (Guided):** Reconfiguration of the overall shape of an assembly via disassembly, migration, and reassembly of peripheral units using interactive light fields (Fig 7e).
        9.  **Infiltration (Guided):** Disassembly into smaller units, migration through narrow channels, and reassembly using interactive light fields (Fig 7f).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors are primarily validated through:
        1.  **Direct Observation:** Experimental snapshots and videos (Supp Movies 1-6) visually demonstrate the collective states and dynamic processes (self-assembly, phase transitions, morphing, etc.).
        2.  **Quantitative Analysis:** Order parameters (ψ6, h6, n_nc, n_bc) are calculated from tracked Magbot positions to objectively classify different phases and analyze transitions (Fig 4b, 6a, 6b, Supp Sec 2). Assembly kinetics are quantified using the Avrami index (Fig 5a).
        3.  **Systematic Parameter Variation:** Phase diagrams are constructed by systematically varying control parameters (A, E) across numerous experiments (Fig 4c, Supp Sec 3.2) and simulations (Supp Sec 4.2), demonstrating reproducibility and predictability of phase formation.
        4.  **Control Experiments (Implicit):** Different magnetic symmetries (Supp Sec 3.5) and chiralities (Supp Sec 3.6) are tested, acting as controls demonstrating the tunability and generality of the phenomena. Compression experiments involve controlled boundary changes (Fig 6a, Methods). Guided behaviors use specific light field algorithms (Methods, Supp Sec 3.4).
        **Limitations:** Robustness quantification is limited. Statistical analysis across multiple runs is mentioned for some experiments (e.g., error bars in Fig 5a), but the extent is not always clear. The influence of system size and boundary conditions is briefly explored (Supp Sec 3.2) but not exhaustively. Simulation results supplement experiments but rely on modeling assumptions.

---

#Key: [yao_fully_2020]

# Fully hardware-implemented memristor convolutional neural network

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a fully hardware-implemented 5-layer memristor-based Convolutional Neural Network (mCNN) designed for image recognition (MNIST dataset). It utilizes eight integrated 2,048-cell memristor crossbar arrays (TiN/TaOₓ/HfOₓ/TiN material stack in a 1T1R configuration) as processing elements (PEs). These arrays perform parallel in-memory multiply-accumulate (MAC) operations, embodying the synaptic weights of the CNN layers (convolutional and fully connected). The system incorporates a customized PCB, an FPGA evaluation board (Xilinx ZC706) for control and interfacing, an ARM core for certain functions (pooling, activation, updates), and peripheral circuits like on-chip decoders, ADCs, multiplexers, and voltage generators. Its purpose is to demonstrate a fast, energy-efficient non-von Neumann computing architecture for deep neural networks, capable of achieving software-comparable accuracy despite device imperfections, leveraging a proposed hybrid training method.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These are key hardware implementation parameters. CNN-specific parameters (kernel sizes, etc.) are also explicit but not listed here for brevity.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is electrical, supplied as voltage pulses to the memristor arrays for programming (SET/RESET operations to write weights) and reading (inference/MAC operations), and for powering the peripheral CMOS circuitry (FPGA, ARM core, ADCs, MUXs, etc.).
    *   Value: Programming: 1.8-4.7 V pulses; Reading: 0.2 V pulses. (Specific power supply values for entire system not detailed)
    *   Units: V (Volts)

### **2.2 Energy Transduction**

    *   Content: 1. **Programming:** Electrical energy (voltage pulses) is converted into electrochemical potential, driving ion/vacancy migration within the TaOₓ/HfOₓ layers. This modulates the conductive filament structure, changing the memristor's resistance state (Stored Potential Energy/Chemical Energy). 2. **Reading (Inference):** Electrical energy (read voltage pulse) is applied across the memristor. Based on its resistance state (stored information), this energy is converted into electrical current (Ohm's Law). Currents from multiple memristors are summed (Kirchhoff's Law) representing the MAC operation. 3. **Peripheral Circuits:** Electrical energy powers standard CMOS logic operations within the FPGA, ARM core, ADCs (converting analog current/voltage signals to digital), MUXs, etc., involving charging/discharging of transistor gates and interconnects, leading to heat dissipation.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper explicitly benchmarks the system's energy efficiency at 11,014 GOP s⁻¹ W⁻¹ (for simulated 8-bit input), claiming it is >100 times better than a state-of-the-art GPU (Tesla V100). This high score reflects the core advantage of in-memory computing, eliminating von Neumann bottlenecks. The benchmarking considers memristor operations and peripheral circuits (ADC, MUX, drivers etc. simulated at 65nm, S&H/ADC from refs). However, the score is not 10 because the benchmarking excludes some functions (pooling, activation, routing - handled by ARM core in the experiment) and relies partly on simulations/external refs for peripheral circuits rather than solely on the fully integrated experimental system's measurement.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation occurs through several mechanisms: 1. **Memristor Programming:** Joule heating during SET/RESET pulses due to current flow through the device and forming/rupturing filaments. 2. **Memristor Reading:** Joule heating due to read current flowing through the memristor resistance. 3. **Peripheral CMOS Circuits:** Dynamic power dissipation (charging/discharging capacitances in FPGA, ARM, ADCs, MUXs, drivers) and static power dissipation (leakage currents). The benchmarking section (Methods, Extended Data Table 1) quantifies energy consumption for key blocks during a 1-bit inference: Memristor Array (0.91 pJ), Drivers/MUX (1.72 pJ), S&H (19.2 pJ), ADC (2.55 pJ per conversion x 32 ADCs x (50ns/50ns) = 81.6 pJ - *calculation corrected based on text stating ADC completes 4 conversions per 1-bit inference cycle, so 32 ADCs * 4 conversions * 2.55 pJ/conv = 326.4pJ for ADC block during 1-bit inference* - Re-reading methods: "8-bit ADC completes four conversations during the 1-bit inference stage" & consumes 2.55pJ *per conversion*. Total ADC energy is 32 ADCs * 4 conv/ADC/cycle * 2.55 pJ/conv = 326.4 pJ. Periph total = 0.91+1.72+19.2+326.4 = 348.23 pJ for 1-bit inference across the array. The relative contribution is dominated by ADCs and S&H blocks. Qualitative Assessment: Medium-High, dominated by analog/mixed-signal peripherals (ADC, S&H).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Hours to Days (Qualitative)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 15 (differential levels per weight); 8 (levels per memristor)
*   Units: levels (quantization states)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |
    | Read (1-bit inference) | ~0.02 pJ/cell/bit * | ~1.82E-5 W/cell/bit * | pJ, W | Medium | Ext. Data Table 1, Methods | Mixed | Energy per array (0.91pJ) / 2048 cells. Power = Energy/Time (0.91pJ/50ns) / 2048 cells. Assumes 1-bit inference relates to 1 bit stored state access. |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | State Distinguishability | Ability to maintain separation between conductance levels | 32 levels separated initially | levels | `MemoryNode` attr: `fidelity_levels` | Fig 1c | Explicit | Fig 1c shows non-overlapping distributions for 32 states initially. |
    | Read Disturbance | Change in conductance after multiple read operations | Minimal change after 10^6 reads | % / μA (from plots) | `MemoryNode` attr: `read_robustness` | Ext. Data Fig 7 | Explicit | Plots show minimal state disturbance after 1M reads. |
    | Yield | Percentage of functional devices | 99.99% (stated for device) | % | `MemoryNode` attr: `reliability_yield` | Methods | Explicit | Stated in Methods for the memristor device itself. Array yield might differ. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip remaining M4 sections.)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Neuromorphic/Analog/Hybrid

### **5.3 Computational Primitive:**

    *   Content: Multiply-Accumulate (MAC) / Vector-Matrix Multiplication (VMM). The material system (memristor crossbar) physically embodies the weighted summation. Input voltage (Vᵢ) applied to a column (bit line) results in current Iⱼᵢ = Gⱼᵢ * Vᵢ through the memristor Gⱼᵢ at the intersection with row j (source line). The total current on source line j is Σᵢ(Gⱼᵢ * Vᵢ), which is the dot product of the input voltage vector and the conductance vector (weights) for that row. This is performed in parallel for all rows.
    *   **Sub-Type (if applicable):** Analog MAC/VMM

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|
| Memristor Array MAC/VMM | Core analog computation engine (128x16 array per PE) | 1164 GOP/s/mm² (System Density)* | 0.91 pJ per array per 1-bit input pulse* | 50 ns (pulse width); 20 MHz (read freq used in benchmark)* | Analog core; Used for 4-bit/2-bit effective weights; 8-bit ADC | Ext Data Table 1&2, Methods | Explicit/Mixed | Performance density & energy/op are explicitly calculated for the benchmark system (macro core). Frequency used in benchmark is explicit. Bit-depth effective usage (4/2/15-level) is explicit. Analog core is implicit from mechanism. |
*Note: Metrics marked with (*) are from the benchmarking simulation/calculation in Methods (Extended Data Tables), not direct measurements from the full 5-layer experimental system operation.*

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Programming Pulse Width | 50 | ns | Fig 1c caption, Methods | Explicit | Explicitly stated for SET/RESET. |
        | Read Pulse Width | 50 | ns | Methods (Benchmarking) | Explicit | Assumed same as programming unless stated otherwise; used in benchmark calc. |
        | Inference Cycle Time (Benchmark) | 50 | ns | Methods (Benchmarking: "1-bit read pulse (0.2 V, 50 ns)") | Explicit | Smallest time unit for benchmark calculation (1-bit). |
        | System Clock Frequency (Benchmark) | 20 | MHz | Methods (Benchmarking: "applying a 1-bit read pulse to all rows at 20 MHz") | Explicit | Frequency used for driving read pulses in benchmark. |
        | ADC Conversion Time (per 4 conversions) | 50 | ns | Methods | Implicit | Derived from "ADC completes four conversations during the 1-bit inference stage" (50 ns). |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is a form of supervised learning applied in-situ to the final FC layer, called "hybrid training". It uses Stochastic Gradient Descent (SGD) with a mini-batch size of 100. For each batch, forward propagation occurs through the hardware mCNN. The error (cross-entropy loss) is calculated externally (likely on ARM core). Gradients of the loss w.r.t. the FC layer outputs are computed. Weight updates (ΔW) for the FC layer are calculated using these gradients and the intermediate inputs to the FC layer (Equation 1: ΔW = η Σ Vᵢ × δᵢ). A threshold learning rule (Equation 2) is applied, where only updates exceeding a threshold (Th = 1.5 μS) trigger a programming operation (SET/RESET pulse via closed-loop writing) on the corresponding memristor pair to adjust its differential conductance. This adaptation compensates for variations/imperfections in preceding layers and the FC layer itself.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behavior is image classification, specifically recognizing handwritten digits from the MNIST dataset. The system takes a 28x28 pixel image as input and outputs probabilities for each of the 10 digit classes (0-9). The class with the highest probability is selected as the classification result. This behavior emerges from the designed structure of the 5-layer CNN and the learned (via hybrid training) conductance values (weights) stored in the memristor arrays, which collectively process the input image through layers of convolution, pooling, and fully connected operations. Parallel convolution using replicated kernels for faster processing is also demonstrated.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The emergent behavior (image classification) is validated experimentally by testing the hardware system on the standard MNIST test dataset (10,000 images). The primary metric is classification accuracy (%). The paper reports accuracy at various stages: software baseline (97.99%), after weight transfer (95.07%), after hybrid training (96.19% in initial demo, final error rate 3.81% => 96.19% accuracy; parallel system 95.83%). Performance is compared against software implementations and GPU benchmarks (energy, density). The effect of drift on accuracy is also quantified over time (Extended Data Fig 3c). Control experiments involve comparing performance before and after hybrid training, and comparing different PE groups in the parallel setup. Reproducibility is suggested by consistent results across multiple PEs/groups (Fig 4g, Extended Data Fig 1). Limitations include potential instability of the test setup for long runs (mentioned in Methods: "Evaluation of recognition accuracy").

---

#Key: [wright_deep_2022]

# Deep physical neural networks trained with backpropagation

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is a hybrid in situ–in silico training algorithm called Physics-Aware Training (PAT) applied to train controllable physical systems, termed Physical Neural Networks (PNNs). PNNs consist of layers of controllable physical systems (e.g., optical, mechanical, electronic) that perform computations through their physical transformations, potentially without direct mathematical isomorphism to conventional Artificial Neural Network (ANN) layers. The purpose is to leverage the potential speed and energy efficiency of physical processes for machine learning tasks (inference), trained using the efficient backpropagation algorithm despite the lack of a perfect digital model. PAT uses the physical system for the forward pass and a differentiable digital model for the backward pass (gradient calculation). The paper demonstrates PNNs based on optical second-harmonic generation (SHG), mechanical oscillations of a plate, and a nonlinear electronic circuit, trained to perform vowel and handwritten digit (MNIST) classification.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters characterize the *performance* and *key physical aspects* mentioned in the excerpt. Implementation details like specific voltages, frequencies, or detailed optical parameters are likely in the full paper/SI.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Specific energy sources depend on the PNN implementation:
        *   Optics (SHG): Laser light (e.g., Titanium:Sapphire laser). Electrical energy for laser, pulse shaper (DMD), spectrometer, control computer.
        *   Mechanics: Electrical energy for audio amplifier driving a speaker (voice coil) to apply forces. Electrical energy for microphone, DAQ, control computer.
        *   Electronics: Electrical energy for the circuit components (transistor, RLC), DAQ, control computer.
        *   Training (PAT): Primarily electrical energy for the digital computer performing the backward pass and parameter updates.
    *   Units: W (Watts) or J (Joules)

### **2.2 Energy Transduction**

    *   Content:
        *   Optics (SHG): Electrical to Optical (Laser); Optical (input pulse modulation via DMD); Optical to Optical (Nonlinear frequency mixing/SHG in crystal); Optical to Electrical (Spectrometer detection).
        *   Mechanics: Electrical to Mechanical (Amplifier/Speaker voice coil generating force); Mechanical (Plate oscillations); Mechanical to Acoustic (Plate vibrations producing sound); Acoustic to Electrical (Microphone detection).
        *   Electronics: Electrical (Input signal from DAQ); Electrical (Processing within nonlinear circuit); Electrical (Output signal to DAQ).
        *   PAT: Physical system's energy transduction during forward pass; Electrical energy used for digital computation (model evaluation, gradient calculation, parameter update) during backward pass.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper *motivates* PNNs by stating conventional DNNs are energy-limited and PNNs have the *potential* to perform machine learning more energy-efficiently (orders of magnitude claimed in Discussion/Supp. Section 5). However, no quantitative efficiency metrics (e.g., Joules per operation, TOPS/W) are provided for the demonstrated PNNs within this excerpt. The *potential* efficiency is the *goal*, not a measured property here. The training process (PAT) itself involves significant digital computation, limiting efficiency gains primarily to the inference phase. Qualitative Assessment: Potential High (for inference), Low/Medium (for training via PAT).

### **2.4 Energy Dissipation**

    *   Content: Mechanisms are implicit based on the physics:
        *   Optics: Absorption and scattering losses in optical components (DMD, crystal, lenses), non-ideal conversion efficiency in SHG, heat in laser, detector noise/heat. Qualitative: Medium/High (expected in nonlinear optics).
        *   Mechanics: Damping in the mechanical oscillator (internal friction, air resistance), heat in the speaker coil/amplifier, acoustic energy radiation. Qualitative: High (mechanical systems are often damped).
        *   Electronics: Resistive losses (I²R heating) in components, heat generated by transistor, noise sources. Qualitative: Medium/High (depends on circuit specifics).
        *   Computation: Heat generated by digital computers during training/control. Qualitative: High (for digital computation).
        Quantification: Not provided in the excerpt.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceed to M3.2 and M3.3.)**

### **3.2 Memory Type:**

    Overall Score reflects good retention and readout, with moderate-to-high capacity determined by design. It's re-writable via retraining. It's closer to synaptic weight storage in ANNs than dynamic state memory.

### **3.3 Memory Retention Time:**

*   Value: Long-term

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: N (system dependent)
*   Units: Parameters

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: Related to PNN classification accuracy (e.g., 87-97% for MNIST)
*   Units: %

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Transferability (Sim-to-Real) | In silico training performs poorly compared to PAT | ~40% vs 93% (Vowel); ~14% vs 93% (MNIST Elec. baseline); Low vs High (general) | % Accuracy | Attribute `sim_to_real_gap` of `MemoryNode` | Fig. 3b, c, Discussion | Explicit | PAT overcomes simulation-reality gap |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog / Neuromorphic / Hybrid

### **5.3 Computational Primitive:**

    *   Content: System-dependent:
        *   Optics (SHG): Nonlinear convolution-like transformation (Eq. 6 provides an idealized approximation: B(ωi)=k∑j A(ωi+ωj)A(ωi−ωj)). Mixes input data and parameters nonlinearly.
        *   Mechanics (Oscillating Plate): Controllable linear convolution-like operations via multimode oscillations driven by time-varying forces (described qualitatively, Supplementary Figs 16, 17 mentioned).
        *   Electronics (Nonlinear Circuit): Nonlinear transient response of an RLC circuit with a transistor. Described qualitatively, Supplementary Figs 12, 13 mentioned.
    *   **Sub-Type (if applicable):** Optics: Nonlinear Mixing/Convolution; Mechanics: Linear Convolution/Filtering; Electronics: Nonlinear Dynamics/Filtering.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | SHG Pulse Duration | ~100 | fs | Methods | Explicit | Value specified for input laser. |
        | SHG Computation Time | ~fs-ps | s | Methods (Implied) | Implicit | Inferred from "ultrafast" description and physics of pulse propagation. |
        | Electronic DAQ Sampling Rate | 1 | MS/s | Methods | Explicit | Value specified for DAQ. |
        | Electronic Computation Time | ~µs | s | Methods (Implied) | Implicit | Inferred from sampling rate (duration of time series). |
        | Mechanical Computation Time | ~ms | s | Methods (Implied) | Implicit | Inferred from use of audio speaker/microphone and typical mechanical oscillation timescales compatible with ~1MS/s DAQ. |
    *   **Note:** Computation times refer to the duration of the physical transformation for a single input forward pass.

### **6.2 Active Inference:**

    *   Content: Partial/Unclear

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is Physics-Aware Training (PAT), which implements gradient-based learning (specifically, backpropagation using stochastic gradient descent or variants like Adam, mentioned for digital model training). The gradient of the loss function with respect to the physical parameters (θ) is estimated using a differentiable digital model (f_m) for the backward pass, while the forward pass uses the actual physical system (f_p). The parameters are updated iteratively based on this estimated gradient (Eqs. 3, 4 in Methods): θ[l] → θ[l] - η * (1/N_data) * ∑_k g_θ[l](k). It's a form of supervised learning via parameter tuning based on error feedback.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**</h3>

    *   Content: The main functional behaviors demonstrated are classification tasks:
        1.  Vowel formant classification (Optics - SHG PNN).
        2.  Handwritten digit classification (MNIST dataset) using Mechanical PNN, Electronic PNN, and a hybrid Optics-Digital PNN.
        The systems learn complex input-output mappings suitable for these classification problems by configuring the physical transformations through training.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The emergent behavior (classification) is validated experimentally by testing the trained PNNs on unseen test data (subsets of vowel data or MNIST). Performance is quantified using standard classification metrics: accuracy (Figs. 3b, 3c, 4c, 4g, 4k) and confusion matrices (Figs. 2c, 4d, 4h, 4l). Control comparisons are made against:
        *   Random guessing baseline (implicit 10% for MNIST, 14% for vowels).
        *   In silico training (shows PAT is superior, Fig. 3b, 3c).
        *   Digital baselines (linear classifier, digital network for hybrid PNN) to show the contribution of the physical computation (Figs. 4c, 4g, 4k references).
        Reproducibility is suggested by averaging results (Fig 3c error bars). Limitations: Validation is task-specific (classification); generalization to other tasks is claimed but not demonstrated. Robustness tests are limited.

---

#Key: [hu_multi-compartment_2022]

# Multi-compartment supracapsules made from nano-containers towards programmable release

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of multi-compartment 'supracapsules' fabricated by the self-assembly of primary dextran-based nanocapsules within evaporating oil droplets. The nanocapsules (building blocks, average diameter 243 ± 92 nm, 5 nm thick shell) are synthesized via polyaddition at a water-in-oil miniemulsion interface and contain various water-soluble cargoes (e.g., fluorescent dyes like Cy5-PEG, FITC-PEG; superparamagnetic nanoparticles). Oil droplets containing dispersed nanocapsules are generated using a microfluidic device (cross-junction geometry). Solvent (cyclohexane) evaporation from these droplets templates the assembly of nanocapsules into larger, precisely sized (tunable between 2-20 µm) spherical supracapsules with a close-packed internal structure (packing density ~0.47). The purpose is to create hierarchical structures where individual nanocapsule compartments retain their cargo and functionality, while the assembled supracapsule exhibits emergent collective properties, specifically programmable, diffusion-dominated cargo release kinetics distinct from single nanocapsules. The structure can also be disassembled on demand using ultrasonication for burst release.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                | Value        | Units      | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :---------------------------- | :----------: | :--------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Nanocapsule Packing Density (fv) | 0.47 ± 0.07  | unitless   | Text (Results / Fig. 2c)  | Explicit          | Medium                          | Derived from fitting Eq. 1        |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Several energy inputs are involved at different stages:
        1.  **Fabrication (Microfluidics):** Pressure-driven flow (implied fluid pumps).
        2.  **Fabrication (Assembly):** Thermal energy for solvent (cyclohexane) evaporation (ambient conditions in fume hood implied).
        3.  **Characterization (VSM):** Magnetic field.
        4.  **Characterization (Confocal/SEM/Cryo-SEM):** Light/Electron beams.
        5.  **Disassembly/Release Trigger:** Ultrasonic energy (sonication).
        Primary energy for *assembly* is thermal energy driving evaporation. Primary energy for *triggered release* is ultrasonic.
    *   Value: Not specified for fabrication energies. Sonication: 20% amplitude, cycle 20s pulse/10s pause (Specific energy/power not quantified).

### **2.2 Energy Transduction**

    *   Content:
        1.  **Assembly:** Thermal energy -> Phase transition (liquid cyclohexane to gas) -> Increased nanocapsule concentration -> Potential energy minimization (van der Waals, capillary forces) -> Structural assembly (kinetic/potential energy stored in ordered structure).
        2.  **Magnetic Response:** Magnetic field energy -> Alignment of superparamagnetic nanoparticle moments -> Macroscopic magnetic moment of supracapsule -> Kinetic energy (movement/rotation) or Potential energy change (separation).
        3.  **Sonication:** Electrical energy -> Acoustic energy (ultrasound) -> Mechanical energy (cavitation, shear forces) -> Breaking inter-nanocapsule bonds (potential energy change) -> Increased surface area/dispersion (kinetic/potential energy). -> Leads to change in release kinetics (chemical potential gradient driving diffusion).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any data or discussion regarding the energy efficiency of the fabrication (microfluidics, self-assembly) or disassembly (sonication) processes. Efficiency values cannot be calculated or estimated.

### **2.4 Energy Dissipation**

    *   Content: Potential dissipation mechanisms include:
        1.  **Microfluidics:** Viscous dissipation during fluid flow.
        2.  **Evaporation:** Latent heat of vaporization removed from the system (or supplied by environment).
        3.  **Assembly:** Possible minor heat release during particle packing (reduction in surface energy).
        4.  **Magnetic Actuation:** Viscous drag during movement in fluid. Possible hysteresis losses within SPIONS (though likely small for superparamagnets).
        5.  **Sonication:** Heat generation due to acoustic absorption and cavitation bubble collapse, viscous dissipation.
        Quantification is not provided. Qualitative assessment: Sonication likely involves significant dissipation as heat. Microfluidics involves standard viscous losses. Evaporation involves defined latent heat transfer. Assembly dissipation likely minimal.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip to Module 4)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The paper does not explicitly define the local interaction rules mathematically. Based on the context of colloidal assembly during drying in a non-polar solvent (cyclohexane), the likely dominant rules are:
        1.  **Steric Repulsion:** Nanocapsules cannot occupy the same space.
        2.  **Van der Waals Attraction:** Attractive forces between nanocapsules at close range in the cyclohexane medium.
        3.  **Capillary Forces:** During the final stages of drying, capillary bridges between particles or forces exerted by the receding solvent front likely contribute significantly to compaction.
        4.  **Confinement:** Interactions are confined within the evaporating droplet.
        The outcome is close packing, suggesting a minimization of free volume driven by attractive forces and confinement.
    * **Implicit/Explicit**: Implicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The emergent global order is a densely packed, roughly spherical cluster (supracapsule) composed of the individual nanocapsules. Cryo-SEM (Fig. 1d) suggests a relatively homogeneous packing throughout the volume, likely amorphous or polycrystalline close-packing rather than a perfect crystal lattice, given the polydispersity of the nanocapsules (± 92 nm). The measured packing density (fv ≈ 0.47) is lower than random close packing (~0.64) or FCC/HCP packing (~0.74), suggesting a potentially less ordered or looser packing structure possibly influenced by polydispersity or assembly kinetics.
    * **Implicit/Explicit**: Mixed

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description             | Parameter            | Value Range    | Units    | Implicit/Explicit | Justification                                         | Protocol                            | Source         |
| :---------- | :---------------------- | :------------------- | :------------- | :------- | :----------------: | :---------------------------------------------------- | :---------------------------------- | :------------- |
| Size        | Supracapsule Diameter   | d_sc                 | 2 - 20         | µm       | Explicit          | Measured from micrographs                           | Optical Microscopy (ImageJ)         | Fig. 2b, Text  |
| Packing     | Nanocapsule Packing Density | fv                   | 0.47 ± 0.07    | unitless | Explicit          | Derived from fitting Eq. 1 to experimental data | Analysis of size vs. concentration data | Fig. 2c, Text  |
| Morphology  | Overall Shape           | Shape Factor (approx)| ~1 (near spherical)| unitless | Implicit          | Inferred from spherical appearance in micrographs   | Optical/SEM Microscopy              | Fig. 1b, Fig 1c |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type                | Description                                             | Predictability (Score 0-10) | Yoneda Score (0-10) | Metrics                                  | Implicit/Explicit | Justification                                                                                                | Source  |
    | :----------------------- | :------------------------------------------------------ | :--------------------------: | :-------------------: | :--------------------------------------- | :----------------: | :----------------------------------------------------------------------------------------------------------- | :------ |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description              | Value        | Units   | Source             | Implicit/Explicit | Justification                                   |
        | :--------------------------------- | :----------: | :-----: | :----------------: | :----------------: | :---------------------------------------------- |
        | Assembly (Evaporation)             | Not specified| hours?  | Text (Methods)     | Implicit          | Implied by collection in open tube in fume hood |
        | Cargo Release (Nanocapsules, Plateau) | ~4           | hours   | Fig. 4b            | Explicit          | Time to reach plateau in release curve        |
        | Cargo Release (5µm Supracapsules, Plateau)| ~6           | hours   | Fig. 4b            | Explicit          | Time to reach plateau in release curve        |
        | Cargo Release Half-life (t_1/2, Nano) | ~0.5-1?      | hours   | Fig. 4b/c (est.)   | Implicit          | Estimated from curve shape/fit in Fig 4b/c    |
        | Cargo Release Half-life (t_1/2, 5µm SC)| ~1.5?        | hours   | Fig. 4b/c (est.)   | Implicit          | Estimated from curve shape/fit in Fig 4b/c    |
        | Cargo Release Half-life (t_1/2, 16µm SC)| ~3?          | hours   | Fig. 4b/c (est.)   | Implicit          | Estimated from curve shape/fit in Fig 4b/c    |
        | Sonication Disassembly (to clusters) | 1            | minute  | Text (Results)     | Explicit          | Time specified for partial disassembly        |
        | Sonication Disassembly (to single NC)| 5            | minutes | Fig. 4d, Text      | Explicit          | Time specified for complete disassembly       |
        | Burst Release (Sonication)         | minutes      | minutes | Fig. 4e, Text      | Explicit          | Qualitative description, rapid release shown  |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors are:
        1.  **Controlled Self-Assembly:** Nanocapsules spontaneously form ordered, size-controlled supracapsules upon solvent evaporation within microfluidic droplets.
        2.  **Multi-Compartmentalization:** Retention of distinct cargoes within individual nanocapsules inside the supracapsule structure.
        3.  **Programmable Cargo Release:** Sustained, diffusion-dominated release of cargo (Cy5) from supracapsules, with release kinetics (e.g., half-life) tunable by controlling the supracapsule size (diffusion path). Release follows first-order kinetics and is slower than from individual nanocapsules.
        4.  **Magnetic Manipulability:** Supracapsules containing magnetic nanoparticles can be separated and manipulated using external magnetic fields.
        5.  **Stimulus-Triggered Disassembly & Burst Release:** Supracapsules disassemble into smaller clusters or individual nanocapsules upon application of ultrasound (sonication), switching the release mode from sustained to rapid burst release.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content:
         *   **Self-Assembly:** Validated by optical and SEM imaging showing formation of spherical supracapsules (Fig 1b, 1c). Size control validated by plotting measured size vs predicted size based on Eq. 1 (Fig 2c). Internal packing validated by cryo-SEM (Fig 1d).
         *   **Multi-Compartmentalization:** Validated by high-resolution confocal microscopy showing distinct localisation of different fluorescent dyes (Cy5-PEG, FITC-PEG) within separate nanoscale compartments (Fig 3a). Cargo retention validated by monitoring dye leakage over 72h (Fig S6).
         *   **Programmable Release:** Validated by measuring cumulative release of Cy5 dye over time for supracapsules of different sizes and comparing to individual nanocapsules (Fig 4b). Size-dependence (programmability) validated by plotting release half-life vs supracapsule diameter (Fig 4c). Diffusion-dominated mechanism supported by fitting release curves and observing quadratic growth of half-life with size.
         *   **Magnetic Manipulability:** Validated by VSM measurements showing superparamagnetic behavior (Fig 3b) and optical tracking of supracapsule movement under external magnetic fields (Fig 3c, 3d, Videos S1, S2).
         *   **Triggered Disassembly & Burst Release:** Disassembly validated by DLS measurements showing decrease in cluster size with sonication time (Fig 4d). Switch to burst release validated by measuring Cy5 release profile with and without sonication (Fig 4e, 4f).

---

#Key: [benjamin_role_2023]

# A role for cortical interneurons as adversarial discriminators

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### 1.1 System Description

    *   Content: The system described is a theoretical framework and computational model proposing that cortical interneurons function as discriminators within an adversarial learning algorithm for unsupervised sensory representation learning in the brain. The system involves two primary phases: a 'wake' phase where an inference network infers representations (z) from sensory inputs (x), and a 'sleep' or 'generative' phase where a generative network generates 'dreamed' inputs (x) from sampled representations (z). A discriminator network (hypothesized interneurons) learns to distinguish between (x,z) pairs generated during these two phases. The purpose is to learn both a generative model of sensory data (p(x,z)) and an inference model (q(x,z)) that approximates Bayesian inference, aligning the distributions of internally generated and externally evoked neural activity patterns. The model leverages plasticity rules that switch between Hebbian and anti-Hebbian based on the phase (wake/sleep or oscillation). Two algorithms are presented: Adversarial Wake/Sleep (Algorithm 1) and an Adversarial Oscillation algorithm (Algorithm 2) designed for hierarchical networks with local discriminators.

### 1.2 Implementation Clarity


### 1.3 Key Parameters

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters are for the computational simulations used to validate the theoretical framework.

## M2: Energy Flow

### 2.1 Energy Input


### 2.2 Energy Transduction


### 2.3 Energy Efficiency


### 2.4 Energy Dissipation


## M3: Memory

### 3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### 3.2 Memory Type:**


### 3.3 Memory Retention Time:**

*   Value: Long-term (Qualitative Descriptor)

### 3.4 Memory Capacity (Optional - if applicable)**

*  Value: High (Qualitative) / Proportional to number of network parameters
*   Units: Parameters / Bits (Theoretical)

### 3.5 Readout Accuracy (Optional - if applicable)**


### 3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Related to weight decay (2e-5 mentioned) but not analyzed as memory degradation.

### 3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### 3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

## M4: Self-Organization and Emergent Order

### 4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

## M5: Computation

### 5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### 5.2 Computation Type:**

    *   Content: Neuromorphic/Hybrid (Analog processing within units, potential digital simulation)

### 5.3 Computational Primitive:**

    *   Content: Weighted Summation followed by Non-linear Activation Function.
    *   **Sub-Type (if applicable):** The specific non-linearity used in simulations is ELU (Exponential Linear Unit) or LeakyReLU. The overall operations involve vector-matrix multiplications and element-wise non-linear transformations, characteristic of standard artificial neural network layers. Stochastic sampling from Gaussian distributions is also a key operation (Methods section).

### 5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### 6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Wake/Sleep Cycle | Hours (Implicitly biological) / Alternating Phases (Algorithmic) | Hours / Algorithmic Steps | Introduction / Algorithm 1 | Implicit / Explicit | Biological timescale implied; algorithmic phases explicit. |
        | Oscillation Cycle | Milliseconds to Seconds (Implicitly biological, e.g., gamma, alpha, theta ranges mentioned) / Alternating Phases (Algorithmic) | ms-s / Algorithmic Steps | Introduction / Results / Algorithm 2 / Discussion | Implicit / Explicit | Biological timescales mentioned; algorithmic phases explicit. |

### 6.2 Active Inference:**

    *   Content: Partial

## M7: Adaptation

### 7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### 7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is adversarial learning via synaptic plasticity. The discriminator network (interneurons) learns to distinguish between 'real' (wake/bottom-up) and 'fake' (sleep/generative/top-down) patterns of activity (x,z pairs). Its output provides a teaching signal. The inference and generative networks adapt their parameters (ϕ and θ) to 'fool' the discriminator, minimizing the objective the discriminator maximizes (e.g., Eq 1 for Wasserstein GAN). This involves calculating gradients of the discriminator's output with respect to the generative/inference parameters and updating weights accordingly (e.g., via backpropagation in simulations). A key hypothesized feature is that the plasticity rule for synapses onto the discriminator switches polarity between phases: Hebbian during wake (to increase firing for 'real' patterns) and anti-Hebbian during sleep (to decrease firing for 'fake' patterns), or vice-versa depending on the objective function sign convention (Results: Plasticity of synapses targeting discriminator cells, Eq 5).

## M8: Emergent Behaviors

### 8.1 Behavior Description:**

    *   Content: The primary functional behavior is unsupervised learning of sensory representations and a generative model. Observable behaviors in the simulations include:
        1.  **Generation:** Producing novel data samples (e.g., MNIST digits, Fig 3d, 3h) that resemble the training data distribution, driven by sampling from the learned prior (z).
        2.  **Inference/Encoding:** Mapping input data (x) to a latent representation (z).
        3.  **Reconstruction:** Mapping an input (x) to its representation (z) and then generating back an output (x'), aiming for x' ≈ x (Fig 3e, 3h).
        4.  **Distribution Matching:** Aligning the joint distribution of activity (x,z) during inference (wake/bottom-up) with the distribution during generation (sleep/top-down), quantifiable via histogram comparisons (Fig 2d, 3c, 3f, 3i).

### 8.2 Behavior Robustness:**


### 8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behavior (realistic generation, distribution matching) are validated primarily through computational simulations.
        *   **Generation:** Visual inspection of generated MNIST digits (Fig 3d, 3h).
        *   **Distribution Matching:** Quantitative comparison of binned histograms of quantized neural activity (subsampled x and z variables) between wake/inference and sleep/generative phases (Fig 2d, 3c, 3f, 3i), following the method of Berkes et al. [3]. KL divergence is calculated for the toy model (Fig 2d).
        *   **Reconstruction:** Visual inspection of input vs. reconstructed digits (Fig 3e, 3h).
        *   **Operational Definitions:** Behaviors are operationally defined by the outputs of the simulation algorithms.
        *   **Control Experiments:** Comparison is made between different algorithms (VAE baseline vs. Adversarial Wake/Sleep vs. Oscillatory Adversarial) on the same task (Fig 3).
        *   **Limitations:** Validation is primarily on MNIST and a small toy task. Quantitative metrics like log-likelihood are noted as intractable for the recurrent models used. Robustness analysis is limited. The link to biological emergence relies on the plausibility of the hypothesized mechanisms.

---

#Key: [xi_emergent_2024-1]

# Emergent intelligence of buckling-driven elasto-active structures

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system, termed "bucklebot," consists of two centimeter-sized self-propelled microbots (Hexbug Nano®) connected by an elastic polyester beam via 3D-printed collars. The microbots generate propulsion through vibration-induced frictional contacts. When connected by the beam, the propulsive forces exerted by the microbots buckle the elastic beam. This buckling aligns the microbots, enabling the entire structure to achieve persistent, directed ballistic motion across a flat substrate. The purpose is to demonstrate how coupling simple active particles with nonlinear elasticity can lead to emergent complex behaviors, such as directed motion, wall-following, passage through constrictions, maze navigation, path probing, and obstacle clustering, which individual microbots cannot achieve. Components include: two Hexbug Nano microbots, a polyester beam of varying length (ℓ) and thickness, 3D-printed PLA collars, inserts, and screws.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Microbot Driving Force (F) | 20 ± 3 | mN | Section IV.B | Explicit | High | Measured via load cell |
        | Microbot Free Velocity (Vf) | 154 ± 15 | mm/s | Section IV.A | Explicit | High | Measured via tracking |
        | Beam Elastic Modulus (E_beam) | 2 | GPa | Section IV.A | Explicit | Medium | Manufacturer data (assumed reliable) |
        | Beam Length (ℓ) | Varied (e.g., implied range ~50-500 mm from Fig 2 inset) | mm | Fig 2 (Insets) | Mixed | Medium | Experimental variable; specific ranges inferred from plots |
        | Beam Thickness | Varied (0.102, 0.191, 0.254) | mm | Fig 2 (Insets), Section IV.A | Explicit | High | Experimental variable; specific values given |
        | Dimensionless Force (Fℓ²/B) | Varied (~10 - 1000) | Dimensionless | Fig 2, Fig 3 | Mixed | Medium | Calculated from measured/controlled parameters (F, ℓ, thickness, E_beam) |
        | Microbot Length (L) | 45 | mm | Section IV.A | Explicit | High | Stated characteristic |
        | Relative Length (λ = L/ℓ) | Varied (depends on ℓ) | Dimensionless | Section V.A, E | Mixed | Medium | Calculated from measured/controlled parameters (L, ℓ) |

    *   **Note:** Bending stiffness B is derived from E_beam, beam width (not explicitly stated but needed), and thickness (B ∝ E*width*thickness³). Its calculation involves implicit steps. Fℓ²/B and λ are key dimensionless parameters derived from primary measurements.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the chemical energy stored in the batteries powering the internal vibrating motors of the Hexbug Nano microbots.

### **2.2 Energy Transduction**

    *   Content: 1. Chemical energy (battery) is converted to electrical energy. 2. Electrical energy powers the internal rotating motor. 3. The motor's rotational kinetic energy is converted into mechanical vibrations of the microbot body/legs. 4. Vibrations, through asymmetric frictional contact with the substrate, are transduced into translational kinetic energy (propulsion force F) of each microbot. 5. The kinetic energy/propulsive force of the microbots does work on the elastic beam, converting into elastic potential energy stored in the buckled beam. 6. The interplay between the stored elastic energy (resisting force/moment) and the microbot propulsion leads to the coordinated translational kinetic energy of the entire bucklebot structure.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Extremely low. The paper does not provide any quantitative efficiency metrics. However, converting chemical energy to vibration and then to directed motion via friction is known to be highly inefficient. Significant energy is lost as heat in the motor, damping within the microbot structure, and especially through frictional interactions with the substrate, which is the intended mechanism but inherently dissipative. The process involves converting directed motor rotation into randomized vibration, then rectifying it back into less efficient directed motion. Qualitative Assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include:
        1.  **Friction:** Translational (drag coefficient γ) and rotational (damping coefficient Γ) friction between the microbot legs/body and the substrate. Explicitly modeled (Eqs. 2a, 2b, 4a, 4b) and stated as dominant (ζ ≫ 1 assumption). Magnitude: High.
        2.  **Internal Damping:** Energy loss within the microbot structure during vibration (implicit). Magnitude: Medium/High (inherent in vibration).
        3.  **Motor Inefficiency:** Heat loss during electrical-to-mechanical conversion in the motor (implicit). Magnitude: Medium/High (typical for small DC motors).
        4.  **Beam Damping:** Internal material damping during buckling/unbuckling cycles (not explicitly modeled, assumed quasi-static beam). Magnitude: Low (relative to friction).
        5.  **Air Resistance:** Negligible compared to substrate friction (implicit). Magnitude: Low.
        Quantification: γ and Γ are included in the model, but their values are not explicitly provided or derived from experiments in the excerpt, though they relate to Vf and F (Vf=F/γ). The overdamped assumption ζ≫1 (Section V.A) highlights the dominance of friction.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are governed by:
        1.  **Beam Dynamics:** 2D Kirchhoff equations (Eqs 1a-c or dimensionless 3a-c) describe the beam's elastic behavior relating internal forces (n), moments (m), curvature (∂θ/∂s), and inertia (ρb). Assumed quasi-static (M ≈ 0).
        2.  **Microbot Dynamics:** Langevin-like equations (Eqs 2a-b or dimensionless/overdamped 4a-b, considering ζ ≫ 1) describe the motion of each microbot based on its driving force (F), orientation (e∥), drag (γ, Γ), and the reaction forces (R) and torques (Q) from the beam.
        3.  **Coupling Constraints:** Clamping conditions (Eqs 5a-d or dimensionless 6a-d) link the beam ends (s=0, ℓ) to the microbots' positions (x₀, x₁) and orientations (ψ₀, ψ₁), ensuring force/moment balance (n=-R, m=-Q at ends) and geometric compatibility (beam tangent matches microbot orientation).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Beam Dynamics | Bending Resistance | B | Varies (depends on thickness³, width) | N·m² | Section II.A, V.A (derived) | Mixed | Derived from E, thickness, width (implicit) |
    | Microbot Dynamics | Driving Force | F | 20 ± 3 | mN | Section IV.B | Explicit | Measured |
    | Coupling | Relative Length | λ = L/ℓ | Varied (e.g., ~0.1 - 0.9 based on Fig 2 inset ℓ) | Dimensionless | Section V.A, E | Mixed | Calculated variable |
    | General | Dimensionless Force | Fℓ²/B | ~10 - 1000 | Dimensionless | Figs 2, 3, Section II.A | Mixed | Calculated variable |

### **4.3 Global Order:**

    *   Content: The primary emergent global order is persistent ballistic motion (MSD ∝ t²) for dimensionless forces 10 < Fℓ²/B < 600. This ordered state is characterized by a steady-state velocity (V) and a stable, bent configuration of the beam defined by the angle ψ between the microbots. For Fℓ²/B > 600, the order changes to rotation combined with slow translation (due to higher buckling modes). Other emergent behaviors built upon this include wall-following, slit passage, and maze navigation.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Beam Dynamics | Elasticity | B (Bending Stiffness) | Varies (f(thickness, E)) | N·m² | Mixed | Primary variable controlling buckling | Sec II.A, V.A |
| Microbot Dynamics | Activity | F (Driving Force) | 20 ± 3 | mN | Explicit | Constant input driving buckling | Sec IV.B |
| Microbot Dynamics | Damping | ζ (Damping Ratio) | ≫ 1 (Assumed) | Dimensionless | Explicit | Assumption simplifying dynamics to overdamped | Sec V.A |
| Coupling | Geometry | λ (Relative Length L/ℓ) | ~0.1 - 0.9 | Dimensionless | Mixed | Controls effective buckling load through leverage | Sec V.A, E, Fig 6 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Ballistic Motion | Translation Speed | V/Vf | ~0 - 1 | Dimensionless | Explicit | Normalized speed characterizing motion | Tracking (Sec IV.B), Fig 2b | Sec II.A |
| Configuration | Bending Angle | ψ | ~0 - π/2 | radians | Explicit | Angle characterizing beam deformation | Tracking (Sec IV.B), Fig 2c | Sec II.A |
| Long-term Dynamics | MSD Exponent | α (MSD ∝ t^α) | ~1.4 (single), ~2 (buckle), <2 (>600Fℓ²/B) | Dimensionless | Explicit | Characterizes diffusive vs ballistic nature | MSD calculation from trajectories (Sec II.A) | Fig 2d |
| Wall Interaction | Residence Time | τr | Varies (f(α, Fℓ²/B)) | s | Explicit | Time spent following wall | Tracking, Fig 3a,b | Sec II.B |
| Slit Interaction | Passage Success | Probability | 0 - 1 | Dimensionless | Explicit | Likelihood of traversing constriction | Repeated trials, Fig 3d | Sec II.B |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | Physics-Behavior | Local physical rules (Eqs 1-6) determining global steady state (V, ψ) | High | 8 | R² (implied from Fig 2b,c data collapse), Model-Experiment Agreement | Mixed | Model accurately predicts key emergent parameters V, ψ from local rules. Deviations explained by λ. | Sec II.A, V.D, Figs 2b,c |
    | Physics-Interaction | Local physical rules determining wall/slit interaction dynamics | Medium-High | 7 | Scaling law match (τr vs sqrt(Fℓ²/B)), Threshold prediction (δ/ℓ vs Fℓ²/B) | Mixed | Model captures trends & thresholds but with some quantitative deviation (e.g., τr magnitude). | Sec II.B, V.F, G, Figs 3b,d |
    | Physics-Complex Task | Local rules enabling maze solving / path probing / clustering | Medium | 6 | Qualitative success, Path length ratio convergence (Fig 4a), Coagulation time τ (Fig 4d) | Mixed | Model explains components (motion, deformation), but direct simulation of maze/clustering not shown. Success linked to predicted basic behaviors. | Sec III, Fig 4 |

    *   **Yoneda Embedding Fulfillment Score [0-10]:** 7 (Rubric: Score reflects how well the *global behavior* (hom(C(A,-), G)) can be predicted solely from understanding the *local interaction rules* (hom(A,-)) applied to the components. 0 = No relation; 5 = Qualitative trends match; 7 = Key quantitative features predicted with some deviation; 10 = Perfect quantitative prediction of all aspects.)
    *   **Metrics:** Comparison of model predictions (derived from local rules) with experimental measurements of global order parameters (V, ψ) and interaction dynamics (τr, passage probability, MSD exponent, cluster dynamics). R² values are not explicitly given but implied by data collapse and theory lines in figures.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Microbot Reorientation (τ) | ~1.3 | s | Sec II.A | Explicit | Characteristic time for single bot direction change |
        | Buckle-bot Motion Onset | ~1 | s | Fig 2a | Explicit | Time to reach steady-state V, ψ |
        | Ballistic Motion Persistence | > 5τ (>~6.5) | s | Sec II.A | Explicit | Duration over which MSD ∝ t² is observed |
        | Wall Residence Time (τr) | Variable (e.g., ~2-10+) | s | Fig 3b | Explicit | Time spent sliding along wall, depends on α, Fℓ²/B |
        | Obstacle Clustering (τ_coag, bucklebot) | 23.3 | s | Sec III, Fig 4d | Explicit | Characteristic time for cluster number reduction (fit) |
        | Obstacle Clustering (τ_coag, 2 bots) | 49 | s | Sec III, Fig 4d | Explicit | Characteristic time for cluster number reduction (fit) |
        | Path Probing (Short Path) | ~14 | s | Fig 4b | Explicit | Time to navigate and return from a specific path |
        | Path Probing (Long Path) | ~25 | s | Fig 4b | Explicit | Time to navigate and return from a specific path |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior is **persistent ballistic motion** (directed translation, MSD ∝ t²) arising from the buckling-induced alignment of two near-diffusive microbots, occurring within a specific range of the dimensionless force parameter (10 < Fℓ²/B < 600). Building on this, the system exhibits several functional behaviors:
        1.  **Wall Following:** Interacting with a planar boundary, the bucklebot slides along the wall for a residence time (τr) before reflecting, typically near β ≈ π/2.
        2.  **Constriction Passage:** The bucklebot can deform its structure (bend the beam further) to squeeze through narrow slits (δ < w, where w is equilibrium width), with success depending on Fℓ²/B and δ/ℓ.
        3.  **Maze Navigation:** Combining ballistic motion and boundary interactions allows the bucklebot to efficiently navigate simple maze structures (Fig 1c).
        4.  **Path Probing:** By navigating into a closed path and returning, the time taken can be used to infer path length (Fig 4a, b).
        5.  **Obstacle Clustering:** The bucklebot pushes and aggregates dispersed light obstacles into clusters more effectively and rapidly than individual microbots (Fig 4c, d).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors are validated through:
        1.  **Direct Observation & Visualization:** Time-lapse photography and video recordings clearly show ballistic motion, maze navigation, wall following, slit passage, and clustering (Fig 1, 2a, 3a, 3c, 4a-c, Movies S1-S4).
        2.  **Quantitative Analysis:** Trajectory tracking (ArUco markers, OpenCV) allows quantification. MSD analysis confirms ballistic motion (MSD ∝ t², Fig 2d). Velocity V and angle ψ measured and plotted vs Fℓ²/B (Fig 2b,c). Wall residence times τr and reflection angles β measured (Fig 3b). Slit passage success rate quantified over multiple trials (Fig 3d). Path length ratio and clustering dynamics (N(t), τ_coag) quantified (Fig 4a, d).
        3.  **Control Experiments:** Comparison with the behavior of individual microbots highlights the emergent nature of the bucklebot's capabilities (e.g., Fig 1b vs 1c, Fig 2d blue vs orange/black lines, Fig 4a blue vs markers, Fig 4d blue vs orange fit).
        4.  **Theoretical Modeling:** A physics-based model (Section V) is developed, and its predictions are compared with experimental results, showing good agreement for key behaviors and parameters (Fig 2b,c, Fig 3b,d), supporting the physical mechanism underlying the emergence.
        **Limitations:** Validation primarily in controlled 2D lab environments. Robustness testing is limited. Direct simulation of complex tasks like maze navigation using the full model is not presented.

---

#Key: [hadorn_specific_2012]

# Specific and reversible DNA-directed self-assembly of oil-in-water emulsion droplets

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of oil-in-water emulsion droplets (EDs) made of diethyl phthalate (DEP) stabilized by phospholipids (POPC, DSPE-PEG, DSPE-PEG-biotin). The surface of the EDs is functionalized with biotinylated single-stranded DNA (btn-ssDNA) oligonucleotides using streptavidin as a linker. The purpose is to achieve specific and reversible DNA-directed self-assembly of these microscopic EDs into macroscopic aggregates. The assembly is driven by the hybridization of complementary ssDNA strands attached to different ED populations. Disassembly can be triggered by changing temperature, electrolyte concentration, or adding competing oligonucleotides. The system is also designed to be recyclable by resetting the surface functionalization via biotin displacement and subsequent re-functionalization.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy inputs are thermal energy (for temperature changes driving assembly/disassembly via DNA melting) and chemical potential differences (driving DNA hybridization, influenced by salt concentration, and biotin-streptavidin binding/displacement). Mechanical energy is used for preparation (sonication, agitation) but not directly for the assembly/disassembly process itself, other than agitation for mixing.
    *   Value: Temperature changes (e.g., Room Temp to 60 °C), Chemical potential gradients (related to concentrations: [NaI], [competing DNA], [biotin]).
    *   Units: °C or K for temperature; J/mol or related concentration units (mM) for chemical potential drivers.

### **2.2 Energy Transduction**

    *   Content: 1. Chemical potential energy stored in ssDNA is converted into binding energy upon hybridization (exothermic process), leading to aggregation. 2. Thermal energy input raises the system temperature above the DNA melting temperature (T<sub>m</sub>), breaking hydrogen bonds (endothermic process), converting binding energy back into potential energy of separated strands and increased entropy, leading to disassembly. 3. Changes in chemical potential due to altered salt concentration affect the stability (and T<sub>m</sub>) of the DNA duplex, modulating the binding energy landscape and driving assembly/disassembly. 4. Addition of competing DNA or soluble biotin alters the chemical potential landscape, favoring dissociation of existing bonds (DNA-DNA or biotin-streptavidin).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any information or metrics to assess the thermodynamic efficiency of the assembly/disassembly processes (e.g., energy input vs. work done in aggregation/disaggregation, or energy cost per bond formed/broken). Assessing efficiency is not a focus of the study.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation mechanisms are not quantified. Qualitatively, dissipation occurs primarily as heat during: 1. DNA hybridization (exothermic). 2. Viscous drag as droplets move during aggregation/sedimentation. 3. Frictional losses during any mixing/agitation steps. 4. Heat exchange with the environment during temperature changes. The dominant dissipation related to the core process is likely heat released during hybridization. Quantification is not provided. Assessment: Low (relative to macroscopic processes).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rule is sequence-specific DNA hybridization between complementary ssDNA strands anchored to the surfaces of adjacent emulsion droplets. This binding is governed by thermodynamics (dependent on temperature, salt concentration, DNA sequence/length). Secondary interactions include steric repulsion from PEG tethers (preventing non-specific aggregation) and van der Waals forces. Binding only occurs significantly when T < T<sub>m</sub> and [Salt] is sufficient (e.g., 25 mM NaI). Non-complementary DNA sequences do not lead to significant interaction/aggregation under the tested conditions. The biotin-streptavidin linkage provides a stable anchor point for the DNA.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | DNA_Hyb | Temperature Dependence | T | < T<sub>m</sub> (aggregation), > T<sub>m</sub> (disassembly, tested at 60°C) | °C | Results, Fig 3 | Explicit | Temp thresholds for assembly/disassembly shown. |
    | DNA_Hyb | Salt Dependence | [NaI] | ≈ 25 mM (aggregation), ≈ 12.5 mM (disassembly) | mM | Results, Fig 3 | Explicit | Salt conc. thresholds for assembly/disassembly shown. |
    | Steric | Non-specific Repulsion | PEG Tether | DSPE-PEG MW 2000 | Da | Introduction, Materials & Methods | Explicit | Component preventing non-specific sticking is defined. |

### **4.3 Global Order:**

    *   Content: The emergent global order is the formation of macroscopic aggregates of emulsion droplets, visible to the naked eye. The paper describes these as "extensive ED3:ED3′ aggregates" and "very large aggregates." The specific geometry or packing structure of the droplets within the aggregates is not characterized in detail, but fluorescence microscopy (Fig. 2B, 3C/D) shows clusters of intermixed green and red droplets.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| DNA_Hyb | Specific DNA Hybridization | T | < T<sub>m</sub> | °C | Explicit | Aggregation occurs below T<sub>m</sub>. | Results, Fig 3 |
| DNA_Hyb | Specific DNA Hybridization | [NaI] | ≈ 25 | mM | Explicit | Aggregation requires sufficient salt. | Results, Fig 3 |
| Comp_Bind | Competitive Displacement | [ssDNA*] | e.g., 10-fold excess | ratio | Explicit | Excess competing strands prevent reassembly. | Results, Materials & Methods |
| Comp_Bind | Competitive Displacement | [Biotin] | e.g., 1000-fold excess | ratio | Explicit | Excess biotin disrupts streptavidin linkage for reset. | Results, Materials & Methods |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| ClusterSize | Size of aggregates | Size | Microscopic to Macroscopic (visible) | μm to mm | Mixed | Micrographs show cluster size; "visible to naked eye" implies mm scale. | Microscopy, Visual | Results, Fig 2, Fig 3 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Functionalization Incubation | 1 | hour | Materials & Methods | Explicit | Time allowed for streptavidin/DNA binding. |
        | Assembly Incubation (population level) | 60 | min | Materials & Methods (step 1) | Explicit | Time allowed for aggregation. |
        | Disassembly Incubation (Temp, Salt) | 30-60 | min | Materials & Methods (step 2f, 3) | Explicit | Time observed for disassembly. |
        | Competitive Disassembly Incubation (ssDNA*) | 30 | min | Materials & Methods (step 4f) | Explicit | Time for competing strands to bind. |
        | Reset Incubation (Biotin @ 60°C) | 24 | hour | Materials & Methods (step 5) | Explicit | Time required for biotin to displace streptavidin at elevated T. |
        | Functionalization Stability Test | 1 (or 7) | week (or days) | Results (Fig 1B), M&M | Explicit | Timeframe over which functionalization integrity was tested. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is stimulus-responsive, sequence-specific aggregation and de-aggregation of emulsion droplets. Under specific conditions (complementary DNA, T < T<sub>m</sub>, sufficient salt), microscopic droplets self-assemble into macroscopic aggregates. This aggregation can be reversed by changing temperature, salt concentration, or adding competing DNA strands. A secondary behavior is the ability to be reset (surface DNA removed) and re-functionalized (re-programmed) with different DNA sequences.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent aggregation are validated through: (1) Direct Observation: Optical and fluorescence microscopy confirm the formation of aggregates from complementary EDs and absence from non-complementary controls (Fig 2B, Fig 3C-F). Macroscopic visibility is noted. (2) Controlled Experiments: Systematic variation of stimuli (temperature, salt, competing DNA, biotin) demonstrates predictable assembly/disassembly cycles (Fig 3, Fig S2). (3) Specificity Controls: Use of non-complementary DNA sequences consistently prevents aggregation, confirming the DNA-directed nature (Fig 1B, Fig 3E-F). Reproducibility is implied through presentation of "representative" micrographs and consistent outcomes across different steps/conditions. Limitations: Quantitative analysis of aggregate size distribution, packing density, or formation kinetics is limited. Robustness testing is not exhaustive.

---

#Key: [goldman_robot_2024]

# Robot swarms meet soft matter physics

__Paper Type:__ Review

## M1: System Overview & Implementation

### 1.1 System Description

    *   Content: The paper reviews the concept of applying principles from soft matter physics (specifically granular media) to the design and control of robot swarms. It posits that a swarm of robotic bodies can be considered a single collective "robot" with emergent properties distinct from its components. The primary examples discussed are "Granulobots" (cm-scale cylindrical robots with internal actuators and magnets for attraction/rolling) and "BOBbots" (simpler active cohesive granular robots). The purpose is to leverage soft matter physics to create reconfigurable robot collectives that can exhibit diverse mechanical behaviors (fluid-like, solid-like, viscoelastic/plastic) and locomotor strategies emerging from simple, local interactions, enabling functions like obstacle navigation and object transport without complex centralized control.

### 1.2 Implementation Clarity


### 1.3 Key Parameters

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** This excerpt focuses on the concept and refers to specific implementations (Granulobots, BOBbots). Detailed quantitative parameters are sparse in this text, mostly referenced from Saintyves et al. (5) and Li et al. (9). Reliability is Medium/Low as the values are cited rather than presented as primary data within this paper.

## M2: Energy Flow

### 2.1 Energy Input

    *   Content: The primary energy source for individual robot activity is internal actuators that rotate off-center magnets. The specific energy source powering these actuators (e.g., batteries) is not explicitly stated but is implied for autonomous robots.

### 2.2 Energy Transduction

    *   Content: Energy stored (likely electrically) is converted into mechanical energy by the internal actuator, causing the rotation of an off-center magnet. This rotation generates forces/torques. When robots interact, the potential energy stored in the magnetic fields between nearby robots is converted into kinetic energy (rolling motion) and potential energy changes as the collective configuration changes. Energy is transduced from individual robot actuation into collective motion and deformation.

### 2.3 Energy Efficiency

    *   Justification/Metrics: The paper does not provide any information regarding the energy efficiency of the actuators, interactions, or collective movement. Qualitative assessment is not possible without more data.

### 2.4 Energy Dissipation

    *   Content: Energy dissipation mechanisms are not explicitly quantified but can be inferred. They include: friction between robots during rolling, friction with the environment/substrate, internal friction/losses within the actuators, and non-ideal energy transfer during magnetic interactions (e.g., slight slipping, inelastic collisions). Qualitative assessment: Likely Medium to High dissipation given the nature of macroscopic robotic interactions and friction.

## M3: Memory

### 3.1 Memory Presence:**

    *   Content: No

**(Skipping M3.2-M3.8 as M3.1 is "No")**

## M4: Self-Organization and Emergent Order

### 4.1 Self-Organization Presence:**

    *   Content: Yes

### 4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rule described for Granulobots is: "When magnets from two robots come close, the robots attract and can roll without slipping against each other." Individual robots also follow control schemes based on monitoring their angular position, acceleration, and rotation, adjusted via open- or closed-loop algorithms driving the internal actuator (rotating magnet). These control schemes influence the *dynamics* of the local interactions. BOBbots are mentioned as having "mechanically induced phase changes" driving aggregation/swarming, implying interaction rules based on mechanical contact triggering state changes.
    * **Implicit/Explicit**: Explicit

### 4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### 4.3 Global Order:**

    *   Content: The global order that emerges includes various collective states analogous to phases of matter: dilute gas-like swarms, dense solid-like aggregates (crystalline or amorphous), and intermediate soft matter states (fluid-like aggregates, viscoplastic/elastic solids/fluids, self-oscillating structures). Specific emergent structures mentioned include aggregates capable of locomotion, deformation around obstacles, and object transport.
    * **Implicit/Explicit**: Explicit

### 4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### 4.5. Local Interaction Rules (for Self-Organization)
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### 4.6. Globally Emergent Order and Order Parameters
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### 4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity


## M5: Computation

### 5.1 Embodied Computation Presence:**

    *   Content: No

**(Skipping M5.2-5.4 as M5.1 is "No")**

## M6: Temporal Dynamics

### 6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |

    *   **Note:** The paper discusses dynamic behaviors but does not quantify the associated timescales. These are inferred relevant timescales based on the described phenomena.

### 6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### 7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Skipping M7.2 as M7.1 is "No")**

## M8: Emergent Behaviors

### 8.1 Behavior Description:**

    *   Content: The main emergent behaviors described are collective states analogous to phases of matter: gas-like (dilute swarms), fluid-like (active flow, deformation around obstacles), solid-like (aggregation, resistance to deformation), and intermediate soft-matter states (viscoplastic/elastic solids/fluids, self-oscillation). Specific functional behaviors include active locomotion of the aggregate, shape change (deformation), aggregation/swarming, and object transport (mentioned for BOBbots). These behaviors arise from simple, local interactions between the individual robots.

### 8.2 Behavior Robustness:**


### 8.3 CT-GIN Emergent Behavior Validation

     *  Content: Validation is primarily achieved through reference to the original research papers (Saintyves et al. (5), Li et al. (9), etc.). The paper mentions "fascinating behaviors" observed even in small collectives (~10 robots, ref 5) and Figure 1 shows an image of Granulobot locomotion over an obstacle (credited to Saintyves et al.). This implies experimental observation and validation in the referenced works. The current paper itself does not present primary validation data but relies on cited studies. Reproducibility and robustness testing details are not provided here.

---

#Key: [dieball_direct_2023]

# Direct Route to Thermodynamic Uncertainty Relations and Their Saturation

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper presents a theoretical framework for deriving Thermodynamic Uncertainty Relations (TURs) directly from the equations of motion for overdamped stochastic dynamics (Langevin equation) and Markov jump processes. It provides a direct proof using stochastic calculus and the Cauchy-Schwarz inequality, unifying and simplifying existing proofs. It also extends TURs to transient systems with explicitly time-dependent currents/densities and derives a sharpened correlation TUR for transient dynamics. The purpose is to establish TURs as inherent properties of these stochastic processes, provide simpler proofs, generalize existing TURs, and identify conditions for their saturation, enabling more accurate thermodynamic inference (lower bounds on dissipation). The core components are mathematical concepts: Langevin equation, Fokker-Planck equation, Markov jump process master equation, stochastic integrals (Itô, Stratonovich), probability density currents (j), generalized time-integrated currents (Jt), densities (ρt), and entropy production/dissipation (Σt).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** The listed parameters are central theoretical quantities defined and used throughout the paper. Their specific values depend on the system being modeled and the chosen U/V functions. Reliability is 'High' as they are precise theoretical constructs.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The framework assumes a system coupled to a thermal bath, implicitly providing thermal energy (manifested as noise in the Langevin equation). The paper sets kBT = 1, measuring energy in units of thermal energy. There is no specific external energy source driving the system *within the general theoretical framework*, although specific applications (like time-dependent driving mentioned contextually) would have one. The focus is on dissipation arising from non-equilibrium dynamics.
    *   Units: kBT (Thermal Energy Units)

### **2.2 Energy Transduction**

    *   Content: The paper does not focus on energy transduction between different forms (e.g., chemical to mechanical). It focuses on the dissipation (entropy production Σt) associated with non-equilibrium processes described by stochastic dynamics. This dissipation quantifies the energy lost to the thermal bath due to irreversible processes, driven by forces (F(x)), driving, or relaxation from non-equilibrium initial states. The energy flow is implicitly from the factors driving the system out of equilibrium (forces, initial conditions) into dissipated heat in the thermal bath.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not define or calculate a thermodynamic efficiency for any process. TURs provide a lower bound on dissipation (a measure of inefficiency) based on fluctuations, but do not quantify efficiency itself.

### **2.4 Energy Dissipation**

    *   Content: The central theme is the mean total entropy production (dissipation), Σt. Equation (5) provides a formula to calculate it for overdamped dynamics: Σt = ∫[0,t] dτ ∫ dx [j(x,τ) ⋅ D⁻¹(x) ⋅ j(x,τ)] / P(x,τ). For Markov jump processes, it's defined via jump rates and probabilities (Eq. 23 related context). The TURs (Eqs. 1, 16, 19, 21, 22, 24) provide lower bounds on Σt based on the mean and variance of currents or densities. The paper aims to find tighter bounds and saturation conditions. Quantification depends on the specific system (F(x), D(x), P(x,τ), rates).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip to Module 4.)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip to Module 6.)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Observation Time Interval | t | s (or arbitrary time units) | Throughout (e.g., Eq 1, 5, 6) | Explicit | Defines the duration over which quantities are accumulated/averaged. |
        | Characteristic System Timescale (Example: Harmonic Trap) | 1/a | s (or inverse freq units) | Fig 1 (uses dimensionless `at`) | Explicit (in example) | Relaxation timescale for the specific example system. General theory is timescale-independent. |
        | Wiener Process Correlation Time | Infinitesimal (δ-correlated) | s | Section "Setup" | Explicit | Fundamental property of the noise model used. |
    *   **Note:** The theory is developed for a general time interval `t`. Specific physical systems modeled by these equations will have characteristic timescales (e.g., relaxation times, diffusion times), as illustrated by `1/a` in the example.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip to Module 8.)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary "behavior" described is the relationship between thermodynamic dissipation (Σt) and the fluctuations (variance) of observable currents (Jt) or densities (ρt) in non-equilibrium systems governed by overdamped stochastic dynamics or Markov jump processes. This relationship is captured by Thermodynamic Uncertainty Relations (TURs), which provide universal lower bounds on dissipation. The paper focuses on proving these relationships directly and exploring their generalizations and saturation conditions. Observable behaviors include the time evolution of the probability density P(x,τ), the mean and variance of currents/densities, and the mean dissipation Σt.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation is primarily through mathematical proof. The paper outlines direct derivations using stochastic calculus and the Cauchy-Schwarz inequality (Sections "Direct proof of TURs", Eq 9-15 for continuous space; Section "Direct route for Markov jump processes", Eq 23-24 for discrete space). The validity of steps relies on established properties of stochastic processes (Wiener process, Fokker-Planck equation). An illustrative example (Brownian particle in a displaced harmonic trap, Fig 1) is provided, showing calculations of quality factors (ratio of TUR sides) based on analytical solutions [63], demonstrating the application and saturation conditions of the derived TURs. Reproducibility relies on the correctness of the mathematical derivations.

---

#Key: [rubenstein_programmable_2014]

# Programmable self-assembly in a thousand-robot swarm

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of a swarm of 1024 low-cost robots ("Kilobots") designed for large-scale collective behavior. Each robot is autonomous, capable of computation (onboard microcontroller), simple sliding locomotion (vibration motors), local sensing (relative distance to neighbours via infrared), and local communication (infrared, range ~10 cm). The system's purpose is to demonstrate programmable self-assembly, where the swarm autonomously forms user-specified two-dimensional shapes through local interactions and a distributed algorithm, without global coordination or position information. The core components are the individual Kilobots and the collective self-assembly algorithm executed by each robot. The algorithm combines edge-following, gradient formation, and localization primitives based on neighbour-to-neighbour communication and distance sensing.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Robot Diameter | ~3.3 (scaled from penny in Fig 1A) | cm | Fig. 1A | Implicit | Medium | Scaled from image based on known US Penny diameter (1.905 cm). |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source for each Kilobot is an onboard rechargeable battery. The paper mentions charging operations for the entire swarm (Fig 1C caption, p. 798 "human intervention (such as charging batteries...)" ), implying electrical energy input during a charging phase. During operation, the stored electrical energy powers the robot.

### **2.2 Energy Transduction**

    *   Content: Stored chemical energy in the battery is converted to electrical energy. This electrical energy powers: 1) The microcontroller for computation (electrical -> computation/heat). 2) The vibration motors for locomotion (electrical -> kinetic/vibrational/acoustic/heat). 3) The infrared transmitter and receiver for communication and sensing (electrical -> infrared light / electrical signal / heat).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide quantitative efficiency metrics. Qualitatively, efficiency is likely Low. Vibration motors are generally inefficient for directed locomotion (converting electrical energy largely into vibration, heat, and noise rather than controlled movement). The need for ~12 hours suggests significant energy consumption for the task relative to the (unstated) battery capacity. No energy harvesting or specific low-power design strategies (beyond component choice) are mentioned.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include: 1) Heat generated by the microcontroller during computation. 2) Heat, friction (between robot and surface, internal motor friction), and acoustic noise from the vibration motors during locomotion. 3) Heat generated by the IR transmitter/receiver electronics. 4) Energy loss during communication (IR light not received or scattered). Quantification is not provided. Dissipation through vibration motors and friction is likely High.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term (operational duration)
*    Units: Qualitative Descriptor

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Task Completion Robustness | All 11+2 experimental trials fully assembled desired shape | 100% (qualitative) | % | `MemoryNode` attribute `robustness_proxy` | p. 798 | Explicit | Explicit statement of task completion in all trials. |

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The local rules are implemented within the self-assembly algorithm's primitives:
        1.  **Gradient Formation:** Robots relay messages, incrementing a value as it propagates. A robot's gradient value depends on the minimum value received from neighbours + 1. (Rule: `my_grad = min(neighbor_grads) + 1`). This establishes geodesic distance from the source.
        2.  **Edge-Following:** A robot moves by maintaining a specific distance range from neighbour(s) identified as being on the edge (based on gradient comparison or shape boundary). Movement involves adjusting vibration motors based on distance measurements to neighbours. (Rule: If `dist_neighbor < target_dist_min`, turn away; if `dist_neighbor > target_dist_max`, turn towards; otherwise move straight). Specific rules for identifying edge neighbours are implied (e.g., comparing gradient values). Robots move only if they are on the outer edge of the initial group (gradient checks) and use randomness to avoid congestion.
        3.  **Localization:** A robot computes its `(x,y)` position using distance measurements to >= 3 non-collinear, already localized, stationary neighbours via distributed trilateration. (Rule: Listen for neighbour positions & measure distance, if >=3 neighbours known, compute `my_pos = Trilaterate(neighbor_pos1, dist1, ..., neighbor_posN, distN)`).
        4.  **State Transitions (FSM):** Robots switch between states (e.g., Idle, GradientListen, EdgeFollow, Localize, Stationary) based on local conditions (e.g., receiving gradient messages, detecting edge, achieving localization, entering target shape, proximity to stationary robot with same gradient). Example rule: If localized *and* inside target shape *and* next to stationary robot with same gradient, transition to Stationary state.
        5.  **Cooperative Monitoring (Fault Recovery):** Robots exchange information with neighbours to detect potential faults (e.g., immobilized robot, pushed robot) based on unexpected distance changes, triggering recovery behaviours (reset motors, signal others to go around, re-evaluate position). (Rule: If `|dist_neighbor(t) - dist_neighbor(t-1)| > threshold` unexpectedly, initiate fault check).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Localization | Minimum Neighbours | Min Neighbour Count | >= 3 | neighbours | Explicit (p. 797) | Explicit | Stated directly in the text. |
    | Gradient Formation | Increment value | Gradient Increment | 1 | unitless | Implicit (p. 796) | Implicit | Standard gradient formation method implied, increment value not explicitly stated. |

### **4.3 Global Order:**

    *   Content: The emerged global order is a two-dimensional aggregate of robots whose overall boundary matches the user-specified target shape (e.g., a starfish, a wrench, a letter 'K'). Robots within the shape are largely stationary and packed together, although packing patterns can vary (Fig 3).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| GradForm | Robots propagate integer gradient values | Gradient Value | Integer >= 0 | unitless | Implicit | Standard practice for gradient protocols. | p. 796 |
| Localize | Robots compute position from >=3 neighbours | Min Neighbours | >= 3 | neighbours | Explicit | Stated in text. | p. 797 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Shape | Overall form achieved by swarm | Target Shape Image | User-defined | pixels/binary array | Explicit | Algorithm input is the shape image. | p. 797 | Input |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    *   **Metrics:** Task completion rate, Qualitative assessment of shape fidelity (MSE mentioned but not quantified).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Digital

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operations performed by the material (swarm robots) include:
        *   Arithmetic: Incrementing gradient values, distance calculations (implicit in trilateration).
        *   Comparison: Comparing gradient values with neighbours, comparing measured distance to thresholds, comparing computed position to target shape boundary.
        *   Logic: Evaluating conditions for state transitions in the Finite State Machine (e.g., IF (localized AND inside_shape AND near_stationary_neighbor_same_gradient) THEN...).
        *   Geometric Calculation: Distributed Trilateration (solving system of equations based on distances).
    *   **Sub-Type (if applicable):** Arithmetic (Increment, Subtraction), Comparison (>, <, ==), Logic (AND, OR), Geometric (Trilateration).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Total Self-Assembly Time (1024 robots) | ~12 | hours | p. 798 | Explicit | Stated in text. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Partial

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism is cooperative monitoring and fault recovery. Robots use neighbour interactions (monitoring relative distances over time) to detect anomalies inconsistent with expected behavior (e.g., a supposedly stationary neighbour moving, an edge-following neighbour not making progress). If a fault is detected (e.g., sensed distance changes unexpectedly), predefined recovery rules are triggered: an immobilized robot might reset its motors or signal neighbours to navigate around it; a pushed robot might re-evaluate its position using localization. This is a reactive adaptation based on detecting deviations from expected behaviour according to the existing algorithm.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is large-scale, distributed self-assembly of user-defined 2D shapes. Secondary emergent behaviors observed include: collective movement via edge-following, gradient formation across the swarm, distributed localization establishing a collective coordinate system, formation of "traffic jams" during edge-following, complex "erosion" patterns in the initial robot group, and achievement of the final shape with some global warping and variable packing. Fault tolerance through cooperative monitoring is also an emergent collective behavior.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation is primarily through physical experimentation with up to 1024 robots. Claims of emergent behaviors (shape formation, traffic jams, erosion, robustness) are supported by:
         1.  **Direct Observation:** Time-lapse videos (referenced as movies S1 to S4) and images (Fig. 3) show the process and phenomena like traffic jams, erosion, and final shapes.
         2.  **Repeatability/Consistency:** Experiments were repeated (e.g., 10 times with ~100 robots for consistency check, p. 798). All trials successfully completed the shape formation task.
         3.  **Quantitative Analysis (Limited):** Shape accuracy was measured using mean square error between true and localized positions (Fig. 3G), demonstrating the effect of accumulated errors (warping). Packing variability was noted qualitatively.
         4.  **Control (Implicit):** The system's success relies on the specific algorithm; without it, or with different rules, the specific shapes would not emerge. Seed robots provide initial conditions, but the process is otherwise autonomous.
     * Limitations: Mathematical models are acknowledged as simplified and unable to predict phenomena like traffic jams or erosion patterns (p.798-799), emphasizing the need for physical validation. Quantitative characterization of some behaviors (e.g., traffic jam dynamics, erosion boundary complexity) is limited.

---

#Key: [sartori_thermodynamic_2014]

# Thermodynamic Costs of Information Processing in Sensory Adaptation

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper presents a theoretical framework for analyzing the thermodynamic costs (entropy production, energy consumption) associated with information processing (measurement, erasure) during sensory adaptation in biological systems. Two specific models are analyzed: 1) A minimal four-state equilibrium feedforward model of a Sensory Adaptation System (SAS) with binary activity (A) and memory (M) states responding to a binary environmental signal (E). 2) A more detailed ten-state non-equilibrium feedback model of *E. coli* chemotaxis, where receptor activity (A, binary) and methylation level (M, 5 levels) adapt to changes in ligand concentration ([L]). The purpose is to establish universal bounds relating thermodynamic costs to information processed (measured and erased) during adaptation and apply these bounds to quantify costs in *E. coli*.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                  | Value           | Units          | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------------------------ | :-------------: | :------------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters selected are fundamental physical constants or key timescales/energies defining the systems. Reliability is High as these are either fundamental constants or standard, cited values for *E. coli* chemotaxis.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: For the minimal equilibrium feedforward model, energy input can be considered trabalho performed by the changing environmental signal (E) on the system via the free energy function F(a,m;e). For the *E. coli* chemotaxis model, the primary energy source is the chemical free energy released from the hydrolysis of S-adenosylmethionine (SAM), quantified by the chemical potential Δμ. Ligand binding/unbinding also changes the system's energy landscape (V(a;[L])).
    *   Value: Δμ = 6 k<sub>B</sub>T (for *E. coli*)
    *   Units: k<sub>B</sub>T or J

### **2.2 Energy Transduction**

    *   Content: In the equilibrium model, work done by the signal change is converted into system free energy (ΔF), which is then dissipated as heat (Q) during relaxation (response and adaptation). In the *E. coli* model, the chemical energy from SAM hydrolysis (Δμ) powers the methylation/demethylation cycle (memory updates), driving the system away from equilibrium and maintaining the non-equilibrium steady state (NESS). Energy from ligand binding alters the receptor's conformational equilibrium (activity changes). This energy is dissipated as excess heat (Q<sub>ex</sub>) and contributes to entropy production during adaptation. Energy flows between chemical potential (SAM), conformational states (Activity A), modification states (Memory M), and the thermal reservoir (Heat Q, Q<sub>ex</sub>).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper quantifies the thermodynamic cost relative to information-theoretic bounds. For *E. coli* adapting to a specific ligand step change (~0.3 bits measured), the energetic cost (W<sub>ex</sub> - ΔF) is ~0.5 k<sub>B</sub>T. The lower bound from Eq (13) is k<sub>B</sub>T * ΔI<sub>meas</sub> ≈ 0.2 k<sub>B</sub>T. The efficiency relative to this fundamental bound is ≈ (0.2 k<sub>B</sub>T) / (0.5 k<sub>B</sub>T) = 40%, or roughly "twice its thermodynamic lower bound" (as stated in the paper). This indicates significant dissipation but still operating comparatively close to the theoretical minimum compared to macroscopic computers. The score reflects being close but not optimal. The feedforward model's efficiency isn't explicitly calculated but implied to be non-zero due to dissipation needed for abrupt changes (Fig 4).

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation occurs as entropy production (ΔS<sub>tot</sub>, ΔS<sub>na</sub>) and heat flow (Q, Q<sub>ex</sub>) to the thermal reservoir. Key dissipation mechanisms:
        1.  **Information Erasure:** Irreversible loss of correlation with the old signal contributes k<sub>B</sub>ΔI<sub>eras</sub> to entropy production (Eq 5, Eq 10). Quantified for models (Fig 4B, Fig 6B).
        2.  **Information Measurement:** The process of gaining correlation with the new signal involves dissipation beyond the free energy change and information gain (Eq 6, Eq 9). Quantified as (W - ΔF - k<sub>B</sub>TΔI<sub>meas</sub>) or (W<sub>ex</sub> - ΔF - k<sub>B</sub>TΔI<sub>meas</sub>) > 0.
        3.  **NESS Maintenance (E. coli):** Continuous hydrolysis of SAM produces adiabatic entropy (ΔS<sub>a</sub>) to sustain the non-equilibrium steady state, even without signal changes (estimated ~6 k<sub>B</sub>T over 10s).
        4.  **Relaxation Dynamics:** Irreversible transitions between states during relaxation to steady state or adaptation dissipate energy as heat.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**

    *   **Retention:** Medium-term (seconds for *E. coli*, relative to ms activity response). It persists longer than the initial response but is actively updated.
    *   **Capacity:** Low. Binary (1 bit max) for the minimal model. For *E. coli*, 5 discrete levels (0-4 methyl groups), allowing storage of < log2(5) ≈ 2.3 bits ideally, but calculated effective storage for a step change is lower (~0.3 bits stored in M long-term, Fig 3A suggests M stores close to 1 bit eventually, but Fig 5/6 calculation gives ~0.3 bits for a specific step). Limited capacity due to discrete levels and fluctuations (S3 Figure discussion).
    *   **Read-out accuracy:** Implicitly linked to adaptation accuracy. The memory state influences the activity adaptation baseline. Errors (E<sub>m</sub>) exist. Limited by fluctuations.
    The score reflects a functional, rewritable physical memory but with limited capacity and medium-term retention compared to biological long-term memory or digital storage.

### **3.3 Memory Retention Time:**

*   Value: t<sub>m</sub> >> t<sub>a</sub>. For *E. coli*, t<sub>m</sub> ≈ 10
*    Units: s (seconds)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 1 (Minimal Model); < 2.3 bits (ideal, *E. coli*); ~0.3 bits (effective, *E. coli* step change)
*   Units: bits

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units         | Uncertainty | Data Source Reference                           | Implicit/Explicit | Justification                                             |
    | :------------------ | :--------------------------: | :-----------------------------: | :-----------: |:-----------:|:---------------------------------------------:|:-----------------:| :-------------------------------------------------------- |
    | E.coli Adaptation   | ~0.5 k<sub>B</sub>T / 0.3 bits = ~1.7 k<sub>B</sub>T/bit | ~0.05 k<sub>B</sub>T/s (rough avg) | k<sub>B</sub>T/bit, k<sub>B</sub>T/s | Medium      | Fig 5C, Extension to NESS section, Discussion | Mixed             | Cost (0.5 k<sub>B</sub>T) and info (0.3 bits) are explicit. Cost/bit is derived. Avg power is estimated (0.5 k<sub>B</sub>T over ~10s). |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :--------------------------------------- | :------------------------ | :---: | :-------------------------: | :---------------------------------------- |:-----------------:| :-------------------------------------------------------------------------- |
    | ΔI<sub>meas</sub>| Measured Information (vs Env Signal) | ~0.3 (E.coli step change) | bits  | `MemoryNode` info content | Fig 3, Fig 5C, Discussion, Eq (3), Eq (13) | Explicit          | Explicitly calculated and discussed as the info stored during adaptation. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**
### **4.2 Local Interaction Rules:**
### **4.2.1 Local Interaction Parameters:**
### **4.3 Global Order:**
### **4.4 Predictability of Global Order:**
### **4.5. Local Interaction Rules (for Self-Organization)**
### **4.6. Globally Emergent Order and Order Parameters**
### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog

### **5.3 Computational Primitive:**

    *   Content: The fundamental computational operations analyzed are:
        1.  **Information Measurement:** Quantifying the correlation gained between the system state (A, M) and the current environmental signal (E<sub>f</sub>), ΔI<sup>meas</sup> = I(A<sub>t</sub>, M<sub>t</sub>; E<sub>f</sub>). This represents gathering information about the present.
        2.  **Information Erasure:** Quantifying the loss of correlation between the system state (A, M) and the past environmental signal (E<sub>i</sub>), conditioned on the present signal (E<sub>f</sub>), ΔI<sup>eras</sup> = I(A<sub>0</sub>, M<sub>0</sub>; E<sub>i</sub>) - I(A<sub>t</sub>, M<sub>t</sub>; E<sub>i</sub> | E<sub>f</sub>). This represents discarding irrelevant old information.
        3.  **Information Storage:** Implicitly, the persistence of information about the signal in the memory state M over timescale t<sub>m</sub>.

### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description                         | Value          | Units | Source                  | Implicit/Explicit | Justification                                       |
        | :-------------------------------------------- | :------------: | :---: | :---------------------- | :----------------: | :-------------------------------------------------- |
        | Activity Response Timescale (t<sub>a</sub>) | ν<sup>-1</sup> or ~1 | ms    | Universal traits, Methods | Explicit          | Explicitly defined as the fast timescale.       |
        | Memory Adaptation Timescale (t<sub>m</sub>) | k<sup>-1</sup> or ~10 | s     | Universal traits, Methods | Explicit          | Explicitly defined as the slow timescale (t<sub>m</sub> >> t<sub>a</sub>). |
    *   **Note:** t<sub>a</sub> and t<sub>m</sub> are the key timescales defining the sensory adaptation dynamics. For specific models, bare rates ν and k define these scales.

### **6.2 Active Inference:**

    *   Content: Partial
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** Track the rate of mutual information gain (dI<sup>meas</sup>/dt) as a proxy for prediction error reduction rate. Analyze the correlation between the memory state (M) and the environmental signal (E) over time as a measure of internal model accuracy. Compare the system's energy expenditure (e.g., W<sub>ex</sub>) versus information gained (ΔI<sup>meas</sup>) under different environmental statistics or prediction tasks to assess efficiency in minimizing surprise.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism depends on the model topology:
        1.  **Feedforward Model (Equilibrium):** Adaptation occurs because the memory state M, influenced directly or indirectly by the signal E, transitions towards a state that minimizes the free energy F(a,m;e) (Eq 1) for the current signal e. These transitions happen on a slower timescale (k) than activity changes (ν). Specifically, M tends to align with E to minimize the D<sub>m</sub> term.
        2.  **Feedback Model (*E. coli*):** Adaptation (methylation changes in M) is driven by the receptor's *activity* A, not directly by the signal [L]. CheR (methylase) and CheB (demethylase) activity depends on the receptor's conformational state (A). When the activity state deviates from its adapted level due to a signal change, this feedback loop adjusts the methylation level M over timescale t<sub>m</sub> until the activity returns to its adapted baseline, effectively storing information about [L] in M. This process requires energy input (SAM hydrolysis) to operate far from equilibrium.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behavior described is **Sensory Adaptation**. Specifically:
        1.  **Response:** A rapid change in the system's activity level (A) upon an abrupt change in the environmental signal (E or [L]).
        2.  **Adaptation:** A slower adjustment of the system's internal memory state (M) to the new signal level, accompanied by the recovery of the activity (A) towards its pre-stimulus baseline level, despite the continued presence of the new signal level.
        This allows the system to be sensitive to *changes* in the environment rather than absolute levels over long times.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The behavior (sensory adaptation) is validated through:
        1.  **Model Simulation:** Simulation results for both the minimal feedforward model (S1, S2 Figures) and the *E. coli* model (implicitly, through parameters based on established models [7, 27, 34-36] and consistency with Fig 1) demonstrate the characteristic response and adaptation dynamics.
        2.  **Theoretical Derivation:** The framework itself is built on established principles of stochastic thermodynamics and information theory, showing how adaptation necessitates information processing and associated thermodynamic costs (Eqs 4-10).
        3.  **Comparison to Biological System:** The framework is applied to the well-studied *E. coli* chemotaxis system, using accepted kinetic models and parameters, and the calculated energetic costs are discussed in the context of the bacterium's energy budget.
        Limitations: Experimental validation within the paper itself is absent; it relies on applying theory to existing models/data. Emergence isn't claimed in the sense of M4; the behavior emerges from the defined model dynamics.

---

#Key: [friston_free_2023]

# The free energy principle made simpler but not too simple

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### 1.1 System Description

    *   Content: The paper describes the Free Energy Principle (FEP) as a mathematical framework derived from the physics of random dynamical systems (described by Langevin equations). It aims to provide a unifying account of self-organisation in non-equilibrium systems, particularly sentient behaviour. The core idea involves partitioning system states (external η, sensory s, active a, internal μ) based on conditional independencies arising from sparse coupling, forming a Markov blanket (b = s, a). The dynamics of internal and active states (autonomous states α = a, μ) are shown to perform variational Bayesian inference, minimizing variational free energy (F) which bounds the surprisal (negative log evidence) of the system's states. This process is interpreted as self-evidencing or active inference, where the system acts to maintain its characteristic states. The framework extends to path integrals, describing planning as inference by minimizing expected free energy (G) over trajectories. Components include states, flows (f(x)), random fluctuations (ω), Markov blankets, variational and expected free energy functionals. The purpose is to provide a first-principles, physics-based explanation for self-organisation, perception, action, and potentially consciousness in systems ranging from particles to agents.

### 1.2 Implementation Clarity


### 1.3 Key Parameters

        | Parameter Name              | Value      | Units        | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :-------------------------- | :--------: | :-----------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters are symbolic representations within the theoretical framework. Units depend on the specific physical system being modeled. Reliability is high in terms of their definition within the theory.

## M2: Energy Flow

### 2.1 Energy Input

    *   Content: The framework primarily deals with variational free energy (an information-theoretic quantity related to surprise or prediction error), not thermodynamic free energy in Joules. Energy input in the thermodynamic sense (e.g., heat, work) required to maintain the non-equilibrium steady state (NESS) is implicitly assumed but not the focus of the derivation presented. The "input" driving the dynamics described is sensory state information (`s`) interacting via the Markov blanket.

### 2.2 Energy Transduction


### 2.3 Energy Efficiency

    *   Justification/Metrics: Thermodynamic efficiency is not calculated. The framework aims for *inferential* efficiency (minimizing variational free energy, finding the most efficient representation/prediction). Figure 3 mentions a link to thermodynamic efficiency via Landauer's principle, suggesting minimizing computational costs (complexity term in VFE) implies energetic efficiency, but this is not derived or quantified in the main text.

### 2.4 Energy Dissipation

    *   Content: Dissipation is explicitly incorporated through the Helmholtz decomposition of the flow `f(x)` (Eq 6). The gradient part of the flow (`-Γ∇ℑ(x)`) represents dissipation, driving the system towards higher probability (lower surprisal) regions of the NESS density, counteracting the effects of random fluctuations `ω`. The magnitude depends on the noise amplitude `Γ`. The paper also mentions the second law applies only to closed systems, while FEP describes open systems exchanging with the environment (Section 7), implying dissipation is necessary to maintain order in an open system. Not quantified in specific units (e.g., Joules/s) but identified as a key component of the dynamics. Qualitative Assessment: Present and essential.

## M3: Memory

### 3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip M3.2-M3.8.)**

### 3.2 Memory Type:**


### 3.3 Memory Retention Time:**


### 3.4 Memory Capacity (Optional - if applicable)**


### 3.5 Readout Accuracy (Optional - if applicable)**


### 3.6 Degradation Rate (Optional - if applicable)**

### 3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### 3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### 4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", include M4.2-M4.7)**

### 4.2 Local Interaction Rules:**

    *   Content: The fundamental local interaction rule is given by the Langevin equation: `x˙(τ) = f(x) + ω(τ)` (Eq 1). This describes how the rate of change of any state `x` depends *only* on the current state `x` (via the flow `f(x)`) and random fluctuations `ω(τ)`. The specific structure of local interactions that enables self-organization via a Markov blanket is defined by constraints on the flow `f(x)`, specifically sparse coupling in its Jacobian `J = ∂f/∂x` (Eq 8, 9, 11, 12, 13). Sparse coupling ensures conditional independencies (e.g., `(μ ⊥ η) | b`, Eq 11) which define the partition into internal, external, and blanket states. These rules are local in state space.
    * **Implicit/Explicit**: Explicit

### 4.2.1 Local Interaction Parameters:**

    | Rule ID         | Description                     | Parameter Name   | Parameter Value Range   | Units         | Data Source   | Implicit/Explicit | Justification                                    |
    | :-------------- | :------------------------------ | :--------------- | :---------------------- | :------------ | :------------ | :---------------- | :----------------------------------------------- |
    | Langevin        | State dynamics flow             | `f(x)`           | system-dep.             | [Units]/s     | Eq 1          | Explicit          | Symbolic definition provided.                    |
    | Langevin        | Noise covariance                | `Γ`              | system-dep. (>=0)       | [Units]^2/s   | Eq 1          | Explicit          | Symbolic definition provided.                    |
    | Sparse Coupling | Jacobian elements               | `J_uv`, `H_uv`, `Q_uv` | 0 or system-dep.      | system-dep.   | Eq 8, 9, 10   | Explicit          | Conditions for sparsity explicitly defined.      |
    | Helmholtz       | Solenoidal flow matrix          | `Q(x)`           | system-dep. (antisym) | [Units]^2/s   | Eq 6, 12      | Explicit          | Symbolic definition provided.                    |
    | Helmholtz       | Grad flow noise dependence     | `Γ`              | system-dep. (>=0)       | [Units]^2/s   | Eq 6, 12      | Explicit          | Explicitly relates gradient flow to noise.       |

### 4.3 Global Order:**

    *   Content: The emergent global order is the Non-Equilibrium Steady State (NESS) probability density `p(x)` over the system's state space (Section 3). This density characterizes the set of states the system is likely to occupy over long timescales (the pullback attractor). It is 'ordered' in the sense that it is typically concentrated in specific regions of state space, reflecting the system's characteristic configurations, maintained despite dissipative flows and fluctuations. The existence of a persistent Markov blanket partitioning is itself a form of emergent structural order.
    * **Implicit/Explicit**: Explicit

### 4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Explicit

### 4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID         | Description               | Parameter        | Value Range         | Units         | Implicit/Explicit | Justification                                    | Source        |
| :-------------- | :------------------------ | :--------------- | :------------------ | :------------ | :---------------- | :----------------------------------------------- | :------------ |
| Dynamics        | Flow function             | `f(x)`           | system-dep.         | [Units]/s     | Explicit          | Defined in Eq 1.                                 | Eq 1          |
| Dynamics        | Noise covariance          | `Γ`              | system-dep. (>=0)   | [Units]^2/s   | Explicit          | Defined in Eq 1.                                 | Eq 1          |
| Partitioning    | Sparse coupling (Jacobian)| `J_µη`, `J_aη`, etc | 0                   | [Units]/s     | Explicit          | Required for conditional independence (Eq 11, 13). | Eq 11, 13     |
| Partitioning    | Hessian non-zero links    | `H_aµ`, `H_sα`      | != 0                | unitless/[Units]^2 | Explicit          | Defines Markov boundary connections.           | Section 4     |

### 4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description                   | Parameter         | Value Range | Units              | Implicit/Explicit | Justification                                            | Protocol     | Source      |
| :---------- | :---------------------------- | :---------------- | :---------- | :----------------- | :---------------- | :------------------------------------------------------- | :----------- | :---------- |
| GO1         | NESS Probability Density      | `p(x)`            | [0, ∞)      | 1/[Units]          | Explicit          | Steady-state solution to Fokker-Planck (Eq 2, 6).         | Theory       | Section 3   |
| GO2         | NESS Surprisal                | `ℑ(x)`            | (-∞, ∞)     | unitless           | Explicit          | `-ln p(x)` (Eq 6). Used in Helmholtz decomp.             | Theory       | Eq 6        |
| OP1         | Entropy of NESS              | `H[p(x)]`         | [0, ∞)      | unitless (nats)    | Explicit          | `E[ℑ(x)]` (Eq 5). Measures uncertainty over states.     | Theory       | Eq 5        |
| OP2         | Variational Free Energy (VFE) | `F(π)`            | (-∞, ∞)     | unitless           | Explicit          | `ℑ(π)` (Eq 18). Bounds surprisal, minimized by dynamics. | Theory       | Eq 18, 23   |
| OP3         | Expected Free Energy (EFE)    | `G(α[τ])`         | (-∞, ∞)     | unitless           | Explicit          | `A(α[τ])` (Eq 38). Path action, minimized by planning.   | Theory       | Eq 38, 39   |

### 4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :---------------- | :------------ | :-----: |

## M5: Computation

### 5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", include M5.2-5.4)**

### 5.2 Computation Type:**

    *   Content: Bayesian Inference (specifically Variational Inference / Bayesian Filtering / Predictive Coding / Optimal Control as Inference). Can be viewed as Analog computation due to continuous state dynamics.

### 5.3 Computational Primitive:**

    *   Content: Gradient Descent / Gradient Flow on Variational Free Energy (`F`) or Expected Free Energy (`G`). The basic operation is updating the state (belief parameters `μ`, action states `a`) based on the gradient of the free energy functional with respect to those states (Eq 17, 22, 23, 31, 33, 34, 41). This implicitly involves calculating prediction errors (discrepancies between predictions based on `μ` and sensory input `s`) which constitute the gradients of `F` or `G`.
    *   **Sub-Type (if applicable):** Gradient Descent.

### 5.4 Embodied Computational Units**
| Unit ID | Description                                    | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source    | Implicit/Explicit | Justification                                                          |
| :------ | :--------------------------------------------- | :--------------- | :--------------- | :--------------: | :-------: | :------------- | :---------------- | :--------------------------------------------------------------------- |
* **Note:** Processing power, energy, frequency, and bit-depth are not quantified in this theoretical framework; they would depend on the specific physical implementation. The computation is fundamentally analog (continuous states).

## M6: Temporal Dynamics

### 6.1 Timescales:**

        | Timescale Description                     | Value                  | Units        | Source         | Implicit/Explicit | Justification                                                                 |
        | :---------------------------------------- | :---------------------: | :-----------: | :-------------: | :---------------- | :---------------------------------------------------------------------------- |
        | Random Fluctuations (`ω`) Correlation Time| ~0 (for white noise)    | s            | Eq 1 assumption| Explicit          | White noise is assumed in Eq 1 for simplicity (or analytic for generalized). |
        | State Dynamics (`x`) Characteristic Time  | system-dep (`f(x)`)   | s            | Eq 1           | Explicit          | Determined by the flow `f(x)`.                                                |
        | Sensory State Change Time                 | system-dep            | s            | Implicit       | Implicit          | Assumed to potentially drive autonomous dynamics.                             |
        | Autonomous Dynamics Off-Manifold Decay    | Potentially Fast      | s            | Eq 21, Fig 2   | Explicit          | Flow towards the centre manifold is described.                                |
        | Autonomous Dynamics On-Manifold Flow      | Potentially Slow      | s            | Eq 22, Fig 2   | Explicit          | Describes belief updating/tracking dynamics.                                  |
        | Parameter/Learning Timescale              | Very Slow             | s            | Footnote 27    | Explicit          | Parameters treated as states changing on slower timescales.                   |
        | Planning Horizon                          | `τ` in path integral  | s            | Eq 3, 36       | Explicit          | Time duration considered for path probability/action.                         |

    *   **Note:** Most timescales are system-dependent or qualitative. The separation of timescales (fast fluctuations, state dynamics, parameters) is a key concept.

### 6.2 Active Inference:**

    *   Content: Yes
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:**
        *   **Prediction Error Reduction Rate:** Track `||∇_s F||` or similar metric over time. Expect minimization. (CT-GIN: Attribute on `ActiveInferenceNode` or `VFENode`).
        *   **Timescale of Anticipation:** If external states follow predictable patterns, measure lead/lag time between internal state (`μ`) estimate and actual external state (`η`). (CT-GIN: Temporal analysis of `StateNode` trajectories).
        *   **Model Evidence / Free Energy:** Track `F(π)` or `G(α[τ])` over time. Expect minimization/stabilization. (CT-GIN: Attribute on `VFENode` or `EFENode`).
        *   **Information Gain Rate (for EFE):** Quantify the rate at which ambiguity is resolved (expected info gain term in Eq 40). (CT-GIN: Attribute on `EFENode`).
        *   **Complexity vs Accuracy Trade-off:** Measure the relative contribution of complexity and accuracy terms in VFE/EFE during inference/planning. (CT-GIN: Attributes on `VFENode`, `EFENode`).

## M7: Adaptation

### 7.1 Adaptive Plasticity Presence:**

    *   Content: Partial

**(Conditional: M7.1 is "Partial", include M7.2)**

### 7.2 Adaptation Mechanism:**

    *   Content: The mechanism for real-time state adaptation is gradient descent on variational free energy (Eq 17, 23, 31, 33, 34), which updates internal states (`μ` – beliefs) and active states (`a` – actions) to minimize surprise/prediction error based on sensory input (`s`). The mechanism for model adaptation/learning (plasticity) is briefly alluded to (footnote 27) as treating model parameters as slowly changing states that also minimize variational free energy, but the specific update rules or mechanisms (e.g., Hebbian-like updates, reinforcement signals) are not specified in this paper.

## M8: Emergent Behaviors

### 8.1 Behavior Description:**

    *   Content: The primary emergent behaviors described are:
        1.  **Self-Organization:** Maintaining structure (NESS density, Markov blanket) against dissipation.
        2.  **Active Inference:** Perception-action cycles minimizing variational free energy, interpretable as belief updating and goal-directed action (including self-evidencing, autopoiesis).
        3.  **Generalized Synchrony:** Mutual tracking/prediction between internal and external states across the Markov blanket.
        4.  **Planning/Goal-Seeking:** Selection of action sequences (policies) that minimize expected free energy, balancing expected value (preferences) and information gain (curiosity).
        5.  **Precise/Classical Dynamics:** For systems with low internal noise (`Γπ→0`), behavior follows predictable paths of least action, potentially exhibiting oscillatory or complex deterministic dynamics (solenoidal flow).

### 8.2 Behavior Robustness:**


### 8.3 CT-GIN Emergent Behavior Validation**

---

#Key: [yang_emergent_2022]

# Emergent microrobotic oscillators via asymmetry-induced order

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of microscale particles (polymeric discs with a nanometre-thick Platinum (Pt) catalyst patch underneath) placed at the air-liquid interface of a hydrogen peroxide (H2O2) droplet. Individual particles catalyze H2O2 decomposition (H2O2 -> H2O + 1/2 O2), generating an oxygen gas bubble that limits the reaction by blocking fuel access. When two or more particles are present, bubble merging and subsequent collapse occur due to hydrodynamic instabilities and restored catalytic activity. This collapse propels particles apart, which are then drawn back together by buoyancy and capillary forces (Cheerios effect), leading to a self-sustained, low-frequency chemomechanical oscillation ("beating"). Robust oscillations in larger ensembles (N>2) require breaking permutation symmetry by introducing a "designated leader" (DL) particle with a larger Pt patch, which stabilizes the periodic behavior via an "asymmetry-induced order" mechanism explained by Rattling Theory. The oscillating system can be modified into a microgenerator by adding a second metal patch (Au or Ru) alongside Pt, creating an on-board fuel cell where the cyclic bubble growth/collapse modulates electrical conductance, generating an oscillatory electrical current capable of driving a microactuator. The purpose is to create autonomous, low-frequency micro-oscillators for microrobotics, capable of on-board power generation without external tethers or complex electronics.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the chemical potential energy stored in hydrogen peroxide (H2O2), which is released through catalytic decomposition on the Pt surface.

### **2.2 Energy Transduction**

    *   Content: 1. **Chemo-Mechanical:** Chemical energy from H2O2 decomposition is transduced into mechanical energy. This occurs via: (a) Gas (O2) production leading to bubble growth, storing potential energy in the deformed interface and compressed gas. (b) Bubble collapse imparts kinetic energy to the particles (impulse) and generates fluid flow. (c) Restoring forces (buoyancy, capillary) convert potential energy (gravitational, interfacial) back into kinetic energy as particles move towards each other. 2. **Chemo-Electrical (Fuel Cell variant):** Chemical energy is transduced into electrical energy via spatially separated redox reactions (Eq 3) on the Pt and Ru/Au electrodes, establishing a potential difference. 3. **Mechano-Electrical Modulation:** In the fuel cell, the mechanical oscillation (bubble growth/collapse) modulates the electrical resistance between electrodes, converting the steady electrochemical potential into an oscillating electrical current.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide an overall energy efficiency value for chemo-mechanical or chemo-electrical transduction. Qualitatively, micro/nanoscale systems driven by chemical reactions often have very low (<1%) chemo-mechanical efficiency due to large viscous dissipation at low Reynolds numbers. The electrical energy generated (measured current/voltage) is small (e.g., ~57 nA short-circuit current for Pt-Ru), suggesting low chemo-electrical efficiency, although a direct calculation isn't provided. The benchmark comparison to a larger oscillator (~47nA peak) suggests comparable, low power output. The score reflects the likely low efficiency typical of such systems and lack of quantification.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include: 1. **Viscous Drag:** Energy lost due to particle motion through the fluid (mentioned as non-Stokesian drag Fd in Supp Note 1, but not quantified in main text). Likely the dominant loss mechanism for mechanical energy. Qualitative assessment: High. 2. **Heat:** The catalytic decomposition of H2O2 is exothermic, releasing heat into the droplet (not quantified). 3. **Sound/Wave Generation:** Bubble collapse likely generates acoustic waves (not quantified). 4. **Electrical Resistance (Fuel Cell):** Energy dissipated as heat (I²R losses) within the electrolyte and electrodes (not quantified). Overall dissipation is expected to be high, typical for low Reynolds number systems.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content:
        1.  **Catalysis:** H2O2 decomposition on Pt surface generates O2 gas (Eq 1). Rate depends on H2O2 concentration and available Pt surface area (modeled via Langmuir-Hinshelwood kinetics, Fig 1j, Supp Note 1).
        2.  **Bubble Growth:** O2 gas accumulates, forming/growing a bubble attached to the particle, progressively blocking catalyst access (self-limiting).
        3.  **Bubble Merger:** Bubbles from nearby particles merge upon contact, influenced by particle proximity and potentially relative bubble sizes (Fig 1b, Fig 3a/b). Merging frees up catalyst surface area.
        4.  **Bubble Collapse:** Merged (or single large DL) bubble grows beyond a stability threshold and collapses/bursts (Fig 1e, Fig 3a/b). Collapse imparts an impulse force on particles.
        5.  **Particle Propulsion:** Bubble collapse drives particles apart.
        6.  **Restoring Forces:** Particles are drawn back together by: (a) Buoyancy `Fg` (radial component directing particles towards apex of concave interface), (b) Capillary Attraction `Fc` ("Cheerios effect" due to local interface deformations). (Fig 1e annotation, text).
        7.  **Fluid Drag:** Particle motion is opposed by fluid drag `Fd` (non-Stokesian mentioned in Supp Note 1).
        8. **Symmetry Breaking (DL dynamics):** Different Pt patch size on DL leads to different bubble growth rate, affecting merger dynamics ('sticking bubble regime') and collapse threshold compared to homogeneous particles (Fig 3a/b, Supp Note 2).

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :--------------------: | :---: | :----------: | :----------------: | :------------: |
    | 1 | Catalysis | Reaction Rate Constant (effective) | Dependent on [H2O2], surface area | varied | 1/s (effective) | Fig 1j, SuppNote 1 | Mixed | Explicitly modeled via LH kinetics, specific constants implicit. |
    | 6a | Buoyancy | Particle Density/Volume | Assumed constant | kg/m³ | Methods (material), SuppNote 1 | Implicit | Particle material specified, but effective buoyant force depends on geometry & interface, derived in model. |
    | 6b | Capillary Force | Surface Tension (H2O) | ~72 | mN/m | Standard Value | Implicit | Not explicitly stated, but intrinsic to Cheerios effect mechanism. Assumed standard value. |
    | 7 | Fluid Drag | Fluid Viscosity (H2O) | ~1 | mPa·s | Standard Value | Implicit | Not explicitly stated, assumed standard water viscosity. |

### **4.3 Global Order:**

    *   Content: The emergent global order is a synchronized, periodic chemomechanical oscillation of the particle collective, referred to as "beating". This is characterized macroscopically by the time-varying "breathing radius" `r(t)` (Eq 2), which measures the average particle distance from the collective centroid. In phase space, this corresponds to a stable limit cycle (Fig 1h, Fig 3c/d). For heterogeneous (DL) systems, robust periodicity persists at larger particle numbers (N).

### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| 1 | Catalysis | H2O2 Concentration | Studied up to ~26 | wt% | Explicit | Varied experimentally. | Fig 1j, Fig 4c |
| 1 | Catalysis | Pt Area | 125 µm vs 175 µm radius | µm² | Explicit | Key parameter for symmetry breaking. | Results |
| 6b | Capillary Force | Particle Separation Distance | Variable (sub-mm) | µm/mm | Implicit | Governs strength of Cheerios effect. | Context |
| 4 | Bubble Collapse Threshold | Bubble Radius | ~1.7x larger for DL system (Fig 3) | µm | Explicit | Key difference between homogeneous and DL. | Fig 3a/b |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| 1 | Oscillation Period | Period (T) | ~3.2 (N=2) to ~14.2 (N=7+DL) | s | Explicit | Measured from `r(t)` and recurrence. | Recurrence Analysis | Fig 1i, Fig 2e/f |
| 2 | Oscillation Amplitude | `r(t)` range | ~150-350 (N=2, Fig 1g) | µm | Explicit | Measured from particle tracking. | Particle Tracking | Fig 1g |
| 3 | Periodicity/Orderliness | Recurrence Entropy | Low (~0.1-0.2 for DL) vs High (~0.6+ for large N homo.) | bits | Explicit | Quantifies predictability from recurrence histogram. | Recurrence Analysis | Fig 3g |
| 4 | Sync Measure | Limit Cycle Stability | Closed loops (N=2, DL) vs collapsed (large N homo.) | - | Explicit | Qualitative assessment from phase portraits. | Phase Portrait Analysis | Fig 1h, Fig 3c/d |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :-------------: | :-----------: | :------ | :----------------: | :------------: | :-----: |
    | Local Interaction Rules -> Global Oscillation | Ability of local physics (catalysis, forces, bubble dynamics) to determine the global collective oscillation characteristics (period, amplitude, stability). | High (for N=2, DL systems, based on model) | 8 | Model Accuracy (Fig 1g), Recurrence Entropy (Fig 3g), Limit Cycle Stability (Fig 1h, 3d) | Mixed | The mechanistic model (SuppNote1) based on local rules successfully reproduces global dynamics (explicit fit shown Fig 1g), suggesting high fidelity. Rattling theory (SuppNote2) links local heterogeneity to global order (Fig 2d). Score reflects model success but acknowledges it's a simplified representation. | Fig 1g, Fig 2d, Fig 3g, SuppNote 1, SuppNote 2 |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 8 (Rubric: 0=No relation; 5=Qualitative agreement; 8=Quantitative model reproduces key features; 10=Perfect predictive model capturing all aspects including fluctuations). The mechanistic model captures detailed dynamics, but complexity/stochasticity at large N is less perfectly modeled.
    *   **Metrics:** Comparison of experimental vs. simulated `r(t)` (Fig 1g), dependence of frequency on [H2O2] (Fig 1j), Recurrence Entropy (Fig 3g), Phase Portraits (Fig 1h, 3c, 3d).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", including M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog

### **5.3 Computational Primitive:**

    *   Content: Oscillation / Frequency Generation. The fundamental operation performed by the material system is the conversion of a steady input (chemical energy) into a periodic, low-frequency output (mechanical motion/electrical current). It acts as a self-contained physical oscillator.
    *   **Sub-Type (if applicable):** Relaxation Oscillator (qualitatively, involving charging/discharging phases related to bubble growth and collapse).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Oscillation Period (N=2, 10.7% H2O2) | ~3.2 | s | Fig 1g, i, j | Explicit | Directly measured via recurrence analysis. |
        | Oscillation Period (N=7+1 DL) | ~14.2 | s | Fig 2e, f | Explicit | Directly measured via recurrence analysis. |
        | Oscillation Period Range (variable [H2O2]) | ~2.5 - 10+ (Period ~0.1-0.4 Hz) | s | Fig 1j | Explicit | Measured experimentally across concentrations. |
        | Bubble Growth/Collapse Cycle | Equals Oscillation Period | s | Fig 1e, Fig 3a/b | Explicit | The bubble cycle *is* the oscillation. |
        | Particle Motion Timescale (within cycle) | < Period | s | Fig 1e, f | Implicit | Particles move significantly within one period. |
        | Reaction Rate Timescale | Faster than oscillation | s | Implicit | Reaction generates gas driving the oscillation; must be fast enough; LH kinetics suggest fast surface reaction. | Context |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: 1. **Chemomechanical Self-Oscillation ("Beating"):** A collective, synchronized, periodic motion of microparticles driven by catalytic H2O2 decomposition and bubble dynamics. Characterized by a low frequency (~0.07-0.4 Hz) and measurable via the collective breathing radius `r(t)`. This behavior emerges robustly for N=2 particles or for larger N when symmetry is broken by a Designated Leader (DL) particle. 2. **Asymmetry-Induced Order:** Stabilization of periodic oscillations in larger ensembles (high N) specifically *because* of the presence of a dissimilar particle (DL), contrasting with the aperiodic behavior of homogeneous large ensembles. 3. **Electrochemical Oscillation Generation:** In the bimetallic fuel cell configuration, the chemomechanical oscillation drives a periodic modulation of electrical resistance, resulting in the generation of an oscillatory electrical current (~sub-0.03 Hz observed in actuator experiment) from a steady chemical fuel source. 4. **Microactuation:** The generated oscillatory current is used to cyclically drive a separate electrochemical microactuator.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behavior (oscillation, asymmetry-induced order) are validated through:
        1.  **Direct Observation:** Video microscopy showing collective particle motion, bubble cycles (Fig 1e, 3b, Supp Movies).
        2.  **Quantitative Tracking:** Particle coordinates tracked over time to calculate breathing radius `r(t)` (Fig 1f,g; Fig 2c,f).
        3.  **Dynamical Systems Analysis:** Phase portraits constructed from `r(t)` show limit cycles for periodic systems (Fig 1h, 3d). Recurrence histograms quantify periodicity by showing sharp peaks at the oscillation period (Fig 1i, 2b, 2e, 3f). Recurrence entropy quantifies the degree of order/periodicity (Fig 3g).
        4.  **Control Experiments:** Comparison between single particle (no oscillation, Fig 1c,d), N=2 (oscillation), homogeneous large N (aperiodic, Fig 2b,c), and DL large N (periodic, Fig 2e,f) systems directly demonstrates the emergence and the effect of symmetry breaking. The actuator experiment includes a control with only one particle (no actuation, Fig 4e).
        5.  **Mechanistic Modeling:** A model based on local physics reproduces the key features of the oscillation (Fig 1g, 1j), supporting the interpretation that the global behavior emerges from these local rules. Rattling theory provides a theoretical framework for asymmetry-induced order (Fig 2d).

---

#Key: [mengaldo_concise_2022]

# A concise guide to modelling the physics of embodied intelligence in soft robotics

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The publication presents a concise guide, framework, and review of mathematical and computational modeling approaches designed to describe the physics underpinning embodied intelligence in soft robotics. It focuses on modeling the internal interactions (soft body mechanics, deformation, actuation like tendons, fluidic actuators, smart materials) and external interactions (with fluid and solid media) of soft robots. The purpose is to provide tools and highlight challenges/opportunities for using these models in the design and control of soft robots, particularly for capturing behaviors emerging from physical interactions with the environment, which is central to embodied intelligence. The core components are the mathematical formalisms (continuum mechanics, Cosserat models, finite-dimensional parameterizations, data-driven methods) and their computational solution strategies (FEM, numerical integration, machine learning).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** The key parameters listed define the categories of models and interactions discussed as the core components of the framework presented in the paper. Specific numerical parameters would depend on the particular robot/scenario being modeled, which is beyond the scope of this guide.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The paper discusses energy input implicitly through actuation forces (internal interactions, C_int) and external interaction forces (C_ext). Specific energy sources mentioned include muscle activation (biological analogy), tendon pulling, pressurized fluids (pneumatic/hydraulic), electric/magnetic fields, or temperature changes for smart materials. External energy exchange occurs via interaction forces with fluids (drag, buoyancy) or solids (contact, friction). The framework models these as forces/stress terms within the equations of motion.

### **2.2 Energy Transduction**

    *   Content: Energy transduction mechanisms are described within the modeling framework. Input energy (via actuation C_int) is transduced into mechanical energy of the soft body (kinetic and potential/strain energy, represented by N_sb term and dynamics Dq_sb). This mechanical energy is then exchanged with the environment (via C_ext), leading to motion and interaction forces (e.g., fluid displacement, solid contact/friction). Smart materials involve transduction from other domains (electrical, thermal, magnetic) to mechanical stress/strain. The models (continuum mechanics, etc.) capture these transformations through constitutive laws (material properties) and coupling terms/constraints.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide quantitative metrics or qualitative assessments of energy efficiency for the modeling approaches or the systems they describe. Efficiency is mentioned indirectly in the context of biological systems achieving "task-efficient performance" (line 41) and the goal of increasing "overall robot efficiency" (line 66) through embodied intelligence, but the models themselves are not evaluated for energy efficiency. Concepts like dissipative forces (friction, viscosity) are mentioned, implying inefficiency, but not quantified in a general sense.

### **2.4 Energy Dissipation**

    *   Content: Dissipation mechanisms are included in the modeling framework. Internal dissipation within the soft body includes material viscosity (line 296) and internal friction (line 474, D term in parametrization models). External dissipation occurs through interaction with the environment, such as fluid drag (viscous forces, turbulent drag - lines 122, 580) and solid friction (nonlinear friction mechanisms - lines 121, 763, 770). These are typically modeled as part of the N_sb term (internal) or C_ext term (external). Quantification depends heavily on the specific system and environment being modeled. Qualitative assessment: Present and significant in soft robotics interactions.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip to Module 4.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6.)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Model Dynamics (Differential Eq. Integration) | Variable | Depends on model (e.g., s, ms) | General (Eq. 1, Sections 2.2, 3.2.1, 3.3.1) | Implicit | Timescales are inherent to solving the time-derivative (D or d/dt) equations but specific values depend on the simulation setup. |
        | Interaction Dynamics (e.g., Fluid/Solid) | Variable | Depends on physics (e.g., flow speed, contact time) | Sections 3.2, 3.3 | Implicit | Interaction timescales (e.g., unsteady flow, contact events) are part of the physics being modeled, but not quantified generally. |
        | Actuation Response Time | Variable | Depends on actuator type | Section 2.1 | Implicit | Different actuators (tendons, fluidic, smart materials) have different response times, mentioned qualitatively but not quantified. |
        | Data-Driven Model Update/Prediction Time | Variable | Depends on model complexity/hardware | Section 2.2.4 | Implicit | Computation time for ML models is relevant but not specified. |
    *   **Note:** The paper deals with dynamic models involving time derivatives (D, d/dt, ∂/∂t in Eq. 1 and subsequent model descriptions), implying the importance of timescales. However, specific quantitative values are not provided as the paper discusses general modeling frameworks.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8.)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behaviors discussed are those arising from the physical interaction between the soft robot's body and its environment, driven by actuation, as central to embodied intelligence. Specific examples mentioned include: locomotion (stepping forward exploiting gravity, adaptation to uneven terrain, octopus arm reaching leveraging buoyancy/water interaction, walking on seafloor, crawling, swimming, jumping, digging), grasping, manipulation, morphing, and growing (lines 50-56, 85, 125-126). These behaviors are proposed to emerge from the interplay of internal forces (actuation C_int), body mechanics (N_sb), and external forces (fluid/solid interactions C_ext) as described by the models.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper focuses on presenting the modeling frameworks rather than validating specific emergent behaviors resulting from them. It mentions that resulting models can be used for design and control (lines 24, 73), implying simulations based on these models could predict behaviors. It cites examples where models have been used (e.g., Fig 4, Fig 6, software like SOFA, Elastica), suggesting validation through comparison with physical experiments or simulations exists in the cited literature, but this paper itself does not present such validation data or methods. The validation discussed for data-driven models involves comparison between model-based and learning-based controllers (lines 518-523).

---

#Key: [archer_ph-responsive_2020]

# pH-Responsive Catalytic Janus Motors with Autonomous Navigation and Cargo-Release Functions

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of multifunctional polymeric Janus colloids, specifically poly(2-vinyl pyridine) (PVP) colloids hemi-coated with platinum (Pt). These colloids exhibit catalytic self-propulsion in hydrogen peroxide (H2O2) fuel. The PVP material swells in response to low pH environments (acidic). This swelling modulates the colloid's propulsion velocity and rotational dynamics. Additionally, the colloids are loaded with a model cargo molecule (Rhodamine B), and the release rate of this cargo is also pH-dependent, increasing upon swelling in low pH. The purpose is to demonstrate a system capable of autonomous navigation (chemotaxis-like accumulation in low-pH regions) coupled with triggered cargo release in response to a chemical stimulus (pH gradient), potentially relevant for microfluidic transport or targeted drug delivery (e.g., near acidic tumor environments).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters selected represent key characteristics defining the system's size, fuel, motion, and cargo release response. Reliability is High as they are directly measured/stated.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the chemical potential energy released from the catalytic decomposition of hydrogen peroxide (H2O2) fuel by the platinum coating on the Janus colloid.

### **2.2 Energy Transduction**

    *   Content: Chemical potential energy from H2O2 decomposition at the Pt surface is transduced into kinetic energy of the colloid (translational and rotational motion). The mechanism is self-diffusiophoresis, where asymmetric catalytic activity generates local concentration gradients of reaction products (e.g., O2), leading to a net fluid flow or pressure gradient around the particle that propels it.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any quantitative measure of energy efficiency (chemical energy input vs. kinetic energy output). Such systems are known to be extremely inefficient, with most energy lost as heat. Efficiency is likely very low (< 1%). Qualitative Assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: The primary dissipation mechanism is viscous drag from the surrounding fluid as the colloid moves. This dissipates the kinetic energy generated by the propulsion mechanism into heat. Additional energy is lost as heat directly from the exothermic H2O2 decomposition reaction. Quantification is not provided, but dissipation due to viscous drag is inherent and significant for motion at this scale (low Reynolds number). Qualitative Assessment: High.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip to Module 4.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial

**(Conditional: M4.1 is "Yes/Partial", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rule governing the emergent accumulation is the individual colloid's response to the local pH. Specifically:
        1.  **Sensing:** The PVP colloid senses the local pH.
        2.  **State Change:** Below a certain pH threshold (~pH 3.7), the colloid swells; the degree of swelling increases as pH decreases (Fig 2).
        3.  **Motion Modulation:** Swelling causes a significant decrease in translational velocity (v) and angular velocity (ω), eventually leading to cessation of propulsion at pH 3.0 (Fig 4a, 4c). Brownian rotational diffusion time (τR) also increases (Fig 4d).
        4.  **Brownian Motion:** Particles undergo translational and rotational Brownian motion described by diffusion coefficients D and Dr (related to τR), which are influenced by particle size (Stokes-Einstein).
        The simulation uses these experimentally derived pH-dependencies (v(pH), ω(pH), τR(pH), R(pH)) embedded in Stokes dynamics equations (Eq 2, 3) which include propulsive force/torque and stochastic terms. Inter-particle interactions beyond excluded volume are simplified or neglected in the simulation description.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | 3 | Motion Modulation (Translation) | v(pH) / v(pH=3.7) | 1.0 -> ~0 | Dimensionless | Fig 4a | Explicit | Experimental data fit |
    | 3 | Motion Modulation (Rotation) | ω(pH) | ~0.8 -> ~0.1 | rad/s | Fig 4c | Explicit | Experimental data fit |
    | 3 | Motion Modulation (Brownian Rotation) | τR(pH) | ~23 -> ~35 | s | Fig 4d | Explicit | Experimental data fit |
    | 2 | State Change (Size) | Radius | ~0.95 -> ~1.15 | µm | Fig 2 | Explicit | Experimental data |

### **4.3 Global Order:**

    *   Content: The emergent global order is a statistical accumulation (increased particle density) within the spherical low-pH region (radial distance < 50 µm). This is a non-uniform spatial distribution. No complex structural patterns (like crystals or lanes) are reported or simulated.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| 1 | pH Sensing / Swelling Response | Threshold pH | ~3.7 | pH units | Explicit | Value where swelling begins in Fig 2. | Fig 2 |
| 2 | Swelling -> Velocity Modulation | Velocity Reduction Factor | 1.0 -> ~0 | Dimensionless | Explicit | Ratio extracted from Fig 4a fits. | Fig 4a |
| 3 | Swelling -> Radius Change | Radius Increase | ~20 | % | Explicit | Calculated from Fig 2 data. | Fig 2 |
| 4 | H2O2 Decomposition | Fuel Concentration | 10 | % (wt/v) | Explicit | Stated in text. | Exp. Section |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| GO1 | Particle Accumulation | Relative Density (PVP) | 1 -> >12 | Dimensionless | Explicit | Ratio of local density to random density from simulation. | Simulation | Fig 5b |
| GO2 | Cargo Accumulation | Relative Dye Conc. | 1 -> ~30 | Dimensionless | Explicit | Ratio combining particle density and release rate enhancement. | Simulation | Fig 6c |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
     | Local Rule -> Global Accumulation | Mapping individual pH-dependent motion changes to collective density increase | High (Simulation) | 6 | Relative Density Profile, Accumulation Timescale | Explicit (Simulation Output) | Simulation shows clear mapping, but lacks rigorous analysis of different local rules/interactions. Score reflects simulation predictability but limited exploration of the mapping's richness/universality. | Fig 5 |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 6 (Rubric: 0=No mapping; 2=Qualitative link; 4=Specific instance simulated; 6=Simulation shows reliable mapping for studied parameters; 8=Mapping explored across parameter variations; 10=Formal proof/understanding of the mapping universality.) The paper simulates one specific scenario based on experimental data, showing a reliable mapping for those parameters (Score 6).
    *   **Metrics:** Relative density as a function of position and time (Fig 5b), Cargo accumulation factor (Fig 6c).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skip to Module 6.)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Swelling Time (PVP, similar) | ~5 | ms | Section 1 (ref [23]) | Explicit (Cited) | Value cited from prior work on similar colloids. |
        | De-Swelling Time (PVP, similar) | <30 | s | Section 1 (ref [23]) | Explicit (Cited) | Value cited from prior work on similar colloids. |
        | Brownian Rotation Time (τR, pH 3.7) | ~23 | s | Fig 4d | Explicit | Derived from MSD analysis. |
        | Brownian Rotation Time (τR, pH 3.2) | ~35 | s | Fig 4d | Explicit | Derived from MSD analysis. |
        | Accumulation Equilibration Time (Simulation, PVP) | >1000 (thousands) | s | Fig 5b, Section 2 | Explicit | Observed from simulation results. |
        | Cargo Release Measurement Duration | 2 | h | Section 2, Fig 6a | Explicit | Duration of the release experiment. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip to Module 8.)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors are:
        1.  **Catalytic Self-Propulsion:** Active motion driven by H2O2 decomposition.
        2.  **pH-Modulated Motility:** Reduction of translational and angular velocity in response to decreasing pH due to colloid swelling.
        3.  **Chemotaxis-like Accumulation:** Statistical aggregation of colloids in low-pH regions resulting from pH-modulated motility within a pH gradient.
        4.  **pH-Triggered Cargo Release:** Increased release rate of encapsulated cargo (Rhodamine B) in low-pH regions due to colloid swelling.
        5.  **Synergistic Targeted Delivery (Simulated):** Combined effect of particle accumulation and enhanced release leading to significantly higher cargo concentration in target low-pH region.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content:
        *   **pH-Modulated Motility:** Validated experimentally via quantitative analysis of particle trajectories (MSD fitting) under different uniform pH conditions. Mean values and trends are reported (Section 2, Fig 4). Robustness demonstrated by analyzing many trajectories (min 36 per condition).
        *   **pH-Triggered Cargo Release:** Validated experimentally by measuring Rhodamine B concentration in supernatant over time at different pH values using UV/Vis spectroscopy (Section 2, Fig 6a, 6b). Release *rates* calculated.
        *   **Chemotaxis-like Accumulation & Synergistic Delivery:** Validated *computationally* using simulations based on experimentally derived motion parameters and release rates within a defined spherical pH gradient (Section 2, Figs 5, 6c). Simulation uses Stokes dynamics. Experimental validation of collective accumulation in stable gradients is mentioned as challenging and not performed.

---

#Key: [kringelbach_thermodynamics_2024]

```markdown
# The Thermodynamics of Mind

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper proposes a theoretical framework called 'Thermodynamics of Mind' to quantify hierarchical brain orchestration using principles from non-equilibrium thermodynamics, specifically focusing on irreversibility ('arrow of time') and entropy production to understand information flow asymmetry in the brain during different states (e.g., rest, cognitive tasks, movie watching, sleep, anesthesia). It connects these thermodynamic concepts to brain dynamics, hierarchy, and computation efficiency, suggesting turbulence as a mechanism for fast information processing despite slow neuronal signaling. The framework is explored using fMRI and MEG data analysis techniques (TENET, INSIDEOUT) and whole-brain modeling. The purpose is to provide a unifying perspective on brain function, state transitions, and the underlying mechanisms of cognition, potentially offering insights into consciousness and neuropsychiatric disorders.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Production Entropy (Hp) | >0 (irreversible), =0 (reversible) | Dimensionless / Information Units (e.g., nats/s) | Box 1, Box 2, Text (Section: Hierarchy and thermodynamics) | Explicit (Concept), Implicit (Value in Brain) | Low (Hard to quantify in high-dim systems like brain) | Estimated via irreversibility proxies |
        | Generative Effective Connectivity (GEC) | Matrix Values | Arbitrary Units / Dimensionless | Fig 1F-I, Fig 3C | Explicit (Concept), Implicit (Specific values) | Medium (Result of whole-brain model fitting) | Model optimization |

    *   **Note:** The paper focuses on concepts and qualitative comparisons (higher/lower irreversibility, flatter/steeper hierarchy) derived from underlying quantitative analyses presented in cited works or illustrative figures. Specific numerical values for parameters like scaling exponents or entropy production rates for the brain are discussed conceptually rather than provided as definitive measurements within this text.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The paper discusses metabolic cost and efficiency implicitly, relating it to the brain's need to perform computation economically. The ultimate energy source is biological metabolism (glucose, oxygen). Schrödinger's "What is Life?" is cited, linking life sustenance (avoiding decay) to metabolism ("eating, drinking, breathing").

### **2.2 Energy Transduction**

    *   Content: The paper focuses on information flow and entropy production rather than physical energy transduction pathways in the brain. It mentions the conversion of electrical signals to chemical signals at synapses and back as a source of slowness, but doesn't analyze the energy transformation itself. The thermodynamic concepts (irreversibility, entropy) are applied to the dynamics of brain signals (information flow), not directly to metabolic energy conversion processes. Turbulence is discussed as facilitating efficient *energy and information transfer* across scales, implying a link between dynamics and energy, but the specific transduction mechanism isn't detailed. FDT relates fluctuations and dissipation (energy loss).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper discusses the brain's need for *efficient* computation ("lowest possible metabolic cost," "efficient distributed computation," "efficient energy and information transfer" via turbulence) but does not provide quantitative metrics or a basis for scoring the overall energy efficiency of the processes described within the thermodynamic framework presented. The focus is on information processing efficiency and hierarchy quantification.

### **2.4 Energy Dissipation**

    *   Content: Dissipation is a central concept in the thermodynamic framework discussed. Irreversibility is linked to entropy production (Hp > 0), implying dissipation. The fluctuation-dissipation theorem (FDT) is explicitly mentioned as relating irreversible dissipation of energy into heat to reversible fluctuations. Turbulence involves an energy cascade ending in dissipation at small scales. However, specific mechanisms (e.g., heat loss, resistance) or quantitative values for dissipation in the brain states analyzed are not provided in this text.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The paper doesn't detail the fundamental local interaction rules (e.g., specific equations for single neuron dynamics or synaptic plasticity) from which global states emerge. Instead, it describes implementations using meso-scale models (whole-brain models) where local dynamics within brain regions are represented by models like Stuart-Landau oscillators (Fig 1G) or are implicitly captured by analysing macroscopic signals (fMRI/MEG). The interaction is shaped by anatomical connectivity (structural connectome) scaled globally, and potentially influenced by neurotransmitters. Turbulence arises from non-linear coupled systems (e.g., oscillators).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | WBM_Oscillator | Local dynamics in whole-brain model (e.g., Stuart-Landau) | Bifurcation parameter (a) | Near 0 (criticality) | Dimensionless | Fig 1G (conceptual), Refs [53,54] | Implicit | Parameter controlling local dynamics' stability, often tuned near a Hopf bifurcation. |
    | WBM_Coupling | Global coupling strength scaling anatomical connectivity | Global Coupling (G) | Variable (fitted to data) | Dimensionless | Text (Section: Understanding brain hierarchy - Glossary: Whole-brain model), Fig 1F-I | Explicit | Scales the influence of anatomical connections on coupled regional dynamics. |
    | Turbulence | Interaction leading to turbulence | Reynolds Number Analogue / Coupling Strength | High values | Dimensionless | Text (Section: Fast brain processing...), Refs [50,53,54] | Implicit | Parameter regime where turbulence emerges in coupled systems (fluids or oscillators). |

### **4.3 Global Order:**

    *   Content: The emergent global order described includes distinct brain states (wakefulness, sleep, anaesthesia, task), characterized by specific patterns of whole-brain network dynamics, information flow (irreversibility), functional hierarchy (flat vs. pyramidal), and potentially turbulent regimes. These states represent macroscopic configurations arising from local interactions.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| WBM_Coupling | Strength of interaction between brain regions via anatomical links | Global Coupling (G) | Fitted to empirical data | Dimensionless | Explicit | Explicitly mentioned as scaling factor in whole-brain models. | Glossary, Fig 1 |
| Local Dynamics | Intrinsic dynamics within a brain region (e.g., oscillatory behavior) | Bifurcation parameter (a) | Tuned near critical point (e.g., a ≈ 0) | Dimensionless | Implicit | Implied by using models like Stuart-Landau oscillators often operated near criticality. | Fig 1G, Refs [53,54] |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Hierarchy | Degree of asymmetry in information flow | Irreversibility Measure | Higher/Lower (qual.) or Quantitative metric | Dimensionless / % / Arbitrary | Explicit | Directly measured using TENET/INSIDEOUT from brain signals. | TENET/INSIDEOUT analysis | Fig 1, Fig 2, Fig 3 |
| State | Characterization of global brain activity pattern | Brain State Label | Rest, Task, Movie, Sleep, Anesthesia | Categorical | Explicit | Defined categories based on experimental conditions. | Experimental Design | Fig 2, Fig 3, Text |
| Dynamics Regime | Qualitative nature of dynamics | Turbulence Level / Synchrony | Low/High / Power law exponent | Dimensionless | Explicit (Turbulence concept), Implicit (Quantification) | Assessed via synchrony analysis, power spectra, structure functions. | Turbulence analysis methods | Fig 4, Refs [53, 55] |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    *   **Metrics:** Goodness-of-fit (correlation between simulated and empirical data), classification accuracy (e.g., TENET), measures of hierarchy/irreversibility/turbulence.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", including M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Neuromorphic / Hybrid (Analog aspects in neuronal signaling, digital aspects in spike timing possibility, complex emergent dynamics)

### **5.3 Computational Primitive:**

    *   Content: The paper doesn't break down the computation into specific primitives like logic gates. Instead, it discusses computation at a higher level: information transfer, integration, segregation, hierarchical processing, and potentially pattern classification/state discrimination (implied by TENET). The underlying physical processes enabling these are information flow asymmetry (irreversibility) linked to hierarchy, and potentially turbulent cascades for efficient processing across scales. FDT connects fluctuations/dissipation to response, potentially implying a form of filtering or signal transformation.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Neuronal Signal Transfer (Synaptic Delay) | ~10-20 | ms | Text (Introduction), Refs [2,3] | Explicit | Explicitly stated as the typical speed between neurons. |
        | fMRI Signal Dynamics (BOLD) | ~seconds | s | Implicit (Nature of fMRI) | Implicit | fMRI measures slow hemodynamic responses on the order of seconds. Assumed context for fMRI analysis (Figs 2, 3). |
        | MEG Signal Dynamics | ~milliseconds | ms | Text (Section: Fast brain processing...), Ref [47] | Explicit | MEG measures fast neural magnetic fields. Mentioned in context of turbulence study. |
        | Cognitive Task Duration | Variable (seconds to minutes) | s / min | Implicit (Context of HCP tasks) | Implicit | Tasks in HCP dataset (referenced for Figs 2, 3) have specific durations. |
        | Brain State Duration | Variable (minutes to hours) | min / hr | Implicit (Nature of states like sleep, rest) | Implicit | Brain states like sleep, wakefulness persist over longer durations. |
        | Turbulent Cascade Timescales | Range (ms to s?) | ms / s | Implicit (Based on fluid dynamics analogy and brain signals) | Implicit | Turbulence involves cascades across multiple timescales, from fast local events to slower global patterns. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main "behaviors" analyzed are the different global brain states (Resting State, Cognitive Tasks [Working Memory, etc.], Movie Watching, Sleep, Anesthesia, Coma, potentially effects of Psychedelics or ADHD/Bipolar/Schizophrenia). These states are characterized by distinct thermodynamic/dynamic properties: levels of irreversibility, hierarchy structure (flat vs. pyramidal), and potentially turbulent dynamics. The framework aims to quantify and differentiate these emergent global states based on underlying information flow dynamics.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation relies on applying the thermodynamic metrics (irreversibility via TENET/INSIDEOUT, turbulence measures) to empirical neuroimaging data (fMRI, MEG) from different conditions (rest, tasks, movie watching, sleep, disease states). Consistent differences in these metrics across states and populations are presented as evidence (Figs 2, 3, 4). Whole-brain models are validated by their ability to reproduce these empirical thermodynamic properties (Fig 1, Fig 3). Control experiments include comparing results to surrogate data (Fig 4C). Reproducibility is suggested by use of large datasets (HCP) and consistency across methods (TENET vs. INSIDEOUT). Limitations include the indirect nature of neuroimaging signals and reliance on specific model assumptions.

---

#Key: [ramstead_bayesian_2023]

# On Bayesian mechanics: a physics of and by beliefs

__Paper Type:__ Theoretical/Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper introduces and reviews Bayesian mechanics, a theoretical framework positioned as a probabilistic mechanics for modeling self-organizing systems. It describes systems endowed with a 'particular partition' (internal states, external states, and intervening Markov blanket states - sensory and active). The core idea is that the internal states (or their dynamics) encode parameters of probabilistic beliefs (probability densities, often variational densities q) about external states (or their dynamics). The system dynamics are described as flows on statistical manifolds (belief spaces) governed by principles like the Free Energy Principle (FEP) or the Constrained Maximum Entropy Principle (CMEP), making the system *appear* to perform approximate Bayesian inference to maintain its structure against dissipation. It aims to provide mechanical theories (equations of motion, potentials, forces) for systems that minimize surprisal (or variational free energy, an upper bound on surprisal) or maximize entropy subject to constraints, linking physical dynamics to information dynamics (belief updating). Key components include the particular partition (internal μ, external η, sensory s, active a states), the generative model p(x), the variational density q(η) parameterized by μ, the variational free energy F, and concepts like synchronization maps and ontological potentials (NESS potentials or constraints). The purpose is to provide a physics-based explanation for self-organization, adaptation, and the appearance of inference in complex systems.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These are defining theoretical quantities within the framework, not specific experimental parameters. Units are typically context-dependent (e.g., nats for information, Joules/kT for physical energy correspondence). NESS = Non-Equilibrium Steady State.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The framework is general and doesn't specify a primary physical energy source. It discusses theoretical energy concepts like variational free energy, potential energy (V(q)), kinetic energy (1/2 mv^2), and thermodynamic concepts like entropy and dissipation. The underlying physical system being modeled would have its own specific energy inputs (e.g., chemical potential, thermal gradients), but these are abstracted away in the general Bayesian mechanics formulation. The "energy" minimized is often variational free energy, an information-theoretic quantity related to surprisal, or action (integral of Lagrangian).

### **2.2 Energy Transduction**

    *   Content: The framework describes dynamics as arising from gradients of potentials (e.g., ontological potential J(x) = -log p(x), variational free energy F, classical potential V(q)). Energy is implicitly 'transduced' into forces driving the system's state evolution (physical dynamics) and belief updates (information dynamics). For example, minimizing the action (integral of Lagrangian L = T - V) leads to equations of motion. Minimizing variational free energy F drives dynamics towards states of low surprisal (belief updating and potentially physical action via active inference). Maximizing constrained entropy drives dynamics towards the most probable macrostate consistent with constraints. The Helmholtz decomposition splits flow into dissipative (gradient-related) and conservative (solenoidal) components, implicitly representing different energy pathways. Gauge theory connects constraints (potentials) to forces acting on the system.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide metrics for physical energy efficiency. It discusses thermodynamic efficiency implicitly in the context of dissipative structures (Sec 1, citing [14]) and minimizing dissipation (Sec 1, citing [1,36]), often linking it to minimizing free energy. The principle of stationary action implies optimal "efficiency" in terms of path selection (no wasted action). However, no quantitative efficiency value for a specific process modeled by Bayesian mechanics is given. Qualitatively, systems minimizing free energy are suggested to be efficient in maintaining their organization against dissipation.

### **2.4 Energy Dissipation**

    *   Content: Dissipation is a central theme, discussed in the context of the second law of thermodynamics, self-organization resisting decay, and non-equilibrium systems. The FEP is framed as managing dissipation ("minimizing entropic dissipation over time" - Sec 1). The Helmholtz decomposition separates flow into a dissipative component (related to gradient descent on the potential, countering fluctuations) and a conservative solenoidal component. The noise term (ξ(t)) in the SDE represents random fluctuations/dissipation from a heat bath. CMEP relates system dynamics to dissipation into the ambient heat bath. Quantification is abstract (e.g., related to the diffusion tensor G or the gradient term in Helmholtz decomposition), not in specific physical units for the general framework. Qualitatively, dissipation is inherently present in the stochastic dynamics considered.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable / System-dependent

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are embodied in the structure of the stochastic differential equations (SDEs) derived from the particular partition (Eq 3.1). Specifically:
        1.  Internal states (μ) dynamics depend only on sensory (s) and active (a) states: `μ_dot = f_μ(s, a, μ) + C_μ ξ_μ(t)`.
        2.  Active states (a) dynamics depend only on internal (μ), sensory (s), and active (a) states: `a_dot = f_a(s, a, μ) + C_a ξ_a(t)`. (Note: Paper uses `f_a(s,a,m)` in partition diagram, implies dependence on μ via s, but direct dependence is often assumed in active inference). Simplified active inference often uses `a_dot = -∂G/∂a`, where G is EFE.
        3.  Sensory states (s) dynamics depend only on external (η), sensory (s), and active (a) states: `s_dot = f_s(η, s, a) + C_s ξ_s(t)`.
        4.  External states (η) dynamics depend only on external (η), sensory (s), and active (a) states (or just η): `η_dot = f_η(η, s, a) + C_η ξ_η(t)`.
        These dependencies define the local information flow across the Markov blanket. The specific *form* of the functions `f_i` and matrices `C_i` defines the precise rule for a given system, often derived from minimizing free energy (for μ, a) or assumed for the environment (η, s).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The emergent global order is the persistent structure and characteristic dynamics of the 'particular system' itself, maintained despite dissipative forces. This can manifest as:
        1.  **NESS Density:** If a steady state exists, the system maintains a specific probability distribution p(x) over its state space (Sec 3.3). The shape of this density represents the global order.
        2.  **Characteristic Paths/Flows:** Even without a strict NESS, the system follows typical trajectories or flow patterns that minimize surprisal / free energy over time (path-tracking dynamics, Sec 3.2). The ensemble of these paths represents the dynamic order.
        3.  **Synchronization:** The coordinated dynamics between internal, external, and blanket states, potentially maintained around a synchronization manifold (Sec 3.3).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| NESS | Probability distribution at steady state | p_NESS(x) | [0, 1] | Probability density | Explicit | Defined as stationary solution to Fokker-Planck Eq. | Solve Fokker-Planck | Sec 3.3 |
| VFE | Variational Free Energy | F(μ, s, a) | Real | Nats or Energy | Explicit | Measures deviation from optimal belief/state | Calculate Eq based on q, p | Sec 3.1 |
| Entropy | System Entropy | H[p(x)] | Real ≥ 0 | Nats or Energy/Temp | Explicit | Measure of uncertainty/disorder | Calculate integral | Sec 1, 5.2 |
| SyncError | Deviation from synchronization manifold | Distance Metric | Real ≥ 0 | System-specific | Implicit | Quantifies mismatch between expected internal/external modes | Define manifold & metric | Sec 3.3 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Probabilistic/Variational Inference

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operation is **Belief Updating** via gradient descent on Variational Free Energy (F) or gradient ascent on constrained entropy (S). For internal states μ parameterizing the belief q, the primitive operation is akin to:
        `μ_dot ∝ -∂F/∂μ` (under FEP)
        or related dynamics derived from CMEP/gauge theory (e.g., parallel transport, Sec 5.2).
        This involves calculating gradients of F (which includes expected log likelihoods and priors) or related quantities and updating μ accordingly. Another primitive is **Action Selection** via gradient descent on Expected Free Energy (G) for active states `a`:
        `a_dot ∝ -∂G/∂a` (Active Inference).
    *   **Sub-Type (if applicable):** Gradient Descent/Ascent, Optimization

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
    *   **Note:** The paper discusses dynamics abstractly without specifying numerical timescales for the general framework. Timescales are implicitly hierarchy-dependent (fast noise vs. slower state evolution).

### **6.2 Active Inference:**

    *   Content: Yes
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** Prediction error reduction rate (decrease in KL divergence or surprisal over time); Timescale of anticipation (how far ahead actions seem planned based on EFE); Complexity of internal models (dimensionality/parameters of q and p); Correlation between EFE gradients and actual action `a_dot`; Entropy of action policy (exploratory vs exploitative behavior). Experimental setups could involve simulating systems under FEP/Active Inference and measuring these quantities, or fitting models to empirical data from adaptive systems.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: If M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The primary mechanism is **Variational Inference / Belief Updating** driven by the minimization of Variational Free Energy (F). Sensory data (s) influences the calculation of F, and the system's internal states (μ) change (typically via gradient descent: `μ_dot ∝ -∂F/∂μ`) to reduce F. This continuously refines the variational density q(η|μ) to better approximate the true posterior p(η|s), effectively adapting the system's internal model to the observed data. If considering adaptation of actions over time (learning a better policy), this would involve updating the parameters governing action selection, potentially through minimizing EFE or related reinforcement learning analogues described in the literature cited ([84-88]). In the CMEP formulation, adaptation involves adjusting the system's state distribution to maximize entropy under potentially changing constraints reflecting the environment. Gauge theory perspective describes how changes in constraints (J) covary with the probability density (p), representing adaptation to new information/conditions via parallel transport. The mechanism is a form of continuous online learning driven by prediction error minimization (implicit in VFE).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main emergent behaviors described are:
        1.  **Self-Organization/Persistence:** Maintaining structural integrity and characteristic dynamics (e.g., NESS) over time despite dissipative forces.
        2.  **Approximate Bayesian Inference:** System dynamics appear to estimate posterior probabilities (beliefs q) about environmental causes (η) given sensory data (s). This includes path-tracking, mode-tracking, and mode-matching dynamics (Sec 1, Sec 3).
        3.  **Active Inference / Self-Evidencing:** Actively selecting actions (a) to minimize expected free energy, thereby seeking out predicted sensory states and reinforcing the system's model/existence (Sec 3.2).
        4.  **Adaptation:** Acquiring the statistical structure of the environment by updating internal states/beliefs over time (Sec 7).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: As a theoretical/review paper, it doesn't present experimental validation itself. Validation relies on:
        1.  **Mathematical Consistency:** Deriving the behaviors (inference, self-organization) from the first principles (particular partition, FEP/CMEP) using statistical physics and probability theory (e.g., Fokker-Planck equation, SDE analysis, variational calculus, gauge theory). Theorems and lemmas (like ABIL) provide formal validation within the mathematical framework (Sec 3.3, Sec 5).
        2.  **Reference to External Work:** Citing numerous other papers ([1-118]) presumably containing specific theoretical derivations, simulations, or experimental applications that support the claims (though these are not detailed here). E.g., citation [81] is mentioned as experimental validation.
        3.  **Conceptual Coherence:** Arguing that the framework provides a principled and unifying explanation for observed phenomena in complex adaptive systems.
        Limitations: Lack of direct experimental validation *within this paper*. Reliance on the validity of cited work. Potential for assumptions in derivations not holding universally.

---

#Key: [cejkova_dynamics_2014]

# Dynamics of Chemotactic Droplets in Salt Concentration Gradients

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of a decanol droplet immersed in an aqueous solution of sodium decanoate (a surfactant). The system's primary function is to exhibit chemotaxis, i.e., directed movement in response to a concentration gradient of sodium chloride (NaCl), which acts as a chemoattractant. Components include the decanol droplet (typically 5 μL), the aqueous sodium decanoate solution (typically 5 or 10 mM), and a source of NaCl gradient (added NaCl solution droplet, NaCl-loaded nitrobenzene droplet, or NaCl-loaded paraffin sphere). The purpose is to investigate this artificial chemotaxis phenomenon, characterize its parameters (induction time, velocity), and demonstrate biomimetic behaviors like path selection, cargo delivery, and stimulus-responsive initiation. The movement is proposed to be driven by Marangoni flow induced by surface tension gradients resulting from the NaCl concentration gradient interacting with the decanoate surfactant system.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name             | Value         | Units   | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------------------- | :-----------: | :-----: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Chemotactic Velocity       | ~0.5 - 2.5    | mm/s    | Fig 1B, Fig 2B, Sec 3.7   | Explicit          | High (Measured), Medium (Est.)  | Image Analysis (Fig 1B, 2B), Estimation (Sec 3.7) |
        | Induction Time             | ~50 - ~350    | s       | Fig 2A                  | Explicit          | High (Measured)                 | Image Analysis                    |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input is the chemical potential difference associated with the non-uniform concentration of NaCl (solute) and potentially sodium decanoate (surfactant) in the aqueous solution. This concentration gradient drives mass transport (diffusion, convection) and induces a surface tension gradient at the decanol droplet interface (Marangoni effect). For temperature-triggered experiments (Section 3.5), thermal energy is input locally to melt paraffin and release NaCl.

### **2.2 Energy Transduction**

    *   Content: Chemical potential energy stored in the NaCl concentration gradient is transduced into kinetic energy of the decanol droplet. The proposed mechanism (Section 3.7) involves:
        1.  Interaction of the NaCl gradient with the decanoate surfactant, altering local surfactant concentration/activity.
        2.  Creation of a surface tension gradient (∂γ/∂x) across the decanol droplet interface due to varying surfactant effects (Marangoni effect). Explicitly measured in Fig 5.
        3.  The surface tension gradient exerts a net force (F_A, Eq 2) on the droplet.
        4.  This force overcomes dissipative forces (viscous drag F_D, wall friction F_W, Eq 1, 3, 4), resulting in directed motion (kinetic energy).
        Convective flows (observed in Fig 4) also contribute to transport and energy transduction. In Section 3.5, thermal energy transduces into kinetic energy (paraffin melting) and then chemical potential energy release (NaCl dissolution), followed by the Marangoni effect.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide an explicit energy efficiency calculation. However, based on the nature of the system (viscous fluid, slow movement driven by relatively weak surface tension forces compared to the total chemical potential energy available in the gradient), the efficiency of converting chemical potential energy into directed kinetic energy is expected to be very low. Most energy is likely dissipated as heat through viscous friction and diffusion processes homogenizing the gradient. The order-of-magnitude force calculations (Section 3.7) show the driving force is only slightly larger than estimated drag forces, implying significant dissipation relative to useful work (motion). Qualitative assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include:
        1.  Viscous Drag (Fluid-Fluid): Resistance from the surrounding aqueous phase. Estimated as F_D = 0.03 x 10^-6 N (Section 3.7, Eq 3). Qualitative: Medium-Low relative to driving force.
        2.  Viscous Drag (Wall Friction): Resistance from the substrate (glass slide). Estimated as F_W = 0.21 x 10^-6 N (Section 3.7, Eq 4). Qualitative: Medium relative to driving force.
        3.  Diffusion: Irreversible mixing process that dissipates the NaCl concentration gradient over time, reducing the energy source. Not quantified. Qualitative: High (as it drives eventual system equilibrium).
        4.  Heat Generation: Due to viscous dissipation and potentially chemical processes (dissolution). Not quantified. Qualitative: High (likely the primary fate of input energy).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes
        1.  **Gradient Following:** The droplet effectively calculates the direction of the steepest ascent of the NaCl concentration gradient and moves accordingly (Section 3.1, Fig 1).
        2.  **Path Selection:** In complex topologies (Section 3.4, Fig 6) and when presented with multiple gradients (Section 3.4, Fig 7), the droplet selects a path (apparently the shortest/strongest gradient path). This implies a physical process solving a minimal optimization problem.
        The computation is performed by the interaction of the droplet's physical properties (surface tension) with the environment (gradient), not by an external controller.

**(Conditional: M5.1 is "Yes", proceeding to M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog / Other: Gradient Ascent Optimization

### **5.3 Computational Primitive:**

    *   Content: **Gradient Ascent / Following:** The fundamental operation is the determination of the local gradient vector (∇C_NaCl, which translates to ∇γ) and generating motion proportional to or aligned with it (v ∝ ∇γ or similar). This can be viewed as a physical implementation of a gradient ascent algorithm on the chemoattractant concentration field (or related surface tension field).

### **5.4 Embodied Computational Units**
| Unit ID        | Description                                     | Processing Power | Energy/Operation | Freq/Resp. Time       | Bit-Depth | Data Source       | Implicit/Explicit | Justification                                      |
| :------------- | :---------------------------------------------- | :--------------- | :--------------- | :--------------------: | :-------: | :---------------- |:-----------------:| :------------------------------------------------- |

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description        | Value      | Units   | Source            | Implicit/Explicit | Justification                                                   |
        | :--------------------------- | :--------: | :-----: | :---------------- | :----------------: | :-------------------------------------------------------------- |
        | Induction Time               | ~50 - ~350 | s       | Fig 2A            | Explicit          | Time delay between stimulus (NaCl addition) and chemotaxis onset. |
        | Migration Time (across slide)| ~50 - ~150 | s       | Fig 1B (derived)  | Implicit          | Estimated from the duration of directed movement in Fig 1B.     |
        | Response to Reversal Stimulus| < 300      | s       | Fig 3 (Interval 5min) | Implicit          | Droplet completes reversal within the 5 min interval (incl. induction). |
        | Gradient Dissipation Time    | > 300      | s       | Fig 3 (implicit)  | Implicit          | Sufficient gradient persists for at least 5 min to allow movement. |
        | Local Random Motion Timescale| Seconds    | s       | Fig 1B (visual)   | Implicit          | Fluctuations occur on the order of seconds before/after chemotaxis. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The system exhibits several key behaviors:
        1.  **Random Motion:** Weak, undirected movement in the absence of a significant gradient (Fig 1).
        2.  **Chemotaxis:** Directed movement up a concentration gradient of NaCl (Fig 1).
        3.  **Gradient Source Selection:** Preferential movement towards a stronger NaCl gradient when presented with two options (Fig 7).
        4.  **Path Navigation:** Ability to follow gradients through simple non-linear paths or mazes (Fig 6).
        5.  **Stimulus-Responsive Initiation:** Initiation of chemotaxis triggered by temperature-induced release of NaCl (Fig 8).
        6.  **Cargo Transport & Reaction:** Movement towards a target (nitrobenzene droplet) carrying a payload (iodine) leading to fusion and reaction (Fig 9).
        7.  **Reversible Motion:** Ability to reverse direction in response to alternating gradient sources (Fig 3).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The key behavior, chemotaxis, is validated through controlled experiments where a gradient is established and droplet movement is tracked over time using video microscopy and image analysis software (Section 2.2, Fig 1). Control experiments (adding water instead of NaCl solution) confirmed that bulk flow was not the cause (Section 2.2). The directionality is clearly shown in trajectory plots (Fig 1A) and coordinate vs. time graphs (Fig 1B). Other behaviors like path selection (Fig 6, 7), stimulus response (Fig 8), and cargo delivery (Fig 9) are validated visually through image sequences and referenced supporting movies. Quantitative analysis focuses on induction time and velocity (Fig 2). Limitations include potential influence of unquantified convection and the simplified nature of the tested topologies (Fig 6). Reproducibility seems implied by systematic parameter variations (Fig 2) but not explicitly quantified (e.g., number of trials per condition not always stated). The term "emergent" is not used by the authors, and the behaviors are largely presented as direct consequences of the imposed conditions and underlying physics (Marangoni flow).

---

#Key: [sitti_physical_2021]

# Physical intelligence as a new paradigm

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper introduces the paradigm of Physical Intelligence (PI) as distinct from, but complementary to, Computational Intelligence (CI) and Embodied Intelligence (EI). PI refers to the physically encoding of sensing, actuation, control, memory, logic, computation, adaptation, learning, and decision-making capabilities directly into the body (material, structure, mechanism) of a physical agent (human-made or biological). The purpose is to enhance the autonomy, robustness, and efficiency of agents operating in complex, unstructured environments, especially at smaller scales where CI is limited or in harsh environments where electronics fail. The paper reviews various methods and examples for creating PI, including passive materials, active/stimuli-responsive materials, metamaterials, structural designs (origami, kirigami, tensegrity), memory encoding, physical computation/logic, adaptation mechanisms, and collective behaviors in swarms. It positions PI as a synergistic field merging mechanics, materials science, robotics, biology, etc.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** The paper discusses PI across scales and cites examples with specific parameters (like timescales), but doesn't present new quantitative parameter measurements for a specific system implementation within this work. The table reflects parameters defining the *scope* and *context* of the PI paradigm as described. Data reliability for cited values is Medium as the primary sources aren't analyzed here.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The paper discusses various potential energy sources for PI systems, emphasizing self-powering through environmental interaction. Examples include: wind energy (Jansen's sculptures), chemical fuels (H2O2 for microrobots, environmental acids, glucose via biofuel cells), light (photocatalysis, ambient light harvesting), mechanical forces (vibrations, flows), thermal gradients, humidity gradients, triboelectrification. The specific source depends on the agent and its environment.

### **2.2 Energy Transduction**

    *   Content: Energy is transduced through various physical mechanisms depending on the PI implementation. Examples include: mechanical linkages converting wind to locomotion (Jansen); chemical reactions generating propulsion (electrophoresis, diffusiophoresis, Marangoni effect); photocatalysis converting light to chemical potential/propulsion; piezoelectric/triboelectric effects converting mechanical stress to electricity; shape memory alloys/polymers converting heat to mechanical work; stimuli-responsive materials converting various stimuli (light, heat, pH, magnetic fields) to shape change/actuation; fluidic/pneumatic systems converting pressure to logic/motion.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper mentions efficiency as a *motivation* for PI (e.g., energy-efficient locomotion, low-power computation) and discusses energy-related concepts like energy harvesting (typically low power, nW-mW range mentioned for cm-scale harvesters) and energy dissipation for safety. However, it does not provide specific efficiency values or a quantitative analysis for any particular PI system discussed. Qualitative assessment: Highly variable depending on the specific PI mechanism and scale; potentially high for some passive mechanisms but low for others, especially energy harvesting.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation is mentioned primarily in the context of safety (using energy-dissipating materials/structures for impact absorption, Section 4.1) and implicitly through thermodynamically irreversible processes inherent in many PI mechanisms (e.g., friction in mechanical systems, heat loss in chemical reactions, resistance in electrical energy harvesting). Quantification is not provided. Qualitative assessment: Dissipation is inherent and likely significant in most real-world PI implementations, but can also be functionally beneficial (e.g., damping, impact absorption).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable (seconds to potentially long-term)
*    Units: s (or Qualitative Descriptor: e.g., "Short-term", "Non-volatile")

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Low (e.g., 1-2 bits for structural examples)
*   Units: bits (for structural memory); Qualitative (shape state)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The paper describes local interactions qualitatively rather than providing specific rules/equations. Examples mentioned include:
        *   **Small Scales (milli/micro):** Dominance of surface/length-related forces like fluidic drag, surface tension, adhesion, friction, van der Waals forces, magnetic interactions (Section 4.6). These physical forces couple neighboring agents.
        *   **Biological Inspiration:** Stigmergy (indirect communication via environmental modification) in social insects (Section 4.6).
        *   **General:** Coupled physical interactions (Section 1, 4.6).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The paper describes several types of emergent global order resulting from local interactions:
        *   Self-organized fluid flow patterns in bacteria swarms (Section 4.6).
        *   Programmable self-assembled or self-organized patterns in magnetic microrobot swarms (Section 4.6).
        *   Collective aggregation and migration (inspired by living cells, Section 4.6).
        *   Collective construction (inspired by termites, etc., Section 4.6).
        *   General self-assembly mentioned as a PI capability (Section 4).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog, Digital, Hybrid. Examples suggest different types:
        *   Analog: Mechanical feedback control (centrifugal governor), potentially some aspects of taxis behavior computation.
        *   Digital: Fluidic/pneumatic logic gates (AND, OR, NAND mentioned), DNA tile computation (algorithmic manipulation), bistable memory elements (implicitly digital).
        *   Hybrid: Systems combining continuous physical dynamics with discrete logic elements might be considered hybrid.

### **5.3 Computational Primitive:**

    *   Content: The paper mentions several computational primitives embodied physically:
        *   Logic Gates (AND, OR, NAND explicitly mentioned for fluidic/pneumatic/bifurcation systems - Section 4.4).
        *   Memory/State Holding (1-bit, 2-bit memory via bistable/multistable structures - Section 4.3).
        *   Feedback Control (Proportional control via mechanical governor, chemomechanical feedback - Section 4.4).
        *   Thresholding/Decision (Implicit in taxis behavior, Venus flytrap signal counting/response - Section 4.5).
        *   Algorithmic Manipulation (via DNA self-assembly - Section 4.4).
    *   **Sub-Type (if applicable):** Logic Gate: AND, OR, NAND; Memory: Bit Storage; Control: Proportional Feedback; Decision: Signal Counting/Thresholding.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Biological Evolution (Neurons) | ~500 Million Years | Years | Section 2 | Explicit | Historical timescale mentioned. |
        | Biological Evolution (Humans) | ~250 Thousand Years | Years | Section 2 | Explicit | Historical timescale mentioned. |
        | Bacterial Chemical Memory | 1-10 | s | Section 4.3 | Explicit | Cited value. |
        | Venus Flytrap AP Counting Window | ~30 | s | Section 4.5 | Explicit | Cited value. |
        | Venus Flytrap Closure Time | ~100 | ms | Section 4.5 | Explicit | Cited value. |
        | Response Time (General PI) | Variable (e.g., ms to hours/days) | s | Implicit | The diverse examples imply a wide range of response times depending on the mechanism (e.g., fast mechanics vs slow diffusion/degradation). |
    *   **Note:** The paper provides some specific timescales for biological examples used for inspiration or comparison, but generally discusses processes without quantifying their characteristic times.

### **6.2 Active Inference:**

    *   Content: Unclear

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The paper describes various physical mechanisms enabling adaptation:
        *   **Shape/Morphology Change:** Via stimuli-responsive materials, origami/kirigami reconfiguration, programmed deformation (e.g., squeezing through gaps, camouflage color change via hydrogel structure).
        *   **Physical Property Tuning:** Active tuning of stiffness/damping (e.g., phase change materials, MR/ER fluids, granular jamming) for locomotion/manipulation. Passive adaptation via material choice (e.g., compliance, impact resistance).
        *   **Behavioral Adaptation (Learning/Decision):** Driven by internal memory/state changes (e.g., bacterial chemical memory for chemotaxis adaptation, slime mold spatial memory for pathfinding, cellular decision hierarchies in Stentor, signal counting/memory in Venus flytrap). Implicitly involves feedback loops where sensory input and internal state history influence future actions. The paper doesn't specify mechanisms like Hebbian or reinforcement learning explicitly but describes functional outcomes (e.g., path optimization, stimulus avoidance).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The paper describes a wide range of functional behaviors achievable through PI:
        *   **Locomotion:** Walking (Jansen's machines, legged robots), flying (flapping wing robots), swimming (microswimmers, jellyfish robot), crawling (soft robots), climbing (using microfibers). Includes multi-modal locomotion.
        *   **Sensing:** Mechanical sensing (Jansen's water avoidance), environmental sensing (stimuli-responsive materials detecting temp, pH, chemicals, light), proprioception, flow sensing (microfibers). Includes taxis behaviors (chemo-, photo-, magneto-, etc.).
        *   **Manipulation:** Grasping (soft grippers with tunable stiffness/adhesion).
        *   **Control/Regulation:** Self-regulation (speed governors, wing rotation), self-response.
        *   **Computation/Logic:** Performing logic operations (fluidic/pneumatic), making decisions (Stentor, flytrap).
        *   **Memory:** Storing/recalling information (shape, state, bit).
        *   **Adaptation/Learning:** Adapting shape/stiffness, learning paths (slime mold), adapting behavior (Stentor).
        *   **Self-X Capabilities:** Self-healing, self-powering, self-cleaning, self-degrading, self-assembly, self-organization.
        *   **Collective Behaviors:** Swarm pattern formation, collective transport/mixing, collective construction.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

---

#Key: [negi_emergent_2022]

# Emergent collective behavior of active Brownian particles with visual perception

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of N "intelligent" Active Brownian Particles (iABPs) simulated in a two-dimensional square box with periodic boundary conditions. Each iABP is self-propelled with a constant velocity v0 along its orientation vector eᵢ and experiences translational and rotational diffusion, as well as excluded-volume interactions modeled by a truncated Lennard-Jones potential. Crucially, each iABP possesses a "vision cone" (VC) characterized by an opening angle 2y and a cutoff radius (4R₀), within which it senses the instantaneous positions of neighboring iABPs. The particle's orientation dynamics includes a steering term that directs it towards the center of mass of perceived neighbors within the VC, weighted by an exponential decay function exp(-rᵢⱼ/R₀) and limited by a maneuverability strength O. The purpose is to study the emergent collective structures and dynamics arising from the interplay between standard ABP physics (self-propulsion, volume exclusion, noise) and this minimal vision-based steering mechanism, specifically focusing on structure formation, phase behavior, and cluster growth dynamics. Components are the iABPs themselves, governed by equations of motion (Eqs 1-5), simulated within a 2D periodic environment.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Units 's' (particle diameter) and 't' (characteristic time) are the simulation's base units as defined in Section 3.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input is implicit and internal to each particle, driving the self-propulsion force Fₐᵢ = gv₀eᵢ. This represents a conversion from some internal energy store (like chemical fuel in biological or synthetic swimmers) into directed kinetic energy. The thermal bath also provides energy via stochastic forces Cᵢ(t).
    *   Value: Fₐᵢ magnitude = gv₀. The activity is characterized by Pe = sv₀/DT, where DT = kBT/g. So, input strength is related to Pe and T.
    *   Units: Force (Energy/Length). In simulation units, gv₀.

### **2.2 Energy Transduction**

    *   Content: 1. Internal Energy -> Directed Kinetic Energy: The active force gv₀eᵢ converts internal energy into directed motion. 2. Kinetic Energy -> Thermal Energy (Dissipation): The friction term -gṙᵢ dissipates kinetic energy into the surrounding medium (represented implicitly by the friction coefficient g). 3. Thermal Energy -> Kinetic Energy: The stochastic force Cᵢ(t) represents energy transfer from the thermal bath to the particle's kinetic energy. 4. Potential Energy -> Kinetic Energy: Repulsive forces -∇U arising from the LJ potential (Eq 2) convert potential energy into kinetic energy during particle collisions/interactions. 5. Kinetic Energy modification via Steering: The vision-based torque (Eq 3) changes the direction of the active force, indirectly influencing kinetic energy distribution (though not directly adding/removing energy, it redirects the active input).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss or quantify the thermodynamic efficiency of the self-propulsion mechanism or any other energy conversion process. Efficiency is not a focus of the study.

### **2.4 Energy Dissipation**

    *   Content: Energy is primarily dissipated through: 1. Viscous Friction: Represented by the term -gṙᵢ in Eq 1. The friction coefficient g dictates the rate of dissipation of translational kinetic energy into the implicit surrounding medium/heat bath. 2. Rotational Diffusion: The stochastic torque Lᵢ(t) in Eq 3, characterized by DR, represents dissipation associated with random rotational motion coupled to the thermal bath. 3. Inelasticity during Collisions (Implicit): While the LJ potential is conservative, the damping (friction term g) ensures that kinetic energy gained during approach is dissipated during separation or subsequent motion, effectively making collisions dissipative overall in the underdamped regime considered ('slightly' underdamped). Quantification is not provided in absolute units, but the processes are inherent to the model. Qualitative assessment: Medium to High, as friction (g = 102 in reduced units) is significant.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8.)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content:
        1.  **Self-Propulsion:** Each particle i moves with velocity v₀ along its orientation eᵢ (Active force Fₐᵢ = gv₀eᵢ in Eq 1).
        2.  **Excluded Volume:** Particles i and j interact via a short-range repulsive Lennard-Jones potential U(rᵢⱼ) (Eq 2) when their distance rᵢⱼ < 2^(1/6)s. Force Fᵢ = -Σⱼ∇ᵢU(rᵢⱼ).
        3.  **Vision-based Steering:** Particle i adjusts its orientation angle φᵢ based on neighbors j within its vision cone (VC defined by angle y and range |rᵢ - rⱼ| ≤ 4R₀, Eq 5). The angular velocity contribution is proportional to O * Σⱼ∈VC [exp(-rᵢⱼ/R₀) * sin(θᵢⱼ - φᵢ)] / Σⱼ∈VC [exp(-rᵢⱼ/R₀)] (derived from Eq 3), where θᵢⱼ is the angle of the vector rⱼ - rᵢ. This rule directs the particle towards the perceived center of mass of neighbors.
        4.  **Translational Noise:** Particles experience random forces Cᵢ(t) from a thermal bath (Eq 1).
        5.  **Rotational Noise:** Particle orientations experience random torques Lᵢ(t) (Eq 3) leading to rotational diffusion DR.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :--------------------: | :---: | :----------: | :----------------: | :------------: |
    | 1 | Self-Propulsion | Pe (related to v₀), g | 1 - 200+, 102 | Dim'less, Dim'less | Section 3 | Explicit | Defined in text |
    | 2 | Excluded Volume | s, e/kBT | 1, (1+Pe) | Length, Dim'less | Section 3 | Explicit | Defined in text |
    | 3 | Vision Steering | O/DR, y, R₀ | 0 - 100+, p/24 - p/2, 1.5 (typ.) | Dim'less, rad, Length | Section 3 | Explicit | Defined in text |
    | 4 | Trans. Noise | T (via kBT), g | 1, 102 | Energy, Dim'less | Section 3 | Explicit | Defined in text |
    | 5 | Rot. Noise | DR | 8e-2 | 1/Time | Section 3 | Explicit | Defined in text |

### **4.3 Global Order:**

    *   Content: The system exhibits several distinct globally ordered phases depending on parameters (Pe, y, O, F):
        *   **Dilute Phase:** Homogeneous, isotropic distribution of largely independent particles (Fig 2d, 5, 10).
        *   **Worm Phase:** Formation of long, flexible, dynamic chains of particles (Fig 2c, 5).
        *   **Worm-Aggregate Coexistence:** Coexistence of worm-like structures and denser clusters (Fig 2b, 5, 8c).
        *   **Aggregate Phase (Fluid-like):** Dense, dynamic clusters with liquid-like internal structure (Fig 8e, 10).
        *   **HCP (Hexagonally Close-Packed) Phase:** Formation of large, dense, solid-like aggregates with local hexagonal packing (Fig 2a, 5, 8a, 8b, 10).
        *   **Dispersed Cluster (DC) Phase:** Small, short-lived clusters dispersed in the system (Fig 8d, 10).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| LJ | Excluded Volume Repulsion | s (diameter) | 1 | Length | Explicit | Defined parameter | Sec 3 |
| LJ | Repulsion Energy | e/kBT | 1+Pe | Dimensionless | Explicit | Defined parameter | Sec 3 |
| Vision | Characteristic Range | R₀ | 1.5 (typ.) | Length | Explicit | Defined parameter | Sec 3 |
| Vision | Max Vision Range | 4R₀ | 6 (typ.) | Length | Explicit | Defined parameter | Sec 2 |
| Vision | Vision Angle | y | p/24 - p/2 | Radians | Explicit | Varied parameter | Sec 3 |
| Vision | Maneuverability | O/DR | 0 - 100+ | Dimensionless | Explicit | Varied parameter | Sec 3 |
| Dynamics | Péclet Number | Pe | 1 - 200+ | Dimensionless | Explicit | Varied parameter | Sec 3 |
| Dynamics | Friction Coefficient | g | 102 | Dimensionless | Explicit | Defined parameter | Sec 3 |
| Dynamics | Rotational Diffusion | DR | 8e-2 | 1/Time | Explicit | Defined parameter | Sec 3 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Clustering | Cluster Size Distribution | N(n) | Varies (Fig 3, 9) | Dimensionless | Explicit | Measured quantity | See Sec 4.1, Eq 7 | Fig 3, 9 |
| Clustering | Fit parameter m | m | 0 - 1.4 | Dimensionless | Explicit | Fitted parameter | Fit to Eq 8 | Table 1 |
| Clustering | Fit parameter n₀ | n₀ | 1 - 13.4 | Particles | Explicit | Fitted parameter | Fit to Eq 8 | Table 1 |
| Structure | Radius of Gyration Scaling | hR<0xC2><0xB2>g | n¹ or n¹·⁴ | Length² | Explicit | Measured scaling | Log-log plot slope | Fig 4 |
| Structure | Kurtosis | K | ~1.4 - 3.8 | Dimensionless | Explicit | Measured quantity | Eq 13 calculation | Fig 11 |
| Dynamics | Velocity Correlation | Cv(r) | -0.1 - 0.6 | Dimensionless | Explicit | Measured quantity | Eq 10 calculation | Fig 6 |
| Clustering | Average Cluster Size | <0xC7><0x83>(t) | Increases with t | Particles | Explicit | Measured quantity | Eq 15 calculation | Fig 13a |
| Clustering | Growth Exponent 1 | z₁ | ~0.3 - 1.0 | Dimensionless | Explicit | Fitted exponent | Fit C(t) ~ t^z₁ | Fig 13b |
| Clustering | Growth Exponent 2 | z₂ | 1/4 | Dimensionless | Explicit | Fitted exponent | Fit C(t) ~ t^z₂ | Sec 5 |
| Clustering | Peak Time Scaling Exp | k | 6/5 | Dimensionless | Explicit | Fitted exponent | Fit tₚ ~ (n/Pe)^k | Sec 5, Eq 17 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | Local Rules -> Global Phase | Mapping parameters (Pe, y, O, F) to emergent phase (HCP, Worm, etc.) | High (See M4.4) | 0 | Phase Diagrams | Mixed | Predictability shown by diagrams, but Yoneda formalism not used or mentioned. Score 0 as framework not applied. | Fig 5, 10 |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 0
    *   **Metrics:** Phase Diagrams (qualitative predictability). No metrics related to Yoneda embedding used.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Simulation Time Step | 10⁻³ | t | Sec 3 | Explicit | Value stated. |
        | Rotational Relaxation Time (τR = 1/DR) | 1 / (8e-2) = 12.5 | t | Sec 3 | Implicit | Calculated from DR. |
        | Equilibration Time | 10⁶ Steps = 1000 | t | Sec 3 | Explicit | Value stated. |
        | Data Collection Time | 10⁷ Steps = 10000 | t | Sec 3 | Explicit | Value stated. |
        | Short-time Cluster Growth Regime | ~1 to ~10 | DRt (Dimensionless) | Sec 5, Fig 13a | Explicit | Inferred range from Fig 13a discussion. |
        | Long-time Cluster Growth Regime | >10 | DRt (Dimensionless) | Sec 5, Fig 13a | Explicit | Inferred range from Fig 13a discussion. |
        | MSD Crossover (Ballistic to Diffusive) | ~1/DR = 12.5 | t | Sec 4.1.3, Eq 12 | Implicit | Theoretical crossover time for ABPs. |
        | Cluster Peak Time (tₚ) | ~5.5(n/Pe)^(6/5)y^(-3/10) | t | Sec 5, Eq 17 | Explicit | Empirical scaling relation derived. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behaviors described are the self-organization into distinct collective phases: dilute fluid, mobile worms, coexistence of worms and aggregates, fluid-like aggregates, hexagonally close-packed (HCP) solid-like structures, and dispersed clusters. Within these phases, specific dynamics occur: ABP-like diffusion in dilute/worm phases, reduced effective diffusion in aggregates, characteristic cluster growth dynamics involving nucleation and merging described by power laws in time (C(t) ~ t^z) and scaling of characteristic times with cluster size and activity (tₚ ~ (n/Pe)^k). Collective motion occurs, particularly the correlated movement of particles within worm structures.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors are validated through:
        1.  **Direct Visualization:** Snapshots of simulations (Fig 2, 8, S1, S7) provide qualitative evidence of the different structures (worms, aggregates, HCP, etc.). Movies (M1-M6, ESI) show dynamics.
        2.  **Quantitative Order Parameters:** Cluster size distribution N(n) (Fig 3, 9, Eq 7-8, Table 1) distinguishes between dilute, worm, and phase-separated states. Radius of gyration scaling <Rg²> vs n (Fig 4, Eq 9) differentiates worms (n¹·⁴) from dense aggregates (n¹). Kurtosis K (Fig 11, Eq 13) characterizes shape/homogeneity of distributions, distinguishing dilute, aggregate, and HCP phases. Spatial velocity correlation Cv(r) (Fig 6, Eq 10) reveals correlations in worms vs HCP. Hexagonal order parameter q₆ (Fig S3, S4, mentioned Sec 4.2.1) confirms HCP structure.
        3.  **Dynamical Analysis:** Mean-square displacement (MSD) analysis (Fig 7, 12, Eq 11-12) quantifies particle mobility in different phases and reveals effective diffusion coefficients D<0xE2><0x82><0x90>.
        4.  **Cluster Growth Kinetics:** Analysis of average cluster size <0xC7><0x83>(t) (Fig 13, Eq 15) and cluster concentration Pn(t) (Fig 14) reveals power-law growth regimes and characteristic timescales (Eq 16-17).
        5.  **Phase Diagrams:** Construction of state diagrams (Fig 5, 10) based on simulation results systematically maps the emergent phases as a function of control parameters, providing strong evidence for predictable emergence.
        6.  **Finite-Size Analysis:** Comparison of results for N=625 and N=1000 (Sec 4.1.1, Fig S7) checks for robustness of conclusions regarding system size. Reproducibility seems implied by averaging/multiple runs mentioned ("up to 10 independent realizations", Sec 3), though not explicitly detailed for all results. Limitations: Validation is purely computational within the defined model; connection to experimental reality is indirect.

---

#Key: [yu_energy_2022]

# The energy cost and optimal design of networks for biological discrimination

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a theoretical and computational framework for analyzing kinetic proofreading (KPR) networks in biological discrimination processes. It investigates the fundamental trade-off between discrimination error (η, ratio of incorrect to correct product formation flux) and energy cost (C, futile cycles per product formed, related to energy dissipation). The framework uses chemical kinetics, reaction flux analysis, and free energy landscape perspectives, modeled via Chemical Master Equations (CMEs) and a derived flux-based formalism. Components include enzymes (E), correct (R) and incorrect (W) substrates, intermediate complexes (e.g., ER, EW, ER*, EW*), products (P), reaction rate constants (k), discrimination factors (f), and futile cycles driven by chemical potential (Δμ\_futile, often from NTP hydrolysis). The purpose is to derive theoretical lower bounds for the error-cost relationship under constraints (fixed discrimination factors) and to analyze how close specific biological systems (T7 DNA Polymerase, E. coli Ribosome, E. coli IleRS) operate to these optimal bounds, considering factors like reaction speed.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These are key parameters defining the models and their performance. η and C are the primary outputs being related. f and n define the network's discrimination capability. γ or Δμ represents the non-equilibrium driving force. Reliability is 'High' in the context of being explicitly defined inputs or variables within the theoretical framework itself.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the chemical potential difference (Δμ\_futile) provided by futile cycles, typically involving the hydrolysis of energy-rich molecules like nucleotide triphosphates (NTPs, e.g., dNTP in DNA replication, GTP in translation, ATP implied in aminoacylation). This drives the system away from equilibrium, enabling proofreading.
    *   Value: e.g., Δμ = 20 kBT (for DNAP example)
    *   Units: kBT (thermal energy units) or J/mol

### **2.2 Energy Transduction**

    *   Content: Chemical energy (Δμ\_futile) is transduced through a network of biochemical reactions (binding, dissociation, conformational changes, catalysis, proofreading/resetting steps). This energy drives the non-equilibrium steady state. Specifically, it powers the 'futile' proofreading cycles (e.g., steps k±3 in Fig 1a, k±(2m+1) in Fig 1b) that reset the enzyme without product formation, enabling lower error rates than equilibrium discrimination would allow. Energy flow maintains fluxes through different pathways (product formation vs. proofreading).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper defines the minimal possible energy cost (C\_min) for a given error (η) via the derived bounds (e.g., Eq 2.10, 2.13, 2.16). Efficiency is assessed by comparing a system's actual cost (C) to C\_min at its operating error η. Systems operating close to the bound (C ≈ C\_min) are considered highly efficient in terms of energy use for accuracy. Examples: T7 DNAP (C is 4.3% larger than C\_min), IleRS (C is 2.6-fold C\_min, but dissipation σ is <20% larger than optimal). Ribosome operates far from the bound in a high-dissipation regime. Therefore, efficiency is system-dependent and quantifiable relative to the theoretical optimum. No single score applies to the framework itself.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation occurs primarily through the futile proofreading cycles. The paper quantifies this dissipation using the dimensionless 'cost' parameter C, defined as the ratio of the total futile proofreading flux to the total product formation flux (Eq 2.5). The physical dissipation rate per correct product formed is given by σ = σ₀ + C(1+η)Δμ\_futile (Eq 2.4). Higher C means higher dissipation for a given Δμ\_futile. The analysis aims to find the *minimum* C required to achieve a certain error η. All non-equilibrium steps contribute to dissipation, but the futile cycles are the key mechanism discussed for *reducing error at the expense of energy*.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", including M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog

### **5.3 Computational Primitive:**

    *   Content: Discrimination / Selection / Classification (based on kinetic parameters). The network effectively compares the kinetics of processing the correct substrate versus the incorrect substrate, amplifying small differences through non-equilibrium steps to achieve a low error rate (high selectivity). Mathematically, it computes the ratio of product formation fluxes J_W / J_R (Eq 2.3).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Reaction Step Timescale | Inverse of rate constants (1/k) | s (or ms, μs etc., depending on k) | Eq 2.2, Models in Figs 1, 2, 5 | Explicit | Rate constants (k) define the timescales of individual reaction steps. |
        | System Relaxation Time | Depends on eigenvalues of K matrix | s (or other time units) | Eq 2.1 | Implicit | The time to reach steady state is determined by the system's dynamics (matrix K), but not explicitly calculated or discussed. |
        | Overall Process Speed | Characterized by product-forming flux (J_R+J_W) | events/s or concentration/s | Section 1, Section 2.6 | Explicit | Speed is mentioned as an important factor, related to product flux. |
    *   **Note:** The paper focuses on steady-state properties (error, cost) determined by *ratios* of rates/fluxes, rather than absolute timescales, although speed is acknowledged as a separate important factor influenced by absolute rates.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behavior is highly specific substrate discrimination, achieving low error rates (η) in selecting correct (R) over incorrect (W) substrates, often below the equilibrium discrimination limit. This is coupled with energy dissipation, characterized by the cost (C). The system exhibits an emergent trade-off behavior where reducing error inevitably increases the minimum required energy cost, following a fundamentally constrained relationship (error-cost bound, e.g., Eq 2.10, 2.13, 2.16). Different systems operate at different points on or above this bound, potentially prioritizing speed over minimizing dissipation (e.g., Ribosome, IleRS).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary emergent behavior claimed is the fundamental error-cost trade-off bound. This is validated theoretically through mathematical derivation using the flux-based formalism applied to CME steady states (Sections 2.2, 2.3, 2.5, Supp Info). It is further validated computationally through extensive random parameter sampling for different KPR models (Hopfield, n-stage DBD, MM-with-proofreading, biological models), demonstrating that all sampled points lie on or above the derived theoretical bound (Figs 1c,d, 5c,d, 6a,b,c). The bound represents a Pareto front for error and cost.

---

#Key: [luo_highly_2023]

# Highly Bionic Neurotransmitter-Communicated Neurons Following Integrate-and-Fire Dynamics

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is an artificial neuron designed to mimic biological neuronal communication using chemical signals (neurotransmitters). It consists of a supercapacitively gated chemotransistor integrated with an acetylcholine releasing unit (microfluidic channel and electrochemical micropump). The chemotransistor uses a graphene nanowall (GNW) extended gate functionalized with acetylcholine esterase and choline oxidase, a poly(3-hexylthiophene) (P3HT) semiconductor channel, and an ion gel dielectric layer. Its purpose is to emulate the reversible integrate-and-fire (I&F) dynamics model observed in biological neurons. Upon acetylcholine detection, an electrochemical reaction charges the GNW gate (mimicking membrane potential integration). When the potential (measured via drain-source current, Ids) reaches a threshold, the GNW discharges (potential recovery), and the micropump releases acetylcholine (mimicking neurotransmitter release), or an integrated axon-hillock circuit generates electrical spikes. The system demonstrates chemical communication between artificial neurons and between artificial neurons and living PC-12 cells. It is intended as a basic unit for constructing bionic neural networks for applications like artificial intelligence, human-machine interfaces, and nerve repair.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters listed focus on key performance aspects related to sensing, memory (decay), and actuation.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Primarily chemical energy from the electrochemical reaction of acetylcholine catalyzed by enzymes on the GNW gate. Secondary electrical energy input via gate voltage pulses (Vg) for modulation/reading and voltage applied to the micropump electrodes for actuation (neurotransmitter release).

### **2.2 Energy Transduction**

    *   Content: 1. Chemical to Electrical: Acetylcholine undergoes enzyme-catalyzed electrochemical reaction on GNWs, generating electrons. 2. Electrical Storage: Generated electrons charge the supercapacitive GNW electrical double layer (EDL), increasing gate potential. 3. Electrical Modulation: GNW gate potential modulates the conductivity of the P3HT channel via the field effect through the ion gel dielectric (Transistor action). 4. Electrical to Electrochemical/Mechanical (Pump): Applied voltage drives electrolysis of KOH solution in the micropump, generating gas pressure. 5. Pressure to Mechanical/Fluidic: Gas pressure deforms the pump membrane, expelling acetylcholine solution. If axon-hillock circuit is used: 6. Electrical (Transistor Ids) to Electrical (Capacitor Charging): Drain current charges Cmem. 7. Electrical (Vmem) to Electrical Spiking: Threshold voltage triggers inverter circuit, generating Vout spikes.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Low. The paper emphasizes low power consumption for the micropump (<20 μW, Fig 4f), which is positive. However, the overall efficiency of converting chemical detection events into signal modulation and potential neurotransmitter release is not quantified and likely low, typical for such multi-step transduction processes involving electrochemical reactions and field effects. No overall system efficiency metric is provided. The score reflects the low power of the pump but acknowledges the likely inefficiencies elsewhere.

### **2.4 Energy Dissipation**

    *   Content: Dissipation occurs via: 1. Electrochemical Reactions: Irreversible chemical transformations and associated heat. 2. Resistive Losses: Current flow through the P3HT channel, GNWs, electrodes, and electrolyte (ion gel). 3. Capacitive Leakage: Imperfect charge holding by the GNW supercapacitor and dielectric layers. 4. Electrolysis Byproducts: Energy converted into chemical potential of H2 and O2 gases in the pump (partially useful work, partially dissipation). 5. Heat Generation: General resistive heating (Joule heating) and heat from chemical reactions. Quantification is not provided, but likely significant across these mechanisms. Qualitative Assessment: High overall dissipation expected.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: ~240
*    Units: ms (decay constant at 200 μM ACh)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Slight changes
    *   Units: Qualitative (performance change)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Neuromorphic / Analog / Hybrid

### **5.3 Computational Primitive:**

    *   Content: Thresholding / Integrate-and-Fire. The core operation involves integrating input signals (chemical leading to charge) over time until a threshold current (Ids_th) is reached, triggering an output event (neurotransmitter release or spike generation) and a reset (potential recovery). The Y-junction demonstrates signal summation leading to threshold activation (OR gate).
    *   **Sub-Type (if applicable):** Integrate-and-Fire.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Decay Constant (Memory Retention) | ~240 | ms | Abstract, Fig 2e text | Explicit | Explicitly stated as decay constant. |
        | Spike Duration (Input) | Variable (e.g., 0.02 used) | s | Fig 3f legend | Explicit | Parameter used in experiments. |
        | Spike Interval (Input) | Variable (e.g., 10-100 tested for PPF) | ms | Fig 3e legend | Explicit | Parameter used in experiments. |
        | Input Pulse Frequency | Variable (e.g., up to 100 tested) | Hz | Fig 3f legend | Explicit | Parameter used in experiments. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is based on the physical dynamics of charge accumulation and dissipation in the supercapacitive GNW gate. Higher frequency, longer duration, or higher voltage input spikes lead to greater charge accumulation (potentiation, higher Ids) within a given time frame. Shorter intervals between pulses result in less charge dissipation between pulses, leading to a larger response to the second pulse (PPF). The decay of this accumulated charge (~240 ms time constant) governs the short-term nature of the plasticity. It is a physics-based emulation of use-dependent synaptic efficacy changes, driven by input signal characteristics and the material properties (capacitance, resistance) of the gate.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: 1. **Neurotransmitter Sensing:** Selective detection of acetylcholine down to pM concentrations. 2. **Signal Integration:** Temporal summation of input signals (chemical/electrical pulses) via charge accumulation on the GNW gate, mimicking membrane potential integration. 3. **Thresholding/Firing:** Activation of an output mechanism (neurotransmitter release via micropump or electrical spike generation via axon-hillock circuit) when the integrated signal (represented by Ids) crosses a threshold. 4. **Reset/Recovery:** Return of the system to a baseline state after firing via GNW discharge. 5. **Short-Term Plasticity:** Modulation of response based on recent input history (PPF, spike timing/voltage dependence). 6. **Chemical Communication:** Transmission of signals between artificial neurons or between artificial and biological cells using acetylcholine. 7. **Logic Operation:** Performing a logical OR function when configured in a Y-shaped junction.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Behaviors are validated through direct experimental measurements:
        *   Sensing/Integration/Thresholding/Plasticity: Validated via drain-source current (Ids) measurements over time in response to controlled acetylcholine concentrations and electrical pulse sequences (Figs 2d-f, 3e-f). Transfer curves (Fig 2b) also validate sensing.
        *   Firing (Chemical): Validated by observing the micropump expelling solution (Fig 4e). Threshold activation linked to Ids (Fig 5 description).
        *   Firing (Electrical): Validated by measuring output voltage spikes from the axon-hillock circuit upon acetylcholine stimulation (Fig S12).
        *   Communication: Validated by demonstrating signal transmission in a Y-shaped OR gate circuit (Fig 5) and by observing response in the artificial neuron when stimulated by neurotransmitters released from PC-12 cells (Fig 4a-c).
        *   Robustness/Stability: Validated via performance tests over time (Fig S3) and specificity tests (Fig 2c).
        *   Limitations: Validation primarily under controlled buffer conditions. Long-term robustness in complex environments or large network variability not fully demonstrated. Claims rely on analogy to biological emergence (I&F) rather than emergent phenomena from self-organization.

---

#Key: [fernando_pattern_2003]

# Pattern Recognition in a Bucket

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system uses the surface waves generated in a bucket of water as a computational medium, analogous to a Liquid State Machine (LSM) or Reservoir Computer. Inputs are provided by electric motors vibrating the water surface. The resulting interference patterns are captured by a camera via an overhead projector setup. These patterns (frames) are pre-processed (Sobel filter, thresholding, averaging) and fed into a simple linear perceptron (or multiple perceptrons in parallel) for classification tasks (XOR, speech recognition). The core idea is that the complex, high-dimensional dynamics of water waves transform temporally complex inputs into spatially complex patterns that are linearly separable by the perceptron. The system components include a water tank, electric motors (inputs), a computer controller, an overhead projector, a screen, a webcam (output sensor), and a computer running perceptron algorithms (readout). Its purpose is to demonstrate pattern recognition using a physical, non-neural dynamical system as the computational pre-processor.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Other parameters like motor frequencies, water depth, tank dimensions are mentioned qualitatively or implicitly but not quantified precisely. Speech pre-processing creates an 8x8 matrix per sample.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy sources are electrical: power supplied to the computer (controlling motors, running perceptron, processing camera data), the electric motors (actuators), the overhead projector (light source), and the webcam (sensor).

### **2.2 Energy Transduction**

    *   Content: 1. Electrical energy (computer control signal) -> Electrical energy (motor driver) -> Mechanical energy (motor vibration) -> Kinetic energy (water waves). 2. Electrical energy -> Light energy (projector bulb). 3. Light energy -> Modulated light pattern (passing through/reflecting off water waves). 4. Modulated light pattern -> Electrical signals (camera sensor pixels). 5. Electrical signals -> Digital data (computer processing & perceptron). The key transduction is electrical input to motors causing mechanical waves, whose spatial pattern (captured optically) represents the computation.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Extremely low. The vast majority of electrical energy input is likely dissipated as heat (motors, projector, computer), sound (motors), and light not captured by the camera. The energy used to create the specific wave patterns relevant for computation is minuscule compared to the total energy consumption of the apparatus. No quantitative efficiency metrics are provided. Qualitative assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include: Heat loss from the computer, electric motors, and overhead projector. Viscous damping of water waves. Sound energy from motors. Light energy from the projector not contributing to the image formation on the camera sensor (scattered/absorbed). Electrical resistance in wiring. Friction in motor mechanisms. Evaporation of water (latent heat). Quantification is not provided. Qualitative assessment: High.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Short-term (Transient)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: High-dimensional

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: XOR: 100% (Training), 85% (Test). Speech: 98.5% (Training), ~65% (Test).
*   Units: % Correct Classification

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Separation Property (Approx.) | Euclidean distance between states vs. distance between inputs (motor count diff.) | Approx. Linear Trend | L2 norm (pixels) vs. Motor count diff. | Attribute of `MemoryNode` | Fig 10 Right | Explicit | Measures how well input differences map to state differences, related to memory distinguishability. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are implicitly the principles of fluid dynamics governing wave propagation and superposition on the water surface (approximated by the wave equation under certain conditions, or more generally, Navier-Stokes equations). Key aspects mentioned include: wave interference (constructive and destructive superposition based on phase), wave propagation (spreading activation), reflection from boundaries (tank walls), and damping (viscosity). These physical laws act locally on elements of the fluid, leading to the global wave patterns.
    * **Implicit/Explicit**: Implicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
### **4.3 Global Order:**

    *   Content: The global order that emerges is the complex, dynamic spatio-temporal pattern of interference waves on the water surface. Specific examples for the XOR task are shown in Figure 3, displaying distinct patterns for different input conditions ([0 0], [1 0], [0 1], [1 1]). For speech, different patterns correspond to "zero" and "one" (Figure 9). This dynamic pattern constitutes the high-dimensional state used for computation.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| WavePattern_1 | Spatio-temporal interference pattern on water surface | Pixel Values (after processing) | 0-255 (Implied) | Intensity | Explicit | Represents the emergent state read by the camera. | Section 2 | Fig 3, 9 |
| Complexity_1 | Mutual Information based complexity measure (Tononi et al.) | MI(Xj; X-Xj) integrated over subset sizes k | Varies (see Fig 10 Left) | Bits (Information) | Explicit | Quantifies statistical complexity of the patterns. | Section 4.1 | Fig 10 Left |
| StateDistance_1 | Euclidean distance between pixel value vectors for different inputs | L2 norm | Varies (see Fig 10 Right) | Pixel Intensity Units | Explicit | Used to measure separation property. | Section 4.2 | Fig 10 Right |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | Input-State | Mapping from motor activations (local input) to global wave pattern (state) | Medium-High (See M4.4) | 4 | Separation Property (Approx.), Classification Accuracy | Mixed | The system reliably maps inputs to distinct, classifiable states, but noise affects fidelity. Predictability score (M4.4=7) suggests medium-high fidelity. Yoneda score reflects reasonable but imperfect mapping. Separation property explicitly measured. | Sec 3, Fig 5, 8, 10 |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 4. Rubric: 0=No relation between local and global; 2=Weak correlation; 4=Discernible mapping, some noise/exceptions; 6=Strong correlation, predictable mapping; 8=Near-perfect functorial mapping; 10=Perfect functorial mapping. The system shows a discernible mapping (input -> wave pattern -> classification), but noise and the limited analysis prevent a higher score. The relationship is primarily demonstrated through task performance rather than a formal functorial analysis.
    *   **Metrics:** Approximate Separation Property (Euclidean distances), Classification Accuracy (XOR, Speech).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Reservoir Computing

### **5.3 Computational Primitive:**

    *   Content: Non-linear spatio-temporal transformation / High-dimensional projection. The fundamental operation performed by the water is the complex, dynamic response to input perturbations, involving wave propagation, superposition (interference), and interaction with boundaries. This transforms the low-dimensional temporal input signal (motor activations over time) into a high-dimensional spatio-temporal state (wave pattern across the surface). This transformation is inherently non-linear due to fluid dynamics and interference effects.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Camera Frame Acquisition | 200 | ms | Section 2 | Explicit | Inverse of 5 fps frame rate. |
        | Input Signal (Speech Task Slice) | 500 | ms | Section 2.2 | Explicit | Each of the 8 time slices per word sample drove motors for 0.5s. |
    *   **Note:** The system operates across multiple timescales, from motor actuation and wave dynamics (likely ms to s) to camera sampling (200ms) and task durations (seconds for speech samples).

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is pattern recognition, specifically: 1. Solving the non-linear XOR problem by classifying input conditions based on resulting wave patterns. 2. Performing speech recognition by discriminating between spoken words ("zero" vs. "one") based on wave patterns generated from pre-processed audio signals. The core emergent behavior enabling this is the system's ability to transform complex temporal inputs into distinct, high-dimensional spatio-temporal patterns (wave interference) that become linearly separable.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation is primarily through standard machine learning methodology: training and testing sets. For both XOR and Speech tasks, the system (water + perceptron) is trained on one dataset and its performance (classification accuracy) is evaluated on a separate, unseen dataset (Sections 2.1, 3.1, 3.2; Figures 5, 8). The ability to generalize to the test set provides evidence that the emergent wave patterns contain task-relevant information. Control experiments (e.g., attempting classification without the water, directly on raw input) were done for speech (Figure 7 Left), showing significantly worse performance, supporting the water's contribution. The complexity and separation property analyses (Section 4) provide further, indirect validation of the underlying dynamics enabling the behavior. Limitations include the specific noise levels in this setup and the relatively small scale of the tasks.

---

#Key: [sakurai_quantum_2022]

# Quantum Extreme Reservoir Computation Utilizing Scale-Free Networks

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a Quantum Reservoir Computer (QRC) model designed for pattern recognition tasks, specifically demonstrated on the MNIST handwritten digit dataset. It consists of four layers: 1) An input layer that preprocesses classical image data using Principal Component Analysis (PCA) and encodes the 2N most significant components onto the initial state of N qubits using single-qubit rotations. 2) A Quantum Hidden Layer (QHL), which acts as the reservoir. This layer is an N-qubit system whose dynamics are governed by the Floquet operator derived from a periodically driven Hamiltonian simulating a discrete time crystal (DTC). The effective Hamiltonian associated with these dynamics generates scale-free network properties under certain conditions (parameter ε). The QHL processes the encoded input state through quantum evolution for 'n' periods. 3) An M-layer where the final quantum state of the QHL is measured in the computational basis, converting the quantum state information into a classical probability distribution vector (~x(k)). 4) An output layer consisting of a classical One-Layer Neural Network (ONN) which is trained (weights W, bias B~) using standard machine learning techniques (gradient descent, backpropagation, mini-batch) on the renormalized measurement outcomes (~x(k)) to perform the final classification. The core idea is to leverage the computationally complex dynamics of the QHL, particularly its scale-free network structure, for information processing, while keeping the quantum part fixed (non-trainable) and performing training only on the classical ONN.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters define the quantum hidden layer's dynamics and the system size used in the simulations. Reliability is high as these are parameters set for the theoretical model/simulations.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The model is theoretical/computational. Energy is implicitly required to drive the quantum dynamics (represented by the Hamiltonian `H(t)`) and perform classical computations (PCA, ONN training). The physical energy source is not specified, as it depends on the eventual hardware implementation (e.g., lasers for ion traps, microwave pulses for superconducting qubits).

### **2.2 Energy Transduction**

    *   Content: 1. **Quantum Dynamics:** Abstract computational "energy" drives the unitary evolution `F̂^n` of the N-qubit state according to the Floquet Hamiltonian. This transforms the encoded input information. 2. **Measurement:** Energy is involved in the physical measurement process (not detailed), converting the quantum state information into classical measurement outcomes (probability distribution). 3. **Classical Computation:** Energy is consumed during PCA data preprocessing and the training/inference of the classical ONN (gradient descent, matrix operations). The specific transduction mechanisms are abstract within this model or depend on the future physical implementation.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss or quantify energy efficiency. It focuses on computational performance (accuracy, training speed inferred from lack of QHL optimization). Efficiency would depend heavily on the specific physical implementation and is not assessable from the provided theoretical description.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation mechanisms (like decoherence in the quantum system, heat in classical processors) are not discussed or quantified. The model assumes ideal quantum evolution. Dissipation is implicitly present in any real physical implementation but not analyzed here.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: [Yes]

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: [~nT for quantum state; Long-term for ONN weights]

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: [Quantum: `2^N` basis states; Classical: Number of weights in ONN (`2^N * 10 + 10`)]
*   Units: [Quantum: States/Dimensions; Classical: Parameters]

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: [Yes]

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rules are defined by the periodically driven Hamiltonian `H(t)` (Eq 1) composed of `H1` (transverse field) and `H2` (long-range Ising-like interaction `J_lm = J0 / |l-m|^α`). The dynamics are governed by the Floquet operator `F̂ = exp(-i H2 T / (2ħ)) exp(-i H1 T / (2ħ))` (Eq 2). The emergent network structure is derived from the effective Hamiltonian `H_eff = iħ/T log[F̂]` using the percolation rule: an edge exists between nodes (basis states) `i` and `j` if `|Ei - Ej| < |Wij|`, where `Ei` are diagonal elements and `Wij` are off-diagonal elements of `H_eff`.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | H_1 | Transverse field term | g | Satisfies 2gT=π | 1/Time | Eq 1 | Explicit | Parameter definition |
    | H_2 | Ising interaction | J0 | 0.06 / T | Energy | Sec IV | Explicit | Value used in simulations |
    | H_2 | Interaction decay | α | 1.51 | Dimensionless | Sec IV | Explicit | Value used in simulations |
    | H(t) | Rotation error / Drive parameter| ε | 0 - 0.1 | Dimensionless | Sec II, IV | Explicit | Parameter controlling network type |
    | Percolation | Energy difference | |Ei - Ej| | Energy | Sec II | Explicit | Part of the rule definition |
    | Percolation | Transition energy | |Wij| | Energy | Sec II | Explicit | Part of the rule definition |

### **4.3 Global Order:**

    *   Content: The emergent global order is the topology of the network representing the effective Hamiltonian `H_eff` in the computational basis, visualized via the percolation rule. Specifically, for certain ranges of ε (e.g., 0.01 to 0.03), this topology exhibits scale-free properties, characterized by a power-law degree distribution P(k) ~ k^-γ (shown qualitatively in Fig 3b). For ε=0, it's locally connected dimers, and for larger ε, it becomes a random network.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Hamiltonain H(t) | Governs quantum dynamics | ε, J0, α, g | See M4.2.1 | Various | Explicit | Defines the system's evolution | Eq 1, Sec II, Sec IV |
| Percolation | Determines network edges from H_eff | |E_i - E_j|, |W_ij| | Energy | Explicit | Defines how network structure emerges | Sec II |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Network Regime | Control Parameter | ε | 0 - 0.1 | Dimensionless | Explicit | Determines the network type | Set in Hamiltonian | Sec II, IV |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | Local Dynamics (H(t), ε) to Global Network Structure (P(k)) | How local qubit interactions and drive parameters determine the overall network topology. | High (for topology class) | 7 | Degree distribution P(k) | Mixed | Predictability discussed in M4.4. Yoneda score reflects good but not perfect mapping due to potential instance variations. | Sec II, IV, Fig 3b |
    | Global Network Structure to Computational Performance (Accuracy) | How the emergent network topology influences the QRC's pattern recognition accuracy. | Medium | 6 | Accuracy vs ε | Explicit | Fig 3 shows correlation, but mechanism isn't fully detailed. Optimal performance in transition regime suggests complex link. | Sec IV, Fig 3 |

    *   **Yoneda Embedding Fulfillment Score [0-10]:** Rubric: 0=No mapping; 3=Weak correlation; 5=Qualitative mapping; 7=Quantitative mapping of key features; 9=Predictive mapping based on mechanism; 10=Complete isomorphic mapping.
        *   *Local-to-Global Structure (Score 7):* The parameter ε clearly dictates the *type* of network (dimers, scale-free, random) and this is shown quantitatively via P(k). The mapping is strong but might miss instance-specific details or effects of finite size.
        *   *Global Structure-to-Performance (Score 6):* Performance (accuracy) is explicitly shown to depend on ε (and thus implicitly on network structure), peaking near the scale-free to random transition. However, the precise mechanism linking specific network features (beyond just being scale-free/transitional) to accuracy isn't detailed, making the mapping good but not fully predictive from structure alone.
    *   **Metrics:** Degree Distribution P(k), MNIST Classification Accuracy Rate.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: [Yes]

### **5.2 Computation Type:**

    *   Content: [Reservoir Computing] (Quantum Reservoir Computing - QRC)

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operation performed by the *quantum* part is the application of the Floquet unitary operator `F̂` (Eq 2) to the quantum state, repeated `n` times. This operator maps the state of the `N`-qubit Hilbert space to another state within the same space, effectively performing a complex, high-dimensional, non-linear transformation (in the sense required for reservoir computing) on the encoded input information over time. `|ψ(t+T)> = F̂ |ψ(t)>`.
    *   **Sub-Type (if applicable):** Unitary Transformation (Floquet Operator Application)

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Floquet Period | T | Time | Eq 1, 2 | Explicit | Defined as period of H(t). |
        | QHL Computation Duration | n * T | Time | Sec IV, Fig 4a | Explicit | Total evolution time (n periods). |
        | Required Computation Duration (n) | ~50 | Periods | Sec IV, Fig 4a | Explicit | Value where performance saturates. |
        | Coherence Time (Implied Limit) | > n * T | Time | Sec II, [25] | Implicit | Mentioned feasibility requires nT < T_coherence. |

    *   **Note:** The absolute value of T is linked to J0 by J0T = 0.06, but T itself isn't specified. n~50 periods are key for QHL computation.

### **6.2 Active Inference:**

    *   Content: [No]

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: [Yes (Classical Part Only)]

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is classical supervised learning applied to the ONN. Specifically, it uses:
        1.  **Gradient Descent:** Weights `w_ij` and biases `b_i` are updated iteratively to minimize a loss function `L`. `w(n+1) = w(n) - η * ∂L/∂w_ij`, `b(n+1) = b(n) - η * ∂L/∂b_i` (Eq C1).
        2.  **Loss Function:** Cross-entropy between the ONN output `~y(k)` (after soft-max activation) and the one-hot target vector `~t(k)` (Eq B3). `L_k = - Σ_l t(k)_l * log(y(k)_l)`.
        3.  **Backpropagation:** Used to calculate the gradients `∂L/∂w_ij` and `∂L/∂b_i` (Eq C2).
        4.  **Mini-batch Method:** Gradients are averaged over a small batch `M` of samples to update the parameters (Eq D1, D3, D4).
        The adaptation modifies the classical processing part based on performance feedback (loss).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior of the system is **pattern recognition**, specifically classifying handwritten digits from the MNIST dataset. Input images are processed, run through the quantum reservoir dynamics, measured, and classified by the trained ONN. Performance is measured by the classification accuracy rate. A secondary observed behavior is the **suppression of overfitting** when dropout is applied to the ONN (Section V, Fig 5).

### **8.2 Behavior Robustness:**

        1.  **Overfitting:** The basic model shows signs of overfitting (gap between training/testing accuracy, Fig 4). Dropout significantly improves robustness against this, maintaining high test accuracy while reducing the training-test gap (Fig 5). This suggests robustness to variations within the training data distribution not captured perfectly by the training set.
        2.  **Parameter Sensitivity (ε):** Performance is sensitive to the parameter ε, peaking in a specific range (Fig 3c, Fig 5c), indicating lack of robustness to variations in this parameter which controls the QHL dynamics/complexity. However, performance is relatively stable within the optimal regime (ε≈0.01-0.05).
        3.  **Parameter Sensitivity (n, N):** Performance improves and saturates with evolution time `n` (Fig 4a) and system size `N` (Fig 4b), suggesting robustness once these are sufficiently large.
        4.  **Network Instance:** Performance is claimed to be insensitive to the *specific* degree distribution instance as long as ε is in the right regime (Sec IV), suggesting robustness to minor structural variations in the QHL network.
        5.  **Noise/Errors:** Robustness to physical noise (decoherence, gate errors, measurement errors) is *not* evaluated. The model is theoretical and assumes ideal conditions.
        The score reflects good robustness shown via dropout and claimed robustness to network instance, but clear sensitivity to ε and unaddressed robustness to physical noise.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary behavior (pattern recognition) is validated through standard machine learning methodology: training on a large dataset (60,000 MNIST samples) and testing on a separate dataset (10,000 samples), quantifying performance using accuracy rate (Sec IV, Fig 3, 4, 5). The effect of parameters (N, n, ε) and techniques (dropout) is systematically evaluated through numerical simulations. Control comparisons are made (e.g., ONN only, different ε values). Reproducibility is implied by standard methods, though code is not provided. Limitations include the theoretical nature (no physical noise) and potential simulation artifacts. The emergence of scale-free networks (related to behavior) is validated by plotting degree distributions (Fig 3b) and citing prior work [21]. The link between network topology (emergent structure) and computational performance (behavior) is established correlationally by plotting accuracy vs ε (Fig 3c).

---

#Key: [bordiga_automated_2024]

# Automated discovery of reprogrammable nonlinear dynamic metamaterials

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of 2D flexible mechanical metamaterials comprising a network of rigid units connected by flexible ligaments. Its purpose is to achieve specific, complex nonlinear dynamic responses (e.g., energy focusing, energy splitting, dynamic protection, nonlinear motion conversion) through automated inverse design. The design process uses a custom-developed fully-differentiable simulation environment and gradient-based optimization (Method of Moving Asymptotes) to tune the geometry (shape of rigid units) to achieve the desired dynamic task encoded in an objective function. The system also demonstrates reprogrammability, where static pre-compression can switch the material between different pre-designed dynamic functionalities (e.g., focusing vs. protection). The system is physically realized using 3D-printed PLA units and polyester shim ligaments.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                | Value                    | Units           | Source (Fig/Table/Section)         | Implicit/Explicit   | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit)   |
        | :---------------------------- | :----------------------- | :-------------- | :----------------------------------- | :------------------ | :----------------------------------- | :-------------------------------- |

    *   **Note:** These are key parameters defining the physical system and its programmable states. Other parameters like damping coefficients, contact parameters, and optimization constraints are also mentioned.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is mechanical excitation applied as a displacement-driven pulse-like signal or harmonic input to specific units, typically at the boundary of the metamaterial domain. Example: `u_input(t) = A/2 * (1-cos(2*pi*f*t))` for `0 <= t <= 1/f`.
    *   Value: Amplitude (A) e.g., 7.5 mm; Frequency (f) e.g., 30 Hz (for pulse width).
    *   Units: mm (Amplitude); Hz (Frequency). Integrated energy mentioned qualitatively, e.g., ~10 mJ focused in target area (Fig 2A).

### **2.2 Energy Transduction**

    *   Content: The input mechanical energy (kinetic energy from imposed displacement) is transduced into kinetic energy of the rigid units (translational and rotational) and potential energy stored in the flexible ligaments (stretching, bending, shearing). Contact between rigid units also involves potential energy storage/release described by a contact model (Eq. 2). Energy flows and transforms between kinetic and potential forms during the dynamic response, propagating through the network of units and ligaments. Nonlinear dynamics govern these transformations, particularly due to large deformations and contact interactions.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The concept of 'efficiency' is task-dependent here. For energy focusing, efficiency could be the ratio of energy concentrated in the target region vs. total input energy or energy elsewhere (J_Ωt / J_Ω\Ωt, explored in Fig 2C). For protection, efficiency means minimizing energy transmission. The optimization aims to maximize/minimize task-specific objectives related to energy distribution, not overall energy conservation efficiency (which is impacted by dissipation). No global efficiency metric is provided.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation is explicitly modeled via a linear viscous damping term `f_damp = -C * q_dot`, where C is a diagonal matrix with translational (c_u) and rotational (c_θ) damping coefficients. Values are provided: c_u = 2.9x10⁻² kg/s and c_θ = 1.2x10⁻⁷ kg·m²/s, determined experimentally. This represents energy loss from the mechanical system, likely converted to heat (though the final form isn't discussed).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", including M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog

### **5.3 Computational Primitive:**

    *   Content: Nonlinear Spatiotemporal Transformation / Filtering. The material structure acts as a complex spatiotemporal filter or transformer for mechanical energy/motion. Based on the optimized geometry (and pre-compression state), it selectively directs, concentrates, blocks, or converts the input dynamic signal over space and time. Specific examples demonstrated correspond to:
        *   Spatial Energy Focusing (Directing energy to a target region).
        *   Spatial Energy Splitting (Distributing energy between target regions).
        *   Spatial Energy Blocking/Protection (Minimizing energy in a target region).
        *   Motion Mode Conversion (Linear input to circular output).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description          | Value               | Units   | Source       | Implicit/Explicit   | Justification                                  |
        | :----------------------------- | :------------------ | :------ | :----------- | :------------------ | :--------------------------------------------- |
        | Input Pulse Width (1/f)        | ~33 (for f=30Hz)    | ms      | Eq. 8        | Explicit            | Defined by input parameters.                   |
        | Integration Time (t_f)         | ~67 (for f=30Hz)    | ms      | Eq. 7 text   | Explicit            | Chosen as 2/f for objective evaluation.        |
        | Dynamic Response Time (Focusing) | ~40-60              | ms      | Fig. 2B, 2D  | Explicit            | Time to peak focusing/observation in plots.    |
        | Oscillation Period (Implied)   | ~ tens of ms        | ms      | Fig. 2F, 4D  | Implicit            | Inferred from dynamic plots; depends on modes. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behaviors are engineered nonlinear dynamic responses:
        1.  **Energy Focusing:** Concentrating input mechanical pulse energy into a predefined target spatial region.
        2.  **Energy Splitting:** Distributing input energy between multiple predefined target regions according to a desired ratio (mentioned in text, detailed in SI).
        3.  **Dynamic Protection:** Minimizing kinetic energy transmission into a predefined target spatial region.
        4.  **Nonlinear Motion Conversion:** Transforming an input motion (e.g., linear oscillation) into a different output motion (e.g., circular oscillation) in a target region.
        5.  **Reprogrammability:** Switching between different functional behaviors (e.g., focusing vs. protection, or focusing at different locations) within the same structure by changing static pre-compression.

### **8.2 Behavior Robustness:**

        *   **Simulation:** Robustness to variations in input pulse amplitude and frequency is shown for energy focusing (Fig. 2C), indicating the optimized behavior persists over a region of input parameter space, not just at the design point.
        *   **Experiment:** Good agreement between simulation and experiments (Fig. 2D, 4C) suggests robustness against inevitable fabrication imperfections and experimental noise. Experimental demonstration of focusing with varying input amplitudes (Fig. 2E, 2F) and the focusing-to-protection switch across different pre-compressions (Fig. 4D) further supports robustness.
        *   Quantification: Fig 2C shows the focusing ratio J_Ωt / J_Ω\Ωt remains high (>4) over a range of A ≈ [4.5, 10.5] mm and f ≈ [25, 40] Hz. Fig 4D shows consistent trends in peak energy vs. pre-compression in both simulation and experiment.
        *   Limitations: Robustness to other factors (e.g., material property variations, temperature, long-term fatigue) is not explored in the excerpt. The score of 7 reflects demonstrated robustness to input variations and experimental realization, but lack of testing against other potential perturbations.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claims of engineered behaviors (focusing, protection, reprogramming, conversion) are validated through:
        1.  **Numerical Simulation:** Using the developed differentiable physics model (Eq. 5 solved numerically) to predict the dynamic response of the optimized designs (e.g., Fig 2B, 3C, 4E, 5B simulation snapshots; Fig 2C, 3D, 4D quantitative plots).
        2.  **Physical Experimentation:** Fabricating the optimized designs (3D printing PLA units, polyester shims) and testing them dynamically using a shaker for excitation and high-speed cameras with DIC tracking for response measurement (e.g., Fig 2D, 4C experimental snapshots; Fig 2F, 4D quantitative plots).
        3.  **Quantitative Comparison:** Direct comparison between simulation predictions and experimental measurements (e.g., Fig 2D, 4C visual comparison; Fig 2F, 4D quantitative overlay plots) shows good agreement, validating both the model and the realization of the designed behavior.
        4.  **Robustness Checks:** Systematically varying input parameters (amplitude, frequency) in simulation (Fig 2C) and experiment (Fig 2E/F) and varying pre-compression in experiment (Fig 4D) demonstrate the behaviors are not limited to a single point condition.
     Limitations: Validation primarily focuses on the specific tasks designed for. Generalizability or emergence of *unintended* behaviors is not explored. Reproducibility is implied by agreement but not explicitly quantified across multiple samples/runs in the excerpt.

---

#Key: [yan_soft_2020]

```markdown
# Soft three-dimensional network materials with rational bio-mimetic designs

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of soft three-dimensional (3D) network materials architected with periodic lattice configurations (cubic, octahedral, octet) where the connections between lattice nodes are formed by 3D helical microstructures. These helical microstructures act as the building blocks. The material is fabricated using polyjet 3D printing with a polymeric material (VeroBlue). The purpose is to create artificial soft materials that mimic the J-shaped stress-strain response characteristic of many biological tissues (e.g., skin, ligaments), achieving this through a bending-to-stretching transition mechanism inherent in the helical filament geometry under tension or compression. The design allows for tunable anisotropic mechanical properties by varying helical geometry (diameter, pitch, coil number, joint length) and lattice topology. Demonstrative applications show potential for flexible bio-integrated devices, such as pressure sensors and stretchable conductors, when coated with conductive layers.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Mooney-Rivlin C10 (VeroBlue @ ~25°C) | 0.578 | MPa | Methods (FEA section) | Explicit | Medium | Fitted from exp. data (Supp Fig 16) |
        | Mooney-Rivlin C01 (VeroBlue @ ~25°C) | 1.364 | MPa | Methods (FEA section) | Explicit | Medium | Fitted from exp. data (Supp Fig 16) |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Mechanical work done on the system via externally applied forces or displacements, resulting in uniaxial stretching or compression.
    *   Value: Not specified as a single value, depends on applied strain/stress. Example energy density can be inferred from stress-strain curves (area under the curve). E.g., for Octahedral case (Fig 2a), at ε=200%, Stress ≈ 2 kPa, Energy Density ≈ 0.5 * 2 * 2000 ≈ 2 kJ/m³ (approximation).
    *   Units: J (Work), Pa or N/m² (Stress), J/m³ (Energy Density)

### **2.2 Energy Transduction**

    *   Content: Input mechanical energy is primarily transduced into elastic potential energy stored within the deformed polymeric helical microstructures. This involves spatial bending, twisting, and eventual stretching of the filaments. At low strains, bending/twisting dominates; at high strains (post-critical strain εcr), stretching dominates. Energy is also dissipated, likely through viscoelastic effects within the polymer, although this is not the focus. In conducting versions, mechanical deformation leads to changes in electrical resistance (mechano-electrical transduction).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify energy efficiency (e.g., ratio of energy returned during unloading to energy input during loading). The focus is on matching the quasi-static stress-strain response. Viscoelastic effects, which would determine efficiency, are intentionally minimized by using a low loading rate but not quantified. Qualitative assessment: Likely Medium to High for quasi-static loading, as hyperelastic models fit well, but would decrease at higher rates.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation mechanisms are not explicitly quantified or discussed in detail. Viscoelasticity inherent to the VeroBlue polymer is the most likely primary dissipation mechanism under dynamic loading, though experiments were quasi-static to minimize this. Internal friction between contacting microstructures might occur under high compression, but this is not analyzed. For conducting versions, Joule heating due to current flow would be a dissipation mechanism, but sensor/conductor operation implies low current/power. Qualitative assessment: Low under quasi-static test conditions, potentially Medium to High under dynamic loading or significant compression contact.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip M5.2-5.4)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Loading Rate (Quasi-static tests) | ~0.3 | mm/min | Methods | Explicit | Specified for mechanical testing to minimize viscoelastic effects. |
    *   **Note:** The primary relevant timescale discussed is the experimental loading rate, chosen to approximate quasi-static conditions. Intrinsic material response timescales (e.g., viscoelastic relaxation times) are not measured or discussed.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is the exhibition of a non-linear, J-shaped stress-strain response under uniaxial tension and compression, mimicking biological tissues. This arises from the designed geometry, specifically the bending-to-stretching transition of the helical microstructures. The system also demonstrates tuneable anisotropy based on lattice structure and loading direction. In conducting versions, it exhibits strain-dependent electrical resistance, functioning as a mechanical sensor (pressure sensor under compression) or stretchable conductor.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The key behavior (J-shaped stress-strain curve) is validated through quasi-static uniaxial tensile and compressive testing on 3D printed samples (Methods, Figs 2, 3, 4, 5). Experimental results are compared quantitatively with Finite Element Analysis (FEA) simulations based on the designed geometry and measured material properties (Mooney-Rivlin model fit from separate tests, Supp Fig 16), showing good agreement (Figs 2, 3, 5). Microstructure deformation mechanisms (bending, twisting, stretching, alignment) are qualitatively validated using optical imaging during testing and compared with FEA deformation predictions (Fig 2d-f, 3i-j). Robustness to defects is validated by testing samples with intentionally introduced random defects (Fig 4a-c). Anisotropy is validated by testing along different crystallographic directions (Fig 3g-j). The behavior is largely presented as a direct consequence of the rational design, rather than strictly emergent in the sense of arising unexpectedly from simple rules. Validation relies on experimental mechanics testing and computational modeling. Reproducibility is implied by the use of multiple samples (error bars shown).

---

#Key: [kim_polymeric_2015]

# Polymeric materials that convert local fleeting signals into global macroscopic responses

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a polymeric material designed to convert local, fleeting stimuli into global macroscopic responses. The core component is a random poly(norbornene) AB copolymer functionalized with three types of units distributed in a 1:2 ratio (A:B). Unit A contains sensing groups (o-nitrobenzyl carbamate) that react to a specific stimulus (300 nm UV light), undergoing a chemical change and releasing chemical reporters (fluoride ions). Unit B contains functionality (tert-butyldimethylsilyl, TBS ethers linked to fluoride release mechanism) that mediates a self-propagating signal amplification reaction upon encountering the reporter. This reaction cleaves the TBS group, changes the polymer's properties (e.g., hydrophilicity), and releases more reporters. The purpose is to create materials that mimic biological systems (like Venus flytraps) capable of global, autonomous reconfiguration in response to localized, low-intensity, and short-duration stimuli, demonstrated here by a switch from hydrophobic to hydrophilic surface wetting.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Mn and PDI values for homopolymers 6 (290 kDa, 1.2) and material dimensions (1 cm x 0.5 cm or 2 cm x 0.5 cm) are also provided but the copolymer parameters are listed as key. Contact angles are approximate graphical readings.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is ultraviolet (UV) light used to initiate the photochemical reaction in the sensing units.

### **2.2 Energy Transduction**

    *   Content: 1. Photon energy (UV light) is absorbed by the o-nitrobenzyl carbamate sensing group (Unit A). 2. This absorbed energy drives a photochemical reaction, overcoming bond energies to cause molecular rearrangement and cleavage. 3. Chemical potential energy stored in the reporter precursor is converted into kinetic energy of the released fluoride ion and potential energy of the chemical gradient. 4. The fluoride ion interacts with the TBS group (Unit B), overcoming the Si-O bond energy barrier (likely lowered by solvent interaction/catalysis) and initiating cleavage. 5. This cleavage releases stored chemical potential energy, driving subsequent reactions that release more fluoride reporters (amplification) and change the polymer structure (hydrophilic groups formation), releasing heat (exothermic reactions). Energy propagates chemically via diffusion of reporters and sequential reactions.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any information or metrics regarding the energy efficiency of the process (e.g., quantum yield of photoreaction, energy stored vs. energy input, thermodynamic efficiency of the cascade). Efficiency is likely very low, typical for photochemical triggers and solution/film reactions losing energy to the environment. Score is 0 due to complete lack of data.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation mechanisms are not explicitly quantified. Potential mechanisms include: (1) Non-radiative decay following photon absorption without reaction (heat). (2) Heat released during exothermic chemical reactions (photoreaction, fluoride release, TBS cleavage). (3) Energy loss during diffusion of fluoride through the solvent/film. (4) Potential light scattering within the film. Qualitative assessment: Likely Medium to High dissipation, given the chemical reaction cascade and interaction with the solvent environment.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**

    *   Retention: The change persists for at least 24 hours (Fig 3a), indicating relatively long retention compared to the stimulus duration. (Score contribution: +1)
    *   Capacity: The system demonstrates a transition between two primary states (hydrophobic and hydrophilic), representing a low-capacity (essentially binary) memory. (Score contribution: +1)
    *   Read-out: The state is read out via contact angle measurements, which show clear changes but have associated error bars (Fig 3). (Score contribution: +1)
    *   Re-writability: The chemical changes appear irreversible under the described conditions, meaning the memory cannot be easily reset or rewritten. This significantly limits its score. (Score contribution: +0)
    Overall score: 3/10, reflecting persistent, low-capacity, read-only chemical state memory.

### **3.3 Memory Retention Time:**

*   Value: > 24
*    Units: hours

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: 2 (Binary)
*   Units: states

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Units: degrees (°)

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | State Separation | Difference between initial and final contact angles | ~16 | degrees (°) | Attribute of `MemoryNode` | Fig 3 | Explicit | Clear change in readout value between states reported. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Skipping M4.2-M4.7 as M4.1 is "No")**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Skipping M5.2-5.4 as M5.1 is "No")**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Stimulus Duration (UV Exposure) | 40 | minutes | Text (p. 3391) / ESI Fig 7 ref. | Explicit | Explicitly stated duration of light exposure. |
        | Photochemical Reaction Completion (Homopolymer 7) | 40 | minutes | Text (p. 3390) / ESI Fig 7 ref. | Explicit | ATR-FTIR showed complete loss of carbamate stretch after 40 min. |
        | Self-Propagation / State Change Duration (Copolymer 1) | > 24 | hours | Fig 3a / Text (p. 3391) | Explicit | Contact angle change progresses for over 24h after stimulus removed. |
        | Self-Propagation Duration (Homopolymer 6, 100mM F-) | ~48 | hours | ESI Fig 3b ref. / Text (p. 3389) | Explicit | Contact angle change tracked over 48h for homopolymer experiment. |
        | Local-to-Global Communication Delay (Fig 3b) | Qual: Slight delay | hours (Implied) | Fig 3b / Text (p. 3391) | Mixed | Explicitly stated "slight delay" observed; timescale implicitly hours based on overall propagation time. |
    *   **Note:** Different experiments show different propagation timescales depending on conditions (e.g., initial fluoride concentration).

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Skipping M7.2 as M7.1 is "No")**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is the autonomous, global transformation of a macroscopic material property (surface wettability from hydrophobic to hydrophilic) in response to a localized and fleeting stimulus (300 nm UV light). This involves signal amplification and spatial propagation of a chemical change throughout the material, even in regions not directly exposed to the initial stimulus.

### **8.2 Behavior Robustness:**

        *   Control experiments (ESI Fig 8 ref.) show the effect requires both the specific copolymer (1) and the stimulus/propagation conditions (light followed by solvent). Homopolymers (6 or 7) alone, or copolymer 1 without light or without solvent, do not show the full effect. This indicates robustness against missing components/conditions.
        *   The system functions with a random copolymer structure, suggesting robustness against precise sequence control.
        *   Experiments were averaged over multiple films (n=6), implying some level of reproducibility.
        *   However, robustness to factors like temperature variations, different solvent environments (beyond i-PrOH/H2O/pyridine), variations in UV intensity, film defects, or long-term stability is not discussed. The propagation relies on specific chemistry (fluoride/TBS) which might be sensitive. Score reflects demonstrated robustness in controls but lack of broader testing.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claim of global change from local fleeting stimulus is validated through:
        *   **Contact Angle Measurements:** Time-dependent measurements on globally exposed films (Fig 3a) and locally exposed films (Fig 3b) quantify the change in wettability over macroscopic areas (>24h).
        *   **Control Experiments:** Referenced ESI Fig 8 demonstrates that the full behavior requires copolymer 1, UV light, and the propagation solvent, excluding artifacts from individual components or conditions. Homopolymer 7 shows initial photoreaction but no propagation; homopolymer 6 doesn't react to light.
        *   **Spectroscopic/Chemical Evidence:** ATR-FTIR confirms photochemical reaction (loss of carbamate stretch, ESI Fig 7 ref.). LCMS confirms expected products from monomer reactions (ESI Figs 2, 5, 6 refs.), supporting the proposed chemical mechanism.
        *   **Spatial Propagation Test:** Experiment exposing only half the film (Fig 3b) directly tests and confirms the propagation of the change to unexposed regions.
        *   **Reproducibility:** Averaging results over multiple films (n=6, mentioned in Fig 3 captions) supports reproducibility.
        *   **Limitations:** Quantification of propagation speed or front sharpness is limited. Dependence on specific solvent system (i-PrOH/H2O/pyridine) is noted. Physical mechanism of propagation (through film vs. through solvent) is suspected to be solvent-mediated but not definitively proven.

---

#Key: [pezzulo_active_2024]

# Active inference as a theory of sentient behavior

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is Active Inference (AIF), a theoretical and computational framework presented as a normative theory of sentient behavior in biological organisms, particularly concerning brain function. It posits that organisms minimize variational free energy (or surprise) by updating internal generative models (beliefs) about the world (perception) and by acting upon the world to make sensory inputs conform to predictions (action). Key components include: generative models (comprising priors, likelihoods, and sometimes transition functions), variational free energy, prediction errors, precision weighting, beliefs (probability distributions), and policies (sequences of actions evaluated via expected free energy). The purpose is to provide a unified, first-principle account of perception, action, learning, planning, and other cognitive functions observed in sentient systems. It aims to bridge computational, algorithmic, and neurobiological levels of description.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name            | Value                                  | Units    | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------------------ | :-------------------------------------: | :-------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These are core theoretical constructs of Active Inference discussed explicitly in the paper. Their "values" are conceptual roles or mathematical definitions rather than specific numerical quantities, as appropriate for a theoretical framework. Units reflect their mathematical nature (information-theoretic or probability). Data Reliability is High as these are definitional within the theory.

## M2: Energy Flow
    *   **Note:** This section assesses 'energy' in the physical sense. Active Inference primarily deals with *information-theoretic* quantities (free energy, surprise). The paper does *not* detail the physical energy consumption of the biological or artificial systems implementing AIF, only the computational/thermodynamic *principles* that might relate to minimizing metabolic cost as an extension of free energy minimization, but this is not the core focus.

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceed with M3.2-M3.8)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable; Short-term (beliefs) to Long-term (model parameters)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order
    *   **Note:** This section assesses physical self-organization. AIF describes how *beliefs* and *models* in a computational/neural system self-organize through free energy minimization, not typically how physical matter self-organizes, although related principles are mentioned for broader applications (morphogenesis).

### **4.1 Self-Organization Presence:**

    *   Content: Partial/Unclear (in the physical sense)

**(Conditional: M4.1 is "Partial/Unclear", M4.2-M4.7 focus on the *representational* self-organization aspect unless otherwise specified.)**

### **4.2 Local Interaction Rules:**

    *   Content: The "local" interaction rules govern the updating of beliefs (e.g., neuronal firing rates or population activities representing expected states and prediction errors) based on variational free energy minimization. In predictive coding (Fig 3), rules involve: calculating prediction errors (ε = signal - prediction), updating expectations (μ) based on weighted prediction errors (weighted by precision), and propagating signals between hierarchical levels. These are message-passing rules derived from the objective of minimizing free energy. Mathematically, they involve gradient descents on free energy.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID             | Description                                      | Parameter Name | Parameter Value Range | Units                  | Data Source       | Implicit/Explicit | Justification                                    |
    | :------------------ | :----------------------------------------------- | :------------- | :-------------------: | :--------------------- | :---------------- | :----------------: | :----------------------------------------------- |
    | Prediction Error    | Calculate discrepancy signal - prediction        | Precision      | > 0                   | Unitless (Inv. Info) | Section 3         | Explicit          | Core concept in AIF/PC                           |

### **4.3 Global Order:**

    *   Content: The emergent "global order" is a coherent set of beliefs (posterior probability distribution) across the entire generative model that best explains the sensory data while remaining consistent with the model's priors. This represents the system's interpretation or understanding of its environment and its own state within it. At a neural level, this could correspond to stable patterns of neural activity across interconnected brain regions.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID             | Description                                      | Parameter | Value Range | Units                  | Implicit/Explicit | Justification                                    | Source            |
| :------------------ | :----------------------------------------------- | :-------- | :----------: | :--------------------- | :----------------: | :----------------------------------------------- | :---------------- |
| Belief Update       | Update state expectation based on errors         | Precision | > 0          | Unitless (Inv. Info) | Explicit          | Core concept in AIF/PC                           | Section 3, Fig 3  |
| Learning            | Update model parameters based on experience      | Step Size | > 0          | Varies                 | Implicit          | Gradient descent parameter, not detailed here    | Section 4 (para 1)|

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID        | Description                         | Parameter              | Value Range      | Units                   | Implicit/Explicit | Justification                                       | Protocol | Source      |
| :----------------- | :---------------------------------- | :--------------------- | :--------------: | :---------------------- | :----------------: | :-------------------------------------------------- | :------: | :---------- |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type              | Description                                      | Predictability | Yoneda Score | Metrics               | Implicit/Explicit | Justification                                                                 | Source |
    | :--------------------- | :----------------------------------------------- | :------------: | :----------: | :-------------------- | :----------------: | :---------------------------------------------------------------------------- | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", proceed with M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Primarily Analog/Neuromorphic and Probabilistic (Bayesian Inference).

### **5.3 Computational Primitive:**

    *   Content: Variational Bayesian Inference (approximated via minimization of prediction errors weighted by precision). Key operations include: calculation of prediction errors (subtraction, potentially non-linear transforms), weighting by precision (multiplication/gain control), integration/summation of errors to update beliefs (gradient descent dynamics). At a higher level: belief updating, policy selection based on expected free energy calculation.
    *   **Sub-Type (if applicable):** Bayesian Belief Updating, Precision-Weighted Prediction Error Minimization.

### **5.4 Embodied Computational Units**
| Unit ID          | Description                         | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification                                       |
| :--------------- | :---------------------------------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-------------------------------------------------- |
*   **Note:** Metrics like processing power, energy/operation, and bit-depth are not specified for the biological implementation in this review. Timescales are inferred from typical neuroscience context. Representation is generally considered analog (e.g., firing rates).

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description         | Value                    | Units               | Source            | Implicit/Explicit | Justification                                                             |
        | :---------------------------- | :-----------------------: | :------------------: | :---------------- | :----------------: | :------------------------------------------------------------------------ |
        | Perceptual Inference          | Fast                     | ms (inferred)       | Section 4 (para 1)| Explicit(Fast/Slow)| Fast synaptic activity timescale assumed for belief updates.                |
        | Learning/Model Update         | Slow                     | seconds to hours+   | Section 4 (para 1)| Explicit(Fast/Slow)| Slower synaptic efficacy changes assumed for learning.                    |
        | Hierarchical Processing       | Multiple (Fast to Slow)  | ms to seconds+      | Section 3, Fig 3  | Explicit          | Higher levels process slower dynamics than lower levels.                  |
        | Action Dynamics               | Variable                 | ms to seconds       | Section 3 (Fig 5) | Implicit          | Timescale depends on the action (e.g., saccade vs. planning).           |
        | Policy Evaluation (Planning)  | Variable                 | seconds+ (inferred) | Section 3         | Implicit          | Planning involves simulating future sequences, likely slower than perception. |

### **6.2 Active Inference:**

    *   Content: Yes
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** Prediction error magnitude over time (should decrease), Rate of convergence to stable beliefs (Free Energy reduction rate), KL divergence between predicted and preferred outcomes (pragmatic value), Mutual information between states and anticipated observations (epistemic value/information gain), Correlation between expected free energy of policies and policy selection probability, Timescale of anticipatory actions relative to predicted events. CT-GIN could model the generative model structure, belief propagation dynamics, and policy selection nodes/edges, annotating them with these metrics.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: Adaptation occurs primarily through learning (parameter updates) and potentially precision tuning. Learning involves modifying the parameters of the generative model (e.g., synaptic efficacy in a neural implementation) to reduce long-term prediction error or free energy. This is typically framed as a gradient descent on variational free energy with respect to model parameters. Precision adaptation involves adjusting the weighting (inverse variance) of prediction errors, potentially reflecting attention or estimated sensory reliability, possibly modulated by neuromodulators (Section 3, Section 4). The underlying principle is Bayesian model updating/averaging driven by minimizing surprise over time.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The framework aims to explain a wide range of sentient behaviors, including: Perception (inferring causes of sensations), Action Selection (choosing policies to fulfill goals/minimize surprise), Planning (evaluating future policies), Exploration vs. Exploitation (balancing information gain and goal achievement via expected free energy), Learning (adapting the internal model), Attention (precision modulation), Homeostasis/Allostasis (regulating internal states), and potentially Social Interaction (inference about others' states/intentions). The specific behavior depends on the generative model and task context (e.g., oculomotor control in Fig 5).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper discusses AIF primarily as a theoretical framework. Validation mentioned involves: accounting for aspects of anatomy and neurophysiology (Section 1, 3), providing theories of psychopathology (Section 1, 3), unifying psychological theories (Section 1), simulating behavioral tasks (e.g., oculomotor task, Fig 5), and applications in robotics/AI (Section 1, 5). The paper advocates for deeper empirical scrutiny (Section 5), suggesting that specific models built using the AIF framework generate testable predictions that can be validated against neurophysiological and behavioral data. It doesn't present new validation data itself.

---

#Key: [wang_collective_2019]

# Collective Behavior of Reconfigurable Magnetic Droplets via Dynamic Self-Assembly

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of multiple millimeter-sized droplets containing carbonyl iron microparticles suspended in benzyl ether at an air-liquid interface. When subjected to an external precessing magnetic field, the microparticles within each droplet self-assemble into rotating chains, causing the droplets themselves to rotate. Interactions (magnetic dipole-dipole, hydrodynamic, capillary) between these rotating droplets lead to their dynamic self-assembly into ordered, rotating patterns (e.g., pentagonal for 5 droplets). The system's purpose is to demonstrate controllable, dynamic self-assembly forming a reconfigurable structure capable of collective behavior, specifically non-contact cargo manipulation (trapping, transport, release) acting as an untethered robotic end-effector. Components include: carbonyl iron microparticles (3 μm avg diameter), water (as suspension medium for particles within droplets), benzyl ether (surrounding fluid), droplets (5 μL water/particle suspension), air-liquid interface, precessing magnetic field generation system (three-axis Helmholtz coils), and cargoes (polypropylene pieces).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the external, time-dependent (precessing) magnetic field generated by a three-axis Helmholtz coil system. This field provides the energy to align and rotate the magnetic microparticle chains within the droplets.
    *   Value: Field parameters (B, f, θ) define the input, e.g., B = 8 mT, f = 3 Hz, θ = 30°.
    *   Units: mT, Hz, degrees.

### **2.2 Energy Transduction**

    *   Content: 1. **Electromagnetic to Mechanical (Particle Scale):** The external magnetic field exerts torques on the magnetic microparticles, causing them to align and assemble into chains that rotate synchronously with the field. 2. **Mechanical (Particle) to Mechanical (Droplet):** Friction between the rotating particle chains and the droplet's internal boundary transduces the rotational motion to the entire droplet. 3. **Mechanical (Droplet Rotation) to Kinetic (Fluid):** The rotating droplets induce fluid flow (vortices) in the surrounding benzyl ether due to viscous coupling. 4. **Potential (Magnetic Field) to Potential (Inter-droplet):** The external field induces magnetic dipole moments in the droplets, leading to tunable magnetic dipole-dipole interaction forces (potential energy) between them. Hydrodynamic interactions (kinetic energy exchange via fluid) and capillary interactions (surface tension energy) also mediate energy exchange between droplets.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify energy efficiency. The system is described as dissipative ("energy-dissipative structures," "dissipative self-assembly process"). Energy is continuously supplied by the magnetic field and dissipated primarily through viscous friction within the droplets, between droplets and the surrounding fluid, and through hydrodynamic interactions. The primary goal is pattern formation and manipulation, not efficient energy conversion. Efficiency is likely very low as most energy input maintains rotation against viscous drag and drives fluid flow rather than performing useful work (like lifting cargo against gravity, which isn't the case here). Qualitative Assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: Energy is primarily dissipated through: 1. **Viscous drag:** Between rotating particle chains and the fluid inside the droplet. 2. **Viscous drag:** Between the rotating droplets and the surrounding benzyl ether fluid. 3. **Hydrodynamic interactions:** Energy transferred to the fluid creating flows and vortices (Fig 4a, 4c), ultimately dissipated as heat. This is explicitly mentioned as the mechanism through which energy supplied by the external field is dissipated. Quantification: The paper calculates Reynolds number (Re ≈ 0.1-1) and shear stress (≈ 0.02 Pa, Fig 4f), which are related to viscous dissipation, but doesn't provide total dissipation rates or power loss. Qualitative Assessment: High (as it's a defining feature of the dynamic self-assembly).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local interactions governing the self-organization are:
        1.  **Magnetic Dipole-Dipole Interaction (Fm):** Time-averaged force between two droplets (i, j) with induced dipole moment 'm', separated by distance 'r', under a precessing field with angle 'θ'. Governed by Eq. 2: `Fm(θ, r) = -3μ₀m²(3cos²θ - 1) / (4πr⁴ * 2)`. This force is repulsive for 0° < θ < 54.7° and attractive for 54.7° < θ < 90°. 'm' depends on particle properties, concentration, and external field B.
        2.  **Hydrodynamic Interaction (Fr):** Repulsive force arising from fluid inertia effects between rotating droplets (i, j) with radius 'a_d', angular velocity 'ω_d', in a fluid of density 'ρ_f'. Estimated by Eq. 3: `Fr = -c·ρ_f·ω_d²·a_d⁷(ri - rj) / |ri - rj|⁴`, where 'c' is a constant. This force scales with ω_d² and inversely with a power of distance 'r'. ω_d itself depends on field frequency 'f' and angle 'θ' (Fig 3a). Operates in the non-zero Reynolds number regime (Re ≈ 0.1-1).
        3.  **Capillary Interaction (Fc):** Attractive force between droplets at the air-liquid interface due to interface deformation. Governed by Eq. 1: `Fc(r) ∝ exp(-r/Lc)`, where Lc is the capillary length `sqrt(γ / (ρg))`, involving surface tension 'γ', fluid density 'ρ', and gravity 'g'. This force becomes significant when droplets are close.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Eq. 2 | Magnetic Dipole-Dipole | θ (Precession Angle) | 0 - 90+ | degrees | Text, Eq. 2, Fig 3 | Explicit | Angle controlling force sign/magnitude. |
    | Eq. 2 | Magnetic Dipole-Dipole | r (Separation Distance) | ~2 - 6+ | mm | Text, Eq. 2, Fig 3c | Explicit | Distance between droplet centers. |
    | Eq. 2 | Magnetic Dipole-Dipole | m (Dipole Moment) | Not specified | A·m² | Eq. 2 | Implicit | Value depends on B, particles, etc., not explicitly given. |
    | Eq. 3 | Hydrodynamic Interaction | ω_d (Droplet Ang. Velocity) | ~0 - 12π (approx) | rad/s | Fig 3a | Explicit | Droplet rotation speed. |
    | Eq. 3 | Hydrodynamic Interaction | r (Separation Distance) | ~2 - 6+ | mm | Text, Eq. 3, Fig 3c | Explicit | Distance between droplet centers. |
    | Eq. 3 | Hydrodynamic Interaction | a_d (Droplet Radius) | ~1 | mm | Fig 1b, Fig 4 | Explicit | Radius of the droplet. |
    | Eq. 1 | Capillary Interaction | r (Separation Distance) | ~2 - few mm | mm | Text, Eq. 1, Fig 2c | Explicit | Becomes dominant at close range. |
    | Eq. 1 | Capillary Interaction | Lc (Capillary Length) | Not specified; depends on fluids | mm | Eq. 1 | Implicit | Material property dependent length scale. |

### **4.3 Global Order:**

    *   Content: The emergent global order consists of stable, dynamically rotating patterns formed by the self-assembled droplets. Specific geometries are observed depending on the number of droplets (e.g., a pentagonal structure for N=5, denser packing for N>5). The entire pattern also precesses as an entity (Fig 2a, 4e). The order is characterized by the specific geometric arrangement and the synchronized rotation/precession.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Eq. 2 | Magnetic dipole-dipole force | θ | 0-90+ | degrees | Explicit | Controls sign and magnitude | Text, Eq. 2 |
| Eq. 2 | Magnetic dipole-dipole force | r | ~2-6+ | mm | Explicit | Distance dependence (r⁻⁴) | Text, Eq. 2 |
| Eq. 3 | Hydrodynamic repulsive force | ω_d | ~0-12π | rad/s | Explicit | Scales with ω_d² | Text, Eq. 3, Fig 3a |
| Eq. 3 | Hydrodynamic repulsive force | r | ~2-6+ | mm | Explicit | Distance dependence (r⁻⁴ approx) | Text, Eq. 3 |
| Eq. 1 | Capillary attractive force | r | ~2 - few | mm | Explicit | Exponential decay with distance | Text, Eq. 1 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| GEO-1 | Geometric Arrangement | N (Number of droplets) | 4, 5, 6, 7... | - | Explicit | Determines pattern (e.g., pentagon for N=5) | Visual Inspection | Fig 2b |
| GEO-2 | Pattern Size | r (Mean separation distance) | ~2.5 - 5 | mm | Explicit | Quantifies expansion/shrinkage | Image Analysis (mean of 5 distances) | Fig 3c |
| DYN-1 | Pattern Precession | Ω (Precession ang. velocity) | ~0 - 1.5 | rad/s | Explicit | Collective rotation of the pattern | Tracking droplet positions | Fig 4e |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | Local Interaction -> Global Pattern | Balance of magnetic, hydrodynamic, capillary forces determines droplet positions and pattern geometry. | High (See 4.4) | 6 | Mean Separation Distance 'r' vs Input Field Parameters (θ, f, B). Global Precession Ω vs Local Rotation ω_d. | Explicit (Relationships shown) | The global pattern state (geometry 'r', dynamics 'Ω') is a predictable function of the local interactions, which are tuned by global field parameters. Local rules reliably map to global order. Score=6: Predictable structure forms from local rules, but formal embedding/functorial relationship isn't mathematically proven in CT terms. | Fig 3c, 4e |
    *   **Metrics:** Functional relationships presented graphically: Mean separation distance `r = f(θ, f, B)` (Fig 3c, 3d), Pattern precession velocity `Ω = g(ω_d)` where `ω_d = h(θ, f)` (Fig 3a, 4e).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Particle Chain Assembly | Fast (not quantified) | s (?) | Fig 1b | Implicit | Described as happening "rapidly after turning on the field". |
        | Pattern Formation | < 120 | s | Fig 2a, Text | Explicit | Time for N=5 droplets to form an ordered pattern from a random-ish start. |
        | Droplet Rotation Period (1/ω_d) | ~0.17 - infinity (derived) | s | Fig 3a | Explicit | Inverse of angular velocity ω_d (e.g. 6π rad/s ≈ 3 Hz -> 0.33s; 2π rad/s -> 1s). |
        | Field Oscillation Period (1/f) | 0.25, 0.33, 0.5 | s | Fig 3 | Explicit | Inverse of applied field frequency. |
        | Pattern Expansion/Shrinkage | ~10s of seconds | s | Fig 3b (visual est.), Video S3 | Implicit | Time taken for pattern size to adjust after changing θ (estimated from figure/video reference). |
        | Pattern Precession Period (1/Ω) | ~4 - infinity (derived) | s | Fig 4e | Explicit | Inverse of pattern precession angular velocity Ω. |
        | Relaxation (Field Off) | ~90+ | s | Fig 2c | Explicit | Time for droplets to drift together via capillary forces after field removal. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: 1. **Dynamic Self-Assembly & Pattern Formation:** Multiple rotating magnetic droplets spontaneously arrange into stable, ordered geometric patterns (e.g., pentagons) at the air-liquid interface under a precessing magnetic field due to balanced local interactions. 2. **Reconfigurable Pattern Control:** The size (mean separation distance) of the formed pattern can be reversibly tuned (expanded or shrunk) by modulating the external magnetic field parameters (precession angle θ, frequency f, strength B). 3. **Collective Rotation/Precession:** The entire assembled pattern rotates (precesses) around its center as a rigid entity, with a precession velocity linked to the individual droplet rotation speed. 4. **Collective Steering:** The entire assembled pattern can be translated (steered) across the air-liquid interface as a single unit by applying an external magnetic field gradient. Automated guidance along a preset path is demonstrated. 5. **Non-Contact Cargo Manipulation:** Utilizing the reconfigurable pattern control (expansion/shrinkage) and collective steering, the droplet assembly can trap (encage), transport, and release macroscopic cargoes (~1.5-1.7 mm) at the interface without direct contact, acting as a robotic end-effector. Trapping is suggested to be aided by induced circular fluid flow.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Emergent behaviors are primarily validated through experimental observation and quantitative measurements documented in figures and videos (referenced):
        *   **Pattern Formation/Control:** Visual confirmation of ordered structures (Fig 2a,b). Quantitative measurement of mean separation distance 'r' as a function of field parameters (θ, f, B) confirms tunable emergence (Fig 3c,d). Repeatability shown via reversible expansion/shrinkage (Fig 3b). Control experiments implicitly include varying parameters and observing predictable outcomes.
        *   **Collective Dynamics:** Measurement of pattern precession velocity (Ω) vs droplet angular velocity (ω_d) validates collective rotation (Fig 4e). Steering velocity vs field gradient measured quantitatively (Fig 5a). Automated trajectory following validates controlled steering (Fig 5b).
        *   **Cargo Manipulation:** Demonstrated through sequential images/videos showing trapping, transport, and release (Fig 5c,d). Finite element simulations support the proposed mechanism for non-contact trapping via induced fluid flow (Fig 4c,f).
        *   **Limitations:** Robustness testing is limited (see M8.2). Statistical analysis of pattern formation reliability/repeatability across many trials is not presented. The precise role and balance of all three forces (magnetic, hydrodynamic, capillary) in determining specific pattern geometries are analyzed qualitatively/via simulation but perhaps not exhaustively proven experimentally for all cases.

---

#Key: [loeffler_neuromorphic_2023]

# Neuromorphic learning, working memory, and metaplasticity in nanowire networks

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of a physical Nanowire Network (NWN) made of self-assembled silver nanowires coated with PVP, drop-cast onto a substrate with electrodes (experimental) or a simulated counterpart based on a graph model. Nanowire-nanowire junctions act as memristive switches. The system functions as a neuromorphic device, implemented here to perform cognitive tasks inspired by the n-back working memory (WM) test and binary classification. Input patterns (representing stimuli) are applied as voltages to source electrodes, and output currents are read from drain electrodes. The system's purpose is to emulate and study brain-like supervised learning, reinforcement learning (Physical Reinforcement Learning - PRL), working memory, and synaptic metaplasticity within a physical neuromorphic substrate. Components are nanowires, memristive junctions, electrodes, and external control/measurement hardware/software (including feedback mechanisms for learning).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Experimental voltages ranged 0.1-0.2V (Methods). Training/Testing times were also specified (e.g., 2s/sample sim, up to 15s exp training).

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is electrical, provided by external voltage sources (NI-DAQ card mentioned) applied to the source electrodes of the NWN device.
    *   Value: 0.1 - 0.3
    *   Units: V

### **2.2 Energy Transduction**

    *   Content: Electrical energy input is transduced within the NWN. 1) Electrical energy drives ion movement (electrochemical metallization) in the memristive junctions when voltage exceeds thresholds (Vset, Vreset), causing changes in the conductive filament structure (state variable λ) and thus modulating the junction's electrical conductance (Gj). 2) This change in conductance alters the electrical energy flow (current pathways) through the network for subsequent inputs. 3) Electrical energy is dissipated as heat (Joule heating) due to current flow through the resistive nanowires and junctions.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify energy efficiency for computation or memory operations. However, memristive systems, especially those relying on ion migration and Joule heating effects (implicit), are generally energy-intensive compared to CMOS logic for switching, although potentially efficient for *in-memory* computation by avoiding data movement. Given the lack of data and the resistive nature, efficiency is assumed to be very low for the switching process itself relative to idealized computation. No specific efficiency metrics (e.g., J/operation) are provided. Assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: Energy is primarily dissipated as heat (Joule heating) as current flows through the resistances of the nanowires and the junctions. The memristive switching process itself (filament formation/rupture) involves overcoming energy barriers and is inherently dissipative. Stochastic thermodynamic breakdown contributing to filament decay (parameter 'b' in Eq. 1) also implies energy dissipation pathways. The paper mentions damage susceptibility due to Joule heating in Ag-PVP wires (Discussion). Quantification is not provided. Assessment: High.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding with M3.2-M3.8)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Seconds to Hours (Qualitative Range)
*    Units: s / hours

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: ~0.4 to 1.0 (Task dependent)
*   Units: % or ratio (Accuracy)

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Governed by 'b' in simulations; Qualitative decay start ~35-90s experimentally.

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", proceeding with M5.2-M5.4)**

### **5.2 Computation Type:**

    *   Content: Neuromorphic / Reservoir Computing / Analog / Hybrid

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operation performed by the material is the **non-linear, history-dependent transformation of input voltage patterns into output current patterns via modulation of junction conductances**. This involves: 1) Threshold-driven memristive switching (conductance change based on voltage history, Eq. 1). 2) Integration of signals through the interconnected network (summation of currents according to Kirchhoff's laws, influenced by pathway conductances). 3) Temporal integration due to the memory effect (current state depends on past states). The reservoir computing paradigm suggests the network performs a high-dimensional non-linear mapping of inputs.
    *   **Sub-Type (if applicable):** Non-linear Temporal Signal Transformation / Physical Reservoir Mapping.

### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Experimental Acquisition Rate | 1 | kHz | Methods | Explicit | Rate at which data was sampled. |
        | Simulation Time Step (Δt) | 0.01 | s | Methods | Explicit | Integration step for simulation. |
        | Simulation Sample Duration | 2 | s | Methods | Explicit | Duration voltage applied per sample in sim. |
        | Experimental Training Time (Max) | 15 | s | Methods | Explicit | Max time per training sample in exp. |
        | Experimental Testing Time (Typical) | 6-7 | s | Methods | Explicit | Duration of testing phase per epoch in exp. |
        | Memory Decay Start (Exp, cited) | 35-90 | s | Methods | Explicit (Cited) | Timescale over which memory begins to fade without reinforcement. |
        | Long-Term Retention (Exp, cited) | Hours / 24 Hours | hours | Methods/Discussion | Explicit (Cited) | Possible retention timescale with reinforcement/tuning. |
        | Experimental Rest Period | 3 / 5 | hours / s | Methods | Explicit | Wait time between trials or samples. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", proceeding with M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism is analogous to synaptic plasticity, specifically involving changes in the conductance (Gj) of memristive nanowire junctions. Physically, this corresponds to the formation, dissolution, or modification of conductive Ag filaments within the insulating PVP matrix, driven by ion migration under electrical fields exceeding Vset or Vreset (Eq. 1). Two specific learning rules are implemented:
        1.  **Supervised Learning:** A gradient descent-like "nudging" algorithm (Eq. 2-3) adjusts drain electrode voltages (Vo) based on the discrepancy between the actual output current (y_target) and a desired target current (d_target). This voltage change indirectly influences junction voltages, driving conductance changes to reduce the error.
        2.  **Physical Reinforcement Learning (PRL):** After testing, the target current threshold (θ) for the *next* epoch is adjusted based on performance (accuracy Acc vs. threshold Acc_θ). If unsuccessful (Acc < Acc_θ), θ_target is increased (reward/reinforcement), and θ_nontarget is decreased (penalty/pruning), effectively biasing future supervised learning and strengthening desired pathways while weakening others. Metaplasticity is observed as the effect of PRL depends on the history of junction modifications (Fig S5, Discussion).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors demonstrated are:
        1.  **Binary Classification:** Distinguishing between two input patterns (2x2 or 3x3 grids) based on output currents at designated drain electrodes (Tasks 1 & 2).
        2.  **Working Memory (Sequence Recall):** Performing an n-back task, identifying if a current input pattern matches one presented n-steps prior (up to n=7 demonstrated in Task 3).
        3.  **Learning:** Improving performance on classification and WM tasks over time (epochs) through supervised learning and PRL.
        4.  **Memory Consolidation:** Maintaining learned information over longer periods (resisting decay) through PRL, involving pathway strengthening and pruning (Task 3, Fig 5).
        5.  **Metaplasticity:** The history dependence of junction plasticity, where prior modifications influence subsequent changes and memory retention (Task 3 simulation results without PRL, Fig S5).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors (learning, WM, consolidation, metaplasticity) are validated through:
        1.  **Quantitative Metrics:** Classification accuracy and recall accuracy are measured and plotted against time (epochs) or task parameter (n) for both experiment and simulation (Figs 2, 3, 4).
        2.  **Control Conditions:** Performance is compared with and without PRL ('reinforcement' vs. 'no reinforcement') to isolate the effect of the learning mechanism (Figs 2, 3, 4). Chance accuracy levels are shown for comparison.
        3.  **Simulations:** A computational model (Methods, Eq. 1) is used to simulate the network dynamics and learning processes. Simulation results are compared with experimental data (Figs 2, 3, 4). Connectivity maps and conductance histograms from simulations (Fig 5, Figs S2, S6, S7) are used to visualize and understand the underlying mechanisms (pathway formation, strengthening, pruning).
        4.  **Parameter Variation (Simulation):** The effect of parameters like the junction decay rate 'b' is explored in simulations to support interpretations (e.g., relating decay to WM limits and metaplasticity, Fig S5).
        Reproducibility is addressed by averaging over multiple epochs or trials (e.g., N=50 epochs in Task 1, N=10 trials per n in Task 2). Limitations include differences between experimental and simulation timescales/complexity and device heterogeneity.

---

#Key: [pervan_algorithmic_2020]

# Algorithmic Design for Embodied Intelligence in Synthetic Cells

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a methodology for algorithmically designing robotic systems, specifically demonstrated on simulated synthetic cells, to achieve embodied intelligence. It aims to encode task information into the physical morphology (sensor-actuator arrangement) rather than relying solely on centralized computation. The components include: 1) A simulated synthetic cell model operating in a 2D chemical environment with defined dynamics. 2) A library of discrete sensors (chemical comparators) and actuators (attraction to chemical sources). 3) An optimization algorithm based on hybrid optimal control (Mode Insertion Gradient - MIG) and information theory (graph entropy for complexity, Kullback-Leibler divergence for task embodiment). The algorithm generates a control policy (a finite state machine mapping sensor states to actuator modes) that minimizes design complexity while maximizing task performance (approaching a target point P). The methodology involves generating an idealized policy and projecting it onto the available discrete sensor/actuator sets. Two design flows are explored: optimizing actuators first then projecting onto sensors, or optimizing sensors first then projecting onto actuators. The purpose is to create computationally limited robots (like micro-scale synthetic cells) that can perform tasks effectively by leveraging their physical design.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Units for v_max and cost weights are not explicitly given but inferred from context. Reliability is High as these are parameters defined for the simulation.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The paper does not explicitly discuss physical energy input/consumption for the synthetic cell. The primary "input" driving the system design process is computational effort for the optimization algorithm. For the simulated cell's *dynamics*, the energy input is implicitly the potential energy landscape created by the switchable chemical sources. The system requires external computational resources to determine the design.

### **2.2 Energy Transduction**

    *   Content: Energy transduction in the physical sense is not addressed. In the computational/design aspect, the algorithm transduces the objective function (cost, complexity, embodiment) into a specific sensor-actuator configuration (the control policy/FSM). In the simulation dynamics, the selection of an actuator mode (chemical source) transduces the potential energy field into kinetic energy (movement) of the cell according to Eq. 1. The control policy itself acts as a transducer, mapping perceived state (sensor readings) to action (actuator mode selection).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Physical energy efficiency is not discussed or quantified. Computational efficiency is mentioned in the context of related work ([23]) but not measured for the proposed algorithm itself. The goal is low *design complexity*, which might correlate with lower energy requirements in a physical realization, but this is not quantified. Task performance (low K-L divergence, distance to target) could be seen as a form of 'task efficiency', but not energy efficiency.

### **2.4 Energy Dissipation**

    *   Content: Physical energy dissipation mechanisms (like fluid drag in a real chemical bath) are not explicitly modeled or discussed, although the velocity limit mimics terminal velocity which implies dissipation. Computational energy dissipation during the design phase is not quantified. Within the simulation, non-optimal paths or oscillations could be considered analogous to wasted effort or energy dissipation relative to the task goal, but this is not framed in terms of physical energy loss.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Infinite (within simulation context)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Related to Design Complexity / Graph Entropy (e.g., h=0 to h=7.6 in examples)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: 100% (within simulation logic)
*   Units: %

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: 0

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: Skipped M4.2-M4.7 as M4.1 is "No")**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Hybrid (Physical implementation of logical/mapping function)

### **5.3 Computational Primitive:**

    *   Content: State/Region Classification and Action Selection. The most basic operation performed by the embodied structure is determining which region of the state space the robot currently occupies (based on sensor readings/comparisons) and outputting the corresponding pre-defined control mode (actuator selection). This corresponds to a transition in the underlying Finite State Machine (FSM) based on state observation. In the projection step (Sec VI, Alg 2), the primitive involves evaluating costs for different actions within a sensor region to assign the best discrete action.
    *   **Sub-Type (if applicable):**Lookup/Mapping (FSM transition)

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Simulation Time Step (t_s) | 0.02 | s | Sec III | Explicit | Time increment for dynamics integration. |
        | Prediction Horizon (T) | 0.1 | s | Sec III | Explicit | Look-ahead time used in cost calculation (MPC context mentioned in Sec IV). |
        | Total Simulation Time (t_f) | 5 | s | Sec III | Explicit | Maximum duration of a single simulation run. |
        | Control Policy Update Rate | 1 / t_s = 50 | Hz | Implicit | Inferred from time step; control is potentially updated each step. |
    *   **Note:** Other relevant timescales, like sensor response time or actuator switching time, are not specified for the simulated system.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: Skipped M7.2 as M7.1 is "No")**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is goal-directed locomotion. The simulated synthetic cell attempts to navigate from a starting position to a desired target point (P) in a 2D state space containing chemical sources that influence its movement. The behavior is governed by the implemented control policy, which selects attraction towards specific chemical sources based on the cell's current state/location.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The goal-directed locomotion behavior is validated through computational simulation. 1000 Monte Carlo simulations with random initial conditions are performed for each designed control policy (Sec V-D, Fig 5d, Fig 12d). Performance is quantified by the average final distance from the desired point P (Fig 5f, Fig 12f) and the Kullback-Leibler (K-L) divergence between the trajectory distribution of the designed robot and an idealized system (Eq. 6, Fig 5e, Fig 12e). Low K-L divergence indicates the behavior closely matches the ideal task execution. No validation through physical experiments is presented. The behavior is designed/optimized, not strictly emergent in the sense of arising unexpectedly from local rules.

---

#Key: [kuncic_emergent_2018]

# Emergent brain-like complexity from nanowire atomic switch networks: Towards neuromorphic synthetic intelligence

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a computational model simulating experimental self-assembled silver nanowire (AgNW) atomic switch networks. It aims to emulate brain-like complexity, specifically nonlinear stochastic dynamics, long-term memory, and scale-invariant fluctuations (1/f noise). The components are simulated AgNWs scattered randomly on a plane, forming junctions where atomic switches are modeled. These switches represent conductive Ag atomic bridges whose formation/dissolution depends on applied voltage and time, causing significant resistance changes (~2 orders of magnitude). The network is abstracted as a graph (wires=vertices, switches=edges) and analyzed using Kirchhoff's laws to determine overall network conductance between two contact points under applied voltage bias. The purpose is to explore emergent brain-like features in these neuromorphic networks computationally.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name             | Value             | Units   | Source (Fig/Table/Section)   | Implicit/Explicit   | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit)   |
        | :------------------------- | :---------------: | :-----: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters listed are key inputs or characteristics mentioned. Specific parameters governing switch dynamics (threshold voltage, growth rate constants) or network geometry (density, exact gamma parameters) are not provided. Reliability is 'Medium' as these are simulation inputs/outputs, potentially based on experimental data not detailed here.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is an external electrical voltage source applying bias across two contact points on the network. Specific input signals include sequences of rectangular pulses and DC bias.
    *   Value: 1.5, 2.0, 0.01 (Examples given)
    *   Units: V (Volts)

### **2.2 Energy Transduction**

    *   Content: Electrical energy input drives the atomic switch dynamics. The applied voltage bias (electrical potential energy) causes the formation (growth) or dissolution of a conductive silver (Ag) atomic bridge within the nanoscale junction. This is likely an electrochemical process driven by the electric field/potential difference. The formation/dissolution of the bridge changes the local junction resistance, thereby altering the flow of electrical current (kinetic energy of electrons) through the network paths. Energy is dissipated as heat (Joule heating) due to current flow through resistive elements (nanowires and switches).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss energy efficiency. While atomic switches are noted for low power consumption (Introduction), no quantitative metrics or analysis of efficiency (e.g., useful work done vs. energy input) are provided for the network operation or the simulated phenomena (memory, computation). Assessing efficiency is not possible based on the text.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation primarily occurs as Joule heating due to electrical current flowing through the resistive components of the network (nanowires and switches). Figure 2 explicitly indicates Joule dissipation visually via colorbar. The paper does not quantify the amount or rate of dissipation. Qualitatively, dissipation would be higher in lower resistance (ON) states where current flow is significant. Energy may also be dissipated during the ionic movement associated with bridge formation/dissolution, but this is not discussed. Assessment: Medium/High during ON states, Low during OFF states.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding to M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: > 10 s (Qualitative: Long-term)
*    Units: s (seconds)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Qualitative: Gradual decline observed after t=20s.
    *   Units: Conductance units / time (e.g., S/s)

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", proceeding to M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content:
        1.  **Switch Dynamics:** At each wire intersection (junction), an atomic switch model governs the formation/dissolution of a conductive Ag bridge. The bridge growth rate is proportional to the differential voltage bias across the junction relative to a minimum threshold. Once the bridge exceeds a critical length, the switch turns "ON" (low resistance); otherwise, it's "OFF" (high resistance). Dissolution occurs when the bias is removed or potentially reversed (implicit). The specific equations are not provided ("simplified mathematical description").
        2.  **Electrical Conduction:** Current flow through the network adheres to Kirchhoff's circuit laws at each junction (sum of currents in = sum of currents out) and Ohm's law across each component (nanowire segments, switches with state-dependent resistance).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description             | Parameter Name          | Parameter Value Range  | Units   | Data Source   | Implicit/Explicit   | Justification                                      |
    | :------ | :---------------------- | :---------------------- | :--------------------: | :-----: | :-----------: | :----------------: | :------------------------------------------------- |

### **4.3 Global Order:**

    *   Content: The emergent global order includes:
        1.  **Conductive State:** The overall network conductance between the two probes, which can switch between a highly resistive ("OFF") state and a highly conductive ("ON") state based on the formation of percolating pathways of ON switches.
        2.  **Memory State:** The persistence of the ON conductive state after stimulus removal.
        3.  **Dynamic Pattern:** The scale-invariant (1/f noise) fluctuations observed in the conductance time series, reflecting the collective, stochastic switching dynamics across the network.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description        | Parameter             | Value Range           | Units   | Implicit/Explicit   | Justification                                    | Source     |
| :------ | :----------------- | :-------------------- | :-------------------- | :-----: | :----------------: | :----------------------------------------------- | :--------- |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description                 | Parameter          | Value Range             | Units    | Implicit/Explicit   | Justification                                    | Protocol                | Source     |
| :---------- | :-------------------------- | :----------------- | :---------------------- | :------: | :----------------: | :----------------------------------------------- | :---------------------- | :--------- |
| 1           | Network Conductance         | Conductance        | Varies (Fig 3)          | S        | Explicit          | Measured output of the simulation                | Simulated double-probe | Fig 3      |
| 2           | Memory State Persistence    | Retention Time     | > 10 (Qual: Long-Term)  | s        | Mixed             | Explicitly shown >10s, "much longer" described   | Post-Stimulus Measurement | Section III|
| 3           | Dynamic Fluctuations        | PSD Slope (alpha)  | approx. 1 (for 1/f^alpha) | Unitless | Explicit          | 1/f shown in Fig 3, mentioned in text            | PSD Analysis of Conductance | Fig 3      |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", proceeding to M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Neuromorphic/Analog

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operation performed by the material is the voltage-dependent resistance switching at the atomic switch junctions. This behaves like a memristive element, where the resistance state depends on the history of applied voltage (specifically, the time integral of voltage above a threshold, controlling bridge growth/dissolution). At the network level, the collective action of these primitives leads to emergent computation like signal integration (implied by memory formation) and potentially pattern recognition or learning (alluded to via STDP analogy).
    *   **Sub-Type (if applicable):** Memristive Switching / Threshold Activation

### **5.4 Embodied Computational Units**
| Unit ID | Description        | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification                                         |
| :------ | :----------------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :---------------------------------------------------- |

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description       | Value                   | Units       | Source        | Implicit/Explicit   | Justification                                         |
        | :-------------------------- | :---------------------: | :---------: | :-----------: | :----------------: | :---------------------------------------------------- |
        | Input Pulse Duration        | ~ms to s ?              | s           | Fig 3         | Implicit          | Pulses shown, ~1s implied duration scale.             |
        | Inter-Pulse Interval        | ~ms to s ?              | s           | Fig 3         | Implicit          | Variable intervals shown, ~1s implied scale.          |
        | Switch ON Time              | Fast (not quantified)   | s           | Fig 3 / Text  | Mixed             | Rapid conductance jump implies fast switching.        |
        | Switch OFF Time (Decay)     | > 10                    | s           | Fig 3 / Text  | Mixed             | Gradual decay observed over seconds.                 |
        | Memory Retention Time       | > 10 (Qual: Long-term)  | s           | Section III   | Mixed             | Retains state for "much longer times" than stimulus. |
        | Dynamic Fluctuation Scale   | Sub-second to seconds   | s           | Fig 3 PSD     | Implicit          | PSD spans frequencies corresponding to these times.    |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", proceeding to M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The proposed mechanism for adaptation relates to the timing of input pulses influencing the collective state of the atomic switches. The paper suggests an analogy to Spike-Timing-Dependent Plasticity (STDP). This implies that the relative timing between voltage pulses arriving at different parts of the network (or successive pulses at the input) could strengthen or weaken conductive pathways by differentially affecting the formation or dissolution rates of Ag bridges at various junctions. The "differential delays in switches closing" is cited as responsible for memory retention, suggesting time-dependent dynamics are key. However, the specific rules governing this timing-dependent plasticity (e.g., equations analogous to STDP learning windows) are not provided. The adaptation manifests as changes in the network's overall conductance pathways and memory state.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors described are:
        1.  **Non-volatile Memory:** Retention of a high conductance state after removal of the writing stimulus.
        2.  **Nonlinear Switching:** Abrupt changes in network conductance in response to input voltage.
        3.  **Stochastic Dynamics:** Fractal-like, fluctuating conductance changes over time.
        4.  **Scale-Invariance:** Power spectral density (PSD) of conductance fluctuations exhibiting 1/f noise characteristics, similar to brain measurements.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claims of emergent behaviors (memory, nonlinear/stochastic/scale-invariant dynamics) are validated through computational modeling. The model simulates the network based on physical principles (random wire assembly, voltage-driven switch dynamics, Kirchhoff's laws). The simulation results (e.g., conductance time series in Fig 3, PSD in Fig 3) demonstrate these behaviors. The model is stated to capture key aspects of experimental AgNW networks (referencing [5]), implying consistency with experimental observations, although direct experimental validation data is not presented within this paper. Limitations include the use of a "simplified" switch model and the inherent limitations of simulation vs. real-world experiment.

---

#Key: [march-pons_consensus_2024]

# Consensus formation and relative stimulus perception in quality-sensitive, interdependent agent systems

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is an agent-based model (the LES model) inspired by honeybee swarm decision-making for nest-site selection. It analyzes how a group of N agents (bees) reaches consensus on the best option among k potential sites, each characterized by intrinsic quality (qα) and discovery probability (πα). Agents can be uncommitted (state 0) or committed to a site α (state α). Transitions involve: (1) Commitment: Uncommitted agents discover sites either independently (probability (1-λ)πα) or through social interaction (probability λfα,t, where fα,t is the fraction committed to α). (2) Uncommitment: Committed agents return to the uncommitted state at a rate rα, which depends on site quality qα (higher quality means lower rate/longer commitment) and a parameter μ (modeling independent quality assessment). The model's purpose is to study consensus formation, the influence of interdependence (λ) vs. individual exploration (πα), compliance with Weber's law, and critical behavior in the context of collective decision-making.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
    *   **Note:** These are the core parameters defining the model's structure and dynamics. Values are often explored across ranges or specific examples are given in figures/simulations. Reliability is high as these are defined model parameters.

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip to Module 4.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content:
        1.  **State Update:** Agents are either uncommitted (sᵢ=0) or committed (sᵢ=α). Direct switching between committed states is forbidden (Sec II, Fig 1).
        2.  **Commitment Rule (sᵢ(t)=0 → sᵢ(t+1)=α):** Occurs with probability pα,t+1 = (1-λ)πα + λfα,t. This depends on the agent's independent discovery probability πα for site α, the global interdependence parameter λ, and the current fraction fα,t = (1/N)∑ⱼ δ(sⱼ(t), α) of agents committed to α (Eq. 2). (Note: fα,t represents local information in the mean-field sense, or neighbours' states in lattice simulations).
        3.  **Uncommitment Rule (sᵢ(t)=α → sᵢ(t+1)=0):** Occurs with probability rα. This rate depends on the site quality qα and the quality assessment parameter μ: rα = q₀/qα^(1-μ) * (μ/K + (1-μ)/qα), approximated as rα = q₀/qα for μ=0 (Eq. 3). Higher quality qα leads to lower rα (longer commitment).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | 2 | Commitment Probability | λ | [0, 1] | Dimensionless | Sec II, Eq. 2 | Explicit | Defines balance between social/individual influence |
    | 2 | Commitment Probability | πα | ≥0, Σπα≤1 | Dimensionless | Sec II, Eq. 2 | Explicit | Independent discovery likelihood |
    | 3 | Uncommitment Probability | qα | ≥0 | Arbitrary/Time⁻¹ | Sec II, Eq. 3 | Explicit | Site quality, determines rα |
    | 3 | Uncommitment Probability | μ | [0, 1] | Dimensionless | Sec II, Eq. 3 | Explicit | Modulates quality sensitivity in rα |

### **4.3 Global Order:**

    *   Content: The primary global order that emerges is **consensus**, typically defined as a stationary state where a significant fraction of the population (fα*) is committed to one option (usually the best quality one, fₖ*), often exceeding a threshold relative to other options or the uncommitted population (e.g., Qₓ = fₖ* - x fⱼ* > 0 or Q'ₓ = fₖ* - max(f₀*, x fⱼ*) > 0, as defined in Sec III.A, Eqs 10, 11). Other global states include multi-opinion states (no clear consensus) or the absorbing uncommitted state (f₀*=1).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| 2 | Social Influence | λ | [0, 1] | Dimensionless | Explicit | Controls impact of fα,t on commitment | Sec II, Eq. 2 |
| 2 | Individual Discovery | πα | ≥0, Σπα≤1 | Dimensionless | Explicit | Probability of finding site α alone | Sec II, Eq. 2 |
| 3 | Quality Sensitivity | qα (via rα) | ≥0 | Arbitrary/Time⁻¹ | Explicit | Higher quality -> lower rα -> longer commitment | Sec II, Eq. 3 |
| 3 | Assessment Type | μ | [0, 1] | Dimensionless | Explicit | Modifies how qα affects rα | Sec II, Eq. 3 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Consensus1 | Fraction committed to site α | fα* | [0, 1] | Dimensionless | Explicit | Stationary fraction of agents | Solve f˙α=0 (Eq. 4-6) | Sec II.A, III.A |
| Consensus2 | Best Option Lead (LES metric) | Qₓ = f<0xE2><0x82><0x96>* - x fⱼ* | Approx [-1, 1] | Dimensionless | Explicit | Difference between top two options (scaled) | Eq. 10 | Sec III.A |
| Consensus3 | Best Option Quorum (Modified LES) | Q'ₓ = f<0xE2><0x82><0x96>* - max(f₀*, x fⱼ*) | Approx [-1, 1] | Dimensionless | Explicit | Compares best option to uncommitted or next best | Eq. 11 | Sec III.A |
| Activity | Fraction of active agents | ρ = 1 - f₀* | [0, 1] | Dimensionless | Explicit | Order parameter for absorbing transition | Eq. 13-14 | Sec III.E |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    *   **Justification**: Category Theory and Yoneda embedding are not mentioned.

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Collective

### **5.3 Computational Primitive:**

    *   Content: Weighted Information Integration / Selection. The core computation involves agents integrating information from two sources weighted by λ: independent assessment (πα, qα via rα) and social influence (fα,t). This integrated information drives transitions between states, ultimately leading to the selection of a dominant state (consensus) based on maximizing positive feedback for higher-quality options (longer commitment 1/rα reinforces higher fα,t, which further increases recruitment if λ>0). It can be seen as a distributed, noisy implementation of a "best-of-N" selection or quality-sensitive voting.
    *   **Sub-Type (if applicable):** Quality-Weighted Voting/Selection

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Simulation Step | 1 (implicit) | Time step | Sec II | Implicit | Discrete time steps assumed |
        | Commitment Duration (Mean) | 1/rα = f(qα, μ) | Time steps | Sec II, Eq. 3 | Explicit | Inverse of uncommitment rate |
        | Convergence Time (to Stationary State) | t_ss | Time steps | Sec III.A (Fig 4c,f), III.B (Fig 6d,e) | Explicit | Measured in simulations |
        | Relaxation Time (Perturbation Decay) | τ | Time steps | Appendix A1 (Fig 13) | Explicit | Calculated from LSA eigenvalues |
    *   **Note:** The paper operates in discrete time steps. Key dynamics are the time agents stay committed (1/rα, depending on quality) and the time the whole system takes to settle (t_ss, τ). t_ss depends strongly on parameters (λ, π, q, N).

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skip to Module 8.)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior is **collective consensus formation**, where the decentralized system of interacting agents converges to a stable state where a majority commits to a single option, typically the one with the highest quality (qₖ). Other behaviors include failing to reach consensus (multi-opinion state) or collapsing to an all-uncommitted state (absorbing state f₀*=1), depending on parameters (λ, πα, qα, N). The system also exhibits **relative stimulus perception** consistent with Weber's Law (Sec III.C) and **critical behavior** near the absorbing phase transition in specific limits (Sec III.E).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors are validated through:
        1.  **Analytical Solutions:** Derivation of stationary states (fα*) from mean-field equations (Eqs. 4-6) predicts consensus/multi-opinion/absorbing states under different parameter limits (Sec II.A). Linear stability analysis (Appendix A) confirms the stability of these predicted states.
        2.  **Numerical Integration:** Numerical solution of the deterministic mean-field equations (Eq. 4) confirms analytical predictions and explores parameter space where analytical solutions are complex (e.g., Figs 2, 3, 5).
        3.  **Agent-Based Simulations:** Stochastic simulations on fully connected networks and lattices (Sec III, Appendix A5) validate mean-field predictions on average and allow study of finite-size effects (Sec III.D, Fig 8, 9), convergence times (Fig 4c, 6d), and critical scaling (Sec III.E, Fig 10, 11).
        4.  **Quantitative Metrics:** Consensus is quantified using metrics Q and Q' (Eqs 10, 11). Weber's law is tested by fitting quality differences (q₂-q₁) vs mean quality (q̄) (Sec III.C, Fig 7). Critical behavior is analyzed using scaling laws for the order parameter (ρ) and susceptibility (χ) (Sec III.E, Eqs 12-14, Fig 10, 11).

---

#Key: [lu_bioinspired_2018]

# A bioinspired multilegged soft millirobot that functions in both dry and wet conditions

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is an untethered, multi-legged soft millirobot fabricated using a modified magnetic particle-assisted molding approach. It consists of a Polydimethylsiloxane (PDMS) body embedded with iron microparticles, featuring multiple tapered soft feet (~650μm long, ~600μm spacing). The robot is actuated remotely by an external magnetic field, enabling locomotion (both discontinuous flap-wave and continuous inverted-pendulum modes) and other functionalities like obstacle crossing and cargo carrying in both dry and wet environments. Its purpose is to demonstrate bio-inspired design principles for soft robotics capable of high performance in harsh conditions, potentially for biomedical or industrial applications.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters selected characterize the robot's physical structure and a key performance metric. Reliability is High as they are directly stated as measured or observed values.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the external magnetic field generated by a permanent magnet, which is moved externally (presumably by a robotic stage or similar mechanism, representing the ultimate energy input). The paper mentions magnetic field strengths up to 200 mT.
    *   Value: Up to 200
    *   Units: mT (magnetic field strength); Energy input to move the magnet is not specified.

### **2.2 Energy Transduction**

    *   Content: The external magnetic field exerts magnetic torque and pulling forces on the iron microparticles embedded within the PDMS structure. This magnetic potential energy is transduced into mechanical energy, causing deformation (bending/alignment) of the soft tapered feet and generating forces that lead to locomotion (kinetic energy). The flexible feet also store and release elastic potential energy during the gait cycle (e.g., PPF and CPF stages store, CD and SS stages release), contributing to the dynamics.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide a direct energy efficiency value (e.g., mechanical work output / magnetic energy input). However, it highlights "ultra-fast locomotion speed" (>40 limb length/s) and efficient locomotion compared to human walking based on stride frequency and step size (Fig 2c). It also notes the elastic energy storage/release reduces energy cost (pg 3). Compared to other magnetic actuation methods, efficiency might be reasonable, but significant energy is likely lost in moving the external magnet and overcoming fluid/surface friction (despite reductions). The score reflects decent locomotion performance but lacks quantitative efficiency data. Qualitative Assessment: Medium.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation primarily occurs through:
        1.  **Friction:** Between the robot's feet and the ground surface (both dry and wet). The paper explicitly states friction is reduced >40 times compared to a footless counterpart due to small contact area and hydrophobic feet (pg 2, pg 3), but it is still present. Magnitude not quantified absolutely. Qualitative: Low (relative to counterpart), but still exists.
        2.  **Viscous Drag:** When operating in wet conditions (liquid film, stomach model). Not quantified. Qualitative: Medium-High depending on liquid viscosity and speed.
        3.  **Internal Damping:** Within the viscoelastic PDMS material during deformation cycles. Not quantified. Qualitative: Medium.
        4.  **External Actuation:** Energy lost in the mechanism moving the external magnet. Not addressed in the paper. Qualitative: Potentially High.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Actuation Frequency (for max speed) | 16 | Hz | Fig.4d | Explicit | Frequency corresponding to max Vl=44 |
        | Gait Cycle Time (DFW, Fig 2a/c example) | ~2 | s | Fig 2a/c | Explicit | Time shown for displacement/deformation plots |
        | Gait Cycle Time (CIP, Fig 2b/c example) | ~2 | s | Fig 2b/c | Explicit | Time shown for displacement/deformation plots |
        | Obstacle Crossing Time (Fig 3c) | ~9 | s | Fig 3c | Explicit | Time shown in snapshots |
        | Natural Frequency (Leg, estimated) | ~27.9 | Hz | Text (pg 5) / Supp. Fig 8 | Explicit | Stated in text, calculation likely in Supp. |
        | Natural Frequency (Body, estimated) | ~3.4 | kHz | Text (pg 5) / Supp. Fig 8 | Explicit | Stated in text, calculation likely in Supp. |
    *   **Note:** Includes actuation frequencies, observed movement times, and estimated material dynamic properties.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors are:
        1.  **Locomotion:** Movement across surfaces (dry and wet) using two distinct gait modes: Discontinuous Flap-Wave (DFW) and Continuous Inverted-Pendulum (CIP), controlled by external magnetic field trajectories. Achieves high normalized speeds (>40 limb lengths/s).
        2.  **Cargo Transportation:** Carrying loads significantly exceeding its own weight (>100 times). Demonstrated carrying a capsule filled with Pb beads and a medical tablet.
        3.  **Obstacle Crossing:** Navigating obstacles, including standing up 90° and crossing obstacles >10 times its body height or climbing slopes up to 60°. This utilizes the flexibility of the feet and the different locomotion modes.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The behaviors described (locomotion modes, speed, carrying capacity, obstacle crossing) are validated through experimental observation and quantitative measurements, documented with images, videos (referenced), and plots (Figs 2, 3, 4). Control comparisons are made (e.g., robot with vs without feet - Supp Fig 4; flexible vs rigid feet - Fig 3c/d; performance vs animals - Fig 4d). Locomotion modes are defined operationally by the magnetic field trajectory and resulting robot motion (Fig 2a/b). Reliability is demonstrated by consistent movement shown over time in figures. Reproducibility is implied by the detailed methods but not explicitly quantified (e.g., number of trials, statistical variance). Limitations include potential variability not captured and reliance on supplementary material for full details.

---

#Key: [horibe_mode_2011]

# Mode Switching and Collective Behavior in Chemical Oil Droplets

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of an oil droplet (nitrobenzene mixed with oleic anhydride) placed in an aqueous solution containing oleate micelles at pH 11 (NaOH). The system exhibits self-propulsion due to a chemical reaction (hydrolysis of oleic anhydride to oleic acid) occurring at the oil-water interface. This reaction creates surfactants (oleate), leading to interfacial tension gradients (Marangoni effect) and internal convection currents, which break symmetry and cause movement. The system's purpose is to study emergent properties like spontaneous mode switching (changes in speed, direction, acceleration) and collective behavior (attraction between droplets) in a simple chemical system as a model for minimal cognition and protocells. Key components are nitrobenzene, oleic anhydride, oleic acid, sodium hydroxide, and water. The paper quantifies single droplet behavioral modes based on size and collective behavior in a two-droplet system.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the chemical potential energy released during the exothermic hydrolysis of oleic anhydride to oleic acid at the oil-water interface. This chemical reaction fuels the droplet's self-propulsion.

### **2.2 Energy Transduction**

    *   Content: Chemical potential energy released by the reaction is transduced into mechanical energy. The reaction produces surfactants (oleate) non-uniformly at the interface, creating gradients in interfacial tension. These gradients induce Marangoni flows (fluid motion along the interface) and internal convection currents within the droplet. The interaction of these flows with the surrounding fluid, coupled with Newton's third law, results in the droplet's kinetic energy (self-movement). Energy is also transduced into surface energy during shape distortions, especially during collisions or in larger, unstable droplets.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any quantification of energy efficiency. However, such chemomechanical systems operating at low Reynolds numbers are known to be highly dissipative. Most chemical energy is likely lost as heat or through viscous dissipation in the fluid. The occasional COR > 1 (Fig 4B) indicates spontaneous conversion of chemical to kinetic energy, but this doesn't imply high overall efficiency. Efficiency is inferred to be very low.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include: 1) Viscous dissipation due to the droplet moving through the aqueous medium and internal convection. 2) Thermal dissipation (heat loss) from the exothermic chemical reaction. 3) Energy loss during inelastic collisions with walls (converting kinetic to surface energy or heat, Fig 4C, D). 4) Energy loss associated with maintaining non-equilibrium internal flow structures. The paper does not quantify these mechanisms, but acknowledges energy loss during collisions ("kinetic energy could be consumed to distort the droplet shape") and thermal dissipation is implied for smaller droplets' low COR. Dissipation is qualitatively High.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*    Units: Qualitative Descriptor: Short-to-Medium Term

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes
        1.  **Droplet Formation:** The oil droplet itself forms through self-assembly when oil is added to the aqueous phase (mentioned in Introduction).
        2.  **Self-Propulsion:** Movement arises spontaneously from local chemical reactions and resulting physical instabilities/flows, not from external propulsion.
        3.  **Behavioral Modes:** Distinct patterns of movement (directional, circular, vibrating, fluctuating) emerge spontaneously based on local physics (reaction rate, convection stability) influenced by droplet size and age (explicitly stated as emergent properties in Abstract, Introduction, Results).
        4.  **Collective Behavior:** Droplets exhibit attraction, leading to clustering, which is described as an emergent higher-order behavior resulting from droplet interactions (sensitivity to own chemical signals, hydrodynamics) (Abstract, Introduction, Section 2.5). This order is not externally imposed.

### **4.2 Local Interaction Rules:**

    *   Content:
        1.  **Chemomechanical Coupling:** Hydrolysis of oleic anhydride at the interface produces oleate surfactant. The rate depends on local reactant availability and surface area (influenced by size, shape).
        2.  **Marangoni Effect:** Non-uniform surfactant concentration creates interfacial tension gradients, driving fluid flow (Marangoni flow).
        3.  **Hydrodynamics:** Internal convection patterns (influenced by droplet size, shape, reaction rate) interact with the external fluid, leading to propulsion via Newton's third law. Instability in convection is linked to mode switching (vibrating, fluctuating).
        4.  **Droplet-Environment Interaction:** Droplets modify their local chemical environment (leaving surfactant trails, consuming reactants). Droplets sense chemical gradients (ref [8]). Droplets interact physically with boundaries (collisions, Section 2.2, 2.3).
        5.  **Droplet-Droplet Interaction:** Droplets influence each other potentially via chemical signals (modification/sensing of environment) and hydrodynamic flows, leading to attraction (Section 2.5, 2.6). Hypothesized mechanism: reciprocal enforcement of convection flows (Section 2.6).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | 1 | Chemomechanical | Droplet Size (Volume) | 1 - 50 | µL | Section 2.1, 3 | Explicit | Affects surface area, reaction rate, stability. |
    | 1, 2, 3 | Chemomechanical/Marangoni/Hydrodynamics | pH | 11 | pH units | Section 3 | Explicit | Affects reaction kinetics/interface properties. |

### **4.3 Global Order:**

    *   Content: The emergent global order includes:
        1.  **Distinct Behavioral Modes:** Single droplets exhibit characteristic spatiotemporal patterns of motion: directional, circular, fluctuating, and vibrating modes (Section 2.1, Fig 1, 2). These modes are identified using SOM analysis based on velocity and turning angle (Section 2.4, Fig 6).
        2.  **Size/Age Dependent Mode Prevalence:** The probability of observing specific modes and transitioning between them changes systematically with droplet size and age (Section 2.4, Fig 7).
        3.  **Collective Attraction:** In multi-droplet systems (specifically 3 µL and 20 µL pairs), droplets tend to decrease their mutual distance over time compared to controls, indicating an emergent attractive interaction leading to spatial clustering (Section 2.5, Fig 8).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Mode Trans. | Probability of switching between modes | Transition Probability | 0 - ~0.8 | Unitless | Explicit | Calculated from SOM mapping over time. | SOM Analysis (Section 2.4) | Fig 7 |
| Collective | Mutual Distance between two droplets | Average Distance | ~2 - 10 | mm (estimated from Fig 8 scale) | Explicit | Measured distance between droplet centroids over time. | Image Analysis (Section 2.5) | Fig 8 |
| Collision | Coefficient of Restitution (Elasticity) | COR = ||v(t+1)|| / ||v(t)|| | ~0 - >1.5 | Unitless | Explicit | Ratio of speeds before/after collision or spontaneous change. | Image Analysis (Section 2.2) | Fig 4 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog

### **5.3 Computational Primitive:**

    *   Content: State Transition / Thresholding. The most basic operation appears to be the switching between distinct behavioral modes (states) based on the system's parameters (size, age) and possibly internal fluctuations or environmental cues crossing certain thresholds (e.g., instability thresholds for convection patterns determining vibrating/circular modes). The response to collisions (change in COR) also suggests a threshold-like response integrated with internal state.
    *   **Sub-Type (if applicable):** State transition based on parameter thresholds.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Observation Duration | Up to 60 | min | Section 2.5 | Explicit | Max duration of collective behavior experiment. |
        | SOM Binning Interval | 20 | s | Section 2.4 | Explicit | Time window used for averaging velocity/angle for SOM. |
        | Droplet Lifetime (Activity Duration) | ~ Tens of minutes | min | Fig 7 (transitions shown over time), Fig 8 (behavior changes over 60 min) | Implicit | Inferred from the duration over which behavior is observed and changes due to aging. |
        | Mode Duration / Transition Frequency | Seconds to Minutes | s / min | Fig 7 (Probabilities imply dwell times), Fig 1/2 (Paths show changes) | Implicit | Characteristic time spent in a mode or frequency of switching, estimated from figures. |
        | Collision/Response Time | < 1 (measurement interval) | s | Section 2.4 (Data recorded every second) | Implicit | COR calculation compares velocity at t and t+1, implies response within 1s interval. |

### **6.2 Active Inference:**

    *   Content: Partial
        1.  **Prediction/Anticipation:** Not explicitly shown, but chemotaxis (ref [8]) implies moving towards predicted favorable chemical environments. Response during collisions (COR variability) might involve state adjustment based on interaction.
        2.  **Action Selection:** Droplets change speed and direction (Section 2.2), switch modes (Section 2.4), and engage in collective behavior (Section 2.5), suggesting selection between different actions/states. This selection is influenced by internal state (size/age) and environment (chemicals, boundaries, other droplets).
        3.  **Internal Model:** The "internal state" (convection pattern, chemical profile) coupled with "memory" (environmental history, previous mode) acts as a rudimentary internal model influencing behavior. The system adjusts behavior (aging effects, mode switching) as the internal state/model evolves.
        However, the level is very basic. There's no evidence of explicit prediction error minimization or complex model updating based on surprise. The actions seem more like responses guided by current state and physics rather than explicit goal-directed planning based on a sophisticated internal world model.
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:**
        *   **Prediction Error Metric:** Quantify deviation from expected path based on simple models (e.g., persistent motion) vs. observed path, especially near stimuli or other droplets. Measure reduction rate over time if adaptation occurs. (Requires trajectory analysis).
        *   **Behavioral Entropy:** Measure the variability or entropy of behavioral choices (mode transitions, turning angles) under different conditions. Lower entropy might indicate more directed behavior based on an internal model. (Requires statistical analysis of trajectories).
        *   **Mutual Information:** Calculate mutual information between environmental state (e.g., chemical gradient, proximity to wall/droplet) and droplet behavior (velocity, mode) to quantify information capture driving action. (Requires simultaneous measurement of environment and behavior).
        *   **(CT-GIN Specific):** Analyze the complexity and update dynamics of the inferred `MemoryNode` and `StateNode` based on environmental interactions mapped via `AdjunctionEdge`s. Track changes in edge weights (e.g., transition probabilities) over time as evidence of model updating.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The primary mechanism of adaptation is the change in the droplet's internal state due to its ongoing chemical reaction and mass loss. Key factors include:
        1.  **Reactant Depletion:** Consumption of oleic anhydride reduces the chemical energy input over time.
        2.  **Volume/Size Change:** The droplet volume decreases over time (mentioned in Section 2.1 related to mode transitions). This alters surface area-to-volume ratio, affecting reaction rates and hydrodynamic stability (Laplace pressure).
        3.  **Accumulation of Products/Byproducts:** Surfactant accumulates at the interface and potentially in the environment, altering interfacial properties and potentially influencing future behavior (environmental memory).
        These changes directly impact the driving forces (Marangoni flow strength) and stability of internal convection patterns, leading to the observed shifts in behavioral modes and overall activity levels. It's adaptation through resource depletion and physical change, not through a learning rule like Hebbian or reinforcement learning.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors described are:
        1.  **Self-Propulsion:** Autonomous movement driven by internal chemical reactions.
        2.  **Mode Switching:** Spontaneous transitions between distinct behavioral patterns (directional, circular, vibrating, fluctuating) characterized by different speeds, turning angles, and stability.
        3.  **Collision Response:** Interaction with physical boundaries (walls), quantified by the Coefficient of Restitution (COR), showing both inelastic and occasionally elastic/super-elastic (COR>1) responses depending on size and state.
        4.  **Collective Attraction:** Tendency for droplets (specifically 3 µL and 20 µL) in the same container to move closer together over time compared to controls.
        5.  **Chemotaxis (Referenced):** Ability to sense and move along chemical gradients (mentioned as prior work, ref [8]).

### **8.2 Behavior Robustness:**

        *   **Size Dependence:** Behavioral modes, COR values, and collective behavior are strongly dependent on droplet size (Figs 1, 4D, 6, 7, Section 2.5). Large droplets (50 µL) show different modes and no collective attraction.
        *   **Time Dependence (Aging):** Velocity, turning angle, mode prevalence, and collective attraction all change significantly over the experiment duration (Figs 3, 7, 8).
        *   **State Dependence:** COR values show wide distributions even for non-collision events (Fig 4B), suggesting sensitivity to the instantaneous internal state. Collisions further alter the COR distribution (Fig 4C).
        *   **Stochasticity:** Mode switching is probabilistic (Fig 7), and paths show spontaneous changes (Fig 3), indicating inherent fluctuations.
        While behaviors are reliably observed under specific conditions (size, age), they are not robust across wide parameter ranges or long timescales.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors (mode switching, collective action) are validated through:
        *   **Quantitative Tracking:** Droplet paths recorded using video, positions tracked over time using software (R, open-cv) (Section 3).
        *   **Feature Extraction:** Velocity and turning angle calculated from trajectories (Section 2.2, 2.4).
        *   **Objective Classification (Modes):** Self-Organizing Maps (SOM) used with U-matrix analysis to identify distinct behavioral modes based on velocity/angle data in an unsupervised manner (Section 2.4, Fig 5, 6). Transition probabilities calculated from SOM mappings (Fig 7).
        *   **Statistical Comparison (Collective):** Mutual distance between droplet pairs measured over time and compared statistically (implicitly via graph representation in Fig 8) to control experiments (superimposed single droplet paths) to demonstrate attraction (Section 2.5, Fig 8).
        *   **Parameter Dependence:** Systematic study across different droplet sizes (Section 2.1, 2.3, 2.4, 2.5).
        *   **Physical Interaction Metric (Collision):** Coefficient of Restitution (COR) calculated to quantify collision dynamics (Section 2.2, Fig 4).
        Reproducibility is implied by presenting results across multiple droplet sizes and experiments (e.g., "all droplets tested" caption for Fig 4B/C). Limitations might include the number of trials for statistical significance (not always stated) and potential environmental variations not controlled for.

---

#Key: [khona_global_2025]

# Global modules robustly emerge from local interactions and smooth gradients

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a theoretical/computational model of neural networks, specifically Continuous Attractor Networks (CANs), designed to explain the emergence of functionally distinct grid cell modules in the mammalian medial entorhinal cortex (MEC). The model proposes that discrete modules with different spatial periods self-organize along a neural sheet (representing the dorsoventral axis of the MEC) due to the interplay of two types of local interactions: one whose properties (e.g., width, strength) vary smoothly (graded) along the axis, and another whose properties remain fixed along the axis. The system's purpose is to demonstrate how global modular structures can arise from purely local interactions and smooth gradients, providing a mechanism ("peak selection") that bridges neuronal properties to circuit function and predicts observed grid module period ratios and self-scaling properties. Components include neurons arranged on a 1D or 2D sheet, synaptic interactions described by kernels (Wg - graded, Wf - fixed), and neural dynamics equations (rate-based model). It performs pattern formation (local grid-like activity) and module formation (global discrete periods). It can also perform velocity integration when driven by velocity inputs.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Fourier Phase (ϕ) | Depends on Wf shape (e.g., ≈0, ≈±π/2) | Radians | Eq 3, Fig 3f-h | Explicit | Medium (Derived from kernel shape) | Theoretical derivation from Wf |

    *   **Note:** Parameters represent typical values used in specific simulations shown; the model's strength lies in robustness across various parameter values. Units are often relative to neuron spacing or simulation time steps.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The paper does not explicitly model metabolic energy input. The system dynamics are driven by abstract neural activity rates and simulated velocity inputs (when applicable for path integration). Energy input is implicitly required for neural computation but not analyzed. For specific simulations involving velocity integration (Fig 2j,k), velocity signals act as information input, driving pattern dynamics.

### **2.2 Energy Transduction**

    *   Content: The model doesn't describe energy transduction in a physical sense. Information is transduced: synaptic inputs (weighted sums of activities) are transformed into neural firing rates via a non-linear function (rectification). Velocity inputs are transduced into phase shifts of the grid pattern (path integration). The "peak selection" mechanism transduces local interaction properties and gradients into global modular structures.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Energy efficiency is not analyzed or discussed in the paper. The model is abstract and does not account for the energy costs of neural activity or synaptic transmission.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation mechanisms (like heat loss from neural activity) are not modelled or discussed. In the abstract sense of information dynamics, numerical errors or noise might be considered analogous to dissipation, but this is not the focus. The system dynamics reach stable states (attractors), implying a dissipation of transients in the state space, governed by the Lyapunov function (mentioned implicitly via Hopfield analogy in ecological model section, Extended Data Fig 2).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term (relative to neural time constants)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Robustness vs Weight Noise | Pattern Variance / Mean Period | < 5% (for 20% noise with peak selection) | % | `MemoryNode` attribute: `robustness_weight_noise` | Fig 5b | Explicit | Quantified improvement with peak selection. |
    | Robustness vs Perturbation | Qualitative Recovery | Recovers within ~τ | ms | `MemoryNode` attribute: `robustness_perturbation` | Fig 5e-g | Explicit | Demonstrated state recovery after silencing/driving. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are defined by the synaptic weight kernel W(Δx; σ(n<sub>DV</sub>)) = Wg(Δx; σ(n<sub>DV</sub>)) + Wf(Δx) (Eq 1). Wg has a width σ(n<sub>DV</sub>) that varies smoothly with position n<sub>DV</sub> along the neural sheet, while Wf has a fixed width d. Specific forms for Wg (Mexican hat, box function) and Wf (localized, diffuse, decaying) are given in Methods (Eq 10, 11, ff.). The interaction contributes to the input sum in the neural dynamics equation (Eq 7): ∑<sub>j</sub> W<sub>0</sub>(i,j)s(j,t). The "peak selection" mechanism arises from how the Fourier transforms of Wg and Wf interact (Eq 2, Fig 3b-d): the fixed kernel Wf creates multiple potential peaks (possible periods), and the graded kernel Wg selects which peak dominates locally, causing jumps as n<sub>DV</sub> changes.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Eq 1 / Eq 7 | Combined Interaction Kernel | Interaction Strength (α<sub>E</sub>, α<sub>I</sub>, α<sub>S</sub>, α<sub>0</sub>, α<sub>T</sub>) | e.g., 1000, 4, -0.25, 25 | Dimensionless (Weight units) | Methods | Explicit | Specific values for example simulations provided. |
    | Eq 1 | Graded Kernel Width | σ<sub>min</sub>, σ<sub>max</sub> (endpoints of σ(n<sub>DV</sub>)) | e.g., 15, 45 (1D box) | Dimensionless (Spatial units) | Fig 4, Methods | Explicit | Defines the range of the gradient. |
    | Eq 1 | Fixed Kernel Width | d | e.g., 84, 135 (1D) | Dimensionless (Spatial units) | Methods | Explicit | Fixed parameter defining Wf scale. |
    | Eq 1 | Fixed Kernel Shape Parameter | ε<sub>S</sub> (for Wf<sub>localized</sub>) | e.g., 4.77 (1D) | Dimensionless (Spatial units) | Methods | Explicit | Defines sharpness of localized kernel. |
    | Eq 3 | Fixed Kernel Fourier Phase | ϕ | e.g., ≈0, ≈±π/2 | Radians | Fig 3f-h | Explicit | Derived from Wf shape, influences period ratios. |

### **4.3 Global Order:**

    *   Content: The emergent global order consists of discrete modules along the neural sheet (DV axis). Within each module, neurons exhibit coherent activity patterns with a fixed spatial period (grid-like pattern). Across modules, the spatial period jumps discontinuously to a different value. The number, size, and period values of these modules emerge from the local rules. Self-scaling of module size with system size (L) is also an emergent property.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Explicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Peak Selection | Interaction kernel definition | Wg width σ(n<sub>DV</sub>) | monotonic gradient (e.g., 15-45) | spatial | Explicit | Defines graded interaction scale | Eq 1, Methods |
| Peak Selection | Interaction kernel definition | Wf width d | fixed (e.g., 84) | spatial | Explicit | Defines fixed interaction scale | Eq 1, Methods |
| Neural Dynamics | Rate update | Neuron time constant τ | fixed (e.g., 30) | time | Explicit | Sets intrinsic timescale of neurons | Eq 7, Methods |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Modularity | Number of Modules | N<sub>mod</sub> | Integer (e.g., 4) | count | Explicit | Predicted by theory, observed in simulations | Eq for N<sub>mod</sub>, Figs 2d, 3e, 4a | Text, Figs 2, 3, 4 |
| Modularity | Module Periods | λ<sub>m</sub> | Discrete values (e.g., 38.8, 48.4 cm - mapped from simulation) | spatial | Explicit | Predicted by Eq 3, observed in simulations and data | Figs 2d, 3e, 3i, 4b | Text, Figs 2, 3, 4 |
| Modularity | Period Ratios | r<sub>m</sub> = λ<sub>m</sub> / λ<sub>m+1</sub> | ≈ (m+1+ϕ/2π) / (m+ϕ/2π) | dimensionless | Explicit | Predicted by Eq 4, matches data | Fig 3j, Eq 4 | Text, Fig 3 |
| Scaling | Module Size | Width of constant period region | Scales ~ L / N<sub>mod</sub> | spatial | Explicit | Observed in simulations, consequence of fixed N<sub>mod</sub> | Fig 4a,b | Text, Fig 4 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    | Peak Selection | Local interactions (Wg, Wf) determining global module structure (periods, boundaries, number). | High (Periods, Number, Scaling); Medium (Boundaries) | 8 | Module Periods (Eq 3), N<sub>mod</sub> (from theory), Period Ratios (Eq 4), Boundary locations (from theory/simulations) | Explicit | Theory derived from local rules accurately predicts most global features. Boundary positions less precise. | Sec: Analytic theory, Figs 3, 4, Eq 2-4, Methods, Supp Info Sec C/E |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 8. (Rubric: 0=No link; 3=Qualitative link; 5=Quantitative link for some features; 7=Quantitative link for most features; 9=Quantitative link for all features with high accuracy; 10=Perfect formal mapping). Score 8 reflects strong quantitative prediction of periods, ratios, number, and scaling, with only boundary locations being less precise.
    *   **Metrics:** Match between theoretical predictions (Eq 2, 3, 4, N<sub>mod</sub> formula) and simulation results/experimental data (Module periods λ<sub>m</sub>, period ratios r<sub>m</sub>, module count N<sub>mod</sub>, boundary locations n<sub>boundary</sub>). R<sup>2</sup> value for ratio prediction. Visual comparison of predicted vs simulated periods/boundaries (Fig 3e, 4b,d).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Neuromorphic

### **5.3 Computational Primitive:**

    *   Content: Pattern Formation (Turing-like instability leading to periodic patterns), Peak Selection (Selecting dominant mode from multiple possibilities based on interaction kernels), Thresholding (Neural activation function ϕ(z)=[z]<sub>+</sub>), Weighted Summation (Synaptic integration ∑<sub>j</sub> W<sub>0</sub>(i,j)s(j,t)), Velocity Integration (Phase update based on velocity input).
    *   **Sub-Type (if applicable):** Pattern Formation: Turing Instability; Thresholding: Rectified Linear Unit (ReLU); Peak Selection: Max-selection based on Fourier modes.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Neural Time Constant | τ ≈ 30 | Arbitrary Time Units (ms implied by context) | Methods | Explicit | Intrinsic timescale of neuron dynamics. |
        | Simulation Time Step | dt = 0.05 | Arbitrary Time Units (scaled relative to τ) | Methods | Explicit | Discretization for numerical simulation. |
        | Module Formation Time | ≈ 1-3 τ | Arbitrary Time Units (ms) | Fig 3a,e | Explicit | Time for modules/patterns to stabilize from initial state. |
        | Perturbation Recovery Time | ≈ τ | Arbitrary Time Units (ms) | Fig 5e-g | Explicit | Time for system to return to stable state after perturbation. |
    *   **Note:** Units are often relative to τ or simulation steps. Real-world timescale mapping depends on assigning a value to τ (often assumed ~10-30ms for neurons).

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior is the self-organization of discrete functional modules along the neural sheet, characterized by discontinuous jumps in the spatial period of grid-like activity patterns. Other key behaviors include: formation of stable periodic (grid) patterns within modules, self-scaling of module size with system size L, generation of specific period ratios between adjacent modules (~adjacent integer ratios), robustness of modules to noise and perturbations, and functional independence of modules during velocity integration (patterns flow independently).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors are validated through several methods:
        1.  **Numerical Simulations:** Direct simulation of the neural dynamics equations (Eq 7) demonstrates the spontaneous formation of modules, period jumps, scaling, and robustness (Figs 2, 3a,e, 4a, 5).
        2.  **Analytical Theory:** Derivation of the "peak selection" mechanism (Eq 2-4, Supp Info Sec C, E) provides a theoretical explanation for the emergence and predicts key properties (periods, ratios, module number, scaling) which match simulation results (Figs 3e, 4b,d).
        3.  **Comparison with Experimental Data:** The predicted period ratios (Eq 4 with ϕ=0) show excellent agreement with experimental data from rat MEC (Fig 3i,j, R<sup>2</sup>=0.999).
        4.  **Robustness Tests:** Specific simulations test robustness against noise, weight heterogeneity, and dynamic perturbations (Fig 5).
        Control experiments (implicit): Simulating only Wg shows graded period variation, not modules (Extended Data Fig 1), confirming the necessity of Wf. Simulating Wf narrower than Wg gives sawtooth patterns (Supp Fig 2), confirming the role of relative widths.
        Limitations: Validation relies on simulations of a specific model class (rate-based CANs) and comparison to existing, selected experimental data.

---

#Key: [pfeifer_morphological_2006]

# Morphological computation for adaptive behavior and cognition

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper introduces and illustrates the concept of "morphological computation," where aspects of computation or control normally attributed to the brain/controller are "offloaded" to the physical structure (morphology, materials) and dynamics of an agent interacting with its environment. It argues that the body's physical properties play a crucial role in generating adaptive behavior and simplifying control. The paper presents several case studies (insect eye morphology, quadruped robot locomotion, fish locomotion, robotic hand grasping) to demonstrate how morphology, materials, and environment interactions contribute to tasks like motion detection compensation, stable running without feedback, diverse swimming behaviors with simple actuation, and adaptive grasping. The purpose is to highlight the importance of embodiment and morphological computation for designing intelligent robotic systems and understanding natural systems, shifting focus from purely neural/control aspects to the interplay between brain, body, and environment. The components discussed include sensors, actuators, limbs, materials (springs, elastic materials, deformable materials), neural systems/controllers, and the environment.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name         | Value                              | Units   | Source (Fig/Table/Section)   | Implicit/Explicit   | Data Reliability (High/Medium/Low)   | Derivation Method (if Implicit)                    |
        | :----------------------- | :--------------------------------- | :------ | :------------------------- | :------------------ | :----------------------------------- | :------------------------------------------------- |

    *   **Note:** The paper provides limited quantitative parameters, focusing on qualitative descriptions of the systems and their behavior. Parameters listed are those explicitly stated or clearly depicted.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Electrical energy for motors (Mini-dog, Wanda fish, Yokoi hand, EyeBot implied) is the primary input driving actuation. Environmental interactions (gravity, friction, fluid dynamics) also contribute energy/forces that are harnessed (e.g., impacting locomotion).

### **2.2 Energy Transduction**

    *   Content: Electrical energy is transduced into mechanical energy by motors. This mechanical energy drives limb/fin motion. In the Mini-dog, motor energy is partially stored and released by springs (elastic potential energy). In the Wanda fish, motor energy creates tail oscillations, which transduce into kinetic energy of the fish and fluid motion (thrust, lift/sink forces). In the Yokoi hand, motor energy actuates tendons, storing elastic energy and performing work on grasped objects via finger deformation. Sensor morphology (EyeBot) transduces light patterns/motion into neural signals (though energy aspects aren't discussed). Environmental forces are transduced through the body's structure (e.g., impact forces on Mini-dog legs, hydrostatic forces on Wanda fish). Energy is dissipated via friction, material damping, and fluid resistance.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper *qualitatively* discusses energy efficiency, particularly concerning "cheap" locomotion (Sections 1, 3.1, 3.2) and grasping (Section 4). It suggests morphological computation *improves* efficiency by exploiting physical dynamics ("physical processes are fast and for free!"). However, no quantitative metrics or efficiency values are provided for any case study. A score cannot be meaningfully assigned based on the text. Qualitative assessment: Morphological computation is presented as potentially leading to Medium/High efficiency compared to purely feedback-controlled systems, but this is not quantified.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation mechanisms are mentioned qualitatively. Friction is explicitly mentioned for physical agents in general (Introduction) and specifically noted as low for the Mini-dog's feet (Section 3.1), implying friction elsewhere (joints, ground interaction) is a dissipation source. Energy dissipation is also mentioned generally for physical agents (Introduction). For the Wanda fish, movement through water inherently involves viscous drag (dissipation). For the elastic components (springs in Mini-dog, tendons/fingertips in Yokoi hand), internal damping likely causes dissipation, though not explicitly stated. Precise quantification is absent. Qualitative assessment: Likely Medium to High dissipation in locomotion examples (friction, drag, material damping), potentially lower in grasping once static.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip to Module 4.)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes
        *   Mini-dog: Stable running gaits (global order) emerge from the interaction of simple sinusoidal motor control, leg springs (local properties/rules), and ground interaction (environment). The system "self-stabilizes" (Section 3.1).
        *   Wanda Fish: Diverse swimming behaviors (forward, turn, up/down - global behaviors) emerge from the interaction of a single DoF tail wiggle (local control), fin elasticity, body buoyancy, and fluid dynamics (local rules/environment), without specific actuators for each direction (Section 3.2).
        *   Yokoi Hand: Adaptive grasping shape (global order) emerges from the interaction of a simple "close" command (control), elastic tendons, deformable fingertips, hand morphology (local properties/rules), and the object's shape (environment) (Section 4).

**(Conditional: M4.1 is "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are primarily the laws of physics governing the system's components and their interaction with the environment:
        *   Mini-dog: Sinusoidal position control applied to motors; Spring dynamics (Hooke's law, damping - implicit); Newtonian mechanics (inertia, gravity, ground reaction forces); Friction laws (implicit).
        *   Wanda Fish: Sinusoidal tail wiggle with variable zero-point offset for turning; Elastic deformation of fin; Fluid dynamics (Bernoulli's principle for lift, drag forces, buoyancy); Newtonian mechanics.
        *   Yokoi Hand: "Close" command to motors; Elastic tendon dynamics; Material deformation mechanics (fingertips, inter-finger material); Contact mechanics/friction with object; Anthropomorphic morphology constraints.
        *   Eyebot (Evolution part): Evolutionary strategy modifying facet positions based on task performance (obstacle distance). EMD (Elementary Motion Detector) responses based on light changes across adjacent facets (implicit underlying rule).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID         | Description                            | Parameter Name               | Parameter Value Range   | Units   | Data Source   | Implicit/Explicit   | Justification                                  |
    | :-------------- | :------------------------------------- | :--------------------------- | :---------------------- | :------ | :------------ | :------------------ | :--------------------------------------------- |

### **4.3 Global Order:**

    *   Content:
        *   Mini-dog: Stable periodic running gaits (e.g., Gait 0, Gait 1 shown in Fig 3), self-stabilization against small perturbations.
        *   Wanda Fish: Behavioral diversity - directional movement (forward, left, right, up, down) emerging from simple tail control.
        *   Yokoi Hand: Self-adaptive grasp configuration matching the shape of the grasped object.
        *   Eyebot: Non-homogeneous arrangement of facets (denser frontally) compensating for motion parallax.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID         | Description                        | Parameter                   | Value Range          | Units   | Implicit/Explicit   | Justification                                  | Source        |
| :-------------- | :--------------------------------- | :-------------------------- | :------------------- | :------ | :------------------ | :--------------------------------------------- | :------------ |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID         | Description                           | Parameter                         | Value Range        | Units   | Implicit/Explicit   | Justification                                         | Protocol                        | Source        |
| :------------------ | :------------------------------------ | :-------------------------------- | :----------------- | :------ | :------------------ | :---------------------------------------------------- | :------------------------------ | :------------ |
| MiniDogGait       | Stable running gait                 | Gait Period, Hopping Height, Vel. | Gait 0 vs Gait 1   | s, m, m/s | Explicit Example    | Fig 3 shows two distinct gaits qualitatively.        | High-speed camera observation | Section 3.1   |
| WandaFishBehavior | Diverse swimming modes              | Direction (X, Y, Z), Speed        | Forward/Turn/Up/Down | Vector  | Explicit Behavior | Behaviors described based on control inputs.        | Observation                     | Section 3.2   |
| YokoiHandGrasp    | Adaptive grasp configuration          | Finger Joint Angles, Contact Pts  | Object-dependent   | Deg, #  | Explicit Behavior | Hand conforms to object shape.                       | Observation                     | Section 4     |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type                        | Description                                                     | Predictability   | Yoneda Score   | Metrics   | Implicit/Explicit   | Justification                                                                                                                     | Source          |
    | :------------------------------- | :-------------------------------------------------------------- | :--------------- | :------------- | :-------- | :------------------ | :-------------------------------------------------------------------------------------------------------------------------------- | :-------------- |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid

### **5.3 Computational Primitive:**

    *   Content: The computational primitives vary by example and are often physical functions rather than logical operations:
        *   EyeBot/Fly Eye: Spatial filtering / Coordinate transformation (compensating for motion parallax, akin to a non-linear mapping based on position and velocity).
        *   Mini-dog: Dynamical system stabilization / Limit cycle generation (finding stable gaits).
        *   Wanda Fish: Non-linear mapping from control input (frequency, amplitude, offset) + physical state (velocity, orientation) to forces/motion (thrust, lift, turning moments) via fluid dynamics.
        *   Yokoi Hand: Constraint satisfaction / Shape adaptation (conforming fingers to object geometry under actuation force).

### **5.4 Embodied Computational Units**
| Unit ID             | Description                                                | Processing Power   | Energy/Operation   | Freq/Resp. Time   | Bit-Depth   | Data Source                   | Implicit/Explicit   | Justification                                                |
| :------------------ | :--------------------------------------------------------- | :----------------- | :----------------- | :---------------- | :---------- | :---------------------------- | :------------------ | :----------------------------------------------------------- |

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description             | Value                    | Units   | Source                 | Implicit/Explicit   | Justification                                                           |
        | :-------------------------------- | :----------------------- | :------ | :--------------------- | :------------------ | :---------------------------------------------------------------------- |
        | Feedback loop delays (General)    | Too slow for rapid locom. | Qual.   | Section 3.1            | Explicit Statement  | Stated as a reason why feedback control isn't used in Mini-dog.         |
        | Physical Process Speed (General)  | Fast                     | Qual.   | Section 3.1            | Explicit Statement  | Stated as an advantage ("physical processes are fast and for free!"). |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes
        *   Mini-dog: Self-stabilizes its gait in response to small perturbations, adapting its dynamic state to maintain locomotion. (Section 3.1)
        *   Wanda Fish: Adapts its movement direction (up/down) based on its speed and turning actions due to hydrodynamic effects, enabling diverse behaviors from simple control. (Section 3.2)
        *   Yokoi Hand: The hand's physical structure (fingers, tendons) adapts its configuration ("self-adapt") to the shape of the object being grasped. (Section 4)
        *   Eyebot: The morphology adapts over evolutionary time based on task performance (maintaining distance). (Section 2)
        This goes beyond simple stimulus-response; it involves the system settling into a functional state determined by the interaction.

**(Conditional: M7.1 is "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanisms are primarily physical and dynamic, not based on explicit learning rules like Hebbian or reinforcement learning in the base examples:
        *   Mini-dog: Physical dynamics (interaction of springs, mass, gravity, ground forces) lead to convergence towards stable limit cycles (gaits). Perturbations are counteracted by these dynamics, leading back to the stable gait (attractor dynamics).
        *   Wanda Fish: Hydrodynamic forces (lift/drag depending on speed, angle of attack during turns) coupling with buoyancy cause changes in vertical motion. The adaptation is a direct physical consequence of the movement pattern.
        *   Yokoi Hand: Passive mechanics (elasticity of tendons, deformability of fingertips, kinematic constraints of the morphology) cause the fingers to conform to the object shape when the closing force is applied. It's a physical settling into a minimum energy configuration under load.
        *   Eyebot: Artificial Evolution (evolutionary strategy) iteratively modifies the sensor morphology parameters based on a performance metric (task success: maintaining distance). This is an optimization process operating over generations.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content:
        *   Eyebot: Maintains a constant lateral distance to an obstacle by evolving sensor morphology to compensate for motion parallax.
        *   Mini-dog: Achieves robust, rapid quadrupedal locomotion (running gaits) without sensory feedback.
        *   Wanda Fish: Exhibits diverse swimming behaviors (forward, turning, vertical movement) using only a single actuated tail fin.
        *   Yokoi Hand: Grasps objects of various shapes adaptively using a simple closing command, leveraging material compliance and morphology.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation methods rely on observation and demonstration within the specific case studies:
         *   Eyebot: Experimental runs using real robots (Eyebot platform) with an evolutionary strategy. Validation shown via final evolved facet distributions across multiple runs (Fig 1c) demonstrating consistent emergence of the desired morphology. (Section 2).
         *   Mini-dog: Observation of running behavior using high-speed camera (Fig 2c), identifying distinct stable gaits (Fig 3). Demonstration of self-stabilization mentioned qualitatively. (Section 3.1).
         *   Wanda Fish: Observation of swimming behaviors resulting from different control inputs (frequency, amplitude, offset). Illustrated with a sequence of upward movement (Fig 4c). (Section 3.2).
         *   Yokoi Hand: Demonstration of grasping different objects using the same simple control scheme. Visualised via grasp sequences and final grasp images (Fig 5b, c, d). (Section 4).
     *   Limitations: Validation is largely qualitative and demonstrative. Quantitative performance metrics, statistical analysis of robustness, or systematic exploration of parameter space are generally lacking in the provided text. Reproducibility is implied by consistent Eyebot results but not explicitly tested for others.

---

#Key: [friston_path_2023]

# Path integrals, particular kinds, and strange things

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper describes a theoretical framework, the Free Energy Principle (FEP), using a path integral formulation derived from stochastic dynamics (Langevin equations) in generalized coordinates of motion. It models the behavior of systems ("particles") defined by a particular partition (Markov blanket) separating internal states (μ) from external states (η) via sensory (s) and active (a) states. The system's purpose is to describe how these particles maintain their characteristic states/paths through a principle of least action, which underpins self-organization and can be interpreted as inference about the environment. This involves minimizing variational free energy (F) or related functionals (Expected Free Energy E, Generalized Free Energy G) which represent the surprisal or implausibility of paths. Different types of particles (inert/active, dissipative/conservative, ordinary/strange) are defined based on the presence and coupling of these states, leading to different behavioral interpretations like active inference, planning, and agency.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These are the core mathematical constructs defining the system. Specific numerical values are not applicable as it's a theoretical framework, but their functional forms and dependencies are specified.

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**

    *   Content: Dissipation is implicitly present through the random fluctuations (ω) in the Langevin equation (Eq. 1), which drive the system away from deterministic paths. The distinction between dissipative (subject to fluctuations ω<sub>s</sub>, ω<sub>a</sub>, ω<sub>μ</sub>) and conservative particles (negligible fluctuations on particular states, `Σ̃<sub>π</sub>` → 0) directly addresses this. Section 7.2 explicitly uses Helmholtz decomposition (Eq. 36) to separate flow into conservative (solenoidal) and dissipative (gradient flow) components, linking the latter to random fluctuations. Quantification is qualitative (present/negligible) or symbolic (`Γ`, `Σ̃`).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Skipping M3.2-M3.8 as M3.1 is "No")**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are encoded in the structure of the flow function `f` within the particular partition (Eq. 10) and the Langevin dynamics (Eq. 1, 2). Specifically:
        1.  External states (η) only directly influence external states and sensory states (s). (`f<sub>η</sub>(η, s, a)`, `f<sub>s</sub>(η, s, a, μ)`) - Note: paper text says `f<sub>η</sub>(η, s)` but Eq 10 shows `f<sub>η</sub>(η, s, a)`. Let's assume Eq 10 is correct. Similarly for `f<sub>s</sub>`.
        2.  Internal states (μ) only directly influence internal states and active states (a). (`f<sub>a</sub>(s, a, μ)`, `f<sub>μ</sub>(s, a, μ)`) - For strange particles, `f<sub>μ</sub>(s, μ)`.
        3.  Sensory states (s) influence internal (μ) and active (a) states.
        4.  Active states (a) influence external (η) and sensory (s) states.
        5.  Dynamics evolve according to `x̃˙ = f(x̃) + ω̃` (Eq. 2) or via the principle of least action / free energy minimization gradient flows (Eq. 14, 21, 32, 34).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :------------------: | :---: | :----------: | :----------------: | :------------: |

### **4.3 Global Order:**

    *   Content: The emergent global order is the persistence of the particle as a distinct entity with characteristic states and dynamics (paths of least action, `x̃`). This manifests as the system continuously minimizing its variational free energy, counteracting dissipative fluctuations, and maintaining itself far from equilibrium (for NESS systems, although NESS is not assumed here). This is interpreted as the particle engaging in inference or "self-evidencing" (Section 1.3, Section 2).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Other (Variational Bayesian Inference)

### **5.3 Computational Primitive:**

    *   Content: Gradient Descent on Free Energy Functional. The core computation involves finding the internal path (μ̃) or autonomous path (α̃) that minimizes the relevant free energy functional (F, E, or G). This is typically achieved via a gradient descent process on the functional (expressed in generalized coordinates, Eq. 14, 21, 32, 34). Minimizing free energy implicitly involves minimizing prediction error (accuracy term) and complexity/divergence, or balancing expected cost and information gain.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Process Interval | `T` | Time | Eq (1) | Explicit | Dynamics defined over `0 ≤ τ ≤ T`. |
        | Generalized Motion Order | `n` | Integer | Section 1.2 | Explicit | Framework uses n-order time derivatives. |
        | Inference vs. Learning | Fast vs. Slow | Qualitative | Section 3, Section 8 | Explicit | Explicitly mentions separation but doesn't quantify. |
    *   **Note:** The use of generalized coordinates inherently deals with multiple timescales (derivatives). The paper explicitly acknowledges a separation between fast inference (focus of the paper) and slow learning (parameter changes, mentioned briefly). Specific numerical values are not provided.

### **6.2 Active Inference:**

    *   Content: Yes
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:**
        *   `Metric: PredictionErrorReductionRate`: Measure the rate at which the accuracy term `-E<sub>q</sub>[L(s̃,α̃|η̃)]` in F (Eq 14) or `-E<sub>q</sub>[L(s̃,μ̃|η̃,ã)]` in G (Eq 32) decreases over time via internal state updates (μ̃).
        *   `Metric: InformationGainRate`: Measure the rate of change of the complexity term `D<sub>KL</sub>[q||p]` in F/G or the explicit expected information gain term `E<sub>p(s̃|α̃)</sub>[D<sub>KL</sub>[p(η̃|s̃,α̃)||p(η̃|α̃)]]` in E (Eq 22) via action selection (α̃).
        *   `Metric: FreeEnergyMinimizationRate`: Measure the rate of decrease of F, G, or E over time.
        *   `Metric: GoalAchievementRate` (for EFE): Measure how often or quickly preferred sensory states (low `L(s̃)`) are achieved.
        *   CT-GIN: These would be attributes of `ComputationNode` (inference dynamics) or `BehaviorArchetypeNode` (active inference behavior).

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Skipping M7.2 as M7.1 is "No")**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior described is **self-maintenance/self-organization** through **variational free energy minimization**. This is interpreted as **inference** (internal states inferring external states) and, for active/strange particles, **active inference** (acting to minimize expected prediction errors/free energy, balancing preferences and information gain), potentially leading to **planning** (selecting action sequences), **agency**, **curiosity**, and **sentient behavior**. Examples simulated include motor control/writing (Fig 5) and visual search/epistemic foraging (Fig 6).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation is primarily theoretical through mathematical derivations presented as Lemmas (Variational Free Energy Lemma, Expected Free Energy Lemma, Generalized Free Energy Lemma). These show how the minimization principles arise from the particular partition and dynamics. Additionally, computational simulations (Figs 5 & 6) are presented as demonstrations that the framework (specifically active inference derived from FEP) can reproduce complex, sentient-like behaviors (handwriting, visual search), lending plausibility to the claims. Reproducibility relies on implementing the specified equations. Robustness is argued theoretically (see M8.2).

---

#Key: [parrondo_thermodynamics_2015]

# Thermodynamics of information

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews the theoretical framework for the thermodynamics of information, focusing on systems operating at small scales where fluctuations are significant. It explores the interplay between thermodynamic quantities (energy, entropy, work, heat) and information-theoretic concepts (Shannon entropy, mutual information). The core purpose is to refine the second law of thermodynamics to explicitly incorporate information and to understand the physical nature and thermodynamic costs of information processing operations like measurement, feedback control, memory storage, and erasure. Key conceptual systems discussed include Maxwell's demon, the Szilárd engine (both theoretical and experimental realizations like single-electron boxes and colloidal particles), general feedback-controlled systems, and models of physical memory (e.g., Brownian particle in a double-well potential). The framework relies heavily on stochastic thermodynamics and fluctuation theorems.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
    *   **Note:** These parameters represent the core concepts defined and used throughout the theoretical framework reviewed in the paper.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source considered is typically a single thermal reservoir (heat bath) at a constant temperature T. Work can also be performed on the system by external agents or measurement/feedback devices (demons). For memory erasure (Landauer's principle), work is input. For work extraction (Szilard engine), heat is drawn from the reservoir.
    *   Units: J

### **2.2 Energy Transduction**

    *   Content: Energy is transduced between heat from the reservoir, work done on/by the system, changes in the system's internal energy (<H₀>ρ), and changes related to the system's information content (via non-equilibrium free energy F = <H₀>ρ - TS(ρ)). Measurement can increase the non-equilibrium free energy (potential to extract work) apparently without energy cost to the *system*, but requires work elsewhere (e.g., memory interaction/reset). Feedback control uses acquired information to manipulate the system, potentially extracting work from the heat bath (e.g., Szilard engine extracts kT ln 2 work via isothermal expansion after measurement). Erasure converts input work into dissipated heat (Landauer's principle).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The review discusses the *limits* of energy conversion, defined by the (generalized) second law (Eq 3, 5, 7, 9, 10, 13). Efficiency bounds are set by quantities like the change in non-equilibrium free energy (ΔF) and mutual information (I). For example, the maximum work extractable in a Szilard cycle is W_ext = kT ln 2, reaching the bound set by the acquired information (H(M)=ln 2). Processes become maximally efficient (zero entropy production) when they are thermodynamically reversible, achieving the equality in the second-law inequalities. Fluctuation theorems (Eq 6, 7) quantify deviations from average behavior and the possibility of transient "violations" compensated by information terms. The efficiency of specific experimental realizations (Fig 1) is not detailed in this review.

### **2.4 Energy Dissipation**

    *   Content: Dissipation occurs in irreversible processes. The dissipated work (W_diss) is equal to the total entropy production (ΔS_tot) multiplied by temperature (TΔS_tot = W_diss = W - ΔF ≥ 0, Eq 3). Sources of dissipation include irreversible relaxation towards equilibrium (Box 1), friction during manipulations, non-quasi-static changes in system parameters, and thermodynamically irreversible measurement or erasure processes. Landauer's principle states that erasing one bit of information necessarily dissipates at least kT ln 2 of heat into the environment (related to W_reset ≥ kT H(M), Eq 9, 10). Entropy production quantifies this dissipation.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*    Units: s (Implicit)

### **3.4 Memory Capacity (Optional - if applicable)**

*   Units: bits or distinct states

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Units: % or error rate

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Units: states/s or % loss per unit time

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Readout Imperfection | Degree of correlation between system state X and memory outcome M | I(X;M) | Nats or J/K | `MemoryReadoutEdge` attribute | Box 2, Eq 4, 5, 13 | Explicit | Mutual information I(X;M) quantifies correlation, implicitly capturing fidelity. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Analog/Physical Process based

### **5.3 Computational Primitive:**

    *   Content: The fundamental operations discussed are:
        1.  **Measurement/Correlation:** Physically correlating the state of one system (memory M) with another (system X), quantified by mutual information I(X;M). (Eq 4, 12, 13)
        2.  **State Manipulation based on Information (Feedback):** Altering the dynamics or Hamiltonian of system X based on the state of memory M (measurement outcome). (Eq 5, 6, 7)
        3.  **State Reset/Erasure:** Driving a memory system M to a standard, known physical state, irrespective of its initial state, quantified by change in H(M) and F(M). (Eq 9, 10)
        4.  **Information-to-Work Conversion:** Utilizing information (e.g., measured state) to extract work, typically from a thermal reservoir (Szilard Engine). (p.131, Eq 5)
        5.  **Work-to-Information Conversion (Measurement Cost):** Inputting work to create correlation/information between systems. (Eq 13)

### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Process Duration | τ | s | p.134 | Explicit | Used in defining forward/reverse protocols for fluctuation theorems. |
        | Measurement Time | t_ms | s | p.134 | Explicit | Time at which measurement occurs within a protocol [0, τ]. |
        | Memory Lifetime/Retention | "long" | s | p.135 | Explicit (Qualitative) | Memory states must persist; duration depends on energy barriers. |
    *   **Note:** The review primarily operates within a framework where processes occur over a duration τ, with specific events like measurement at t_ms. Physical timescales like relaxation or memory retention are mentioned qualitatively or implied.

### **6.2 Active Inference:**

    *   Content: Partial
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:**
        *   **Prediction Error Proxy:** The stochastic mutual information I(γ,m) = ln[p(m|x(t_ms))/p_m] (Eq below Eq 6) could be interpreted as a measure of "surprise" or information gain from the measurement relative to the prior p_m. Its average is I(X(t_ms);M). Quantifying how feedback protocols minimize expected future work cost (related to free energy) based on this information gain.
        *   **Model Complexity:** Could be related to the information capacity of the memory M (e.g., number of bits used to encode the measurement outcome) or the complexity of the mapping from measurement m to protocol λ_m(t).

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The primary adaptation mechanism discussed is **feedback control**. Information (m) about the system's state (X) at time t_ms is obtained via measurement. This information is then used to select or modify the control protocol λ_m(t) applied to the system for t ≥ t_ms. The goal is typically to optimize some thermodynamic quantity, such as maximizing work extraction (Szilard engine) or minimizing dissipation, by exploiting the acquired information. The specific mapping from measurement outcome 'm' to protocol 'λ_m(t)' defines the adaptation strategy, often designed externally (by the demon/experimenter) to be optimal according to thermodynamic criteria (e.g., designing reversible protocols, p.135). This is a form of closed-loop control based on measured state information.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behaviors analyzed are:
        1.  **Work Extraction from a Single Heat Bath:** Using information (e.g., measurement outcome) to seemingly violate the traditional second law by extracting work in a cyclic process coupled to only one reservoir (e.g., Szilard engine extracting kT ln 2).
        2.  **Information Processing Cost Minimization:** Operating physical devices (memories, information channels) at or near the fundamental thermodynamic limits for tasks like measurement, erasure, and computation (e.g., Landauer limit kT ln 2 for erasure).
        3.  **Controlled Dynamics:** Manipulating the trajectory or statistical state of a small system using feedback based on acquired information.
        4.  **Information as Fuel:** Utilizing an ordered memory (low entropy state) as a resource to drive processes or extract work (information reservoirs).
        These behaviors emerge from the interplay of stochastic system dynamics, interaction with thermal reservoirs, and the physical embodiment/manipulation of information.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The review cites experimental work that validates key theoretical predictions:
        1.  **Szilárd Engine Realizations:** Experiments with colloidal particles (Refs 13, 16) and single electrons (Ref 14) demonstrated the principle of extracting work (kT ln 2) using information (Fig 1).
        2.  **Fluctuation Theorem Verification:** The integral fluctuation theorem incorporating information (Eq 7) was experimentally verified using a single-electron Szilárd engine (Ref 14). The generalized Jarzynski equality under feedback (Eq derived from Eq 7, related to Eq 5) was verified using a colloidal particle (Ref 13).
        3.  **Landauer's Principle Verification:** The minimum energy cost for bit erasure was experimentally verified using colloidal particles in a double-well potential (Ref 15, 41, 42), confirming the kT ln 2 bound. Ref 16 validated symmetry-breaking aspects related to erasure cost.
        These validations rely on careful experimental control, precise measurement of work, heat, and information (often inferred from measured positions/states), and statistical analysis of fluctuations over many trials. Limitations often involve measurement precision, achieving true quasi-static/reversible processes, and perfectly isolating the system.

---

#Key: [muller_what_2017-1]

# What Is Morphological Computation? On How the Body Contributes to Cognition and Control

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper analyzes the concept of "Morphological Computation" (MC), defined as the contribution of the physical body (morphology) to cognition and control in natural and artificial agents. It argues that the common "offloading computation from brain to body" perspective is misleading. The paper investigates four characteristic case studies often cited as MC: (1) passive dynamic walkers, (2) self-stabilizing mechanisms (robots, geckos, grippers), (3) insect eye morphology facilitating perception, and (4) physical reservoir computing. The purpose is to clarify the concept of MC, categorize different phenomena attributed to it, and argue that true morphological *computation* is rare. Instead, the paper distinguishes morphology that facilitates control, morphology that facilitates perception, and morphological computation proper (identified primarily with reservoir computing). The focus is on understanding how body structure contributes to intelligent behavior, rather than simply offloading computation.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                       | Value   | Units   | Source (Fig/Table/Section)   | Implicit/Explicit   | Data Reliability (High/Medium/Low)   | Derivation Method (if Implicit)   |
        | :----------------------------------- | :------: | :------: | :-------------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters characterize the *analysis framework* and key *concepts* discussed in the paper, rather than a single implemented system. Reliability is High as these are explicitly defined or enumerated within the paper's text. Reservoir computing properties listed are high dimensionality, nonlinearity, fading memory, plus others implied (requirements for physical realization).

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Varies depending on the case study. E.g., Gravitational potential energy (Passive Walker), Chemical energy (muscle actuation), Electrical energy (robot motors, computation), Light energy (Fly Eye). The paper doesn't focus on quantifying this.

### **2.2 Energy Transduction**

    *   Content: Varies. E.g., Potential to Kinetic (Passive Walker), Chemical to Mechanical (Muscles/Motors), Electrical to Mechanical (Motors), Light to Electrical/Chemical (Photoreceptors). The paper focuses more on information/control flow than detailed energy transduction pathways. Reservoir computing involves transduction from input streams to physical system dynamics (e.g., mass-spring oscillations) and then to output readouts.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper mentions energy efficiency conceptually (e.g., passive walkers are energy-efficient [Sec 2.1, 2.2], RHex aims for energy efficiency [Sec 2.2]). However, it does not provide quantitative efficiency metrics or a comparative analysis that would allow for a general score for "morphological computation" concepts. The efficiency varies greatly depending on the specific system and task.

### **2.4 Energy Dissipation**

    *   Content: Implicitly present in all physical systems discussed (e.g., friction in walkers, heat in computation, viscoelastic damping in soft bodies). Not quantified or analyzed as a primary topic. Reservoir computing relies on "fading memory," which implies energy dissipation within the reservoir dynamics.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceed with M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial
        *   Passive Dynamic Walker (Sec 2.1): The stable walking gait emerges spontaneously from the local interactions (gravity, inertia, contact forces) acting on the pre-defined morphology on a slope, without explicit global control dictating the pattern. The order (stable gait) is designed *into* the morphology and environment interaction, but the dynamic pattern itself emerges.
        *   Reservoir Computing (Sec 2.4): The complex spatiotemporal patterns within the reservoir emerge from the local interactions (e.g., spring forces, neuronal connections) driven by input signals and the system's intrinsic dynamics. The specific useful computation relies on tapping into this emergent dynamic behavior.
        *   Self-stabilization (Sec 2.2): Stability emerges from the interplay of local mechanical feedback loops and interaction with the environment.

**(Conditional: If M4.1 is "Partial", include M4.2-M4.7 where applicable)**

### **4.2 Local Interaction Rules:**

    *   Content: Varies by example:
        *   Passive Walker: Newtonian mechanics (gravity, inertia, friction, contact forces, constraints of joints). Equations of motion describe these local rules (referenced, e.g., [52]).
        *   Self-Stabilization (RHex): Mechanical feedback loops, material properties (elasticity), motor reflexes (simple control loops connecting local sensors and actuators). Governed by mechanics and simple control laws (referenced, e.g., [5, 42]).
        *   Reservoir Computing (Mass-Spring): Hooke's Law for springs, Newton's second law for masses, potentially damping forces. Input forces applied to specific masses, readout measured from others. Defined by system's physical equations (referenced, e.g., [27, 28]).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID             | Description                  | Parameter Name       | Parameter Value Range   | Units       | Data Source       | Implicit/Explicit   | Justification                                  |
    | :------------------ | :--------------------------- | :------------------- | :-------------------- | :---------- | :---------------- | :----------------: | :--------------------------------------------- |

### **4.3 Global Order:**

    *   Content:
        *   Passive Walker: Stable, periodic walking gait down an incline.
        *   Self-Stabilization: Maintenance of a stable state or trajectory (e.g., stable walking gait, balanced posture) despite perturbations. Attracting properties of certain gaits.
        *   Reservoir Computing: Complex, high-dimensional, recurrent spatiotemporal patterns of activity within the physical system (the "reservoir state"). Can include limit cycles if feedback is applied.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID             | Description                    | Parameter          | Value Range   | Units      | Implicit/Explicit   | Justification                 | Source             |
| :------------------ | :----------------------------- | :----------------- | :---------- | :--------- | :----------------: | :---------------------------- | :----------------- |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID           | Description             | Parameter           | Value Range   | Units      | Implicit/Explicit   | Justification                        | Protocol               | Source          |
| :-------------------- | :---------------------- | :------------------ | :---------- | :--------- | :----------------: | :-----------------------------------| :--------------------- | :-------------- |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "Yes", include M5.2-5.4, focusing on the cases identified as computation, i.e., Reservoir Computing)**

### **5.2 Computation Type:**

    *   Content: Analog / Reservoir Computing / Natural Computation

### **5.3 Computational Primitive:**

    *   Content: Spatiotemporal transformation / Nonlinear filtering / Projection into high-dimensional state space. The core operation of the physical reservoir is to take an input time series and map it onto a high-dimensional, nonlinear, dynamic state trajectory within the physical system. This transformation unfolds the temporal information into a spatial pattern at any given time, allowing simple readouts to perform complex tasks (e.g., classification, prediction, function approximation). Hauser et al. [27, 28] are cited, describing it as viewing the morphology as a "fixed nonlinear kernel" providing high-dimensional projections and nonlinear combinations of input (Sec 2.4). It can also autonomously generate complex dynamics (e.g., limit cycles) when feedback is added (Sec 2.4, 4.4).

### **5.4 Embodied Computational Units**
| Unit ID         | Description                       | Processing Power   | Energy/Operation   | Freq/Resp. Time   | Bit-Depth   | Data Source       | Implicit/Explicit   | Justification                                                |
| :-------------- | :-------------------------------- | :----------------- | :----------------- | :---------------- | :---------- | :---------------- | :----------------: | :----------------------------------------------------------- |
* **Note:** Processing power is task-dependent (e.g., classification accuracy, prediction error). Energy/Operation not discussed. Speed/frequency depends heavily on the physical substrate (slow for macroscopic bodies, fast for optoelectronics). Bit-depth is inherently Analog for the physical dynamics.

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description        | Value         | Units       | Source                  | Implicit/Explicit   | Justification                                                                    |
        | :--------------------------- | :------------ | :---------- | :---------------------- | :----------------: | :------------------------------------------------------------------------------- |
    *   **Note:** Specific quantitative values are not provided in this review paper. Timescales depend heavily on the specific physical implementation of each case study. Reservoir computing explicitly operates in continuous time.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Partial
        *   Reservoir Computing (Sec 2.4): Adaptation occurs during the *training phase* where readout weights are modified via learning algorithms (e.g., linear regression) to achieve the desired input-output mapping. The reservoir dynamics themselves are typically fixed ("view the morphological structure as some fixed nonlinear kernel" - Sec 2.4 quoting Hauser). If feedback loops are trained, the *generation* of patterns becomes adaptive during training.
        *   Rückert & Neumann study (Sec 3.2.5): Shows that optimal morphology *depends* on the controller, and controllers are *learned* (adapted) for each morphology, demonstrating adaptation *within the control system* interacting with morphology.
        *   Other examples (Passive Walker, Gecko, Fly Eye): Presented as largely fixed systems evolved/designed for specific tasks, not exhibiting ongoing adaptive plasticity in their core morphological function as described here. Evolution itself is an adaptive process that shapes morphology, but the paper focuses on the resulting morphology's contribution, not the evolutionary process itself.

**(Conditional: If M7.1 is "Partial", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: For Reservoir Computing readouts: Supervised learning algorithms (e.g., linear regression) are used to adjust the weights connecting the reservoir states to the output units based on training data (Sec 2.4). For trainable feedback loops, similar learning algorithms would apply. For the Rückert & Neumann example: Stochastic optimal control methods / Reinforcement learning to learn control policies (Sec 3.2.5). The paper does not delve into the specific algorithms.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The paper discusses several functional behaviors emerging from morphology:
        *   Locomotion: Passive walking (Sec 2.1), Actuated walking/running (RHex, Sec 2.2), Climbing (Gecko, Stickybot, Sec 2.2).
        *   Manipulation: Grasping (Coffee balloon gripper, Sec 2.2).
        *   Stability/Control: Self-stabilization during locomotion (Sec 2.2), Balancing (Rückert & Neumann pendulum, Sec 3.2.5).
        *   Perception/Sensing: Distance gauging/obstacle avoidance via visual processing (Fly eye, robot vision, Sec 2.3), Sensing terrain properties (Tensegrity robot, Sec 2.4).
        *   Computation/Pattern Generation: Performing spatiotemporal transformations, classification, prediction (Reservoir Computing, Sec 2.4), Emulating dynamical systems (e.g., oscillators, gaits) (Reservoir Computing, Sec 2.4).

### **8.2 Behavior Robustness:**

        *   Self-stabilization implies robustness to small perturbations (Sec 2.2). RHex exhibits "unprecedented mobility" suggesting robustness (Sec 2.2). Passive walkers operate robustly within their designed environment (Sec 3.2.5 mentions maintaining balance robustly). Taga's work cited for human locomotion notes robustness against perturbations (Sec 2.2).
        The paper does not provide quantitative robustness metrics or a general assessment. The conclusion mentions that exploiting morphology may lead to less versatile solutions dependent on specific morphology/environment (Sec 6.3), implying potential fragility outside the design domain.

### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper validates its claims about behavior primarily by citing and summarizing findings from existing literature (experiments, simulations, theoretical analyses) for each case study.
        *   Passive Walker: Cites McGeer [52], Collins et al. [15, 14] reporting experimental and simulation results (Sec 2.1, Fig 2a, 2b).
        *   Self-Stabilization/RHex: Cites Blickhan et al. [5], Koditschek et al. [42], Saranli et al. [71] reporting analysis and experimental results (Sec 2.2, Fig 2c).
        *   Gecko/Stickybot: Cites Autumn et al. [2], Kim et al. [39] reporting experimental findings (Sec 2.2, Fig 2d).
        *   Gripper: Cites Brown et al. [8] reporting experiments (Sec 2.2, Fig 2e).
        *   Fly Eye/Robot: Cites Franceschini et al. [25], Floreano et al. [23] reporting analysis and robot implementation (Sec 2.3, Fig 3).
        *   Reservoir Computing: Cites Hauser et al. [27, 28], Nakajima et al. [57, 58], Caluwaerts et al. [10, 11] reporting simulation and experimental results (Sec 2.4, Fig 4).
       Limitations: Validation relies on the cited works; this paper performs a conceptual analysis based on them.

---

#Key: [di_ventra_parallel_2013]

# The parallel approach

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper describes "memcomputing," a proposed computing paradigm utilizing "memelements" (memory circuit elements) - specifically memristive, memcapacitive, and meminductive systems. These are defined as two-terminal passive devices whose response `y(t) = g(x,u,t)u(t)` depends on internal state variables `x` that evolve according to `dx/dt = f(x,u,t)`, driven by input `u(t)`. The purpose is to achieve massively parallel computation where information processing and storage are physically co-located within the memelement network, overcoming the von Neumann bottleneck. The system consists of arrays of memelements, potentially combined with traditional circuit components (like CMOS, though minimized), connected to external voltage/current sources. Computation occurs through the collective evolution of the system's state.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name             | Value        | Units   | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------------------- | :----------: | :-----: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
    *   **Note:** Values are cited from referenced works (19, 20) as examples meeting the criteria, not necessarily measured within this commentary itself. Reliability is Medium as they are cited examples, not primary data from this paper.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is external electrical power, supplied via voltage or current sources connected to the memelement network to drive the state changes and perform computation.

### **2.2 Energy Transduction**

    *   Content: Electrical energy input (voltage/current) is transduced into changes in the internal physical state variables (`x`) of the memelements. This could involve ionic motion (e.g., in nanoionic switches), changes in spin polarization, or other nanoscale physical phenomena. This state change, in turn, modulates the electrical properties (resistance, capacitance, inductance) of the device, affecting the overall circuit dynamics and potentially dissipating energy as heat (Joule heating). Memcapacitors and meminductors are noted to store energy in electric and magnetic fields, respectively.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper emphasizes low power consumption as a desirable feature (Criterion 6 rationale) and cites a potentially very low write energy (~5 x 10^-14 J/bit). It also notes that memcapacitors and meminductors could consume "little or virtually no energy," suggesting high efficiency is a key goal and potentially achievable. However, overall system efficiency isn't quantified. The score reflects the stated goal and cited low energy figure, balanced by the lack of overall system analysis.

### **2.4 Energy Dissipation**

    *   Content: The primary dissipation mechanism, especially for memristive systems, would be Joule heating due to current flow through the device resistance. The magnitude depends on the device's state (resistance) and the applied signal. The paper aims to minimize power consumption, implying minimization of dissipation, but doesn't quantify dissipation mechanisms directly. Memcapacitors and meminductors are suggested as potentially more energy-efficient (less dissipative). Qualitative Assessment: Medium (for memristors, intended to be low), Low (potentially for memcapacitors/inductors).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: ~7 years (example); "Sufficiently long"; Ideally "Non-volatile"
*    Units: Time (years, seconds), cycles

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :-------------------------------------- | :-----------: | :----: | :-------------------------: | :---------------------------------------------------------: |:-----------------:| :--------------------------------------------------------------- |
    | Endurance | Number of write cycles before failure | >10^5, 10^12  | cycles | `MemoryNode`.`endurance`      | Criterion 2 (citing refs 19, 20)                             | Explicit          | Explicitly cited as key characteristic.                          |
    | Retention | Duration state is held without power    | ~7            | years  | `MemoryNode`.`retention_time` | Criterion 2 (citing ref 19)                                  | Explicit          | Explicitly cited as key characteristic.                          |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes (Functional)

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are governed by the physics of the memelements themselves and the circuit they are embedded in. Specifically: (1) The state `x` of each memelement evolves based on its *local* input `u(t)` according to `dx/dt = f(x,u,t)`. (2) The output `y(t)` (e.g., voltage) of each memelement depends on its state `x` and input `u(t)` via `y(t) = g(x,u,t)u(t)`. (3) Kirchhoff's laws dictate how these local inputs and outputs relate within the network connecting the memelements. The state change of one device influences the voltage/current distribution in the network, thereby affecting the input `u(t)` and subsequent state evolution `dx/dt` of *other* connected devices.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID                 | Description                 | Parameter Name | Parameter Value Range | Units | Data Source                                     | Implicit/Explicit | Justification                             |
    | :---------------------- | :-------------------------- | :------------- | :-------------------- | :---- | :---------------------------------------------- | :---------------- | :---------------------------------------- |
    | State Variable Contrast | Difference in g in limits   | e.g., R_on/R_off | >1 (Implicitly Large) | ratio | Criterion 4 ("strong memory content")           | Mixed             | Need for distinct states is explicit. |

### **4.3 Global Order:**

    *   Content: The emergent global order described is the *solution* to a computational problem, such as finding the optimal path(s) in a maze. This solution manifests as a specific configuration of the internal states (`x`) of the memelements across the network after the system dynamics settle or evolve for a sufficient time. For the maze problem, high/low resistance states might represent blocked/open paths.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID             | Description                                      | Parameter                       | Value Range           | Units | Implicit/Explicit | Justification                                                                       | Source                     |
| :------------------ | :----------------------------------------------- | :------------------------------ | :-------------------- | :---- | :---------------- | :---------------------------------------------------------------------------------- | :------------------------- |
| Network Coupling    | Kirchhoff's Laws / Circuit Topology            | Resistance, Capacitance etc.    | Network Dependent     | Ohms, F | Implicit          | Governs how local outputs/inputs are related across the network.                    | Standard Circuit Theory    |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID   | Description                                                              | Parameter            | Value Range                          | Units | Implicit/Explicit | Justification                                                              | Protocol          | Source                 |
| :------------ | :----------------------------------------------------------------------- | :------------------- | :----------------------------------- | :---- | :---------------- | :------------------------------------------------------------------------- | :---------------- | :--------------------- |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :---------------- | :------------ | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog/Hybrid

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operation is the state-dependent transfer function defined by `y(t) = g(x,u,t)u(t)` coupled with the state evolution `dx/dt = f(x,u,t)`. This represents a time-varying, history-dependent modulation of the input signal `u(t)` based on the internal state `x`. Depending on the specific forms of `f` and `g` and the network configuration, this primitive can implement various functions, including (as mentioned) logic operations (like gates) or contribute to solving optimization problems (like maze solving).
    *   **Sub-Type (if applicable):** State-dependent signal modulation / Differential equation solver / Logic gate (specific examples)

### **5.4 Embodied Computational Units**
| Unit ID    | Description                                  | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth   | Data Source             | Implicit/Explicit | Justification                                   |
| :--------- | :------------------------------------------- | :--------------- | :--------------- | :--------------: | :----------: | :---------------------- |:-----------------:| :---------------------------------------------- |

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description     | Value           | Units            | Source                 | Implicit/Explicit | Justification                                            |
        | :------------------------ | :-------------: | :--------------: | :--------------------- | :---------------- | :------------------------------------------------------- |
        | Write Speed (Example)     | < 10            | ns               | Criterion 2 (ref 19)   | Explicit          | Explicitly cited characteristic.                         |
        | Memory Retention (Example)| ~7              | years            | Criterion 2 (ref 19)   | Explicit          | Explicitly cited characteristic.                         |
    *   **Note:** Timescales relate to individual device operations (write, retention) and overall computation.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism is the physical change of the internal state variables `x` within each memelement, driven by the input signal `u(t)` according to the function `f(x,u,t)`. Examples of `x` include ion positions or spin polarization. This change is intrinsic to the device physics. In networks, this local adaptation, coupled through circuit laws, leads to collective adaptation, such as the self-reinforcement observed in the maze-solving example where paths leading to the solution might experience state changes (e.g., resistance reduction) that favor them over time. It's essentially adaptation driven by physical dynamics and network feedback.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behavior described is massively parallel computation, specifically applied to tasks like optimization problems (e.g., maze solving) and potentially implementing logic functions (memristive logic). The system's collective dynamics evolve to find solutions or perform operations. A key behavioral characteristic is the co-location of information storage and processing.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper cites external work (ref 11) for the maze-solving example, implying validation through simulation or experiment in that cited work. Within this commentary, the validation is primarily conceptual, arguing that the defined properties and collective dynamics *lead* to these behaviors. Criterion 6 (Robustness) is justified by analogy to the brain and the observed behavior in the maze example handling topological changes (citing ref 11). No new experimental data or detailed simulation results validating emergent behavior are presented *within this paper*.

---

#Key: [muller_what_2017]

# What Is Morphological Computation? On How the Body Contributes to Cognition and Control

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper analyzes the concept of "morphological computation," questioning the idea that body morphology "offloads" computation from the brain. It investigates four characteristic case studies: (1) Passive Dynamic Walkers (purely mechanical robots walking down an incline), (2) Self-Stabilizing Machines/Gecko Feet/Coffee Grippers (systems using morphology for stability, adhesion, or grasping, often with simple control), (3) Insect Eyes (fly eyes with non-uniform photoreceptor distribution preprocessing visual information), and (4) Physical Reservoir Computing (systems like mass-spring networks or soft bodies used as computational reservoirs). The purpose is to clarify when morphology truly computes versus when it facilitates control or perception, concluding that true morphological computation is rare and the focus should be on how morphology orchestrates behavior.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                      | Value                 | Units     | Source (Fig/Table/Section)   | Implicit/Explicit   | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit)   |
        | :---------------------------------- | :--------------------: | :--------: | :-------------------------: | :------------------: | :-------------------------------: | :---------------------------------: |

    *   **Note:** As a conceptual/review paper analyzing different systems, specific implementation parameters with numerical values are not the focus and are generally absent in the provided excerpt. The table lists the key *categories* analyzed and essential *qualitative properties* discussed for Reservoir Computing.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: For Passive Dynamic Walkers (Case 1), the primary energy source is potential energy due to gravity on an incline. For other cases (active robots, animals, reservoir computers), energy sources vary (e.g., motors/actuators powered electrically, metabolic energy for animals, input signals for reservoir computers) but are not detailed in the excerpt beyond mentioning actuators or the need for power in non-passive systems. Reservoir computing examples use input data streams as their primary "computational" input, which require energy to generate/transmit, but the physical energy source for the reservoir itself (e.g., maintaining mass-spring system) isn't specified.

### **2.2 Energy Transduction**

    *   Content: Passive Walker: Potential energy to kinetic energy of motion. Self-Stabilizing/Actuated Systems: Chemical/electrical energy to mechanical work via actuators/muscles, mechanical energy dissipated/stored in compliant structures. Gecko Feet: Metabolic energy potentially involved in limb movement, adhesion relies on van der Waals forces (intermolecular energy), not direct energy transduction for sticking itself. Fly Eye: Light energy to electrochemical signals via photoreceptors. Reservoir Computing: Input signal energy transformed/spread through the physical dynamics of the reservoir (e.g., kinetic/potential energy in mass-spring system); output requires energy for readout mechanism.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper mentions passive dynamic walkers are studied partly for their minimal energy use (Sec 2.1) and that exploiting body interaction can lead to energy-efficient locomotion (Sec 2.2, citing Collins et al. [14]). It contrasts stiff robots needing complex control and being slow (Fig 1c) with compliant systems reducing computation/energy (Fig 1a, 1b). However, no quantitative efficiency values or formal metrics are provided for any case study in the excerpt. Qualitative assessments suggest passive/compliant systems are potentially more efficient than stiff, heavily controlled ones. Reservoir computing energy efficiency isn't discussed.

### **2.4 Energy Dissipation**

    *   Content: Not explicitly discussed in terms of mechanisms or quantification. Implicitly, all physical systems involve dissipation. Passive Walkers: Friction, inelastic collisions during foot contact. Actuated Systems: Heat loss in motors/electronics, friction in joints, material damping. Animals: Metabolic heat, friction, damping. Fly Eye: Heat from neural processing. Reservoir Computing: Damping in physical system (required for fading memory), heat from readout computation. Assessment: Medium/High for most active systems, potentially Lower for idealized passive walker.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: **Passive Walker (Case 1):** Newtonian mechanics (gravity, contact forces, friction, constraints of joints) governing segment motion, impact dynamics at foot contact. Parameters include segment lengths, mass distribution, foot shape, slope angle (Sec 2.1). **Reservoir Computing (Case 4):** Physics of the specific reservoir (e.g., Hooke's law for springs, collision dynamics, fluid dynamics for water bucket). Input signals act as driving forces/perturbations. Random but bounded connections within the reservoir are mentioned for neural network versions, implying complex local coupling (Sec 2.4). For physical reservoirs like mass-spring systems, interactions are forces between connected masses (Sec 2.4, Fig 4). Hauser et al. [28] mentioned linear feedbacks are sufficient with nonlinear morphology (Sec 2.4).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID                  | Description                             | Parameter Name                | Parameter Value Range   | Units   | Data Source       | Implicit/Explicit   | Justification                                    |
    | :----------------------- | :-------------------------------------- | :---------------------------- | :---------------------- | :------: | :----------------: | :------------------: | :-------------------------------------------------: |

### **4.3 Global Order:**

    *   Content: **Passive Walker (Case 1):** Stable walking gait (periodic motion down the slope) (Sec 2.1). **Reservoir Computing (Case 4):** Complex spatiotemporal patterns of activity within the reservoir that represent transformed input streams (Sec 2.4). Can generate stable limit cycles (e.g., Van der Pol oscillator, Lissajous figures) when feedback is added (Sec 2.4). Specific gaits for a quadruped were generated using outputs from a mass-spring reservoir (Sec 2.4).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID                  | Description                     | Parameter                  | Value Range   | Units     | Implicit/Explicit   | Justification                      | Source             |
| :----------------------- | :------------------------------ | :------------------------- | :------------ | :--------: | :------------------: | :---------------------------------: | :-----------------: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID        | Description                   | Parameter               | Value Range   | Units        | Implicit/Explicit   | Justification                         | Protocol                | Source           |
| :----------------- | :---------------------------- | :---------------------- | :------------ | :-----------: | :------------------: | :------------------------------------ | :---------------------- | :---------------: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type                | Description                                     | Predictability    | Yoneda Score   | Metrics   | Implicit/Explicit   | Justification                                                                 | Source                 |
    | :----------------------- | :---------------------------------------------- | :---------------- | :------------- | :-------- | :------------------: | :------------------------------------------------------------------------------:| :-----------------------: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Reservoir Computing (potentially Analog/Hybrid)

### **5.3 Computational Primitive:**

    *   Content: Spatiotemporal Transformation / Nonlinear Kernel Mapping. The core function of the physical reservoir is described as performing a complex, nonlinear, dynamic transformation of input streams into a higher-dimensional state space, effectively acting as a nonlinear kernel or filter where temporal information is spread out spatially (Sec 2.4, 4.4). This high-dimensional projection allows simple linear readouts to perform complex tasks like classification, prediction, or emulation of nonlinear dynamical systems (Sec 2.4). Hauser et al. [27, 28] are cited viewing it as a fixed nonlinear kernel providing high-dimensional projections and nonlinear combinations of input (Sec 2.4).

### **5.4 Embodied Computational Units**
| Unit ID            | Description                                    | Processing Power   | Energy/Operation   | Freq/Resp. Time   | Bit-Depth   | Data Source   | Implicit/Explicit   | Justification                                       |
| :----------------- | :--------------------------------------------- | :----------------- | :----------------- | :---------------- | :----------: | :------------: |:-----------------:| :--------------------------------------------------: |

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description               | Value              | Units   | Source             | Implicit/Explicit   | Justification                                       |
        | :---------------------------------- | :-----------------: | :------: | :-----------------: | :------------------: | :---------------------------------------------------: |
        | Reservoir Fading Memory             | Fading/Short-term   | s       | Sec 2.4, 4.4        | Explicit (Qual.)    | Explicitly described as "fading memory"              |
        | Reservoir Dynamics Response Time    | Slow (for macro)    | s       | Sec 5.3             | Explicit (Qual.)    | Stated macroscopic systems are "very slow"           |

    *   **Note:** Quantitative timescales are generally not provided. The assessment relies on qualitative descriptions or inferences from the system's behavior.

### **6.2 Active Inference:**

    *   Content: Unclear

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes (Partial)

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: For Reservoir Computing (Case 4), the adaptation mechanism discussed is supervised learning (e.g., linear regression) applied *only* to the readout weights (connections from the reservoir state to the output layer). The internal reservoir weights and input weights are typically left untrained (fixed random values) (Sec 2.4). The goal is to adjust the readout to correctly interpret the reservoir's complex state and produce the desired output for tasks like classification, prediction, or dynamical system emulation. If feedback loops are added and trained, the system can learn to generate autonomous output streams (Sec 2.4). The paper mentions Beer [4] involves categorization task learning (Sec 2.4).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: **Case 1 (Passive Walker):** Stable walking locomotion down an incline (Sec 2.1). **Case 2 (Self-Stabilizing etc.):** Energy-efficient locomotion on level ground (actuated walker, RHex), self-stabilization against perturbations, climbing smooth vertical surfaces (gecko/Stickybot), grasping diverse objects (coffee gripper) (Sec 2.2). **Case 3 (Fly Eye):** Nonlinear transformation (preprocessing) of visual input to facilitate distance gauging/obstacle avoidance (Sec 2.3). **Case 4 (Reservoir Computing):** Spatiotemporal transformation of input streams, classification/prediction, emulation of complex nonlinear dynamical systems (limit cycles, gaits) (Sec 2.4).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper primarily relies on citing existing work (experimental and simulation) that demonstrates the behaviors. For passive walkers, citations refer to McGeer [52] and Collins et al. [15] (Sec 2.1, 4.1). For self-stabilization, Blickhan et al. [5], Koditschek et al. [42], Taga [76, 77] are cited (Sec 2.2). For gecko adhesion, Autumn et al. [2] and Kim et al. [39] (Stickybot) are cited (Sec 2.2). For the gripper, Brown et al. [8] is cited (Sec 2.2). For the fly eye, Franceschini et al. [25] and other artificial eye designs [73, 23] are cited (Sec 2.3). For reservoir computing, numerous theoretical and simulation/experimental works are cited [47, 22, 27, 28, 38, 10, 11, 19, 57, 58] covering simulation and physical implementations (Sec 2.4, 4.4). Validation relies on the results reported in these cited studies (often demonstrated through physical robots, simulations, or biological experiments). The paper itself performs conceptual analysis and classification based on these cited validations.

---

#Key: [ciaunica_nested_nodate]

# Nested Selves: Self-Organization and Shared Markov Blankets in Prenatal Development in Humans

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### 1.1 System Description

    *   Content: The paper describes a theoretical/conceptual system analyzing biological self-organization during human pregnancy. The core 'system' consists of two nested, interacting biological self-organizing entities: the developing fetus and the pregnant person (mother). The analysis framework primarily uses Active Inference (FEP), Markov blankets, and concepts from immunology and developmental biology. The purpose is to understand the emergence of selfhood and biological self-organization when one system (fetus) develops within another (mother), focusing on the role of the interacting immune systems and the placenta as a shared boundary/interface. It proposes that pregnancy involves a dynamic interplay and negotiation between these nested systems, mediated by shared Markov blankets, influencing co-homeostasis and the individuation process. Components include the mother, fetus, their respective immune and neural systems, the placenta, and theoretical constructs like generative models and Markov blankets. The system *does* theoretical analysis and modeling of biological processes, particularly immune interactions and self-organization during pregnancy.

### 1.2 Implementation Clarity


### 1.3 Key Parameters

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Gestational Stage Dependency | Pro-/Anti-inflammatory phases | Qualitative stages | Section 3 | Explicit | Medium | Based on cited immunological studies |
        | Immune Cell Populations (Decidua) | NK cells (~70%), Macrophages (~20-25%), Dendritic cells (~1.7%) | % of decidual leukocytes | Section 3 | Explicit | Medium | Citing Bulmer et al. (1988), King et al. (1997), Mor et al. (2006) |
        | Fetal Oxygen Environment (pO2) | 16 - 27 | mm Hg | Section 4 | Explicit | Medium | Cited as "Mount Everest in utero" concept |

    *   **Note:** Parameters listed relate to the biological system being modeled and the theoretical framework used. Values are mostly qualitative or reference typical biological ranges/concepts. Reliability is Medium as values are cited from other sources or are conceptual.

## M2: Energy Flow

### 2.1 Energy Input

    *   Content: Biological metabolic energy derived from the mother's nutrient intake, sustaining both the mother and the developing fetus via the placenta.

### 2.2 Energy Transduction

    *   Content: Biochemical energy from metabolism is transduced into various forms to support fetal growth, maternal physiological changes, immune system activity (e.g., cell production, cytokine release), neural activity, and maintenance of homeostasis/allostasis in both organisms. Energy is used for biosynthesis, transport across the placenta, mechanical work (fetal movement, maternal bodily changes), and information processing (neural, immune signaling).

### 2.3 Energy Efficiency

    *   Justification/Metrics: The paper does not discuss or quantify the thermodynamic efficiency of the metabolic or information processing aspects of the mother-fetus system in a way that allows for scoring. Biological systems are generally far from thermodynamically reversible, implying low-medium efficiency, but this is not assessed in the text.

### 2.4 Energy Dissipation

    *   Content: Energy is dissipated primarily as heat due to metabolic processes, immune responses (inflammation), physical activity, and the inherent inefficiency of biological energy transformations, consistent with the second law of thermodynamics. The paper mentions avoiding entropic dissipation (Schrödinger, 1956) as a characteristic of living systems (Section 1), implying dissipation occurs but is managed through self-organization. Quantification is not provided. Assessment: High (inherent in biological systems).

## M3: Memory

### 3.1 Memory Presence:

    *   Content: Yes

**(Conditional: M3.1 is "Yes", proceeding with M3.2 and M3.3.)**

### 3.2 Memory Type:**

    *   **Immune Memory:** Biological, adaptive, long-term molecular/cellular memory (High fidelity potential, but mechanism detail limited in excerpt).
    *   **Active Inference Priors:** Information-theoretic/computational memory encoded in the parameters of generative models (potentially complex, adaptable, influencing behavior). Fidelity depends on model structure and learning.
    *   **Developmental Trajectory (Homeorhesis):** Persistence of developmental state influencing future progression (Structural/dynamic memory).
    *   **Transgenerational Information (Flavors):** Chemical information transfer with lasting effects (Implicit memory).
    The score reflects the presence of sophisticated biological (immune) and theoretical (Active Inference priors) memory concepts central to the argument, although the mechanistic details and quantitative aspects (capacity, readout accuracy) are not specified for this specific context in the excerpt. Retention varies (immune memory can be long-term, priors are constantly updated, developmental states persist).

### 3.3 Memory Retention Time:**

*   Value: Varies (Short-term to Long-term)
*    Units: Qualitative Descriptor

### 3.4 Memory Capacity (Optional - if applicable)


### 3.5 Readout Accuracy (Optional - if applicable)


### 3.6 Degradation Rate (Optional - if applicable)

### 3.7 Memory Operations Energy Cost (Optional - if applicable)
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### 3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### 4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", proceeding with M4.2-M4.7)**

### 4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are primarily biological and informational:
        1.  **Cell-Cell Interactions:** During embryogenesis, determining individual boundaries within the blastoderm (Section 4). Based on bioelectrical and biochemical signaling (Implicit).
        2.  **Immune Cell Interactions:** Recognition (self/non-self/aberrant-self), signaling (cytokines), activation, suppression, migration, and cooperation between maternal and fetal immune cells, particularly at the placental interface (trophoblast, decidua - Section 3). Involves pattern recognition receptors, antigen presentation, etc. (Explicit mention of components, implicit detail of rules).
        3.  **Metabolic Exchange:** Transfer of nutrients, gases, waste across the placenta governed by concentration gradients, transport mechanisms (Section 4). (Implicit detail).
        4.  **Hormonal Signaling:** Hormones like hCG, progesterone, estrogen regulating maternal and fetal development (Section 3). (Explicit mention, implicit detail of rules).
        5.  **Active Inference Dynamics:** Bayesian belief updating based on prediction errors minimizing free energy. Sensory states (interoceptive, immune signals) update beliefs (priors), which generate predictions and guide actions (physiological adjustments, immune responses) (Section 2, 4). Rules are formalized by Bayesian probability and variational calculus (Implicit detail of specific equations).
    * **Implicit/Explicit**: Mixed

### 4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |

### 4.3 Global Order:**

    *   Content: The emergent global order includes:
        1.  **Organismic Integrity:** Maintenance of distinct, viable mother and fetus as self-organizing systems.
        2.  **Co-Homeostasis/Co-Allostasis:** Dynamically regulated physiological balance achieved through the interaction and mutual influence of the two systems.
        3.  **Immune Tolerance/Cooperation:** Successful establishment and maintenance of pregnancy despite the fetus being semi-allogeneic, involving modulated immune responses rather than rejection.
        4.  **Placental Structure/Function:** The emergent relational organ mediating exchange and communication.
        5.  **Individuation/Selfhood:** The (theorized) emergence of a distinct self for the developing fetus through interaction with the maternal environment.
    * **Implicit/Explicit**: Mixed

### 4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### 4.5. Local Interaction Rules (for Self-Organization)
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| ActiveInf | Minimization of Free Energy | Variational Free Energy | Scalar | Nats | Explicit | Core principle governing dynamics in AI framework. | Section 2, 5 |

### 4.6. Globally Emergent Order and Order Parameters
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### 4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 6
        * Rubric: Score reflects how well behaviors/properties at a higher scale can be understood as functors preserving the structure of interactions at the lower scale. 0 = No relationship; 5 = Qualitative relationship observed; 10 = Formal functorial mapping demonstrated.
        * Example: Immune cell interactions (local) map to overall immune tolerance (global). The mapping is complex and not fully formalized but conceptually present.


## M5: Computation

### 5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: M5.1 is "Yes", proceeding with M5.2-5.4)**

### 5.2 Computation Type:**

    *   Content: Other (Bayesian Inference / Predictive Processing)

### 5.3 Computational Primitive:**

    *   Content: The most basic computation is the calculation of prediction error (discrepancy between predicted and actual sensory states) and the subsequent Bayesian belief updating (adjusting posterior beliefs based on priors and likelihood/prediction error) to minimize variational free energy.
    *   **Sub-Type (if applicable):** Bayesian Belief Update / Prediction Error Calculation. Mathematically, involves operations like integration, differentiation, and probabilistic calculations (e.g., using Bayes' theorem or variational approximations).

### 5.4 Embodied Computational Units
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|


## M6: Temporal Dynamics

### 6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Immune Response (Innate) | Minutes to Hours | Time | Implicit | General knowledge, not specified. | Biological timescale |
        | Immune Response (Adaptive) | Days to Weeks | Time | Implicit | General knowledge, not specified. | Biological timescale |
        | Hormonal Changes (e.g., hCG peak) | Weeks | Time | Implicit | General knowledge, linked to Section 3 mention. | Biological timescale |
        | Gestational Stages (Trimesters) | Months (approx. 3 per stage) | Time | Explicit | Section 3 describes 3 stages. | Biological timescale |
        | Active Inference Belief Updating | Milliseconds to Seconds (Neural) | Time | Implicit | Typical neural processing timescales, mentioned in Section 2. | Biological/Computational timescale |
        | Fetal Development | Months | Time | Implicit | Overall process duration. | Biological timescale |

### 6.2 Active Inference:**

    *   Content: Yes
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:**
        *   Rate of free energy minimization during specific physiological challenges (e.g., maternal infection, glucose fluctuation).
        *   Mutual information between maternal and fetal states mediated by the placental Markov blanket.
        *   Complexity (e.g., KL divergence) of the generative models required to explain observed physiological stability/adaptation across different gestational stages.
        *   Timescale of anticipatory regulation (allostasis) vs reactive regulation (homeostasis) in response to simulated perturbations.
        *   Quantification of prediction error signals in simulated immune responses (e.g., cytokine levels deviating from expected).

## M7: Adaptation

### 7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: M7.1 is "Yes", proceeding with M7.2)**

### 7.2 Adaptation Mechanism:**

    *   Content: Several mechanisms are involved:
        1.  **Active Inference:** Bayesian belief updating. Generative models (priors) are updated based on sensory evidence (prediction errors) to better predict future states and guide actions (minimizing free energy). This is a form of unsupervised learning.
        2.  **Immune Modulation:** Changes in immune cell populations (e.g., regulatory T cells), cytokine profiles, and receptor expression leading to tolerance. This is driven by signaling between maternal, fetal, and placental cells (e.g., trophoblast 'educating' decidual immune cells, Section 3). Related to biological learning/adaptation mechanisms in immunology.
        3.  **Physiological Adaptation:** Adjustments in hormonal levels, metabolic pathways (e.g., insulin sensitivity changes, Section 3), and cardiovascular function to meet the demands of pregnancy. These are homeostatic/allostatic/homeorhetic processes.

## M8: Emergent Behaviors

### 8.1 Behavior Description:**

    *   Content: The main emergent functional behaviors described are:
        1.  **Biological Self-Organization:** Maintenance of distinct organismic integrity and non-equilibrium steady state (life) for both mother and fetus.
        2.  **Co-Homeostasis/Co-Allostasis:** Successful mutual regulation of physiological states between mother and fetus across the placenta.
        3.  **Immune Tolerance/Cooperation:** The establishment and maintenance of a state where the maternal immune system tolerates the semi-allogeneic fetus, involving active modulation and cooperation rather than simple suppression.
        4.  **Successful Gestation & Development:** The progression of pregnancy leading to a viable offspring.
        5.  **Self-Individuation (Theoretical):** The emergence of a distinct 'self' within the developing fetus through interaction within the nested context.

### 8.2 Behavior Robustness:**


### 8.3 CT-GIN Emergent Behavior Validation

     *  Content: The paper validates claims through theoretical coherence and citation of existing biological and immunological empirical findings. It synthesizes concepts from Active Inference, immunology, and developmental biology to build a consistent explanatory framework. Specific emergent behaviors like immune tolerance, hormonal changes, and metabolic shifts are supported by references to empirical studies (e.g., Section 3 cites studies on immune cell populations, insulin sensitivity). The concept of co-homeostasis and shared Markov blankets is presented as a theoretical proposal building on these foundations. No new experimental validation is presented in the excerpt. Limitations include the reliance on existing literature and the theoretical nature of some core concepts (like the specific role of shared Markov blankets in mediating co-homeostasis).

---

#Key: [freitas_emergent_2022]

# Emergent second law for non-equilibrium steady states

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper describes a general class of open systems governed by autonomous Markov jump processes on a discrete state space {n}. These systems interact with an environment (e.g., thermal reservoir at temperature T) and are driven out of equilibrium by external work sources (Wρ). Key components include states (n), transition rates between states (λ ρ(n)), state energies (E(n)), internal entropies (S(n)), and work inputs (Wρ(n)). The purpose is to derive a fundamental relationship between the non-equilibrium steady state (NESS) probability distribution Pss(x) (specifically its self-information I(x) = -log(Pss(x))) and the macroscopic entropy production (Σ) along deterministic trajectories that emerge in a macroscopic limit (Ω→∞, where x = n/Ω). The central result is an "emergent second law" inequality: Σ + kbΔI ≥ 0, which bounds changes in self-information along deterministic paths by the entropy produced. An example application to a CMOS memory cell is provided to illustrate the theory.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** These parameters are fundamental to the theoretical framework presented. Values are typically variable or defined functionally, except for β and the limit for Ω.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input driving the system away from equilibrium is external work, denoted Wρ(n) for a transition ρ from state n. This work is provided by external sources (e.g., chemical potentials, electrostatic potentials/voltage sources in the CMOS example).
    *   Value: Varies depending on the transition and state. Represented by Wρ(n) or Ẇ = <Ẇ>.
    *   Units: J (for Wρ) or W (for Ẇ)

### **2.2 Energy Transduction**

    *   Content: Energy is transduced via state transitions. The external work input Wρ(n) during a transition, along with changes in internal energy ΔE = E(n+Δρ) - E(n), results in heat exchange Qρ(n) with the environment. The relationship is Qρ(n) = ΔE - Wρ(n) based on energy conservation. The system transduces energy from the external work source into changes in internal energy and heat dissipated to the environment.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not define or quantify thermodynamic efficiency in the traditional sense (e.g., work output / heat input). The focus is on entropy production (Σ̇) as a measure of dissipation or irreversibility, which is related to energy loss but not framed as efficiency. Minimizing Σ̇ is mentioned implicitly as desirable (e.g., CMOS design minimizing static power, coarse-graining yielding minimal entropy production).

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation is quantified by the entropy production rate Σ̇ (or its scaled version σ̇). According to Eq. 7 and 20, Σ̇ ≥ 0, representing the irreversible generation of entropy due to the system being out of equilibrium. This entropy production is directly related to heat flow to the environment (TΣ̇e = -<Q̇>) and the non-conservative work input (TΣ̇ = -d<F>/dt + <Ẇ>). Specific mechanisms depend on the system (e.g., heat dissipation in resistors in the CMOS circuit example, related to Q̇). The adiabatic contribution Σ̇a specifically represents housekeeping heat. Σ̇ is explicitly calculated from transition rates and probabilities (Eq. 7, 10, 20).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Long-term / Infinite (in theory for NESS)
*    Units: s

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: 0 (in theory for NESS)
    *   Units: s^-1

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are encoded in the transition rates λ ρ(n) between discrete microstates n and n+Δρ. These rates depend only on the current state n (and potentially the transition ρ itself). Thermodynamic consistency is imposed locally via the Local Detailed Balance (LDB) condition (Eq. 4), which relates forward and backward rates to free energy differences (Φ(n)) and work (Wρ(n)): log[λ ρ(n)/λ−ρ(n+Δρ)] = −β[Φ(n+Δρ)−Φ(n)−Wρ(n)]. In the macroscopic limit, these scale to ω ρ(x) = lim Ω→∞ λ ρ(Ωx)/Ω, which also obey a corresponding LDB (Eq. 14).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | LDB | Local Detailed Balance | β (Inverse Temp) | > 0 | J^-1 | Eq. 4 | Explicit | Definition |
    | LDB | Local Detailed Balance | Φ(n) (Free Energy) | System-dependent | J | Eq. 4 | Explicit | System Property |
    | LDB | Local Detailed Balance | Wρ(n) (Work input) | System-dependent | J | Eq. 4 | Explicit | External Input |
    | Master Eq | State Transition | λ ρ(n) (Rate) | ≥ 0 | s^-1 | Eq. 5 | Explicit | Model Input |

### **4.3 Global Order:**

    *   Content: The global order that emerges is twofold: 1) The deterministic dynamics described by the rate equations dx/dt = u(x) (Eq. 15), where the system evolves along predictable trajectories in the macroscopic limit. 2) The Non-Equilibrium Steady State (NESS) probability distribution Pss(x) ≈ exp[-ΩIss(x)], characterized by the rate function Iss(x) (Eq. 12, 16). This distribution represents the long-term, time-invariant probability landscape over the state space, potentially exhibiting features like multiple stable states (e.g., bistability in Fig. 1).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| LDB | Local Detailed Balance | β (Inv. Temp) | > 0 | J^-1 | Explicit | Defined in Eq. 4 | Eq. 4 |
| LDB | Local Detailed Balance | ΔΦ (Free Energy Change) | System-dep. | J | Explicit | Defined in Eq. 4 | Eq. 4 |
| LDB | Local Detailed Balance | Wρ(n) (Work Input) | System-dep. | J | Explicit | Defined in Eq. 4 | Eq. 4 |
| Rates | Transition Rate | λ ρ(n) | >= 0 | s^-1 | Explicit | Defined in Eq. 5 | Eq. 5 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| NESS | Steady State Distribution | Pss(x) | 0 to 1 | Dimensionless | Explicit | Defined Eq. 1, 2, 12 | Solve Master Eq. (Eq. 5) or HJB Eq. (Eq. 16) | Eq. 16 |
| NESS | Rate Function | Iss(x) | >= 0 | Dimensionless | Explicit | Defined Eq. 12 | Solve HJB Eq. (Eq. 16) | Eq. 16 |
| Dynamics | Deterministic Trajectory | x(t) | System-dep. | System-dep. | Explicit | Defined Eq. 15 | Solve Rate Eq. (Eq. 15) | Eq. 15 |
| Dynamics | Deterministic Velocity | u(x) | System-dep. | System-dep./s | Explicit | Defined Eq. 15 | Calculate from rates ωρ(x) | Eq. 15 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Microscopic Transition Time | ~1/λρ | s | Implicit (from rates) | Implicit | Inverse of transition rates sets microscopic timescale. |
        | Macroscopic Relaxation Time | System-dependent | s | Fig. 1, Eq. 15 | Implicit | Timescale over which system relaxes along deterministic trajectory x(t) towards steady state. Determined by eigenvalues of the Jacobian of u(x) near fixed points. |
        | Observation Time (for Σ) | T | s | Eq. 3 | Explicit | Duration over which entropy production Σ is integrated. |
        | Steady State Persistence | Infinite (Ideal NESS) | s |Implicit (definition of NESS) | Implicit | NESS is time-invariant by definition. |

    *   **Note:** Relevant timescales include the microscopic jump times, the macroscopic relaxation time towards steady state along deterministic trajectories, and the persistence time of the steady state itself.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behaviors described are: 1) The establishment of a Non-Equilibrium Steady State (NESS), characterized by a specific probability distribution Pss(x) and self-information Iss(x). 2) The emergence of macroscopic deterministic dynamics (relaxation trajectories x(t)) governed by Eq. 15 in the large system limit. 3) The adherence to the "Emergent Second Law" (Eq. 3 or 18), which constrains the relationship between entropy production Σ along deterministic trajectories and changes in the steady-state self-information ΔIss. For the CMOS example, specific behaviors include bistability (information storage) and the associated phase transition.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The main emergent behavior (Emergent Second Law, Eq. 3, 18) is presented as a theoretical derivation based on the positivity of adiabatic entropy production (Eq. 17, 18) within the framework of stochastic thermodynamics and large deviation theory applied to Markov jump processes satisfying LDB. Validation for the specific CMOS memory example involves: (a) showing the model fits the general theoretical requirements (Section: Example), (b) Calculating the exact rate function Iss(x) (Eq. 24) and comparing it with bounds derived from the theory using computed entropy production σ̇ along deterministic trajectories (Fig. 2b, 2c, 4). Agreement between the bound and the exact function serves as validation for this specific system. The linear response regime saturation (Eq. 23) is also presented as a consistency check (Fig 2a). Limitations include the reliance on the macroscopic limit (Ω→∞) for the strict inequality, though applicability to meso/micro systems is claimed if sub-extensive terms are negligible.

---

#Key: [feng_optimal_2020]

```markdown
# Optimal Machine Intelligence at the Edge of Chaos

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper presents a general theory for generic non-linear dynamical systems, represented by the discrete operator xxx<sub>t+1</sub> = fff(xxx<sub>t</sub>), where xxx is an N-dimensional vector. The theory focuses on the transition from ordered (stable fixed point or periodic) dynamics to chaotic dynamics. It proposes that the "edge of chaos," defined by the normalized asymptotic Jacobian norm (√1/N ||J*||<sub>F</sub> = 1), corresponds to the point of optimal information processing. Components include the dynamical operator (fff), its state vector (xxx), the Jacobian matrix evaluated at the asymptotic attractor (J*), and the attractor states (xxx*). The purpose is to establish a theoretical foundation for the "edge of chaos" hypothesis in generic systems and validate it using deep neural networks (DNNs) in computer vision tasks, showing that optimal DNN performance correlates with operation near this edge. The theory also links this edge to Neimark-Sacker bifurcation and maximal Lyapunov exponents.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** The table lists key theoretical parameters defining the system's dynamics and the edge of chaos condition. Specific values for trained networks vary by epoch (see Figs 2, 3, S1-S9). Reliability is high for theoretical definitions, medium for numerical measures in specific experiments.

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

**(Conditional: If M3.1 is "No", skip to Module 4. If "Yes", include M3.2 and M3.3.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Diverging near critical point (Edge of Chaos)
*    Units: Iterations (Qualitative Descriptor: Long-term / Diverging)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Maximal at the edge of chaos (Proportional to log L)
*   Units: Bits (information content) or Number of States (L) (Qualitative descriptor)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The fundamental local interaction rule is defined by the discrete dynamical operator xxx<sub>t+1</sub> = fff(xxx<sub>t</sub>). This rule governs how the state vector xxx evolves from one time step to the next based on its current value. For the DNNs used in validation, fff represents the action of multiple layers (weights, biases, activation functions like ReLU or tanh implicitly) transforming an input activation pattern to an output pattern. The stability and emergent phase depend on the properties derived from this rule, specifically the asymptotic Jacobian J* = ∂fff/∂xxx evaluated at the attractor mean µµµ.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | f(x) | DNN Operator Dynamics | Network Weights & Biases | Real numbers | Dimensionless | Sec III, IV, S4 | Implicit | Specific weight values are learned during training and not listed, only the architecture types are given. |
    | f(x) | Logistic Map Example | Parameter r | e.g., ≈ 3.57 for chaos onset | Dimensionless | Fig 1C, Sec II/S3 | Explicit | The parameter 'r' is explicitly used and varied. |

### **4.3 Global Order:**

    *   Content: The global order that emerges corresponds to the dynamical phase of the system's attractor:
        1.  **Stable Fixed Point:** System converges to a single, unchanging state vector xxx*. (Ordered Phase, ρ < 1)
        2.  **(Pseudo)Periodic Cycle:** System converges to a repeating sequence of L states. (Ordered Phase, ρ crosses 1, √1/N ||J*|| < 1)
        3.  **Chaotic:** System exhibits sensitive dependence on initial conditions, state does not repeat, often confined to a strange attractor. (Disordered Phase, √1/N ||J*|| ≥ 1)
        The specific structure of the attractor (visualized via Poincare plots in Fig 1A, 1C, 2) represents the emergent global state.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| R2 | Jacobian Calculation | J* = ∂fff/∂xxx | Matrix elements | Dimensionless | Explicit | Used for stability analysis | Sec II |
| R3 | DNN Weight Update | Adam Optimizer Rule | Learning rate, decay rates, etc. | Various | Implicit | Standard algorithm used, specific parameters not detailed | Sec III, S4 |
| R4 | Logistic Map Rule | x<sub>t+1</sub>=rx<sub>t</sub>(1-x<sub>t</sub>) | r | Dimensionless | Explicit | Specific example used | Sec II, S3 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| P2 | Stable Fixed Point Phase | Spectral Radius ρ | ρ < 1 | Dimensionless | Explicit | Condition for local stability | Sec II |
| P4 | Periodic Cycle Phase | Spectral Radius ρ | ρ ≥ 1 (approx) | Dimensionless | Explicit | Condition for Neimark-Sacker bifurcation | Sec II |
| P5 | Periodic Cycle Phase | Normalized Jacobian Norm | √1/N ||J*|| < 1 | Dimensionless | Explicit | Condition for stability within periodic phase | Sec II |
| P7 | Chaotic Phase Threshold | Normalized Jacobian Norm | √1/N ||J*|| = 1 | Dimensionless | Explicit | Critical threshold for chaos onset | Eqn 3, S15 | Sec II |
| P8 | Chaotic Phase Stability | Max Lyapunov Exponent γ | γ > 0 | 1/iteration | Explicit | Characteristic of chaos | Eqn S22 | Sec S2 |
| P9 | Stability Measure | Asymptotic Separation | |δx<sub>∞</sub>|=0 (Stable), >0 (Chaotic) | Arbitrary | Explicit | Numerical indicator of chaos | Fig 3 | Sec III, S2 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

**(Conditional: If M5.1 is "No", skip to Module 6. If "Yes", include M5.2-5.4)**

### **5.2 Computation Type:**

    *   Content: Neuromorphic / Analog

### **5.3 Computational Primitive:**

    *   Content: The fundamental operation is the non-linear transformation performed by the operator fff: xxx<sub>t+1</sub> = fff(xxx<sub>t</sub>). In the context of DNNs, this transformation involves weighted sums followed by non-linear activation functions (like ReLU), effectively performing feature extraction and pattern mapping. The paper analyzes the *information processing* capability (specifically Input-Output Mutual Information I(x<sub>0</sub>, x<sub>1</sub>) or asymptotic information log L) as the key computational function optimized at the edge of chaos.
    *   **Sub-Type (if applicable):** Non-linear transformation / Information Transfer / Feature Extraction (in DNNs)

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Basic Time Step | 1 | iteration | Sec II | Explicit | Dynamics are discrete time. |
        | Convergence to Attractor | t → ∞ (Asymptotic) | iteration | Sec II | Explicit | Analysis focuses on long-term behavior. |
        | Lyapunov Time (Chaos) | 1/γ | iteration | Eqn S22, S27 | Explicit | Characteristic time for divergence of nearby trajectories (γ is Lyapunov exponent). |
        | Periodic Cycle Length | L | iteration | Sec II, Fig 1A | Explicit | Duration of repeating sequence in periodic phase. |
        | Memory Lifetime | Diverges at edge of chaos | iteration | Sec II | Explicit | Timescale over which past state information persists. |
        | DNN Training Epoch | 1 | epoch | Fig 2, 3, S4-S9 | Explicit | Timescale for weight adaptation and performance evaluation during training. |
    *   **Note:** The table highlights the key timescales relevant to the system dynamics, stability, memory, and adaptation (during training).

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is the training process of the Deep Neural Networks. The paper explicitly mentions using the Adam optimizer [21] and implies the use of backpropagation (standard for training DNNs). This involves iteratively adjusting the network's weights and biases based on the error (loss) calculated between the network's output and the target labels for the training data (e.g., FashionMNIST, CIFAR10 datasets). This feedback-driven process aims to minimize the loss function, leading to improved classification accuracy and, as the paper shows, a concurrent shift in the network's dynamical properties towards the edge of chaos. Regularization techniques like weight decay and dropout are also mentioned as influencing this process (Sec IV, Fig S9C).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior described is the system operating within distinct dynamical phases (stable fixed point, periodic, chaotic) determined by its parameters, particularly the Jacobian norm. The key functional behavior highlighted is **optimal information processing** (maximal mutual information, maximal number of metastable states L) occurring specifically at the **edge of chaos** (the boundary between periodic and chaotic phases, √1/N ||J*|| = 1). In the context of DNN validation, this translates to **optimal computational performance** (highest test accuracy, lowest test loss) occurring when the trained network's dynamics are near this edge. The training process itself exhibits the behavior of **converging towards the edge of chaos** as performance improves. Another related behavior is **generalization**, which the paper links to stability (better generalization in stable/edge phases, overfitting linked to chaos).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper validates the emergent behavior (optimal performance at the edge of chaos) using computational experiments on various DNN architectures (MLP, CNN, ResNet, DenseNet) trained on benchmark datasets (FashionMNIST, CIFAR10). Validation methods include:
        1.  Tracking test accuracy and loss during training alongside stability metrics (normalized Jacobian norm √1/N ||J*|| or asymptotic separation |δx<sub>∞</sub>|). (Figs 2, 3).
        2.  Generating Poincare plots of the dynamics of trained network operators at different epochs to visually confirm the dynamical phase (stable, periodic, chaotic). (Figs 1A, 2 insets, Fig 3 insets).
        3.  Demonstrating that the optimal performance (peak accuracy/lowest loss) occurs when the stability metrics indicate operation near the edge of chaos (√1/N ||J*|| ≈ 1 or |δx<sub>∞</sub>| ≈ 0). (Figs 2, 3).
        4.  Showing that varying model parameters (e.g., reducing weights in Fig 1A) induces expected phase transitions (Neimark-Sacker bifurcation).
        5.  Calculating mutual information for a toy model (logistic map) to support the claim of maximal information transfer at the edge (Sec S3, Fig S3).
        Limitations: Validation mainly correlational (optimal performance *coincides* with edge of chaos dynamics); direct causal link manipulation is limited. Robustness primarily assessed via generalization gap, not direct perturbation tests.

---

#Key: [kos_nematic_2022]

# Nematic bits and universal logic gates

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system utilizes topological defects (+1/2 and -1/2 disclination lines) in nematic liquid crystals (LCs) as fundamental units called "nematic bits" (nbits). The state of an nbit is defined by the local director field orientation near the defect core, mathematically mapped to a point on the Poincaré-Bloch sphere using quaternions (SU(2) representation). The purpose is to demonstrate information storage and processing capabilities analogous to classical and potentially nonclassical computation. Single-nbit operations (logic gates like Pauli, Hadamard) are implemented by controlling the director field orientation using applied electric fields. Multi-nbit operations (classical universal NOR and NAND gates, generalized continuous functions) are realized by exploiting nematoelastic interactions between nearby, pinned defect lines. The system is primarily theoretical/computational, based on electro-nematic LC theory and simulations, but proposed as experimentally feasible. Components include the nematic LC material, topological defects (nbits), pinning sites (implicit), and electrodes for applying electric fields.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                  | Value              | Units   | Source (Fig/Table/Section)   | Implicit/Explicit   | Data Reliability (High/Medium/Low)   | Derivation Method (if Implicit)   |
        | :------------------------------ | :----------------: | :------: | :-------------------------: | :-------------------: | :---------------------------------: | :-------------------------------: |
        | Elastic Constant (K)            | 5                  | pN      | Discussion (MBBA example) | Explicit            | Medium                              | Literature Value (Ref 38, 42)     |
        | Rotational Diffusion Const. (Γ) | 0.076              | Pa·s    | Discussion (MBBA example) | Explicit            | Medium                              | Literature Value (Ref 38, 42)     |
        | Dielectric Anisotropy (εa)      | -0.7 * ε0          | F/m     | Discussion (MBBA example) | Explicit            | Medium                              | Literature Value (Ref 38, 42)     |
    *   **Note:** Values presented are primarily from the example feasibility calculation in the Discussion section using MBBA parameters. The electric field and radius are example values used for timescale estimates, not necessarily the exact values used in all simulations. ε0 is the vacuum permittivity.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy input for manipulating nbit states (performing logic operations) is electrical energy, supplied via applied electric fields E(z). For multi-nbit interactions, stored elastic energy in the director field configuration also plays a crucial role.
    *   Units: J (Joules) or related (e.g., J/m³ for energy density).

### **2.2 Energy Transduction**

    *   Content: Electrical energy is transduced into mechanical work on the LC director field. For LCs with negative dielectric anisotropy (εa < 0), the electric field exerts a torque that tends to align the director field perpendicular to the field, but due to the defect structure and boundary conditions, it results in alignment of the defect's normal axis (Ω) with the field E (Eq. 5). This reorientation involves overcoming elastic stresses (related to K) and viscous drag (related to Γ). Energy is stored elastically in the deformed director field configuration. In multi-nbit systems, this elastic energy mediates interactions, coupling the orientations of different nbits. Energy flows from the electric field source to the kinetic energy of director rotation, then stored as elastic potential energy, and finally dissipated viscously.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The system is inherently dissipative, governed by Eq. 5 which includes the rotational diffusion constant Γ representing viscous losses. Eq. 6 gives the energy _dissipated_ (cost) for a transformation, scaling inversely with the operation time τ (ℰ ≃ π⁴/(3τ) Γ L R²). Faster operations are explicitly stated to require proportionally more energy. This indicates significant energy loss, especially for faster computations. No efficiency metric (e.g., output work / input energy) is provided, but the focus on dissipation suggests low efficiency from a purely energetic standpoint (energy is used to change state against viscosity, not primarily to perform output work). Qualitative Assessment: Low efficiency.

### **2.4 Energy Dissipation**

    *   Content: The primary dissipation mechanism is viscous drag associated with the rotation of the LC director field in response to the electric field, quantified by the rotational diffusion constant Γ in Eq. 5. The energy dissipated during an nbit transformation (e.g., ∣0⟩ to ∣1⟩) over time τ is explicitly estimated as ℰ ≃ π⁴/(3τ) Γ L R², where L is defect length and R is characteristic radius. This indicates significant energy loss, particularly for fast operations. Thermal fluctuations also represent a form of energy exchange/dissipation, but the system is argued to be thermally stable over typical operational timescales. Quantified dissipation for a specific operation: ℰ ≃ π⁴/(3τ) Γ L R². Qualitative Assessment: High dissipation.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: 1.7 (estimated)
*    Units: days (for MBBA example at room temp, LR² ~ (10 μm)³)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Continuous (theoretically)

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: ~ 1 / 1.7 days⁻¹ (Thermal flip rate estimate)
    *   Units: s⁻¹ or days⁻¹

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |
    | State Flip (e.g., ∣0⟩→∣1⟩) | ℰ ≃ π⁴/(3τ) Γ L R²          | P ~ ℰ/τ ≃ π⁴/(3τ²) Γ L R²    | J     | Medium (Order of magnitude estimate) | Eq. 6, Discussion | Explicit formula, Implicit parameters | Formula derived explicitly, depends on parameters (τ, L, R) whose specific values for an operation are not fixed. |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Thermal Stability Time (τB) | Estimated time for thermal flip | ~1.7 | days | `MemoryNode` attribute | Discussion | Explicit | Calculated based on MBBA parameters. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: 1. **Director Alignment (Electric Field):** In response to an electric field E, the director normal Ω tends to align with E (for εa < 0). Governed by the torque term in Eq. 5: -(εaΓ/4) Tr(ησzη†E) Eησz. 2. **Director Relaxation (Elastic):** The director field tends to relax towards a minimum elastic energy configuration, minimizing gradients. Governed by the elastic term in Eq. 5: KΓ d²η/dz². 3. **Nematoelastic Interaction (Multi-nbit):** Nearby defects interact via the elastic distortions they induce in the director field. The system minimizes the free energy functional (Eq. 8 implicitly for equilibrium states in Figs 5-7, or a related functional underlying Eq. 5 dynamics), leading to preferred relative orientations (Ω₁ = -Ω₂ or Ω₁ = Ω₂ depending on configuration/far-field). This interaction is mediated locally through the continuous director field.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID                 | Description                          | Parameter Name          | Parameter Value Range              | Units   | Data Source       | Implicit/Explicit   | Justification   |
    | :---------------------- | :----------------------------------- | :---------------------- | :--------------------------------: | :------: | :--------------: | :----------------: | :------------: |
    | Electric Field Align.   | Torque due to dielectric anisotropy  | εa                      | -0.7 * ε0 (MBBA)                   | F/m     | Discussion      | Explicit           | Material property |
    | Elastic Relaxation      | Resistance to deformation            | K                       | 5 (MBBA)                           | pN      | Discussion      | Explicit           | Material property |
    | Viscous Dissipation     | Resistance to director rotation      | Γ                       | 0.076 (MBBA)                       | Pa·s    | Discussion      | Explicit           | Material property |
    | Nematoelastic Coupling  | Interaction strength via elasticity  | K (implicit dependence) | 5 pN (MBBA) - strength scales with K | pN      | Fig 5, Eq 8 ref | Implicit strength | Interaction arises from minimizing elastic energy (Eq 8 involves K and L). |

### **4.3 Global Order:**

    *   Content: In multi-nbit systems, the emergent order is the specific relative orientation (correlation) between the normal vectors Ω of interacting nbits. Examples include the antiparallel alignment (Ω₁ = -Ω₂) in the Ψ- ensemble (Fig. 5B) or parallel alignment (Ω₁ = Ω₂) in the Φ+ ensemble (Fig. S6, mentioned in text). This correlated state emerges globally across the interacting pair(s) from local interactions.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID                 | Description                          | Parameter | Value Range              | Units   | Implicit/Explicit   | Justification           | Source          |
| :---------------------- | :----------------------------------- | :-------- | :-----------------------: | :------: | :----------------: | :---------------------- | :--------------: |
| Electric Field Align.   | Torque due to dielectric anisotropy  | εa        | -0.7 * ε0 (MBBA)          | F/m     | Explicit           | Material Property.        | Discussion      |
| Elastic Relaxation      | Tendency to uniform alignment        | K         | 5 (MBBA)                  | pN      | Explicit           | Material Property.        | Discussion      |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type                       | Description                                                                 | Predictability   | Yoneda Score | Metrics                  | Implicit/Explicit   | Justification                              | Source              |
    | :------------------------------ | :-------------------------------------------------------------------------- | :--------------- | :----------- | :----------------------- | :----------------: | :----------------------------------------- | :------------------ |
    *   **Metrics:** Predictability assessed qualitatively based on deterministic simulation outcomes and theoretical descriptions of energy minima. Control fidelity shown via simulation results matching intended states (Fig 3).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Hybrid (Analog and Digital capability)

### **5.3 Computational Primitive:**

    *   Content: The fundamental operation is the controlled rotation of the nbit state (director field / quaternion / Bloch vector) via electric fields or nematoelastic interactions. Specific computational primitives demonstrated include:
        *   **Single-Nbit Rotations:** Analogous to quantum gates (Pauli X, Y, Z; Hadamard H; √X; Phase Shift Rϕ). These are specific transformations (rotations) on the Poincaré-Bloch sphere implemented via E-field protocols (Fig 4).
        *   **Projection:** Measurement-like operation projecting state onto subspaces (e.g., ∣0⟩ or ∣1⟩) via strong E-field alignment (Fig 4G).
        *   **Multi-Nbit Conditional Logic:** NAND and NOR gates, where the state of output nbits depends conditionally on the states of input nbits via nematoelastic coupling (Fig 6).
    *   **Sub-Type (if applicable):** Logic Gate: Pauli-X, Pauli-Y, Pauli-Z, Hadamard, √X, Rϕ, NAND, NOR; Projection Operator: P_E.

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description        | Value     | Units     | Source     | Implicit/Explicit   | Justification                               |
        | :--------------------------- | :--------: | :--------: | :---------: | :----------------: | :------------------------------------------ |
        | Electric Response Time (τ_electric) | ~0.049    | s         | Discussion | Explicit           | Calculated for MBBA example (Γ/(|εa|E²)).      |
        | Elastic Relaxation Time (τ_elastic) | ~1.5      | s         | Discussion | Explicit           | Calculated for MBBA example (ΓR²/K).          |
        | Thermal Stability Time (τ_B)    | ~1.7      | days      | Discussion | Explicit           | Estimated for MBBA example (Energy barrier vs kBT). |
        | Gate Operation Time (τ)      | > τ_electric | s         | Discussion | Implicit           | Must be slow enough for adiabatic following.  |
        | Measurement Time (τ_meas)    | Variable  | s         | Discussion | Explicit           | Compared to τ_elastic for projective vs non-invasive regimes. |
    *   **Note:** Values are primarily estimates based on the MBBA example in the Discussion. τ represents the duration over which an E-field protocol is applied.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8. If "Yes", include M7.2)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behaviors demonstrated are:
        1.  **Information Storage:** Representing states (bits or continuous values) via persistent nbit orientations.
        2.  **Single-Nbit Logic Operations:** Performing controlled state transformations analogous to quantum gates (Pauli, Hadamard, etc.) using electric field protocols.
        3.  **Multi-Nbit Logic Operations:** Implementing universal classical logic (NAND, NOR) and generalized continuous functions through nematoelastic interactions between nbits.
        4.  **State Projection:** Performing measurement-like operations by forcing nbits into specific subspaces (e.g., ∣0⟩ or ∣1⟩) using strong electric fields.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary behaviors (logic gate operations) are validated through numerical simulations based on established electro-nematic LC theory (Eq. 5 derived from LC hydrodynamics, Eq. 8 for free energy minimization). Simulation results visually demonstrate the intended state transformations for single-nbit gates (Fig 4) and the correct truth tables for multi-nbit NAND/NOR gates (Fig 6). Generalized continuous functions are also shown via simulation (Fig 7). The emergence of correlated states (Ψ-, Φ+) from nematoelastic interactions is shown as equilibrium states in simulations (Fig 5). Validation relies on the fidelity of the computational model to real LC physics. Experimental validation is proposed as feasible but not performed in this work. Robustness is partially validated by the thermal stability estimate.

---

#Key: [crepaldi_evidence_2022]

# Evidence of In-Memory Computing in a Ferrofluid

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of a Fe3O4 water-based ferrofluid (FF) contained in a vial with two feed-line electrodes. Quasi-DC voltage signals are used to program (write) the ferrofluid's state, which alters its impedance characteristics. The state is read out using Radio Frequency (RF) signals via a Vector Network Analyzer (VNA) measuring S-parameters, which are then converted to impedance values (specifically ZC11, ZC12, ZC21, ZC22, representing summed impedance magnitudes over the frequency range). The purpose is to demonstrate that the ferrofluid exhibits memristive behavior, including short and long-term memory/plasticity, and can perform in-memory computation tasks like digit classification using both a custom scheme and Physical Reservoir Computing (PRC). Key components include the ferrofluid (EMG601P, 5ml), an ABS vial with gold-plated RF connector feed lines, a two-port VNA (PicoVNA 106), a DC bias generator (Micropython board + MAX4426T OpAmp), bias tees (TCBT-14+), and control software on a PC.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | VNA Frequency Range | 10 MHz - 6 | GHz | Fig. 1B Caption, Materials and Methods (PicoVNA 106 range is 300kHz-6GHz, used range 10MHz-6GHz implied) | Mixed | High | Part explicitly stated, part implied by usage. |

    *   **Note:** ZC values are highly dynamic and depend on the state and port; the range provided covers typical values seen in figures.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Two primary energy inputs: 1) Quasi-DC electrical energy from the DC bias generator for programming/writing the ferrofluid state. 2) RF electrical energy (at 0 dBm) from the VNA for reading the state via S-parameter measurement.
    *   Value: DC: up to ±10 V; RF: 0 dBm (1 mW)
    *   Units: Volts (DC); dBm or mW (RF)

### **2.2 Energy Transduction**

    *   Content: 1) Applied DC electrical energy is transduced into potential energy changes within the ferrofluid, likely related to the reorganization of magnetic nanoparticles (alignment, chaining, clustering) under the electric field, possibly involving electrophoretic or dielectrophoretic effects, and potentially torque on particle spins/moments (mentioned in S10). This alters the material's collective dielectric/magnetic properties. 2) Incident RF electrical energy interacts with the ferrofluid's current state (nanoparticle configuration). Part of this energy is reflected/transmitted, detected by the VNA as S-parameters. The interaction likely involves dielectric losses, potentially magnetic losses related to spin resonance (mentioned in S10 hypothesis), and changes in conductivity pathways.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide any metrics for energy efficiency for either the writing (DC) or reading (RF) process, nor for the computation itself. Given the nature of manipulating bulk fluid properties with DC fields and the low power RF probing, the efficiency related to direct computation is likely extremely low compared to conventional electronics. The DC energy primarily rearranges nanoparticles and likely involves significant ohmic losses/heating (though not quantified). The RF energy is primarily for probing, not performing work in the computational sense. Score is assigned based on lack of evidence for efficiency and inferred high losses. Qualitative Assessment: Very Low.

### **2.4 Energy Dissipation**

    *   Content: Dissipation mechanisms are not explicitly quantified. Likely mechanisms include: 1) Ohmic heating due to DC current flow through the ferrofluid (which has finite conductivity). 2) Viscous dissipation related to nanoparticle movement/reorientation under DC fields. 3) Dielectric losses during RF probing. 4) Magnetic relaxation losses (if applicable, as per Sec. S10 hypothesis) during RF probing. 5) Heat loss to the vial and surroundings. Qualitative Assessment: Likely High for DC writing (due to current flow), Low for RF reading (low RF power).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Minutes to > 40 minutes (Explicitly demonstrated); "Longer duration" possible (Claimed).
*    Units: seconds/minutes

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: N=16 demonstrated; Analog claimed ("power of continuum").
*   Units: Discrete states (N); Qualitative (Analog).

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: 90% (In-memory classification); 90.6% (PRC classification); Low variance in Hold phase (Memory test).
*   Units: % (Accuracy); Ω² (Variance)

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Qualitative: Dynamics reduction/shrinking over days of repeated testing. Reversible.

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes (Potentially/Implicitly)

### **4.2 Local Interaction Rules:**

    *   Content: The paper does not provide explicit equations or detailed descriptions of the local interaction rules between nanoparticles. Implicitly, these rules would involve:
        1.  Dipole-dipole interactions between magnetic nanoparticles (mentioned in S10).
        2.  Interaction of particle charges/dipoles with the applied external DC electric field (electrophoresis/dielectrophoresis).
        3.  Interaction with electrode surfaces.
        4.  Steric repulsion/surfactant interactions preventing aggregation.
        5.  Hydrodynamic interactions mediated by the solvent.
        6.  Brownian motion/thermal fluctuations (mentioned in Sec. S9 context).
    * **Implicit/Explicit**: Implicit

### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**

    *   Content: The emergent global order is the specific microscopic configuration/arrangement of the nanoparticles throughout the ferrofluid volume between the electrodes. This microscopic order is not directly observed but manifests macroscopically as a specific set of frequency-dependent impedance values (Z11, Z12, Z21, Z22, and consequently the summed ZC values) measured between the electrodes. Different particle arrangements correspond to different impedance states, encoding the memory and enabling computation. The chaotic dynamics (Sec. S9, S10) suggest this order is complex and sensitive.
    * **Implicit/Explicit**: Implicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Impedance State | Macroscopic manifestation of particle arrangement | ZC11, ZC12, ZC21, ZC22 | ~2-17 | kΩ | Explicit | Represents the global state | VNA Measurement | Figs 1-4, S1-S8 |


### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Reservoir Computing (PRC); Analog In-Memory Computing (Custom Scheme).

### **5.3 Computational Primitive:**

    *   Content: The fundamental operation appears to be **Non-linear Transformation and Integration with Memory**. The ferrofluid transforms the input voltage sequence (representing pixels) into a time-varying impedance trajectory (ZC values). This transformation is non-linear (evidenced by chaotic dynamics and PRC feasibility) and integrates the input over time, with the result influenced by the material's memory state (hysteresis, plasticity).
    *   **Sub-Type (if applicable):** For PRC, this non-linear transformation projects the input into a higher-dimensional state space. For the custom scheme, it functions similarly to a weighted integration or correlation where matching inputs cause larger impedance changes. Other implicit primitives include thresholding (for decision making in the custom scheme, Fig. 3F) and potentially filtering (mentioned conceptually in Fig. 3A discussion).

### **5.4 Embodied Computational Units**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | DC Voltage Step Duration (Hysteresis) | 1 | s | Fig. 1C | Explicit | Duration of each voltage step in the hysteresis measurement. |
        | DC Pulse Duration T_P (Memory Write) | 4 to 64 (Fig. 2); 0.25 to ~1.6 (Fig. S2B pulses); 0.25, 4.5 (Classification Weighting Fig. 3) | s | Fig. 2A, Fig. S2B, Fig. 3C-E | Explicit | Duration of programming pulses used in memory and classification tasks. |
        | Memory Hold Phase Duration | Up to ~2500 | s | Fig. 2B, Fig. S2A | Explicit | Duration for which memory state stability was observed. |
        | PRC Pixel Serialization Duration | 2 | s | Sec. S11 (Text for Fig. 4) | Explicit | Duration each pixel voltage is applied during PRC input. |
        | VNA Measurement Time | ~0.7 (Control Loop Tick); Not specified for single scan | s | Fig. S7 Caption; Materials & Methods | Mixed | Control loop tick implies measurement time. Single scan time not given, but likely fast relative to DC changes. |
        | Short-term Memory Fading | ~Seconds | s | Fig. S3B Detail | Implicit | Visual estimate from plot showing impedance decay after stimulus removal. |
        | Long-term Dynamics Reduction | Days | days | Sec. S7, Fig. S5 Caption | Explicit | Timescale over which performance degrades with repeated testing under specific reset. |
        | Reset Phase Duration | Variable (Control Loop) / Seconds-Minutes (Manual) | s / min | Fig. 2A, Fig. 4A, Sec. S7 | Mixed | Control loop duration depends on state; manual reset duration mentioned qualitatively. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The precise mechanism is not fully elucidated but is linked to the history-dependent rearrangement of nanoparticles under DC electric fields. The application of DC voltage alters the configuration of nanoparticles, and this configuration persists to some degree, influencing subsequent impedance measurements and responses (memristive effect / plasticity, Fig. 1C, Fig. 2). For PRC, the adaptation occurs primarily in the external readout neural network, which learns to map the complex, history-dependent impedance states generated by the ferrofluid reservoir to the desired output classifications. The ferrofluid itself provides the necessary rich dynamics and fading memory, but its internal "rules" are not described as adapting in a supervised or goal-directed way during PRC training. The "Progressive Adaptation" in Sec. S6 suggests that repeatedly driving the system across its hysteresis loop with specific inputs can selectively enhance or suppress impedance changes for certain inputs, implying the underlying particle configuration evolves or "adapts" based on the input statistics and the bias point on the hysteresis curve. The dynamics reduction (Sec. S7) suggests an adaptation towards a less responsive state under prolonged, specific cycling conditions.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors demonstrated are:
        1.  **Memristive Behavior:** Exhibiting pinched hysteresis loops in the voltage-impedance relationship (Fig. 1C).
        2.  **Analog Memory Storage:** Storing information encoded as distinct impedance levels based on the duration of programming pulses (Fig. 2, Sec. S4). Includes short-term (fading) and long-term (plastic) components.
        3.  **In-Memory Classification (Custom):** Performing digit classification (8x8 pixels) directly within the ferrofluid using a custom voltage sequencing and weighting scheme, where the final impedance state indicates the recognized digit (Fig. 3). Achieves 90% accuracy for 9/10 digits.
        4.  **Physical Reservoir Computing (PRC):** Serving as a physical reservoir whose complex, history-dependent dynamics (impedance response to serialized input) are used to perform digit classification (4 digits) when coupled with a trained external readout neural network (Fig. 4). Achieves 90.6% accuracy.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper validates the claimed behaviors through direct experimental demonstration:
        *   **Memristive Behavior:** Measured impedance (ZC) vs. applied voltage (Vp) sweeps showing pinched hysteresis loops (Fig. 1C). Repeated sweeps show evolution over time.
        *   **Memory Storage:** Applying controlled DC pulses (varying duration T_P) and measuring the resulting distinct ZC22 levels during a hold phase (Fig. 2B). Low variance during hold is shown (Fig. 2C). Longer tests presented in Fig. S2.
        *   **In-Memory Classification:** Applying specific weighted voltage sequences corresponding to serialized digits and measuring the final ZC22 value. Showing that the target digit sequence results in the lowest impedance (Fig. 3C-F). Accuracy calculated based on lowest impedance rule. Failure case (3 vs 8) identified.
        *   **PRC:** Applying serialized digit inputs, measuring the ZC22 trajectory, training an external NN readout layer on this data (using 50 sequences, described in Methods and S11), and then testing the trained NN on new real-time measurement data. Validation via confusion matrix and accuracy calculation (Fig. 4B, Sec. S11). Necessity of specific reset protocol justified (Sec. S11).
        *   **Limitations:** Validation primarily relies on demonstrating the functionality under specific protocols. Statistical significance (e.g., number of trials for classification accuracy beyond the PRC training/test set split) could be stronger. Robustness testing is limited (Sec. S7 shows degradation). Direct visualization of nanoparticle organization is absent.

---

#Key: [boniface_self-propulsion_2019]

# Self-propulsion of symmetric chemically active particles: Point-source model and experiments on camphor disks

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper investigates the self-propulsion of chemically active particles, specifically focusing on symmetric ones where motion arises from spontaneous symmetry breaking. It develops a theoretical "point-source model" applicable to both phoretic and interfacial swimmers, treating the swimmer as a point releasing a chemical (φ) that diffuses (coefficient D) and is advected by the fluid flow induced by the swimmer's motion (velocity v). The model predicts propulsion speed based on parameters like swimmer size (a), asymmetry (b/a), release rate (J), and physical properties (mobility M or surface tension coefficient κ, viscosity η). This model is compared with experiments performed on millimeter-scale camphor-loaded agar disks floating on water. The disks release camphor, lowering local surface tension, and the resulting Marangoni stresses cause self-propulsion. The experiments measure velocity as a function of disk size and induced asymmetry (by adding holes). The purpose is to understand the mechanism of self-propulsion in symmetric active particles, particularly the role of advection and symmetry breaking, and to validate the point-source model's predictions, especially in the advection-dominated regime relevant to interfacial swimmers like camphor disks.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value             | Units         | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---------------: | :-----------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Release rate (J, for 4mm disk) | ~5 x 10⁻⁹       | mol/s         | Table II, Fig 3 (Inset)   | Mixed             | Medium                          | Estimated from concentration measurements (Fig 3) and model (Eq 32), adjusted for single half-immersed disk. |
        | Péclet number (Pe = av/2D) | Varies (~15-120) | Dimensionless | Sec II.B, Figs 5, 8       | Mixed             | Medium                          | Calculated from measured `a`, `v` and literature `D`. |
        | Dimensionless activity (M) | Varies (Large)  | Dimensionless | Sec II.C                  | Implicit          | Low                             | Estimated using model definitions (e.g., κJ/CηD²) and other parameters. Order of magnitude estimate. |

    *   **Note:** Péclet number and M are crucial dimensionless parameters linking model and experiment. Their values are derived/estimated rather than directly measured as fundamental constants.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is the chemical potential energy stored within the solid camphor embedded in the agar disk. This energy is released upon dissolution of camphor from the disk into the water phase and its subsequent spreading over the interface.

### **2.2 Energy Transduction**

    *   Content: 1. Chemical Potential Energy -> Chemical Gradient Energy: Camphor dissolves/sublimates from the disk, creating concentration gradients (φ) in the surrounding water bulk and/or at the air-water interface. 2. Chemical Gradient Energy -> Surface Energy Gradient: The camphor concentration gradient at the interface modifies the local surface tension (γ = γ₀ - κφ), creating a surface tension gradient (∇γ). 3. Surface Energy Gradient -> Mechanical Work/Kinetic Energy: The unbalanced surface tension exerts a net force (Marangoni force, Fs in Eq. 17) on the disk, overcoming the viscous drag force (Fd) and propelling the disk, thus converting surface energy into kinetic energy of the disk and the surrounding fluid.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not quantify energy efficiency. Qualitatively, the efficiency of converting chemical potential energy into directed kinetic energy is expected to be very low for such systems. Significant energy is lost to diffusion (increasing entropy), evaporation, and viscous dissipation in the fluid. The observed speeds (cm/s) are achieved with milligram-scale camphor release rates. Low score assigned based on typical efficiency ranges for similar molecular/colloidal motors and dissipative processes involved.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include: 1. Viscous Drag: Energy dissipated into the fluid due to the motion of the disk (Fd = Cηav is used in the model, though reality is more complex, Sec III.C.ii). This is the primary mechanism balancing the propulsive force in steady state. 2. Diffusion: The random spreading of camphor molecules represents an increase in entropy and is inherently dissipative. 3. Marangoni Flows: The paper acknowledges (Sec III.C.i) that surface tension gradients induce fluid flows (Marangoni flows) separate from the disk motion, which dissipate energy through viscosity. 4. Evaporation: Camphor evaporating from the surface carries energy away (mentioned in Sec II.B.3 context). Quantification is not provided, but viscous drag (balancing propulsion) is implicitly the largest sink related to motion. Qualitative Assessment: High overall dissipation.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Units: s

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The local rules governing the system are the physical laws described by the model:
        1.  **Chemical Release:** Source term Jδ(r-R(0)) in Eq. 4 (simplified as point release). Experimentally, release occurs from the disk surface, modeled in Sec III.A.2.
        2.  **Advection-Diffusion:** ∂φ/∂t + v·∇φ = DΔφ (generalized from Eq. 4 for the fluid frame, potentially with evaporation -αφ term). This describes how the local concentration φ changes due to fluid flow (advection) and random motion (diffusion).
        3.  **Surface Tension Dependence:** γ = γ₀ - κφ (Eq. 16). Local surface tension depends linearly on the local interfacial camphor concentration.
        4.  **Force Generation:** Net propulsive force Fs arises from integrating surface tension gradient forces around the disk boundary (implicit in Eq. 17). Fs = -∫∇γ dl.
        5.  **Fluid Dynamics / Force Balance:** In steady state, Fs = Fd (Eq. 17). The drag force Fd depends on velocity v and fluid properties (e.g., Fd = Cηav in Stokes regime). This links the swimmer's velocity to the forces generated by the concentration field. (Note: The point-source model simplifies fluid dynamics significantly by assuming uniform advection).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID                  | Description                     | Parameter Name | Parameter Value Range | Units         | Data Source | Implicit/Explicit | Justification |
    | :----------------------- | :------------------------------ | :------------- | :-------------------- | :-----------: | :----------: | :----------------: | :------------: |
    | AdvectionDiffusionRule | Diffusion Rate                  | D              | 0.72 x 10⁻⁹ (water) | m²/s          | Table II    | Explicit          | Cited value.   |
    | AdvectionDiffusionRule | Advection Velocity              | v              | 0.025 - 0.14          | m/s           | Fig 8       | Explicit          | Measured.      |
    | SurfaceTensionRule     | Surface Tension Coefficient     | κ              | 3 x 10⁻³              | Nm²/mol       | Table II    | Explicit          | Cited value.   |
    | ForceBalanceRule         | Drag Coefficient Factor         | C              | 16/3 (disk, Stokes)   | Dimensionless | Table II    | Explicit          | Theoretical.   |
    | ForceBalanceRule         | Fluid Viscosity                 | η              | 1 x 10⁻³              | Pa·s          | Table II    | Explicit          | Standard value. |
    | ChemicalReleaseRule    | Total Release Rate (4mm disk) | J              | ~5 x 10⁻⁹           | mol/s         | Table II    | Mixed             | Estimated.     |

### **4.3 Global Order:**

    *   Content: The emergent global order is the state of steady, directed self-propulsion characterized by a constant velocity vector **v**. This represents a breaking of the initial spatial symmetry.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID                  | Description                     | Parameter | Value Range         | Units         | Implicit/Explicit | Justification | Source |
| :----------------------- | :------------------------------ | :-------- | :------------------ | :-----------: | :----------------: | :------------: | :-----: |
| AdvectionDiffusionRule   | Coupling of flow and conc.      | Pe        | ~15-120 (Expt)      | Dimensionless | Mixed             | Calculated.    | Eq 5, Fig 8 |
| ForceBalanceRule         | Balance of propulsion and drag | v         | 0.025 - 0.14 (Expt) | m/s           | Explicit          | Measured.      | Fig 8 |
| SurfaceTensionRule       | Conc. effect on surf. tension   | κ         | 3 x 10⁻³            | Nm²/mol       | Explicit          | Cited value.   | Table II |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID              | Description                     | Parameter | Value Range         | Units         | Implicit/Explicit | Justification | Protocol | Source |
| :----------------------- | :------------------------------ | :-------- | :------------------ | :-----------: | :----------------: | :------------: | :------: | :-----: |
| SteadyPropulsionVelocity | Magnitude of velocity vector    | |v|       | 0.025 - 0.14 (Expt) | m/s           | Explicit          | Measured avg velocity. | Particle Tracking | Fig 8 |
| VelocityScaling          | Size dependence of velocity     | ν (v∝a^ν) | ~1/3 (Expt)         | Dimensionless | Mixed             | Fitted exponent. | Data fitting | Fig 8, Sec III.B |
| AsymmetryInfluence       | Velocity dependence on χ        | ∂v/∂χ   | ≈ 0 (Expt, small χ) | m/s           | Mixed             | Inferred from Fig 7. | Hole punching | Fig 7, Sec III.B |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type             | Description                                     | Predictability | Yoneda Score | Metrics           | Implicit/Explicit | Justification | Source |
    | :-------------------- | :---------------------------------------------- | :------------- | :----------- | :---------------- | :----------------: | :------------: | :-----: |
    | Local Rules -> Global V | Predicting velocity from physical parameters. | Moderate       | 6            | v vs a, v vs χ | Mixed             | Model predicts trends but overestimates magnitude. Consistency validates mechanism. | Sec II, III |
    *   **Yoneda Embedding Fulfillment Score [0-10]:** 6. (Rubric: 0=No relation, 5=Qualitative agreement, 8=Good quantitative agreement, 10=Perfect prediction). The model successfully maps local physics (diffusion, advection, surface tension) to the global emergent behavior (velocity), capturing key qualitative features like size dependence and robustness to asymmetry, demonstrating a reasonably faithful local-to-global mapping despite quantitative discrepancies.
    *   **Metrics:** Comparison of predicted vs. measured velocity scaling exponent (ν), predicted vs. measured dependence on asymmetry parameter (χ).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: Skipped M5.2-5.4 as M5.1 is "No")**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description          | Value             | Units | Source          | Implicit/Explicit | Justification |
        | :----------------------------- | :---------------: | :---: | :-------------: | :----------------: | :------------: |
        | Advection time (a/v)           | ~0.01 - 0.1       | s     | Calculated      | Implicit          | Calculated from typical 'a' and measured 'v' (Fig 8). |
        | Diffusion time (a²/D)          | ~1 - 300          | s     | Calculated      | Implicit          | Calculated from typical 'a' and literature 'D' (Table II). |
        | Experimental Measurement Interval| 200               | s     | Sec III.B       | Explicit          | Stated duration (700-900s). |
        | Long-term Velocity Decay Scale | Hours (~10⁴ s)    | s     | Fig 5           | Explicit          | Observed timescale of velocity decrease in 4h run. |
        | Release Characteristic Time (τ)| ~1.2 x 10⁵        | s     | Fig 3           | Explicit          | Fitted parameter from bulk release experiment. |
        | Internal Diffusion Time (δ²/Dg)| Varies with δ     | s     | Sec III.A.2     | Implicit          | Timescale from the growing diffusive layer model. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: Skipped M7.2 as M7.1 is "No")**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behavior is the steady self-propulsion of an initially symmetric camphor disk on a water surface. This directed motion arises spontaneously due to the breaking of symmetry by the advection of the released camphor, which creates an asymmetric surface tension gradient driving the disk. Key observed characteristics include velocity dependence on size (v ∝ a^ν with ν ≈ 1/3) and robustness against small, deliberately introduced geometric asymmetries.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claim of emergent behavior (symmetry-breaking self-propulsion) is validated by: 1. **Model-Experiment Comparison:** The point-source model, which incorporates the symmetry-breaking mechanism via advection, predicts key features observed experimentally: self-propulsion of symmetric disks, sublinear velocity scaling with size (v ∝ a^ν, model predicts ν=1/3 or 1/2 depending on dimension, experiment finds ≈1/3, Fig 8), and irrelevance of small intrinsic asymmetry in the high-Pe regime (model prediction Sec II.C, experiment Fig 7). The qualitative agreement supports the proposed mechanism. 2. **Controlled Experiments:** Experiments specifically testing the effect of induced asymmetry (Fig 7) directly probe the robustness of the symmetry-breaking mechanism against intrinsic defects. 3. **Systematic Parameter Variation:** Measuring velocity across a range of disk sizes (Fig 8) allows testing the model's scaling predictions. Quantitative discrepancies are acknowledged (Sec III.C), indicating limitations of the simplified model but not invalidating the core emergent mechanism. Reproducibility is addressed by averaging over multiple runs/swimmers (Sec III.B).

---

#Key: [hadorn_hierarchical_2012]

# Hierarchical Unilamellar Vesicles of Controlled Compositional Heterogeneity

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system consists of Hierarchical Unilamellar Vesicles (HUVs), which are giant unilamellar vesicles (GUVs, 1-100 µm diameter) internally compartmentalized by smaller, non-concentric GUVs ("intermediate GUVs"). The system is fabricated using a sequential vesicle-in-water-in-oil (v/w/o) emulsion transfer technique. The inner "intermediate" GUVs and the outer confining HUV GUVs can have controlled, different membrane compositions (e.g., presence/absence of biotinylated lipids, fluorescent labels) and different internal aqueous cargo (e.g., sucrose vs. glucose solutions, different fluorophores). The purpose is to create biomimetic structures resembling eukaryotic cells with organelles, and potential applications include multi-agent drug delivery systems and complex artificial microreactors. The key components are phospholipids (POPC, bPEG2000-DSPE, cfPEG2000-DSPE), sucrose, glucose, Atto565-Biotin, light mineral oil, water, and streptavidin-coated surfaces for isolation.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
    *   **Note:** Additional parameters like cargo concentrations (900mM sucrose/glucose), centrifugation speeds (1500g, 3400g), and filter pore size (12 µm) are also provided explicitly in the Methods section.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy inputs are: 1) Mechanical energy via centrifugation to force emulsion droplets across the oil-water interface and to sediment vesicles. 2) Mechanical energy for agitation during emulsion preparation. 3) Acoustic energy via sonication for dissolving phospholipids in oil. 4) Chemical potential energy difference due to density gradients (sucrose vs. glucose solutions) driving sedimentation.
    *   Value: Centrifugation: 1500 g and 3400 g. Sonication: Not specified (thermostated bath sonicator used). Chemical Potential: Related to Δρ ≈ 10 kg/m³ (derived from sucrose/glucose density difference).

### **2.2 Energy Transduction**

    *   Content: 1) Mechanical energy (centrifugation) is transduced into kinetic energy of droplets/vesicles and potential energy change as they cross the interface/sediment. It also drives the fusion of phospholipid monolayers into a bilayer upon interface crossing. 2) Acoustic energy (sonication) is transduced into kinetic energy of molecules to facilitate phospholipid dissolution. 3) Chemical potential energy (density difference) is transduced into gravitational potential energy driving sedimentation, resulting in kinetic energy until terminal velocity is reached, where it is balanced by viscous drag.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The efficiency is likely very low. The primary goal is material fabrication, not energy conversion. Significant energy is input via centrifugation and sonication, but most is likely dissipated as heat and overcoming viscous forces. The chemical potential energy driving sedimentation is utilized, but the process itself is slow and subject to drag. No quantitative efficiency metrics are provided or calculable from the text. Qualitative assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: Major dissipation mechanisms include: 1) Viscous drag during sedimentation of vesicles (quantified via Stokes' Law analysis in Results/Discussion). 2) Heat generated during centrifugation. 3) Heat generated during sonication (thermostated bath used, suggesting heat generation). 4) Energy loss during the non-elastic process of monolayer fusion. Quantification is limited to the drag force calculation based on Stokes' Law (Eq. 3). Other mechanisms are qualitatively assessed as present but not quantified (High/Medium/Low assessment not explicitly possible from text).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial

**(Conditional: If M4.1 is "Partial", include M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: 1) **Phospholipid Assembly:** Amphipathic lipid molecules minimize contact between hydrophobic tails and water, and maximize contact between hydrophilic heads and water. This drives the formation of monolayers at the oil/water interface and bilayers in aqueous environments. (Governed by thermodynamics, minimizing free energy). 2) **Biotin-Streptavidin Binding:** Specific molecular recognition between biotinylated lipids in the intermediate GUV membrane and streptavidin coated on the isolation chamber surface. Enhanced by sodium ions. (Governed by binding affinities, kinetics). 3) **Density-Driven Sedimentation:** Vesicles containing denser cargo (sucrose solution) experience a net downward gravitational force in a less dense surrounding medium (glucose solution), described by buoyancy principles and Stokes' Law for drag. (Governed by gravity, density differences, viscosity).
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Binding | Biotin-Streptavidin Binding | Sodium Ion Conc. (enhancer) | 25 | mM | Methods (HS2) | Explicit | Concentration provided. |
    | Sedimentation | Density-Driven Sedimentation | Density Difference (Δρ = ρ_IS - ρ_HS) | ~10 | kg/m³ | Results (Eq. 4) | Explicit | Value used in calculation. |
    | Sedimentation | Density-Driven Sedimentation | Dynamic Viscosity (μ_HS) | 1.5956 x 10⁻³ | kg s⁻¹ m⁻¹ | Results (Eq. 3) | Explicit | Value used in calculation (cited from [29]). |
    | Sedimentation | Density-Driven Sedimentation | Gravitational Acceleration (g) | 9.81 | m/s² | Results (Eq. 1) | Explicit | Standard value used. |
    | Sedimentation | Density-Driven Sedimentation | Centrifugation Rate (x) | 1, 1500, 3400 | dimensionless (rel. to g) | Methods / Results (Eq. 4) | Explicit | Values used experimentally and in calculations. |

### **4.3 Global Order:**

    *   Content: The emergent global order, facilitated by directed assembly, is the Hierarchical Unilamellar Vesicle (HUV): a larger GUV containing one or more smaller, non-concentric GUVs, where inner and outer vesicles have distinct, controlled compositions. An associated outcome of the process is the separation of these intact HUVs from released (non-encapsulated) intermediate GUVs.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Sedimentation | Density difference driving movement in gravity/centrifugation | Density difference (Δρ) | ~10 | kg/m³ | Explicit | Value stated/used in calculations. | Results |
| Sedimentation | Viscous drag resisting motion | Viscosity (μ_HS) | ~1.6x10⁻³ | kg s⁻¹ m⁻¹ | Explicit | Value stated/used in calculations. | Results |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Structure | Hierarchical Vesicle Formation | Presence of encapsulated GUVs | Yes/No | Binary | Explicit | Primary outcome observed by microscopy. | Microscopy | Figs 2, 3 |
| Structure | Vesicle Size (Inner) | Mean Radius | 2.27 ± 0.7 | µm | Explicit | Measured from micrographs. | Image Analysis | Fig 4 |
| Structure | Vesicle Size (Outer) | Mean Radius | 9.94 ± 2.6 | µm | Explicit | Measured from micrographs. | Image Analysis | Fig 4 |
| Composition | Membrane Heterogeneity | Biotin (Inner) vs No Biotin (Outer) | Yes/No | Binary | Explicit | Designed difference used for separation. | Methods | Methods |
| Composition | Cargo Heterogeneity | Sucrose (Inner) vs Glucose (Outer) | Yes/No | Binary | Explicit | Designed difference used for sedimentation. | Methods | Methods |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | PS Incubation | 10 | minutes | Methods | Explicit | Stated incubation time. |
        | Intermediate GUV Centrifugation | 3 | minutes | Methods | Explicit | Stated centrifugation time. |
        | HUV Centrifugation | 3 | minutes | Methods | Explicit | Stated centrifugation time. |
        | Sedimentation (Preparation) | 180 | s | Results (Eq 4, Dt1) | Explicit | Time used in Stokes' Law calculation. |
        | Isolation Incubation (Binding/Sedimentation) | ~2 | hours | Methods | Explicit | Stated total time for sedimentation/binding. |
        | Isolation Step Incubation (Turning) | 60, 30, 30 | minutes | Methods | Explicit | Stated incubation times for separation steps. |
        | Sedimentation (Imaging) | 7200 | s | Results (Eq 4, Dt2) | Explicit | Time (2 hrs) used in Stokes' Law calculation. |
    *   **Note:** These are process timescales, not inherent dynamic timescales of the material itself (like memory decay or adaptation).

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors demonstrated are: 1) **Hierarchical Compartmentalization:** Formation of stable vesicle-within-vesicle structures with distinct membrane/cargo compositions. 2) **Density-Based Sedimentation:** Differential sedimentation based on encapsulated cargo density differences (sucrose vs. glucose). 3) **Affinity-Based Immobilization:** Selective binding of biotinylated vesicles (released GUVs) to streptavidin-coated surfaces for separation. 4) **Separation/Isolation:** Successful isolation of intact HUVs from released intermediate GUVs by exploiting differences in density and membrane composition (biotinylation).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The primary behaviors are validated through direct observation using light and fluorescence microscopy (Figs 2, 3). Hierarchical compartmentalization is confirmed by observing red fluorescent cargo (inner GUVs) enclosed within green fluorescent membranes (outer HUV). Sedimentation is implicitly validated by the ability to collect vesicles via centrifugation and its use in Stokes' Law calculations. Separation is validated by comparing micrographs before (Fig 2 A-C) and after (Fig 2 D-J) the isolation procedure, showing enrichment of intact HUVs. Quantitative analysis includes size distribution measurements (Fig 4) and fluorescence intensity distributions (Fig 6). Limitations include the lack of direct measurement of unilamellarity (assumed based on method [25]) and challenges in accurately counting encapsulated GUVs (mentioned in Results).

---

#Key: [ceylan_mobile_2017]

# Mobile microrobots for bioengineering applications

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews untethered, micron-scale mobile robots (microrobots) designed for bioengineering applications. These systems aim to navigate complex physiological environments (in vivo, organ-on-a-chip) to perform tasks such as targeted therapeutic/cargo delivery, tissue engineering assembly, single-cell manipulation, and potentially microsurgery or biosensing. The components vary depending on the design but generally include a body (often synthetic, sometimes biohybrid) and an actuation mechanism. Actuation is broadly categorized into off-board (externally powered/guided, e.g., magnetic, acoustic, light fields) and on-board (self-propelled, e.g., chemical reactions, integrated biological components like bacteria or sperm). The primary purpose is to access and operate in hard-to-reach or enclosed microenvironments non-invasively or minimally invasively for diagnostics and therapeutics.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters selected represent key physical constraints and characteristics discussed across different microrobot types in the review. Values are generally cited explicitly.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Varies by type:
        *   **Off-board:** External fields (Magnetic, Acoustic, Light).
        *   **On-board (Chemical):** Chemical potential energy from fuels (e.g., H2O2, glucose, water reacting with Mg/Zn).
        *   **On-board (Biohybrid):** Chemical energy from cellular metabolism (e.g., ATP utilized by bacteria flagella, sperm tails, muscle cells).

### **2.2 Energy Transduction**

    *   Content:
        *   **Magnetic:** External field energy -> Magnetic torque/force -> Kinetic energy (rotation/translation). Mechanism: Interaction of field with magnetic material in the robot.
        *   **Acoustic:** Acoustic field energy -> Radiation force/Acoustic streaming/Bubble oscillation -> Kinetic energy. Mechanism: Momentum transfer from sound waves, fluid dynamics.
        *   **Light:** Light energy -> Thermal energy (absorption) -> Thermocapillary flow/Phase transition/Material deformation -> Kinetic energy. Mechanism: Surface tension gradients, material properties change.
        *   **Chemical (Bubble):** Chemical energy (fuel decomposition) -> Gas production/pressure -> Mechanical work (bubble expansion/ejection) -> Kinetic energy. Mechanism: Catalysis, fluid dynamics, momentum conservation.
        *   **Chemical (Self-phoretic):** Chemical energy (fuel decomposition) -> Local chemical/thermal gradients -> Fluid flow/phoretic force -> Kinetic energy. Mechanism: Catalysis, diffusiophoresis/thermophoresis.
        *   **Biohybrid:** Chemical energy (ATP) -> Mechanical work (flagellar rotation, tail beating, muscle contraction) -> Kinetic energy. Mechanism: Biological motors, cellular mechanics.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The review does not provide quantitative efficiency values. However, it implicitly suggests low efficiency for many designs operating at low Reynolds numbers, where viscous forces dominate. The text mentions challenges like overcoming drag (implying significant energy loss). Magnetic torque actuation is noted as potentially more efficient at smaller scales (<100 μm) than gradient pulling. Biohybrid systems are qualitatively mentioned as having comparatively high efficiency. Overall, efficiency seems low and is a major challenge, hence the low score. Efficiency here refers to the conversion of input energy (field, chemical) into desired kinetic energy for propulsion.

### **2.4 Energy Dissipation**

    *   Content: The primary dissipation mechanism mentioned or implied across all fluid-based propulsion methods is viscous drag, dominant at low Reynolds numbers. For magnetic systems, potential heating due to time-varying fields or high gradients is a safety concern (implying energy dissipation as heat). For light-actuated systems using thermal gradients, heat conduction into the surrounding medium is inherent. Chemical reactions release heat (exothermic processes). Biohybrid systems dissipate energy via metabolic heat and mechanical work against fluid viscosity. Quantification is not provided in the review. Qualitative assessment: Viscous drag is High for all swimmers. Thermal dissipation is relevant for magnetic, light, and chemical systems (Low to Medium depending on design/operation).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8.)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The review provides limited detail on specific local interaction rules driving self-organization or swarm behavior.
        *   **Self-Assembly (Ref 44):** Not detailed in this review. Assumed to be driven by physical forces (e.g., magnetic, capillary) between constituent particles under specific environmental conditions, leading to the propeller shape.
        *   **Swarm Behavior (Refs 66, 70):** Mentions magnetic attraction/repulsion between microrobots due to their individual magnetic fields/gradients, controllable via external field orientation. Also mentions interference affecting net velocity. Hydrodynamic interactions are implicitly present but not detailed.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :-------------------- | :---: | :----------: | :----------------: | :------------: |
    | SwarmInteraction | Magnetic Dipole Interaction | Inter-robot Distance | Variable | m | Fig 1f / Ref 66 | Explicit | Distance is key to interaction strength. |
    | SwarmInteraction | Magnetic Dipole Interaction | External Field Orientation | Variable | degrees/radians | Section "Magnetic actuation" / Ref 66 | Explicit | Control input for interaction. |

### **4.3 Global Order:**

    *   Content:
        *   **Self-Assembly:** Formation of specific structures (e.g., "propeller-shaped microswimmers").
        *   **Swarm:** Coordinated movement (though potentially less efficient than solitary swimmers), controllable spacing/formation (Ref 66), collective signaling (e.g., fluorescence for tracking, Ref 70).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| MagneticSwarm | Control of attraction/repulsion via external field orientation | External Field Angle | Variable | degrees/rad | Explicit | Explicitly stated as control method in Ref 66 description. | Section "Magnetic actuation", Fig 1f |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| SwarmMotion | Average speed of swarm | Mean Velocity | e.g., 6.8 (in vivo example) | μm/s | Explicit | Explicitly given for one in vivo experiment (Ref 70). | Tracking Fluorescence | Section "Magnetic actuation" / Ref 70 |
| SwarmFormation | Control of inter-robot distance | Separation Distance | Variable | μm | Explicit | Control objective described based on Ref 66. | Magnetic Control | Section "Magnetic actuation", Fig 1f |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |
     | Local Interaction to Global Structure (Self-Assembly) | How local forces yield final shape | Medium (Implied) | 3 | Shape Fidelity | Implicit | Predictability inferred, Yoneda score based on lack of detailed mapping in review. | Ref 44 mention |
     | Local Interaction to Swarm Behavior (Motion/Formation) | How inter-robot forces affect group dynamics | Medium | 4 | Mean Velocity, Formation Stability | Mixed | Predictability assessed in M4.4. Yoneda score reflects some link but lack of rigorous model in review. | Section "Magnetic actuation", Refs 66, 70 |
        * Rubric: 0: No link shown. 3: Qualitative link described. 5: Quantitative correlation shown. 7: Predictive model based on local rules demonstrated. 10: Formal mathematical proof of local-to-global mapping.
    *   **Metrics:** Mean speed, Swarm formation stability/controllability, Self-assembly yield/fidelity (mostly qualitative or single data points in the review).

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Response Time (Chem. Speed Control, UV) | ~0.2 | s | Section "Self-propelled chemical" / Ref 118 | Explicit | Stated value for UV activation response. |
        | Response Time (Chem. Speed Control, Ultrasound) | < 0.1 | s | Section "Self-propelled chemical" / Ref 115 | Explicit | Stated value for ultrasound control response. |
        | Microrobot Lifetime (Bubble-propelled example) | weeks | time | Section "Self-propelled chemical" / Ref 121 | Explicit | Stated potential operational duration. |
        | Muscle Contraction Frequency (Biohybrid) | 1-5 | Hz | Section "Biohybrid cell-driven" / Ref 158 (context) | Explicit | Stated range for muscle cell actuation. |
        | Half-life (Sensitive Cargo in serum) | very short (qualitative) | time | Section "Introduction" / Ref 16 | Explicit | Qualitative description. |
    *   **Note:** These are examples mentioned in the review; many other relevant timescales (e.g., field response times, diffusion times) exist but are not explicitly quantified in the review text.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary behaviors are:
        1.  **Controlled Locomotion/Navigation:** Moving through fluid environments (in vivo, microfluidic channels) under external guidance (magnetic, acoustic, light) or via self-propulsion (chemical, biohybrid). This includes 3D maneuverability, path following, and penetration through barriers (e.g., mucin gel, thrombus).
        2.  **Cargo Transport and Delivery:** Carrying therapeutics, imaging agents, genetic material, or micro-objects to targeted locations.
        3.  **Manipulation/Assembly:** Picking, placing, gripping, or assembling micro-objects (e.g., cells, hydrogels) in 2D or 3D.
        4.  **Sensing (Implicit/Potential):** Some systems intrinsically respond to environmental cues (e.g., chemotaxis, pH-taxis), suggesting potential for sensing, though dedicated sensing/reporting functions are less emphasized than mobility. Biohybrid systems integrate cellular sensing.
        5.  **Collective Behavior:** Swarming, synchronized motion, pattern formation (as discussed in M4).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The review primarily summarizes reported behaviors from cited primary literature. Validation methods *implied* by the descriptions include:
        *   **Locomotion/Navigation:** Microscopy-based tracking (position vs. time), demonstrating controlled movement along paths or towards targets (e.g., Fig 1f discusses control, Ref 70 tracks fluorescence). Speed and precision metrics are sometimes reported (e.g., Refs 70, 153).
        *   **Cargo Delivery:** Visualization of cargo release at target sites, functional assays demonstrating therapeutic effect (e.g., cell killing, gene transfection - Refs 49, 150, 152).
        *   **Manipulation/Assembly:** Visual confirmation of successful pick-and-place or assembly of structures (e.g., Refs 11, 12). Repeatability may be assessed.
        *   **Collective Behavior:** Observation of swarm formation/motion, quantification of group velocity (Ref 70), control over relative positioning (Ref 66).
        *   **Limitations:** As a review, it doesn't perform new validations. The robustness and reproducibility under varying, complex conditions are often not fully explored or reported in the summarized studies. Claims of "intelligence" are often based on observed targeted behavior rather than rigorous testing against cognitive science definitions.

---

#Key: [iyer_directed_2024]

# Directed motion of cognitive active agents in a crowded three-way intersection

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system studied is a simulation model of cognitive active agents navigating a crowded three-way intersection. It uses intelligent active Brownian particles (iABPs) simulated via Langevin dynamics in two spatial dimensions. The purpose is to understand emergent collective behavior in pedestrian dynamics, specifically focusing on navigation strategies in semi-dense crowds. Key components are:
        1.  **Agents (iABPs):** Modeled as point particles with position `r_i`, orientation `e_i`, and constant speed `v_0`. Associated with a type `t_i` indicating goal direction.
        2.  **Propulsion Force:** `f_p * e_i`.
        3.  **Friction:** `-γ * r_dot_i`.
        4.  **Rotational Dynamics:** Governed by noise `Λ_i`, a vision-based avoidance torque `M_vis`, and a goal-following torque `M_goal`. (Eq. 2, 8).
        5.  **Vision-based Avoidance:** Non-reciprocal interaction torque (Eq. 4) dependent on relative positions (`r_ij`), orientations (`e_i`, `e_j`), a vision cone (`VC`) defined by angle `ψ` and range `R_v`, and a weighting factor `T_ij` distinguishing oncoming/co-moving agents (Eq. 5, 6). Interactions are limited to agents within the vision cone.
        6.  **Goal Following:** Torque `M_goal` aligning agent orientation `e_i` with the goal direction `d_hat(t_i)` (Eq. 7). Strength `K`.
        7.  **Rotational Noise:** Gaussian white noise `Λ_i` (or `ξ_i` in Eq. 8) with diffusion coefficient `D_r`.
        8.  **Environment:** A circular region with three inflow points (separated by 2π/3) and corresponding outflow regions, simulating a multi-stream intersection (Fig 1a). Agents are added at a specific inflow rate `Γ`.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name         | Value                     | Units       | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :--------------------- | :------------------------: | :---------: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
    *   **Note:** R<sub>0</sub> is the characteristic length scale, τ<sub>r</sub> = 1/D<sub>r</sub> is the rotational diffusion time. Parameter values are often varied across simulations to explore phase space.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The energy input is implicitly represented by the constant propulsion force `f_p` acting along the agent's orientation `e_i`. This force drives the agent's motion against the frictional drag. It's not a physical energy source but a parameter defining the agent's self-propulsion activity in the model.
    *   Units: N (Force)

### **2.2 Energy Transduction**

    *   Content: 1. **Propulsion to Kinetic:** The propulsion force `f_p` is transduced into translational kinetic energy, balanced by frictional dissipation (`-γ * r_dot_i`) resulting in a constant speed `v_0` in the overdamped limit. 2. **Interactions/Goal to Rotational:** Agent interactions (via `M_vis`) and goal alignment (`M_goal`) exert torques that change the agent's orientation `e_i`, thus redirecting the propulsion force and influencing kinetic energy allocation. 3. **Noise to Rotational:** Random thermal/internal fluctuations are modeled as a stochastic torque (`Λ_i` or `ξ_i`), transduced into changes in orientation.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not discuss energy efficiency. The model operates in the overdamped limit, focusing on trajectories and collective states rather than energy conversion efficiency. Concepts like thermodynamic efficiency are not applicable to this model as presented.

### **2.4 Energy Dissipation**

    *   Content: The primary dissipation mechanism explicitly modeled is friction, represented by the term `-γ * r_dot_i` in the translational equation of motion (Eq. 1). This term represents energy loss to the surrounding medium (analogous to viscous drag). Rotational dynamics also involve dissipation implicitly through the rotational diffusion term `D_r`, balancing the stochastic input. Quantification: The magnitude of translational dissipation is proportional to the friction coefficient `γ` and the agent's velocity `r_dot_i`. Qualitative Assessment: High, as the system is explicitly in the overdamped regime where inertial effects are negligible and motion is dominated by the balance of propulsion and friction.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*    Units: τ<sub>r</sub> (or seconds if parameters were dimensionalized)

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**
    *   Units: τ<sub>r</sub><sup>-1</sup>

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The local rules governing agent behavior are:
        1.  **Vision-based Avoidance Torque (M<sub>vis</sub>):** An agent `i` adjusts its orientation based on other agents `j` within its vision cone (`VC`, Eq. 6: angle `ψ`, range `R_v`). The torque (Eq. 4) depends on the relative position `r_ij`, relative orientation `e_i \cdot e_j` (via `T_ij`, Eq. 5, which penalizes head-on approaches), and the number of visible neighbors `N_i`. Strength `Ω`. This interaction is non-reciprocal for `ψ < π`.
        2.  **Goal-Following Torque (M<sub>goal</sub>):** An agent `i` tries to align its orientation `e_i` with its designated goal direction `d_hat(t_i)`. The torque (Eq. 7) depends on the angle between `e_i` and `d_hat(t_i)`. Strength `K`.
        3.  **Rotational Noise (Λ<sub>i</sub> or ξ<sub>i</sub>):** A stochastic torque representing random fluctuations, characterized by rotational diffusion coefficient `D_r` (Eq. 2, 3, 8).
        4.  **Propulsion/Friction:** Constant speed `v_0` resulting from propulsion `f_p` balanced by friction `γ` (Eq. 1). Although primarily translational, it determines the rate at which agents encounter each other, influencing interactions.
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID          | Description                  | Parameter Name         | Parameter Value Range     | Units       | Data Source | Implicit/Explicit | Justification                             |
    | :--------------- | :--------------------------- | :--------------------- | :------------------------: | :---------: | :----------: | :----------------: | :---------------------------------------- |
    | Vision Avoidance | Strength of avoidance torque | Ω                      | Δ * K = Δ * 8 * D<sub>r</sub> | τ<sub>r</sub><sup>-1</sup> | Text (Eq. 2, 8) | Explicit          | Ω is defined relative to K via Δ.       |
    | Vision Avoidance | Angle of vision cone         | ψ                      | π/4 to π                  | Radians     | Text, Fig 2  | Explicit          | Parameter varied in simulations.         |
    | Vision Avoidance | Range of vision cone         | R<sub>v</sub>            | 4 * R<sub>0</sub>            | R<sub>0</sub>    | Text          | Explicit          | Fixed parameter value stated.            |
    | Goal Following   | Strength of goal torque      | K                      | 8 * D<sub>r</sub>            | τ<sub>r</sub><sup>-1</sup> | Text          | Explicit          | Fixed parameter value stated.            |
    | Rotational Noise | Strength of random torque    | D<sub>r</sub>            | Implicitly 1/τ<sub>r</sub> | τ<sub>r</sub><sup>-1</sup> | Text (units)  | Mixed             | Sets the timescale τ<sub>r</sub>=1/D<sub>r</sub>. |
    | Relative Strength| Avoidance vs Goal            | Δ                      | 1 to 8+                   | Dimensionless | Text, Fig 2  | Explicit          | Ratio Ω/K, primary parameter varied. |

### **4.3 Global Order:**

    *   Content: Several distinct global (or system-spanning) ordered states emerge depending on the parameters (Δ, ψ, Γ):
        1.  **Unhindered Passage (Low Δ):** Agents largely ignore each other, moving almost directly towards goals (Fig 2b).
        2.  **Scattering State (Intermediate Δ, ψ ≥ π/2):** Complex, disordered motion with frequent avoidance maneuvers (Fig 2c). No stable global order like lanes.
        3.  **Jamming/Percolation State (High Δ, ψ=π):** Strong clustering, agents form large groups (clusters) that can span the interaction zone ('percolation'), exhibiting power-law size distributions (Fig 2d, Fig 3a,b). High density regions form.
        4.  **Localized Flocking State (Intermediate/High Δ, ψ < π/2):** Agents form local co-moving clusters by aligning with oncoming agents as an avoidance strategy (Fig 2e, Fig 3c,d). Characterized by parallel trajectories within the interaction zone.
        5.  **Rotational Flow/Roundabout Motion (High Γ, ψ=π/2):** Agents form vortex-like structures, flowing around the center of the interaction zone (Fig 6c, Fig 7b). Creates a low-density 'eye' at the center.
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID          | Description                     | Parameter              | Value Range                | Units          | Implicit/Explicit | Justification                               | Source           |
| :--------------- | :------------------------------ | :--------------------- | :-------------------------: | :------------: | :----------------: | :------------------------------------------ | :--------------- |
| Vision Avoidance | Controls aversion to neighbors  | Relative Maneuverability (Δ) | Varied (e.g., 1 to 8+)    | Dimensionless  | Explicit          | Key parameter controlling interaction strength vs goal. | Text, Figs 2, 5  |
| Vision Avoidance | Determines field of view        | Vision Angle (ψ)       | Varied (e.g., π/4 to π)     | Radians        | Explicit          | Key parameter controlling interaction scope.  | Text, Figs 2, 5  |
| Goal Following   | Controls adherence to goal path | Goal Fixation (K)      | 8 * D<sub>r</sub>                | τ<sub>r</sub><sup>-1</sup>      | Explicit          | Fixed parameter setting baseline goal drive. | Text (Results)   |
| System Density   | Controls crowding             | Inflow Rate (Γ)        | Varied (e.g., 0.4 to 4)     | τ<sub>r</sub><sup>-1</sup>      | Explicit          | Controls agent density in interaction zone.   | Text, Fig 6      |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID           | Description                                     | Parameter                    | Value Range                  | Units       | Implicit/Explicit | Justification                                                | Protocol             | Source         |
| :-------------------- | :---------------------------------------------- | :--------------------------- | :---------------------------: | :---------: | :----------------: | :----------------------------------------------------------- | :------------------- | :------------- |
| Jamming/Percolation   | Cluster size distribution decay                 | Power-law exponent           | ~2.2                         | Dimensionless | Explicit          | Value consistent with percolation universality class stated. | Cluster analysis     | Fig 3a         |
| Localized Flocking    | Average alignment within clusters               | Average Cluster Polarization (P<sub>c</sub>) | ~0.5 (low Δ) to ~1 (high Δ) | Dimensionless | Explicit          | Defined in Eq. 16, plotted in Fig 3c.                      | Clustering, Eq. 16 | Fig 3c         |
| Localized Flocking    | Average size of clusters (all types)          | Mean Cluster Size (⟨n<sub>c</sub>⟩) | Increases strongly with Δ   | Dimensionless | Explicit          | Plotted in Fig 3d.                                         | Cluster analysis     | Fig 3d         |
| Localized Flocking    | Number of clusters                              | Number of Clusters (N<sub>c</sub>) | Increases strongly with Δ   | Count         | Explicit          | Plotted in Fig 3d.                                         | Cluster analysis     | Fig 3d         |
| System Flow           | Average speed of agents in interaction zone     | Average Velocity (⟨v⟩)     | Decreases with ρ (density) | v<sub>0</sub>    | Explicit          | Measured and plotted vs density in Fig 6b.                 | Velocity averaging | Fig 6b         |
| System Flow           | Overall throughput                              | Flux (J)                     | J = ρ⟨v⟩, see Fig 6a         | v<sub>0</sub>/R<sub>0</sub><sup>2</sup> | Explicit          | Measured and plotted vs density in Fig 6a.                 | Flux calculation   | Fig 6a         |
| System Density        | Average density in central interaction zone     | Density (ρ)                  | Varied via Γ, see Fig 6     | R<sub>0</sub><sup>-2</sup> | Explicit          | Calculated based on neighbor distance (Methods), plotted.    | Density calculation  | Fig 6a,b       |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description           | Value                | Units                 | Source           | Implicit/Explicit | Justification                                                                          |
        | :------------------------------ | :-------------------: | :-------------------: | :--------------: | :----------------: | :------------------------------------------------------------------------------------- |
        | Rotational Diffusion Time       | τ<sub>r</sub>        | s (implicit units)  | Text (Results)   | Explicit          | Defined as 1/D<sub>r</sub>, used as the fundamental unit for time scaling.               |
        | Simulation Time Step            | 0.0005               | τ<sub>r</sub>         | Methods          | Explicit          | Value used in Velocity-Verlet integration scheme.                                      |
        | Total Simulation Time           | 4000                 | τ<sub>r</sub>         | Methods          | Explicit          | Duration of each simulation run.                                                       |
        | Goal Passage Time (approx)      | t<sub>0</sub> = 2R<sub>int</sub>/v<sub>0</sub> | τ<sub>r</sub> (if Pe given) | Fig 4 caption    | Explicit          | Characteristic time to cross diameter, used for averaging path lengths.                  |
        | Correlation Time (Qualitative)  | Long (relative to τ<sub>r</sub>) | τ<sub>r</sub>         | Text (Dynamics...) | Mixed             | Inferred from H > 0.5 and slow C(t) decay; specific value not extracted.             |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors are emergent collective movement patterns of the agent population within the three-way intersection, depending on parameters:
        1.  **Unhindered Passage:** Agents move towards goals with minimal deviation. (Low Δ)
        2.  **Scattering:** Disordered, seemingly random movement with frequent avoidance maneuvers. (Intermediate Δ, ψ ≥ π/2)
        3.  **Jamming/Percolation:** Formation of high-density clusters, slow movement, and system-spanning clusters. (High Δ, ψ=π)
        4.  **Localized Flocking:** Formation of temporary, co-moving clusters aligned locally. (Intermediate/High Δ, ψ < π/2)
        5.  **Rotational Flow:** Organized circular motion around the intersection center. (High Γ, ψ=π/2)
        Individual agent behavior is characterized by super-diffusive motion resembling fractional Brownian motion or Lévy walks (Fig 5).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Claims of emergent behaviors are validated through:
        1.  **Visualisation:** Agent trajectories are plotted for different parameter regimes, providing qualitative visual evidence of patterns like scattering, flocking, jamming, and rotation (Fig 2b-e, Fig 6c). Snapshots and movies (Supplementary) support this.
        2.  **State Diagram:** A phase diagram (Fig 2a) maps observed collective states onto the parameter space (Δ, ψ), quantitatively delineating regions where different behaviors emerge.
        3.  **Quantitative Analysis:**
            *   Cluster Analysis: Cluster size distributions (Fig 3a) are analyzed, showing power-law scaling for the jammed/percolation state, consistent with percolation theory. Average cluster size and polarization (Fig 3c,d) quantify the localized flocking state.
            *   Dynamical Analysis: MSD and orientational correlation functions (Supp Fig S1) are calculated and fitted to fBM/LW models (Fig 5a,b) to characterize single-particle dynamics within the collective states. Path length distributions are analyzed (Fig 4).
            *   Flow Analysis: Fundamental diagrams (Flux vs Density, Velocity vs Density) are constructed (Fig 6) to characterize macroscopic flow properties and identify jamming transitions. Local density plots (Fig 7) visualize flow patterns.
        *   **Limitations:** Validation relies on simulation results. Comparison to experimental pedestrian data is limited (mention of log-normal path lengths matching antipode experiments). Robustness tests against model variations or different noise types are not presented.

---

#Key: [ritort_nonequilibrium_2024]

# Nonequilibrium work relations for energy and information

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper reviews fundamental concepts in the nonequilibrium thermodynamics of small systems, with an emphasis on single-molecule biophysics experiments (e.g., pulling DNA/RNA/proteins using optical tweezers, studying molecular folding/unfolding) and their connection to energy and information processing. It covers key theoretical frameworks like fluctuation theorems (FTs), Jarzynski equality, Crooks FT, and relates them to experimental measurements of work, heat, and entropy production in systems like optically trapped beads and biomolecules. The paper also discusses the thermodynamics of information, referencing Maxwell's demon, Szilard's engine, Landauer's principle, and feedback control in nonequilibrium systems. The purpose is to provide an overview of the theoretical and experimental advances in understanding energy and information at the nanoscale, particularly in biological contexts, and their implications for the foundations of statistical mechanics. The "system" described is the collection of concepts, theories, and experimental setups relevant to nonequilibrium physics of small systems.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Temperature (T) | 298 (Standard condition example) | K | Section "SMALL SYSTEMS...", Fig 2 Caption | Explicit | High | Standard experimental condition |
        | Persistence Length (P) - dsDNA | 50 | nm | Section "SMALL SYSTEMS..." | Explicit | High | Typical experimental value quoted |
        | Persistence Length (P) - ssDNA | 0.75 | nm | Section "SMALL SYSTEMS..." | Explicit | High | Typical experimental value quoted |

    *   **Note:** Parameters listed are key physical constants or representative values for the systems discussed (DNA, trapped beads) to illustrate the concepts.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: Energy input mechanisms discussed include: mechanical work done by pulling forces (e.g., via optical tweezers moving relative to a pipette or bead), work done by moving an optical trap potential through a viscous medium, thermal energy from the environment (manifesting as Brownian motion and fluctuations). In the context of Maxwell's Demon/Szilard Engine, the energy input is heat from a thermal bath.
    *   Units: Joules (J) or pN*nm. Power units (Watts or pN*nm/s) are relevant for rates.

### **2.2 Energy Transduction**

    *   Content: Energy transduction mechanisms discussed include:
        1.  Mechanical Work -> Potential Energy (stored in stretched/unfolded molecule, e.g., DNA WLC elasticity).
        2.  Mechanical Work -> Heat (dissipation due to friction/viscosity when pulling molecules or dragging beads through fluid, σ = (1/T)d¯Q/dt ≥ 0).
        3.  Heat -> Mechanical Work (in the idealized Szilard engine, where thermal energy drives particle expansion against a piston).
        4.  Chemical Energy (e.g., ATP hydrolysis, mentioned for molecular motors) -> Mechanical Work (briefly mentioned in Outlook).
        5.  Light Energy (laser trap) -> Potential Energy (trapping bead) -> Mechanical Work (exerting force).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Efficiency is discussed conceptually, particularly for molecular motors and information-to-energy conversion. Molecular motors are mentioned as having "astonishing large efficiencies" near 100% (Section "Fluctuation Theorems", Outlook). The Szilard engine ideally converts heat to work with efficiency related to information (k<sub>B</sub>T log2 per bit). The efficiency of experimental processes like pulling is related to dissipation (W<sub>d</sub> = W - ΔG); faster pulling is less efficient (more dissipated work). No overall efficiency score can be assigned to the review's scope, as it depends heavily on the specific process (ideal engine vs. real experiment). Qualitative assessment for *discussed* ideal systems: High (Szilard Engine, ideal motors). Qualitative assessment for *real* experimental pulling: Low to Medium (efficiency decreases with pulling speed).

### **2.4 Energy Dissipation**

    *   Content: Dissipation is a central theme, defined as irreversible transformation of work into heat, leading to positive entropy production (σ ≥ 0). Mechanisms explicitly mentioned:
        1.  Friction/Viscous Drag: Heat generated when pulling molecules or dragging beads through a fluid (Eq. 11 describes friction γ). Dissipated work W<sub>d</sub> = W - ΔG in transient processes (Eq. 9 context). Entropy production S<sub>t</sub> in dragged bead experiment (Fig 4, Eq 13).
        2.  Information Erasure: Landauer's principle implies heat dissipation (k<sub>B</sub>T log2 per bit erased) to restore the second law in Maxwell's Demon scenarios.
    Quantification: Dissipated work W<sub>d</sub> is quantifiable from work measurements and free energy differences using FTs (Eq. 9). Entropy production S<sub>t</sub> measured in bead experiment (Fig 4). Average entropy production rate σ = <S<sub>t</sub>>/t (Eq 10). Landauer limit quantifies minimum dissipation for erasure. Values are process-dependent.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skip to Module 4.)**

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skip to Module 5.)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: If M5.1 is "No", skip to Module 6.)**

### **5.2 Computation Type:**


### **5.3 Computational Primitive:**


### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Molecular Collision Timescale | Implicitly very short (<ps) | s | Introduction | Implicit | Timescale below which motion appears reversible. |
        | Bead Relaxation Time (τ<sub>r</sub> = γ/k<sub>b</sub>) | Variable (e.g., ms range mentioned for Fig 4) | s | Eq (12), Exercise 3, Fig 4 discussion | Explicit | Parameter defining response time of bead in trap. |
        | Experimental Time (t) | Variable (e.g., 10-60 ms in Fig 4; seconds or longer for pulling) | s | Fluctuation Theorems section, Fig 4, Fig 5 | Explicit | Duration over which processes are observed/driven. |
        | Pulling Rate (r = ḟ) | e.g., 20; 1, 5, 15 | pN/s | Exercise 2d; Fig 5 Caption | Explicit | Rate at which force is ramped in experiments. |
        | Measurement Interval (τ) in CMD | Variable | s | Section "ENERGY, INFORMATION...", Fig 7, Fig 8 | Explicit | Time between measurements in continuous feedback protocols. |
    *   **Note:** Specific values are often context-dependent or illustrative.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: If M7.1 is "No", skip to Module 8.)**

### **7.2 Adaptation Mechanism:**


## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main behaviors described arise from the fundamental physics of nonequilibrium small systems:
        1.  Fluctuations: Observable deviations from average behavior (e.g., negative entropy production events, fluctuations in work/heat). Governed by Fluctuation Theorems (Eq 8, 9).
        2.  Irreversibility: Tendency towards positive entropy production on average (Second Law), quantified by average dissipated work or entropy production rate.
        3.  Elastic Response: Force-extension behavior of polymers like DNA (Fig 2), described by models like WLC (Eq 1, 2).
        4.  Stochastic Dynamics: Random motion (Brownian motion) and state transitions (e.g., hairpin folding/unfolding, Fig 3, 5) driven by thermal noise and external forces (Eq 11).
        5.  Information-Energy Conversion: Conceptual behavior of Maxwell's Demon/Szilard Engine converting information into work (Fig 6, 8, Eq 15, 16).
        6.  Response under Feedback: System dynamics modified by measurement and control loops (Section on feedback).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The "emergent" behaviors discussed (FTs, statistical laws) are validated through:
        1.  Theoretical Derivations: Mathematical proofs based on statistical mechanics principles (referenced, e.g., [45-48, 5]).
        2.  Experimental Verification: Quantitative measurements in controlled experiments. Examples: Evans' bead-in-trap experiment validating work FT (Fig 4, [49]); DNA/RNA hairpin unzipping experiments testing Crooks FT and Jarzynski equality (Fig 5, [42, 53]); Szilard engine realizations measuring work distributions (Fig 8, 9, [70-73]). Validation involves comparing measured probability distributions (e.g., P(W)) or averages (e.g., <exp(-Wd/k<sub>B</sub>T)>) with theoretical predictions (Eq 8, 9, 14). Reproducibility is demonstrated by collapsing data from different conditions (Fig 5 right panel).

---

#Key: [wang_liquid_2022]

# Liquid Crystal Biosensors: Principles, Structure and Applications

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is Liquid Crystal (LC) biosensors. These utilize the sensitivity of LC molecular orientation to external stimuli and surface interactions to detect biological molecules. Binding events between target biomolecules and receptors immobilized on sensing interfaces (LC-solid, LC-aqueous, LC-droplet) disrupt the LC alignment (e.g., homeotropic to planar/random), causing a macroscopic change in optical properties (birefringence). This change is typically visualized using Polarized Optical Microscopy (POM) as a dark-to-bright transition or quantified via spectral shifts in Whispering Gallery Mode (WGM) microcavities integrated with LCs. Components include liquid crystals (primarily nematic like 5CB, E7), functionalized substrates (glass, polymers like PDMS for microfluidics), interface modifiers (e.g., DMOAP, CTAB, phospholipids, aptamers, antibodies), target analytes (proteins, DNA, ions, enzymes, cells), buffer solutions, and detection instrumentation (POM, spectrometers for WGM). The purpose is label-free detection and quantification of various biological targets.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | LC Film/Droplet Size (Influence mentioned) | ~2 to >10 (droplets influence) | µm | Sec 3.3.2, [116] | Mixed | Medium (Qualitative impact explicitly stated, range implicitly covers typical experimental values) | Inference from discussion |

    *   **Note:** Parameters listed are examples illustrating the types of information provided or discussed. Detection limits are key performance metrics explicitly listed in tables summarizing cited works. LC types used in various studies are also explicitly mentioned. The influence of LC dimension (droplet size) is explicitly discussed, with example sizes cited in context.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy inputs triggering the sensing event are typically chemical (Gibbs free energy of binding between analyte and receptor, enzymatic reaction energy) or interfacial (changes in surface energy due to adsorption/binding). For detection, optical energy is input (polarized light for POM, pump laser for WGM). Electrical energy is used in some configurations (e.g., electrodes mentioned in ref [2-6], but not the main focus for sensing trigger).

### **2.2 Energy Transduction**

    *   Content: Chemical/interfacial energy changes from biomolecular interactions at the interface are transduced into mechanical energy associated with the reorientation of LC molecules against elastic forces. This molecular reorientation alters the material's bulk optical properties (effective refractive index, birefringence). This change in optical properties modulates the input optical energy (light for POM) or the resonant condition of an optical cavity (WGM), transducing the initial binding event into a detectable optical signal (change in light intensity/polarization or shift in resonant wavelength).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: Energy efficiency is not discussed or quantified in the review. The focus is on detection sensitivity and limits, not the energy conversion efficiency of the sensing process. Qualitatively, the amplification effect (molecular event causing macroscopic optical change) suggests high sensitivity, but not necessarily high energy efficiency in a thermodynamic sense.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation mechanisms are not explicitly discussed or quantified. Potential mechanisms include viscous dissipation during LC reorientation, light absorption/scattering (though LCs are often transparent in visible), heat generated from WGM laser absorption, and non-radiative decay processes. Frictional losses in microfluidic systems could also occur. Assessment: Low (based on typical LC device operation, but not specified in the text).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

**(Conditional: M4.1 is "Yes", including M4.2-M4.7)**

### **4.2 Local Interaction Rules:**

    *   Content: The primary local interaction rules are:
        1.  **Intermolecular Forces:** Anisotropic forces (e.g., van der Waals) between LC molecules favoring parallel alignment of long axes (Sec 2.2).
        2.  **Surface Anchoring:** Interactions between LC molecules and the functionalized substrate/interface (solid, aqueous, droplet). These interactions impose preferred orientations (e.g., homeotropic via DMOAP/CTAB/phospholipids, planar via rubbed polymers) (Sec 2.1, 3.1.1, 3.2.1, 3.3.1). Anchoring energy quantifies the strength of this interaction.
        3.  **Analyte-Receptor Binding:** Specific binding events disrupt the local surface interactions (Rule 2), changing the anchoring conditions for nearby LC molecules (Sec Abstract, 1, 3).
        4.  **Elasticity:** LC materials resist spatial variations in the director field (bend, splay, twist deformations). This elastic coupling propagates local orientation changes caused by binding over a certain distance (up to ~100 µm mentioned in Sec 1).
    * **Implicit/Explicit**: Explicit

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | 1 | Intermolecular Forces | Order Parameter (S) | 0 to 1 (Theoretically) | Dimensionless | Sec 2.2 (Eq 3) | Explicit | Definition and range provided. |
    | 2 | Surface Anchoring | Pretilt Angle (θ) | 0° to 90° (Mentioned near-homeotropic ~85°, planar ~0°) | Degrees (°) | Sec 2.2 (Fig 1c), Sec 3.1.1 (discussion of tilted state) | Mixed | Concept explicit, specific values examples/implicit range. |
    | 4 | Elasticity | Correlation Length | Max ~100 | µm | Sec 1 | Explicit | Explicitly stated propagation distance. |

### **4.3 Global Order:**

    *   Content: The emergent global order is the macroscopic orientation pattern of the LC director field within the sensor geometry (cell, droplet, microchannel). Common initial states are uniform homeotropic (molecules perpendicular to surface, dark POM image) or planar (molecules parallel to surface). In droplets, radial or bipolar configurations emerge. Binding events cause a transition from this ordered state to a less ordered or different configuration (e.g., tilted, random, planar from homeotropic, bipolar from radial), resulting in a change in the global optical appearance (e.g., bright POM image, WGM shift).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| 1 | Intermolecular Alignment | Order Parameter (S) | 0-1 | Dimensionless | Explicit | Defined in Eq 3. | Sec 2.2 |
| 3 | Binding Disruption | Analyte Concentration | Variable (e.g., pM to mg/mL) | M, g/mL, etc. | Explicit | Central variable in detection experiments. | Tables 1, 2 |
| 4 | Elastic Propagation | Interaction Distance | ~100 (max) | µm | Explicit | Explicitly stated propagation limit. | Sec 1 |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| GO2 | Optical Appearance (POM) | Image Brightness / Grayscale Intensity / Area Coverage | Variable | Arbitrary units / % | Explicit | Primary detection method discussed. | POM Imaging | Sec 2.1, 3 |
| GO3 | Optical Retardation | Phase Difference (δ) | Variable | Radians or nm | Explicit | Defined in Eq 1. | Interferometry (Indirectly via POM) | Sec 2.1 |
| GO4 | WGM Resonance | Wavelength Shift (Δλ) | Variable | nm or pm | Explicit | WGM sensing principle. | Laser Spectroscopy | Sec 3.3.1, 4 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Analyte Binding/Reaction Time | Variable | Seconds to Minutes (Implicit Range) | Sec 3.1.1 ("60-min fixation time") | Mixed | Specific example given, general range implicit. |
        | LC Molecular Reorientation Time | Milliseconds to Seconds (Implicit Range) | ms to s | General LC Physics (Implicit) | Implicit | Not specified, inferred from general LC knowledge. |
        | Microfluidic Flow Time | Variable (Depends on setup) | Seconds to Minutes (Implicit Range) | Sec 3.1.2, 3.2.2 | Implicit | Microfluidics implies flow, but times not given. |

    *   **Note:** The review mentions "rapid response speed" qualitatively. A specific fixation time for an aptamer is mentioned (60 min). Other timescales are implicit based on the physical processes involved.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary functional behavior is **signal transduction and amplification**. Local molecular binding events at an interface trigger a collective reorientation of LC molecules (often over distances up to 100 µm), leading to a macroscopic change in optical properties. This emergent optical change (e.g., dark-to-bright transition in POM, WGM spectral shift) serves as the detectable signal indicating the presence and/or concentration of the target analyte. Different geometries (interfaces, droplets) exhibit specific orientational transitions (e.g., homeotropic-planar, radial-bipolar) as part of this behavior.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The emergent behavior (optical signal change) is validated primarily through:
        1.  **Polarized Optical Microscopy (POM):** Visual observation and imaging of the dark-to-bright (or other configuration) transition correlating with analyte concentration (Explicitly described, e.g., Fig 2, Fig 4, Sec 2.1, 3.1.1). Quantification involves measuring grayscale intensity or bright area coverage (Explicit, e.g., Sec 3.1.1, 3.2.1).
        2.  **Spectroscopy (WGM):** Measuring the shift in resonant wavelength of WGM microcavities upon analyte binding, correlating the shift magnitude with concentration (Explicitly described, e.g., Fig 7, Sec 3.3.1, 4).
        3.  **Control Experiments:** Implicitly necessary (though not detailed in this review excerpt) to demonstrate specificity (response only to target analyte) and rule out non-specific binding or other artifacts causing signal changes.
        Reproducibility is implicitly claimed by presenting quantitative detection limits and linear ranges from cited studies. Limitations might include sensitivity to environmental factors (temperature) and potential for non-specific interactions affecting the signal.

---

#Key: [levin_multiscale_2024]

# The Multiscale Wisdom of the Body: Collective Intelligence as a Tractable Interface for Next-Generation Biomedicine

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system described is the living body, viewed as a multi-scale collective intelligence. It comprises components ranging from molecular networks (e.g., GRNs, pathways) and single cells (microbes, somatic cells) to tissues, organs, and the whole organism. The paper hypothesizes that these components, particularly through bioelectric networks, act as an "agential material" capable of navigating anatomical, physiological, and transcriptional state spaces. Its function/purpose, from this perspective, is development, regeneration, repair, cancer suppression, and anatomical homeostasis – essentially, achieving and maintaining specific morphological and functional goal states. The core idea is to interface with this "physiological software" for therapeutic interventions, treating it as a reprogrammable, goal-seeking system.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name                     | Value                                   | Units   | Source (Fig/Table/Section)   | Implicit/Explicit   | Data Reliability (High/Medium/Low)   | Derivation Method (if Implicit)                 |
        | :--------------------------------- | :-------------------------------------- | :------ | :--------------------------- | :------------------ | :----------------------------------- | :---------------------------------------------- |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source for the biological systems described is chemical energy derived from metabolism. External interventions (e.g., optogenetics mentioned implicitly via citations, electrical stimulation for defibrillation) also provide energy input.

### **2.2 Energy Transduction**

    *   Content: Chemical energy from metabolism is transduced into various forms: maintaining ion gradients (potential energy stored across membranes), driving ion channel/pump activity (bioelectrical signals), powering cellular processes like proliferation, differentiation, migration (mechanical/chemical work for morphogenesis/regeneration), and synthesizing molecules (chemical energy). External stimuli (e.g., drugs binding receptors, light activating channels) trigger changes in these endogenous energy transduction pathways, particularly modulating ion flow and downstream signaling cascades leading to gene expression changes. Bioelectric signals themselves transduce information that influences cellular behavior and large-scale patterning.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide metrics or discussion regarding the energy efficiency of the described biological processes (morphogenesis, regeneration, bioelectric signaling) or the interventions. Assessing the thermodynamic efficiency of development or regeneration is highly complex and beyond the scope of the text.

### **2.4 Energy Dissipation**

    *   Content: Energy dissipation is inherent in all biological processes described (maintaining gradients against leakage, cellular work, signaling cascades) primarily as heat, consistent with the second law of thermodynamics applied to living systems. The paper does not quantify or specifically discuss dissipation mechanisms. Qualitative assessment: Likely High, typical for complex biological processes maintaining non-equilibrium states.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Variable (ranging from transient to "permanent")

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: High (Qualitative)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: High (Qualitative)

### **3.6 Degradation Rate (Optional - if applicable)**
    *   Value: Low (Qualitative, for stable memories) / Potentially increases with aging

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------------------------------------- | :---------------- | :------ | :--------------------- | :---------------- | :------------------ | :------------------------- |
---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Yes

### **4.2 Local Interaction Rules:**

    *   Content: The paper focuses heavily on endogenous bioelectric signaling as a key local interaction rule. Rules include: 1) Cell resting potential (Vmem) determined by ion channel activity influencing cell state (Sec 4.1, Fig 3). 2) Propagation of Vmem changes through gap junctions, creating tissue-level bioelectric patterns (Sec 4.1, Fig 3). 3) Bioelectric states influencing downstream cell behaviors (differentiation, proliferation, migration) via mechanisms like neurotransmitter signaling, calcium pathways, gene expression regulation (Sec 4.2, implied). 4) Cell-cell recruitment (e.g., ectopic eye formation, Fig 4D, Sec 4.3). 5) Chemical signaling (implied, standard developmental biology). 6) Mechanical interactions (implied, standard developmental biology). Equations/algorithms are not provided, but the functional roles are described.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID              | Description                         | Parameter Name                 | Parameter Value Range   | Units   | Data Source                      | Implicit/Explicit   | Justification                        |
    | :------------------- | :---------------------------------- | :----------------------------- | :---------------------- | :------ | :------------------------------- | :------------------ | :----------------------------------- |
    | BioelectricSignaling | Resting Potential Pattern           | Vmem                           | Pattern-dependent       | mV      | Fig 4B, Fig 5A, Sec 4.3          | Explicit            | Specific patterns shown/described    |

### **4.3 Global Order:**

    *   Content: The emergent global order is the species-specific anatomical structure (e.g., correctly formed brain, face, limb, eye, planarian body plan) or functional state (e.g., regenerated structure, suppressed tumor, normal physiology). This order represents the target state or attractor in the anatomical/physiological morphospace (Sec 3.1, Fig 2).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Implicit

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID              | Description                         | Parameter Name                 | Value Range             | Units   | Implicit/Explicit   | Justification                        | Source                           |
| :------------------- | :---------------------------------- | :----------------------------- | :---------------------- | :------ | :------------------ | :----------------------------------- | :------------------------------- |
| BioelectricSignaling | Cell resting potential              | Vmem                           | Pattern-dependent       | mV      | Explicit            | Specific patterns shown/described    | Fig 4B, Fig 5A, Sec 4.3          |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID         | Description          | Parameter             | Value Range                | Units   | Implicit/Explicit   | Justification                           | Protocol          | Source                           |
| :------------------ | :------------------- | :-------------------- | :------------------------- | :------ | :------------------ | :-------------------------------------- | :---------------- | :------------------------------- |
| PhysiologicalState  | Homeostasis          | Relevant biomarker    | Normal range               | Varies  | Explicit            | Goal-seeking behavior described         | Measurement       | Sec 1, Sec 3.2                   |
| PatternFormation    | Bioelectric Prepattern | Vmem distribution     | Specific spatial pattern   | mV      | Explicit            | "Electric face" map described/shown    | Voltage Imaging   | Fig 4B, Fig 5A, Sec 4.3          |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type                           | Description                                           | Predictability      | Yoneda Score   | Metrics        | Implicit/Explicit   | Justification                                                         | Source                           |
    | :---------------------------------- | :---------------------------------------------------- | :------------------ | :------------- | :------------- | :------------------ | :-------------------------------------------------------------------- | :------------------------------- |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Analog/Biological/Other (Proto-cognitive dynamic system)

### **5.3 Computational Primitive:**

    *   Content: The paper suggests several levels of computational primitives:
        *   **Molecular Level:** Learning (Habituation, Sensitization, Associative Conditioning) in GRNs/pathways (Fig 1B, Sec 2.2). Basic signal integration/thresholding within pathways (implied).
        *   **Cellular Level:** Decision-making (e.g., differentiation, migration based on signals), Context-dependent response (Sec 2), Signal processing/perception (Sec 2).
        *   **Network/Tissue Level:** Integration of bioelectric signals across cells (Sec 4.1), Pattern matching/completion (implied by anatomical homeostasis, Fig 2C), Error detection/correction relative to setpoint (Sec 3.2, Fig 2B). Navigation/problem-solving in state space (morphospace, physiological space) (Sec 1, Sec 3.1, Fig 2).
    *   **Sub-Type (if applicable):** Learning: Associative; Decision-Making: Cell Fate Choice; Navigation: Morphospace Traversal.

### **5.4 Embodied Computational Units**
| Unit ID            | Description                     | Processing Power   | Energy/Operation   | Freq/Resp. Time   | Bit-Depth   | Data Source        | Implicit/Explicit   | Justification                   |
| :----------------- | :------------------------------ | :----------------- | :----------------- | :---------------- | :---------- | :----------------- | :------------------ | :------------------------------ |

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description              | Value                  | Units      | Source           | Implicit/Explicit   | Justification                                     |
        | :--------------------------------- | :--------------------- | :--------- | :--------------- | :------------------ | :------------------------------------------------ |
        | Neural Bioelectric Activity        | Milliseconds           | time (ms)  | Sec 4.2 (implied)| Implicit            | Standard neuroscience timescale context           |
        | Somatic Bioelectric Activity       | Seconds to Hours/Days  | time       | Sec 4.2          | Explicit            | Contrasted with neural timescale                  |
        | Morphogenesis / Regeneration       | Days to Weeks / Months | time       | Sec 4.2          | Explicit            | Stated timescale for morphogenetic change         |
        | Molecular Network Learning         | Variable (stim. dep.)  | time       | Sec 2.2          | Implicit            | Depends on stimulus pattern/duration               |
        | Planarian Memory Rewriting (Exp.)  | Hours (Manipulation)   | time (hr)  | Sec 4.3 (Ref 130)| Explicit (Ref)      | Experimental duration cited                       |
        | Anatomical Homestasis Response Time| Variable               | time       | Sec 3.2          | Implicit            | Depends on perturbation and regenerative capacity |

### **6.2 Active Inference:**

    *   Content: Yes/Partial
    *   **If Yes/Partial, provide examples of testable CT-GIN metrics that *could* be used to quantify active inference:** Rate of reduction of deviation from target morphology during regeneration; Correlation between bioelectric predictors and subsequent cell behavior; Model complexity vs. prediction accuracy trade-offs in simulations based on experimental data; Mutual information between sensory input (e.g., wound signals) and corrective actions (e.g., cell proliferation rate). CT-GIN could model the internal generative model (`InternalModelNode`) and its updates based on sensory input (`SensoryInputNode`) leading to action (`ActionOutputNode`) via `ActiveInferenceEdge`.

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: Mechanisms vary by scale:
        *   **Molecular:** Changes in dynamic states of GRNs/pathways through repeated stimulation (learning, Sec 2.2); alteration of gene expression (e.g., in response to barium, Sec 2.2).
        *   **Bioelectric:** Modification of ion channel/gap junction activity leading to altered Vmem patterns, which serve as new setpoints or guides (e.g., planarian head reprogramming, Sec 4.3; brain defect rescue via HCN2 modulation correcting Vmem pattern, Sec 4.4, Fig 5). This is presented as rewriting the "software".
        *   **Cellular/Tissue:** Changes in cell behavior (proliferation, differentiation, migration) driven by altered signaling (bioelectric, chemical); recruitment of cells; modifications to cell number/shape to maintain overall organ size despite ploidy changes (Table 1). Feedback loops adjusting activity to achieve homeostatic goals (Sec 3.2).
        *   **Conceptual:** The paper suggests mechanisms analogous to behavioral science (training, Sec 1), cybernetics (rewriting setpoints, Sec 1, 3.2), and active inference (error minimization, Sec 6.2).

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The primary emergent behaviors described are complex morphological outcomes resulting from collective cell activity:
        *   **Morphogenesis:** Development of specific anatomical structures (eyes, limbs, brain, face, overall body plan) from simpler beginnings (Sec 3.1, 4.3, 4.4).
        *   **Regeneration/Repair:** Reconstitution of correct anatomical structures after damage or perturbation (Sec 3.2, 4.3, 4.4, Table 1). This includes anatomical homeostasis – reaching and maintaining the target morphology.
        *   **Cancer Suppression/Induction:** Viewed as the emergent outcome of cooperation (suppression) or defection (induction/progression) within the cellular collective, linked to the size of the "cognitive light cone" and bioelectric state (Sec 5).
        *   **Physiological Homeostasis:** Maintenance of stable physiological states (implied context for goal-seeking behavior, Sec 1).
        *   **Learning/Adaptation:** Emergent property of molecular networks and cellular collectives (Sec 2.2, Sec 7.1).
        *   **Biobot Formation/Behavior:** Self-assembly of dissociated cells into novel motile constructs with emergent capabilities (e.g., healing neurons, Fig 6C, Sec 6.1).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: Validation relies on observational evidence from developmental biology and regeneration experiments cited throughout the paper. Methods include:
         *   **Perturbation Experiments:** Observing outcomes after physical injury (amputation, Fig 2C), genetic manipulation (Notch mutants, Fig 5; Pax6 induction, Sec 4.3), chemical exposure (teratogens, Sec 4.4; barium, Sec 2.2), cell transplantation (chimeras, Sec 1), or modulation of signaling (bioelectric manipulations, Sec 4.3, 4.4, Fig 4, Fig 5; gap junction blockade, Sec 4.3).
         *   **Imaging/Observation:** Tracking morphological changes over time (development, regeneration, tadpole face remodeling - Ref 67). Bioelectric imaging reveals prepatterns (Fig 4B, 5A). Histology confirms structure (ectopic eyes, Fig 4C'').
         *   **Functional Assays:** Learning tests after brain repair (Ref 124); chemotaxis after planarian regeneration (Ref 88).
         *   **Molecular Analysis:** Transcriptomics reveal adaptive changes (barium exposure, Ref 66). Gene expression analysis confirms tissue identity/correction (Fig 5).
     *   **Limitations:** While providing strong evidence for the *phenomena*, the paper doesn't always provide rigorous quantitative validation specifically for the "collective intelligence" *interpretation* itself within a formal CT-GIN framework. Reproducibility is implied by referencing established biological findings.

---

#Key: [ziyin_universal_2023]

# Universal thermodynamic uncertainty relation in nonequilibrium dynamics

__Paper Type:__ Theoretical/Computational

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The paper describes a theoretical framework for deriving a universal Thermodynamic Uncertainty Relation (TUR) applicable to arbitrary observables in general Markovian systems, including those with absolute irreversibility and time-dependent dynamics. The system is abstractly defined as a trajectory of stochastic events [x] = (x1, ..., xM) with probability P([x]), evolving according to Markovian transition probabilities P(xi|xi-1, ti-1). Key components are the forward path probability P([x]), the time-reversed path probability P*([x]*), the entropy production σS = -log(P*([x]*)/P([x])), an arbitrary observable f([x]), and a reference dynamics Q([x]). The purpose is to establish universal bounds on the fluctuations (variance) of observables based on thermodynamic quantities like entropy production and the degree of irreversibility/nonstationarity. It aims to unify TURs in physics with inequalities in theoretical finance.

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**


### **2.2 Energy Transduction**


### **2.3 Energy Efficiency**


### **2.4 Energy Dissipation**


## M3: Memory

### **3.1 Memory Presence:**

    *   Content: No

**(Conditional: M3.1 is "No", skipping M3.2-M3.8.)**

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: No

**(Conditional: M5.1 is "No", skipping M5.2-5.4)**

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Trajectory Duration | M steps (discrete) or τ (continuous implied) | steps or time | Intro, Appendix D | Explicit | The framework considers trajectories over M discrete steps or implied continuous time τ. |
        | Step Duration (if discrete) | Variable (t<0xE1><0xB5><0xA2> - t<0xE1><0xB5><0xA2>₋₁) | time | Intro | Explicit | Transition probabilities can be time-dependent. |
        | Relevant timescale for finance example | Day (implied unit time) | days | Application II | Explicit | Price trajectory x₀...x<0xE2><0x82><0x9C> uses time unit corresponding to a day. |


### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: No

**(Conditional: M7.1 is "No", skipping M7.2)**

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The "behavior" described is the fluctuation of an arbitrary observable f([x]) around its mean value ⟨f⟩, specifically its variance Var[f]. The main result bounds this variance from below using thermodynamic quantities (entropy production σS, degree of irreversibility γ, or nonstationarity D). Examples include fluctuations of thermodynamic currents (Application I) and financial quantities like price return r or wealth return rate R (Application II).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

---

#Key: [lee_nanograin_2023]

# Nanograin network memory with reconfigurable percolation paths for synaptic interactions

__Paper Type:__ Experimental

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is a memory device based on a single silicon nanowire (Si NW) featuring two distinct segments: one with a solid single-crystal Si core and a porous Si shell (nanograin network), and the other composed only of the solid Si core. Two electrodes are placed on each segment. The device utilizes reconfigurable current percolation paths within the porous shell's nanograin network to achieve memory behavior and emulate synaptic functions. Its primary purpose is to demonstrate simultaneous data storage and processing (in-memory computation) and neuromorphic functionalities like potentiation, habituation, and synaptic elimination, controllable both electrically and photonically. Electrical charging forms low-resistance percolation paths (space-charge-limited current) within the high-resistance nanograin network (electron hopping dominated when uncharged). Laser illumination can reversibly suppress current by annihilating charges and disconnecting these paths (photonic habituation).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy sources are electrical energy supplied via bias voltage pulses/sweeps and optical energy from laser illumination.
    *   Value: Electrical: up to 5 V (pulsed/swept), -1 V (reset); Optical: 17 - 720 µW (habituation), 2.1 mW (elimination)
    *   Units: Electrical: Volts (V); Optical: microWatts (µW), milliWatts (mW)

### **2.2 Energy Transduction**

    *   Content: 1. Electrical to Charge Storage: Applied voltage drives charge carriers (electrons) into the nanograin network, storing energy capacitively (self-capacitance of nanograins) and lowering resistance by forming percolation paths. 2. Electrical to Current Flow: Stored charge enables space-charge-limited current (SCLC) flow through percolation paths; otherwise, higher resistance electron hopping occurs. 3. Optical to Charge Annihilation: Incident photons excite stored charges in the nanograins, leading to their removal/annihilation, increasing resistance and suppressing current (photonic habituation). This is contrary to typical photocarrier generation enhancing current. 4. Electrical to Heat: Resistive losses (Joule heating) occur during current flow, especially through higher resistance hopping paths or constriction points.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide quantitative energy efficiency metrics for memory operation or computation. However, the operating currents are in the µA range with voltages up to 5V, suggesting power consumption in the µW to tens of µW range per device during active operation. Hopping transport (low current state) is high resistance, implying significant resistive losses. SCLC path formation consumes energy for charging. Photonic habituation requires external laser power (µW to mW). Compared to state-of-the-art digital logic or optimized memristors, the efficiency appears relatively low, especially considering the need for external lasers for erasure/habituation. Qualitative Assessment: Low.

### **2.4 Energy Dissipation**

    *   Content: Primary dissipation mechanisms include: 1. Joule Heating: Due to current flow through the resistive nanograin network (both hopping and SCLC paths) and the solid core Si NW. This is likely the dominant dissipation mechanism during electrical operation. 2. Non-radiative Recombination: Following photonic excitation for charge annihilation (habituation), energy might be lost via non-radiative pathways (e.g., phonon emission), though the primary intended effect is charge removal. 3. Capacitive Charging/Discharging Losses: Energy loss associated with charging and discharging the self-capacitance of the nanograins through resistive paths. Quantification is not provided. Qualitative Assessment: Medium-High (due to µA currents and likely significant resistance in hopping/percolation paths).

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: Seconds timescale (Qualitative); ~10s (from Fig 4j)
*    Units: s

### **3.4 Memory Capacity (Optional - if applicable)**


### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Erase Linearity (Photonic Habituation) | Deviation from linear conductance decrease per pulse | ν = 0.37 | Dimensionless | `MemoryNode` attribute | Methods (Simulation section) | Explicit | Nonlinearity parameter ν explicitly stated for simulation based on experimental fit. |
    | Erase Linearity (Electrical Habituation) | Deviation from linear conductance decrease per pulse | ν = 0.9 | Dimensionless | `MemoryNode` attribute | Methods (Simulation section) | Explicit | Nonlinearity parameter ν explicitly stated for simulation based on experimental fit (Fig S9). |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: No

**(Conditional: M4.1 is "No", skipping M4.2-M4.7)**

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Neuromorphic / Analog

### **5.3 Computational Primitive:**

    *   Content: Analog Weight Update / Synaptic Plasticity (Potentiation/Habituation/STDP). The basic operation is the modulation of the device's conductance (synaptic weight) based on electrical pulses (potentiation, STDP) or light (habituation, STDP, elimination).
    *   **Sub-Type (if applicable):** Potentiation (Conductance Increase), Habituation (Conductance Decrease), Spike-Timing-Dependent Plasticity (STDP, conductance change based on relative timing of pre/post spikes).

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Potentiation/Habituation Pulse Width | 100 | ms | Fig 3a, 3d caption | Explicit | Stated in figure captions describing pulse parameters. |
        | Potentiation/Habituation Pulse Interval (Δt) | 200 - 2000 | ms | Fig 3a, 3b, 3d caption | Explicit | Stated in figure captions and varied in Fig 3b. |
        | PSC Decay / Memory Retention | ~ seconds | s | Fig 3a, S5, 4j | Implicit | Inferred from the time axis of decay plots. |
        | STDP Spike Interval (Δt) | -600 to +600 | ms | Fig 3c | Explicit | Range shown on x-axis of STDP plot. |
        | Characteristic Charging Time (τ, model) | 1 | s | Methods (Transport model) | Explicit | Stated as a parameter in the theoretical model. |
        | Laser Cycling Period (Fig 2d) | 14 | s | Fig 2d (7s on, 7s off) | Explicit | Directly observable from the time axis and description. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The mechanism is based on modulating the stored charge within the nanograin network of the porous Si shell. Electrical potentiation involves applying voltage pulses (e.g., +5V) that increase the stored charge, likely creating more or stronger percolation paths, thus increasing conductance (decreasing resistance). Photonic habituation uses laser pulses to annihilate stored charges, disconnecting percolation paths and decreasing conductance (increasing resistance). STDP is achieved by applying pre- and post-synaptic voltage pulses with varying time intervals (Δt), where shorter intervals lead to larger conductance changes, mimicking Hebbian-like learning rules (timing-dependent modification of charge storage/percolation). Synaptic elimination uses continuous laser illumination during potentiation pulses on one device to suppress its conductance increase while enhancing it in an adjacent, unilluminated device.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: 1. Non-volatile Memory: Exhibits hysteresis in I-V characteristics, retaining conductance state after voltage sweeps. 2. Analog Conductance Modulation: Conductance can be gradually increased (potentiation) or decreased (habituation) via electrical or optical stimuli. 3. Synaptic Plasticity Emulation: Demonstrates short-term plasticity (Paired-Pulse Facilitation, PPF) and long-term plasticity (Spike-Timing-Dependent Plasticity, STDP). 4. Photonic Current Suppression/Erasure: Laser illumination actively reduces persistent current levels (photonic habituation). 5. Synaptic Elimination Mimicry: Selective photonic habituation on one of two adjacent devices suppresses its potentiation while enhancing the other's, mimicking competitive synaptic dynamics.

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The paper validates the claimed behaviors through: 1. **Direct Measurement:** I-V curves (Fig 2c) validate memory hysteresis. Current vs. time measurements under specific pulse/light conditions (Figs 2d, 3a, 3d, 4f-h) validate potentiation, habituation, PPF, photonic suppression, and elimination dynamics. 2. **Parameter Dependence:** Systematic variation of laser power (Fig 2e) and wavelength (Fig S4) validates controllability of photonic habituation. Variation of pulse interval Δt (Fig 3b, 3c) validates PPF and STDP characteristics. 3. **Modeling & Simulation:** A transport model based on percolation theory and charge dynamics (Fig 1, Methods) provides a physical basis for the observed memory and habituation. Pattern recognition simulations using experimentally derived parameters (linearity) validate the potential for neuromorphic computation (Fig 3g, h). 4. **Control Experiments:** Comparison with pure Si NW (Fig S2, linear I-V) confirms the porous shell is crucial for memory. Reproducibility over limited cycles is shown (Fig 2d, S6). **Limitations:** Long-term stability/endurance and device-to-device variability across a larger population are not extensively validated. Noise characteristics are not explicitly quantified.

---

#Key: [kamsma_brain-inspired_2024]

# Brain-inspired computing with fluidic iontronic nanochannels

__Paper Type:__ Hybrid

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: The system is an aqueous volatile memristor based on a tapered microfluidic channel filled with a rigid face-centered cubic (fcc) colloidal crystal structure (charged silica spheres) forming a nanochannel network membrane (NCNM). The channel connects two reservoirs containing an aqueous electrolyte (10 mM KCl). The device utilizes voltage-driven transient salt concentration polarization due to an inhomogeneous ionic space charge density between colloids to achieve memristive behavior, mimicking short-term synaptic plasticity. Its purpose is to serve as a synaptic element for iontronic neuromorphic computing, specifically demonstrated in a reservoir computing framework for classifying time-series data (encoded handwritten digits).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |
        | Approx. Surface Charge (σ<sub>c</sub>) | -0.01 | C m<sup>-2</sup> | Section 1 | Explicit | Medium | Estimated value |
        | Colloidal Volume Fraction (η) | ~0.74 | Dimensionless | Section 1 | Explicit | Medium | Assumed near-close-packed fcc |

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The primary energy source is electrical energy supplied by an external voltage source applying voltage pulses (V<sub>app</sub>(t)) across the device reservoirs.
    *   Value: ~1-10 (write), ~0.01-0.1 (read)
    *   Units: μJ (write), μJ (read)

### **2.2 Energy Transduction**

    *   Content: Electrical energy input drives ion transport (K<sup>+</sup>, Cl<sup>-</sup>) through the nanochannel network within the aqueous electrolyte. This ion movement constitutes an ionic current. Due to the tapered geometry and inhomogeneous space charge, the applied voltage leads to salt concentration polarization (accumulation/depletion of ions), changing the local conductivity. This effectively transduces electrical energy into changes in the potential energy stored in concentration gradients and subsequently modulates the electrical resistance (conductance) of the device. A portion of the energy is dissipated as Joule heating due to ionic current flow through the resistive medium.

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper provides energy consumption per pulse (~1-10 μJ for write, ~10-100 nJ for read). Compared to biological synapses (femtojoules to picojoules) or advanced solid-state memristors (picojoules or less), this is relatively high. The primary function contributing to computation (conductance change) relies on ion movement and concentration polarization, inherently involving significant resistive losses (Joule heating) and diffusive relaxation, suggesting low thermodynamic efficiency for the computation/memory aspect itself, although efficiency is not explicitly calculated or discussed in the paper. The score reflects the relatively high energy per operation compared to biological/solid-state benchmarks.

### **2.4 Energy Dissipation**

    *   Content: The primary dissipation mechanism is Joule heating due to the flow of ionic current (I) through the resistive electrolyte within the nanochannels under the applied voltage (V). Frictional losses associated with ion movement against the solvent (water) and channel walls also contribute. Energy is also dissipated during the relaxation of concentration gradients (diffusion) when the driving voltage is changed or removed. Quantification is not provided, but given the ionic current and applied voltages (up to 5V), Joule heating (I*V) is likely the dominant dissipation mode during operation. Assessed qualitatively as High during write pulses, Medium-Low during read pulses.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**


### **3.3 Memory Retention Time:**

*   Value: ~1.62 (for L=150 μm); Tunable via L<sup>2</sup>/D
*    Units: s (seconds)

### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Effectively Analog (Continuous)
*   Units: Dimensionless (Conductance ratio g/g0)

### **3.5 Readout Accuracy (Optional - if applicable)**

*   Value: ~95% (for simple digits), 81% (for MNIST) classification accuracy. SD ~ few % to 10% of conductance value.
*   Units: % accuracy; Dimensionless (SD)

### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Operation | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|
    | Stability (Cycle-to-cycle) | Standard deviation of current response over 50 cycles | Max ~7% | % | `MemoryNode` attribute `cycleVariability` | Fig 2 | Explicit | Max SD reported relative to mean response. |
    | Stability (Longer term / pulse trains) | Standard deviation of conductance over 4h / 16 patterns | Typically few %, Max ~10% | % | `MemoryNode` attribute `longtermVariability` | Section 1 (ref SI) | Explicit | Max SD reported relative to mean conductance. |
    | Noise Robustness (Reservoir Computing) | Classification accuracy when noise (based on experimental SD) is added during training/inference | 95% (simple), 81% (MNIST) | % | `BehaviorArchetypeNode` attribute `noiseRobustness` | Section 2 (Fig 4E, 4F) | Explicit | Reported accuracy under noise conditions. |

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial (Fabrication only)

**(Conditional: M4.1 is "Partial", relevant only to fabrication. Operational self-organization is absent. Skipping M4.2-M4.7 as they pertain to operational emergent order.)**

### **4.2 Local Interaction Rules:**


### **4.2.1 Local Interaction Parameters:**


### **4.3 Global Order:**


### **4.4 Predictability of Global Order:**


### **4.5. Local Interaction Rules (for Self-Organization)**

### **4.6. Globally Emergent Order and Order Parameters**

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**


## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Reservoir Computing (Analog, Neuromorphic)

### **5.3 Computational Primitive:**

    *   Content: The most basic computational operation performed by the material is a non-linear temporal transformation with fading memory. Input voltage V(t) is transformed into a time-dependent conductance g(t) according to the differential equation dg(t)/dt = [g<sub>∞</sub>(V(t)) - g(t)] / τ (Eq. 2), where g<sub>∞</sub>(V) is the non-linear steady-state conductance and τ is the memory timescale. This represents a leaky integration or a form of temporal filtering/mapping characteristic of reservoir computing nodes.
    *   **Sub-Type (if applicable):** Non-linear leaky integration / Temporal mapping

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Memory Retention (τ, L=150μm) | ~1.62 | s | Eq. 1, Discussion | Explicit | Calculated from theory, supported by Fig 3 exp. |
        | Memory Retention (τ Scaling) | ∝ L<sup>2</sup>/D | s | Eq. 1 | Explicit | Theoretical derivation, supported by Fig 3 exp. |
        | Write Pulse Duration | 0.75 | s | Section 1 (Fig 1F descr.), Section 2 | Explicit | Stated experimental parameter. |
        | Read Pulse Duration | 50 | ms | Section 1 (Fig 1E descr.) | Explicit | Stated experimental parameter. |
        | Pulse Interval (STP & RC) | 0.75 | s | Section 1 (Fig 1F descr.), Section 2 | Explicit | Stated experimental parameter. |
        | Reservoir Computing Sequence Duration (4 pulses) | (0.75+0.75)*4 - 0.75 = 5.25 | s | Derived from Section 2 | Implicit | Calculated from pulse duration and interval. |
        | Hysteresis Loop Frequency Range | 0.01 - 2 | Hz | Fig 3 Caption, SI Ref | Explicit | Range of frequencies tested experimentally. |

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: The adaptation mechanism is the inherent short-term memory resulting from transient ion concentration polarization. Facilitation occurs when successive voltage pulses (e.g., positive) arrive faster than the memory retention time (Δt < τ), causing further accumulation of ions and thus increasing conductance. Depression occurs similarly for negative pulses causing depletion. The "adaptation" is governed by the physics described in Eq. 2: the current conductance state g(t) depends on the integral of past voltage inputs V(t'), weighted by an exponential decay related to τ. It is not based on Hebbian or reinforcement learning but is an intrinsic dynamic property of the physical system.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors described are:
        1.  **Volatile Memristance:** Exhibiting history-dependent conductance (pinched hysteresis loop, Fig 1C).
        2.  **Short-Term Synaptic Plasticity Emulation:** Demonstrating facilitation and depression in response to pulse trains (Fig 1F).
        3.  **Time Series Processing/Separation:** Transforming input voltage time series into distinct final conductance states within a reservoir computing context (Fig 4A).
        4.  **Input Classification Support:** Providing sufficiently distinct outputs for an external (in silico) readout function to classify input patterns (handwritten digits) with reasonable accuracy (Fig 4F).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: The claimed behaviors are validated through:
        *   **Memristance:** Experimental I-V curves showing pinched hysteresis loops under sinusoidal voltage (Fig 1C), compared with theoretical predictions.
        *   **STP:** Experimental measurements of conductance change in response to specific voltage pulse trains, demonstrating facilitation and depression (Fig 1F), compared with theory. Reproducibility shown via averaging over multiple devices/measurements and low SD (Fig 2).
        *   **Reservoir Computing:** Applying voltage pulse trains corresponding to input patterns (bit strings, digits) and measuring the resulting conductance states (Fig 4A). Validation includes showing distinguishability of outputs for different inputs and successful classification using a standard benchmark (MNIST) with an in silico readout (Fig 4F), achieving accuracy comparable to other platforms. Robustness tested by incorporating experimental noise into simulations (Fig 4E). Control experiments implicitly include applying different pulse trains and observing different outputs. Reproducibility across devices assessed (Fig 4A description).

---

#Key: [jiao_mechanical_2023-1]

# Mechanical metamaterials and beyond

__Paper Type:__ Review

## M1: System Overview & Implementation

### **1.1 System Description**

    *   Content: This perspective paper reviews the field of mechanical metamaterials (MMs), defined as artificial materials achieving unique mechanical properties (e.g., negative Poisson's ratio, ultra-stiffness, programmability) through the rational design of microstructural units (e.g., origami, chiral, lattice). It explores MMs beyond classical mechanics, discussing their integration with functional materials for capabilities like sensing, energy harvesting, actuation, adaptation, computation, and information processing. The purpose is to provide an overview, discuss AI-driven design approaches, highlight challenges, and propose roadmaps toward next-generation active, responsive, and potentially "intelligent" MMs capable of interacting with their environment and adapting, forming sense-decide-respond loops. Key components are the base materials (metallic, polymeric), microstructural units (unit cells), and functional materials (piezoelectric, magnetic, thermal, light-driven, conductive polymers).

### **1.2 Implementation Clarity**


### **1.3 Key Parameters**

        | Parameter Name | Value | Units | Source (Fig/Table/Section) | Implicit/Explicit | Data Reliability (High/Medium/Low) | Derivation Method (if Implicit) |
        | :------------- | :---: | :---: | :-----------------------: | :-----------------: | :-----------------------------: | :-------------------------------: |

    *   **Note:** Parameters listed are general scale ranges for MM design described conceptually or specific cited examples, not parameters for one implemented system analyzed in depth by this paper. Data Reliability is High for explicitly stated scales/cited values.

## M2: Energy Flow

### **2.1 Energy Input**

    *   Content: The paper discusses multiple energy inputs for MMs beyond mechanical loading, primarily for functionalities like energy harvesting and actuation. Sources include: Mechanical (vibrations, waves, strain), Thermal (temperature fluctuations), Magnetic fields, Electrical fields, Light (e.g., LED), Acoustic waves.

### **2.2 Energy Transduction**

    *   Content: Several transduction mechanisms are discussed:
        *   **Electro-mechanical:** Piezoelectric effect (mechanical strain -> electrical energy/vice versa), Triboelectric effect (contact electrification/mechanical motion -> electrical energy). Used for energy harvesting and sensing.
        *   **Thermo-mechanical:** Shape Memory Polymers/Alloys (thermal energy -> mechanical work/shape change), Thermoelectric effect (temperature gradient -> electricity). Used for actuation and energy harvesting.
        *   **Magneto-mechanical:** Magnetic materials integrated into structure (magnetic field -> mechanical actuation/force/state change). Used for actuation, memory switching.
        *   **Opto-mechanical:** Light-driven materials like Liquid Crystal Elastomers (light energy -> mechanical deformation/actuation).
        Energy is transduced from the input form (mechanical, thermal, etc.) into electrical signals (for sensing/power), mechanical motion (for actuation), or stored potential energy (in bistable states).

### **2.3 Energy Efficiency**

    *   Justification/Metrics: The paper does not provide quantitative energy efficiency data for the discussed transduction mechanisms or systems. It mentions goals like "efficient energy harvesting" (Ref 12, 158) and near-unity efficiency for a specific electromagnetic harvester (Ref 8), but doesn't provide a generalizable metric or score for the broad range of systems reviewed. Efficiency would vary greatly depending on the specific mechanism, material, design, and application. Qualitative assessment: Highly variable (Low to potentially High depending on system).

### **2.4 Energy Dissipation**

    *   Content: While not a primary focus, dissipation mechanisms are implicitly present in the described systems. Examples include: mechanical damping/friction within the structure during deformation or vibration, electrical resistance in conductive components or circuits, heat loss during thermal processes (thermo-mechanical actuation, thermoelectricity), material fatigue/damage under load. Energy absorption is mentioned as a desirable property (e.g., for impact resistance, vibration reduction, Table 1), which involves controlled energy dissipation. Quantification is absent. Qualitative assessment: Present and likely significant in many applications (e.g., damping, thermal losses), but magnitude varies.

## M3: Memory

### **3.1 Memory Presence:**

    *   Content: Yes

### **3.2 Memory Type:**

    *   **Bistability/Multistability:** Mechanical structures with multiple stable equilibrium states (e.g., magnetic m-bits, buckled structures, SMPs in different configurations). This allows discrete state storage. (Explicit, Ref 4, 9, 15, 22, 65, 128)
    *   **Shape Memory:** Material remembers a predefined shape and returns to it upon specific stimulus (e.g., heat for SMPs). (Explicit, Ref 13, 22)
    *   **Information Storage (Digital Analogue):** Using arrays of bistable elements (m-bits) to store digital information (0s and 1s) mechanically. (Explicit, Ref 128, 160)
    The score of 5 reflects the presence of demonstrably stable, re-writable (in the case of m-bits) memory states, suitable for basic information storage or programmed responses. However, it's often limited to discrete states, potentially volatile depending on the mechanism (e.g., requiring sustained fields) or requiring specific stimuli (heat) for readout/reset, and doesn't encompass complex associative or adaptive memory types discussed in cognitive science. Retention is present but not always detailed quantitatively; capacity can be scaled by array size; readout accuracy depends on the mechanism.

### **3.3 Memory Retention Time:**


### **3.4 Memory Capacity (Optional - if applicable)**

*  Value: Scalable (by number of unit cells/m-bits)
*   Units: Bits (for digital storage analogue) or Number of stable states

### **3.5 Readout Accuracy (Optional - if applicable)**


### **3.6 Degradation Rate (Optional - if applicable)**

### **3.7 Memory Operations Energy Cost (Optional - if applicable)**
    | Memory Operation ID | Energy Consumption per Bit | Power Usage during Operation| Units | Uncertainty | Data Source Reference | Implicit/Explicit | Justification |
    | :------------------ | :--------------------------: | :-----------------------------: | :---: |:-----------------:|:-----------------:|:-----------------:| :------------------ |

### **3.8 Memory Fidelity & Robustness Metrics (Optional - if applicable)**
    | :-------- | :---------- | :----: | :---: | :-------------: | :----------: |:-----------------:| :-----------------:|

---

## M4: Self-Organization and Emergent Order

### **4.1 Self-Organization Presence:**

    *   Content: Partial/Unclear

**(Conditional: If M4.1 is "No", skip to Module 5. If "Yes", include M4.2-M4.7)**
*Assessments below assume the "Partial" interpretation focusing on emergent *properties* from *designed* structures.*

### **4.2 Local Interaction Rules:**

    *   Content: The local interaction rules are embodied in the physical structure and material properties of the unit cells. Examples include:
        *   **Geometric Constraints:** How units connect, fold (origami), rotate (chiral), or deform relative to neighbors dictates the macroscopic mechanical response. E.g., Re-entrant honeycomb geometry leads to negative Poisson's ratio. (Implicit/Explicit - geometry is explicit, rule is implicit).
        *   **Elasticity/Plasticity:** Material constitutive laws govern how struts bend, buckle, or yield, influencing overall stiffness, strength, and energy absorption. (Implicit - relies on standard mechanics principles).
        *   **Contact Mechanics:** Interactions between surfaces during large deformations or folding. (Implicit).
        *   **Functional Material Response:** Local rules defined by piezoelectric, magnetic, thermal responses integrated into the unit cell. E.g., local strain induces local voltage based on piezoelectric coefficients. (Mixed - functional material type is explicit, exact local rule often implicit).
        These are physical laws applied locally due to the designed structure, rather than programmable algorithmic rules.
    * **Implicit/Explicit**: Mixed

### **4.2.1 Local Interaction Parameters:**

    | Rule ID | Description | Parameter Name | Parameter Value Range | Units | Data Source | Implicit/Explicit | Justification |
    | :------ | :---------- | :------------- | :---------- | :---: | :----------: | :----------------: | :------------: |
    | Geometric | Unit cell geometry | Strut length/thickness, angle, fold pattern, chirality | Variable (design parameter) | Length, Angle | Design/Fig 2a | Explicit (types), Implicit (values) | Geometry types are shown, specific values depend on the design. |
    | Elastic | Material stiffness | Young's Modulus (E), Poisson's Ratio (ν) | Variable (material property) | Pa, Dimensionless | Material Selection/Table 1 | Explicit (concept), Implicit (values) | Material choice dictates these, not specified generally. |
    | Functional | Piezoelectric coupling | Piezoelectric coefficient (d33, d31 etc.) | Variable (material property) | C/N or m/V | Functional Material Choice/Table 2 | Implicit | Specific functional material parameters not listed. |

### **4.3 Global Order:**

    *   Content: The emergent global "order" is the desired macroscopic property or functionality, such as: Negative Poisson's Ratio (Auxeticity), Negative Stiffness, Ultra-Stiffness, Ultra-Lightweight, Specific Wave Propagation characteristics (Bandgaps), Programmable Mechanical Response, Sensing Capability (global response from local sensing), Energy Harvesting Output (collective contribution), Defined Actuation Behavior. This is functional order rather than purely structural order (like a crystal lattice, though the MM itself often *is* a lattice).
    * **Implicit/Explicit**: Explicit

### **4.4 Predictability of Global Order:**

    * **Implicit/Explicit**: Mixed

### **4.5. Local Interaction Rules (for Self-Organization)**
| Rule ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Source |
| :------ | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :-----: |
| Geometric | Constraints from unit cell shape/connectivity | Design parameters (lengths, angles, topology) | Variable | Length, Angle, Dimensionless | Mixed | Geometry is designed (explicit basis), parameters vary (implicit range). | Fig 2a, Section 2 |
| Material | Constitutive behavior (e.g., linear elastic, hyperelastic) | Material Moduli (E, G, K), Yield Strength | Variable | Pa | Implicit | Standard mechanics principles assumed but not detailed per example. | Section 2, Table 1 |
| Functional | Physics of embedded functional material (e.g., piezoelectric equations) | Material coefficients (piezoelectric, magnetic susceptibility, thermal expansion) | Variable | Various | Implicit | Functional mechanisms described, but specific governing equations/parameters not listed. | Table 2, Section "Actuation..." |

### **4.6. Globally Emergent Order and Order Parameters**
| Property ID | Description | Parameter | Value Range | Units | Implicit/Explicit | Justification | Protocol | Source |
| :---------- | :---------- | :-------- | :---------- | :---: | :----------------: | :------------: | :------: | :-----: |
| Mechanical | Effective Elastic Modulus | E_eff | Variable (Positive, Negative possible) | Pa | Explicit (Concept) | Property is explicitly discussed. | Homogenization, FEA, Experiment | Section 2, Table 1 |
| Mechanical | Effective Poisson's Ratio | ν_eff | Variable (<0 to >0) | Dimensionless | Explicit (Concept) | Property is explicitly discussed. | Homogenization, FEA, Experiment | Section 2, Table 1 |
| Mechanical | Effective Density | ρ_eff | Variable (Often low) | kg/m³ | Explicit (Concept) | Property is explicitly discussed. | Calculation, Measurement | Section 2, Fig 2b, Ref 1, 85 |
| Functional | Energy Harvesting Output | Voltage / Power | Variable | V / W | Explicit (Concept) | Functionality discussed. | Measurement | Table 2, Section 3 |
| Functional | Sensing Response | Change in Resistance/Voltage/Capacitance | Variable | Ohm / V / F | Explicit (Concept) | Functionality discussed. | Measurement | Table 2, Section 3 |
| Functional | Actuation Displacement/Force | Strain / Force | Variable | Dimensionless / N | Explicit (Concept) | Functionality discussed. | Measurement | Table 2, Section 3 |

### **4.7 Yoneda Embedding and Local-to-Global Mapping Fidelity**

    | Link Type | Description | Predictability | Yoneda Score | Metrics | Implicit/Explicit | Justification | Source |
    | :-------- | :---------- | :------------- | :----------- | :------ | :----------------: | :------------: | :-----: |

## M5: Computation

### **5.1 Embodied Computation Presence:**

    *   Content: Yes

### **5.2 Computation Type:**

    *   Content: Hybrid (Mechanical Logic, potentially interfaced with Electrical), Digital Analogue (using bistable states for bits)

### **5.3 Computational Primitive:**

    *   Content: The basic computational operations identified include:
        *   **Logic Gate:** Realizing Boolean logic (AND, OR, NOT, XOR, etc.) through mechanical deformation/buckling modes coupled with electrical conductivity (Ref 23), acoustic interactions (Ref 18), or sequential mechanical excitations (Ref 129). (Sub-Type: Boolean Logic Gate)
        *   **State Switching (Binary):** Reversibly switching between two stable mechanical or magnetic states, analogous to a digital bit flip (Ref 128, 160). (Sub-Type: Bistable Switch)
        *   **Thresholding:** Implicit in buckling-based logic or state switching, where a certain input threshold must be overcome. (Implicit)

### **5.4 Embodied Computational Units**
| Unit ID | Description | Processing Power | Energy/Operation | Freq/Resp. Time | Bit-Depth | Data Source | Implicit/Explicit | Justification |
| :------ | :---------- | :--------------- | :--------------- | :--------------: | :-------: | :----------: |:-----------------:| :-----------------:|
    *   **Note:** Performance metrics like processing power, energy, speed are generally not quantified in this review.

## M6: Temporal Dynamics

### **6.1 Timescales:**

        | Timescale Description | Value | Units | Source | Implicit/Explicit | Justification |
        | :-------------------- | :---: | :---: | :----: | :----------------: | :------------: |
        | Actuation Time (Thermal - SMP) | Variable (seconds to minutes) | s to min (Qualitative range) | Implicit Inference (Based on general knowledge of SMPs) | Ref 13, 22 (Concept) | Thermal diffusion is relatively slow. |
        | Actuation Time (Magnetic/Electric) | Variable (Potentially ms or faster) | ms (Qualitative estimate) | Implicit Inference | Table 2, Section "Actuation..." | Faster than thermal, depends on field strength, material. |
        | Computational Operation Time (Mechanical Logic) | Variable (Likely >> electronic speeds) | ms to s (Qualitative estimate) | Implicit Inference | Ref 17, 23, 129 | Limited by mechanical wave speed or deformation rates. |
    *   **Note:** The paper discusses dynamic properties and time-dependent behaviour (e.g., vibration, wave propagation, actuation speed) but rarely quantifies specific timescales. Values are qualitative estimates based on the underlying physics.

### **6.2 Active Inference:**

    *   Content: No

## M7: Adaptation

### **7.1 Adaptive Plasticity Presence:**

    *   Content: Yes

### **7.2 Adaptation Mechanism:**

    *   Content: Adaptation mechanisms rely heavily on integrating functional materials or designing specific structural features:
        *   **Functional Materials:** Using materials whose properties change in response to stimuli (leading to changes in the MM's overall behavior). Examples:
            *   Shape Memory Polymers/Alloys (SMP/SMA): Temperature changes induce shape recovery/change (Ref 13, 22).
            *   Electroactive Polymers (EAP)/Dielectric Elastomers (DE): Electric fields cause deformation/stiffness change (Ref 131).
            *   Magneto-responsive materials: Magnetic fields cause deformation/stiffness change/state switching (Ref 131, 135, 136).
            *   Photo-responsive materials (LCEs): Light causes deformation (Ref 131, 145).
        *   **Structural Reconfiguration:** Designing MMs that can be actively switched between different stable or meta-stable configurations with distinct properties (e.g., origami folding/unfolding, bistable element switching, programmable self-locking Ref 78, 79, 81, 128, 129). Mechanical (Ref 129), magnetic (Ref 128), or other stimuli can trigger reconfiguration.
        *   **AI-Driven Inverse Design:** While not a physical mechanism *within* the material, AI is used to *design* structures that exhibit desired adaptive or tunable responses (Section AI-driven design). This is an external design loop enabling adaptive behavior.

## M8: Emergent Behaviors

### **8.1 Behavior Description:**

    *   Content: The main functional behaviors discussed are:
        *   **Unusual Mechanical Properties:** Negative Poisson's ratio (auxeticity), negative stiffness, negative compressibility, ultra-stiffness, ultra-lightweight, high energy absorption, vibration isolation/damping, tunable stiffness/damping.
        *   **Sensing:** Detecting strain, pressure, contact, potentially chemical/thermal stimuli via integrated functional materials (piezoelectric, triboelectric, chemo-responsive, thermo-responsive).
        *   **Energy Harvesting:** Converting ambient energy (mechanical vibrations, thermal gradients, EM waves) into electrical energy (via piezoelectric, triboelectric, thermoelectric, EM harvesting elements).
        *   **Actuation:** Changing shape, stiffness, or position in response to external stimuli (thermal, electrical, magnetic, light).
        *   **Wave Manipulation:** Controlling propagation of elastic, acoustic, or potentially other wave types (bandgaps, focusing, guiding, topological wave states).
        *   **Memory/Information Storage:** Storing information in discrete stable states (mechanical bits).
        *   **Computation:** Performing logic operations (mechanical logic gates).
        *   **Self-Powering/Autonomy:** Combining sensing, energy harvesting, potentially computation/actuation towards autonomous operation (aspirational goal).

### **8.2 Behavior Robustness:**


### **8.3 CT-GIN Emergent Behavior Validation**

     *  Content: As a review/perspective, this paper validates claims primarily through **citation** of original research articles (experimental and theoretical/computational). Figures (e.g., Fig 2b, Fig 3a, Fig 4) visually summarize reported behaviors from cited works. Tables (e.g., Table 1, 2, 3, 4) categorize and reference studies demonstrating specific properties, functionalities, or AI methods. The validation relies on the peer-reviewed status of the cited literature. The paper itself does not present new experimental data or simulations to validate specific emergent behaviors. Limitations include the lack of direct reproducibility within this paper and potential variability in the rigor of the cited studies.

---
